{
  "metadata": {
    "target_file": "kernel/bpf/verifier.c",
    "start_ref": "v6.3",
    "end_ref": "v6.13",
    "patchset_count": 119
  },
  "patchsets": [
    {
      "merge_hash": "a8e1a3ddf7246cd43c93e5459fcc1b4989853a06",
      "merge_subject": "Merge branch 'explicit-raw_tp-null-arguments'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nExplicit raw_tp NULL arguments\n\nThis set reverts the raw_tp masking changes introduced in commit\ncb4158ce8ec8 (\"bpf: Mark raw_tp arguments with PTR_MAYBE_NULL\") and\nreplaces it wwith an explicit list of tracepoints and their arguments\nwhich need to be annotated as PTR_MAYBE_NULL. More context on the\nfallout caused by the masking fix and subsequent discussions can be\nfound in [0].\n\nTo remedy this, we implement a solution of explicitly defined tracepoint\nand define which args need to be marked NULL or scalar (for IS_ERR\ncase). The commit logs describes the details of this approach in detail.\n\nWe will follow up this solution an approach Eduard is working on to\nperform automated analysis of NULL-ness of tracepoint arguments. The\ncurrent PoC is available here:\n\n- LLVM branch with the analysis:\n  https://github.com/eddyz87/llvm-project/tree/nullness-for-tracepoint-params\n- Python script for merging of analysis results:\n  https://gist.github.com/eddyz87/e47c164466a60e8d49e6911cff146f47\n\nThe idea is to infer a tri-state verdict for each tracepoint parameter:\ndefinitely not null, can be null, unknown (in which case no assumptions\nshould be made).\n\nUsing this information, the verifier in most cases will be able to\nprecisely determine the state of the tracepoint parameter without any\nhuman effort. At that point, the table maintained manually in this set\ncan be dropped and replace with this automated analysis tool's result.\nThis will be kept up to date with each kernel release.\n\n  [0]: https://lore.kernel.org/bpf/20241206161053.809580-1-memxor@gmail.com\n\nChangelog:\n----------\nv2 -> v3:\nv2: https://lore.kernel.org/bpf/20241213175127.2084759-1-memxor@gmail.com\n\n * Address Eduard's nits, add Reviewed-by\n\nv1 -> v2:\nv1: https://lore.kernel.org/bpf/20241211020156.18966-1-memxor@gmail.com\n\n * Address comments from Jiri\n   * Mark module tracepoints args NULL by default\n   * Add more sunrpc tracepoints\n   * Unify scalar or null handling\n * Address comments from Alexei\n   * Use bitmask approach suggested in review\n   * Unify scalar or null handling\n   * Drop most tests that rely on CONFIG options\n   * Drop scripts to generate tests\n====================\n\nLink: https://patch.msgid.link/20241213221929.3495062-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-12-13 16:24:54 -0800",
      "commits": [
        {
          "hash": "c00d738e1673ab801e1577e4e3c780ccf88b1a5b",
          "subject": "bpf: Revert \"bpf: Mark raw_tp arguments with PTR_MAYBE_NULL\"",
          "message": "This patch reverts commit\ncb4158ce8ec8 (\"bpf: Mark raw_tp arguments with PTR_MAYBE_NULL\"). The\npatch was well-intended and meant to be as a stop-gap fixing branch\nprediction when the pointer may actually be NULL at runtime. Eventually,\nit was supposed to be replaced by an automated script or compiler pass\ndetecting possibly NULL arguments and marking them accordingly.\n\nHowever, it caused two main issues observed for production programs and\nfailed to preserve backwards compatibility. First, programs relied on\nthe verifier not exploring == NULL branch when pointer is not NULL, thus\nthey started failing with a 'dereference of scalar' error.  Next,\nallowing raw_tp arguments to be modified surfaced the warning in the\nverifier that warns against reg->off when PTR_MAYBE_NULL is set.\n\nMore information, context, and discusson on both problems is available\nin [0]. Overall, this approach had several shortcomings, and the fixes\nwould further complicate the verifier's logic, and the entire masking\nscheme would have to be removed eventually anyway.\n\nHence, revert the patch in preparation of a better fix avoiding these\nissues to replace this commit.\n\n  [0]: https://lore.kernel.org/bpf/20241206161053.809580-1-memxor@gmail.com\n\nReported-by: Manu Bretelle <chantra@meta.com>\nFixes: cb4158ce8ec8 (\"bpf: Mark raw_tp arguments with PTR_MAYBE_NULL\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241213221929.3495062-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-12-13 16:24:53 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/test_tp_btf_nullable.c"
          ]
        },
        {
          "hash": "838a10bd2ebfe11a60dd67687533a7cfc220cc86",
          "subject": "bpf: Augment raw_tp arguments with PTR_MAYBE_NULL",
          "message": "Arguments to a raw tracepoint are tagged as trusted, which carries the\nsemantics that the pointer will be non-NULL.  However, in certain cases,\na raw tracepoint argument may end up being NULL. More context about this\nissue is available in [0].\n\nThus, there is a discrepancy between the reality, that raw_tp arguments can\nactually be NULL, and the verifier's knowledge, that they are never NULL,\ncausing explicit NULL check branch to be dead code eliminated.\n\nA previous attempt [1], i.e. the second fixed commit, was made to\nsimulate symbolic execution as if in most accesses, the argument is a\nnon-NULL raw_tp, except for conditional jumps.  This tried to suppress\nbranch prediction while preserving compatibility, but surfaced issues\nwith production programs that were difficult to solve without increasing\nverifier complexity. A more complete discussion of issues and fixes is\navailable at [2].\n\nFix this by maintaining an explicit list of tracepoints where the\narguments are known to be NULL, and mark the positional arguments as\nPTR_MAYBE_NULL. Additionally, capture the tracepoints where arguments\nare known to be ERR_PTR, and mark these arguments as scalar values to\nprevent potential dereference.\n\nEach hex digit is used to encode NULL-ness (0x1) or ERR_PTR-ness (0x2),\nshifted by the zero-indexed argument number x 4. This can be represented\nas follows:\n1st arg: 0x1\n2nd arg: 0x10\n3rd arg: 0x100\n... and so on (likewise for ERR_PTR case).\n\nIn the future, an automated pass will be used to produce such a list, or\ninsert __nullable annotations automatically for tracepoints. Each\ncompilation unit will be analyzed and results will be collated to find\nwhether a tracepoint pointer is definitely not null, maybe null, or an\nunknown state where verifier conservatively marks it PTR_MAYBE_NULL.\nA proof of concept of this tool from Eduard is available at [3].\n\nNote that in case we don't find a specification in the raw_tp_null_args\narray and the tracepoint belongs to a kernel module, we will\nconservatively mark the arguments as PTR_MAYBE_NULL. This is because\nunlike for in-tree modules, out-of-tree module tracepoints may pass NULL\nfreely to the tracepoint. We don't protect against such tracepoints\npassing ERR_PTR (which is uncommon anyway), lest we mark all such\narguments as SCALAR_VALUE.\n\nWhile we are it, let's adjust the test raw_tp_null to not perform\ndereference of the skb->mark, as that won't be allowed anymore, and make\nit more robust by using inline assembly to test the dead code\nelimination behavior, which should still stay the same.\n\n  [0]: https://lore.kernel.org/bpf/ZrCZS6nisraEqehw@jlelli-thinkpadt14gen4.remote.csb\n  [1]: https://lore.kernel.org/all/20241104171959.2938862-1-memxor@gmail.com\n  [2]: https://lore.kernel.org/bpf/20241206161053.809580-1-memxor@gmail.com\n  [3]: https://github.com/eddyz87/llvm-project/tree/nullness-for-tracepoint-params\n\nReported-by: Juri Lelli <juri.lelli@redhat.com> # original bug\nReported-by: Manu Bretelle <chantra@meta.com> # bugs in masking fix\nFixes: 3f00c5239344 (\"bpf: Allow trusted pointers to be passed to KF_TRUSTED_ARGS kfuncs\")\nFixes: cb4158ce8ec8 (\"bpf: Mark raw_tp arguments with PTR_MAYBE_NULL\")\nReviewed-by: Eduard Zingerman <eddyz87@gmail.com>\nCo-developed-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241213221929.3495062-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-12-13 16:24:53 -0800",
          "modified_files": [
            "kernel/bpf/btf.c",
            "tools/testing/selftests/bpf/progs/raw_tp_null.c"
          ]
        },
        {
          "hash": "0da1955b5bd2af3a1c3d13916df06e34ffa6df3d",
          "subject": "selftests/bpf: Add tests for raw_tp NULL args",
          "message": "Add tests to ensure that arguments are correctly marked based on their\nspecified positions, and whether they get marked correctly as maybe\nnull. For modules, all tracepoint parameters should be marked\nPTR_MAYBE_NULL by default.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241213221929.3495062-4-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-12-13 16:24:53 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/raw_tp_null.c",
            "tools/testing/selftests/bpf/progs/raw_tp_null_fail.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "cf8b876363da4fccdcc4ba209d4d098ec0f1ffac",
      "merge_subject": "Merge branch 'bpf-track-changes_pkt_data-property-for-global-functions'",
      "merge_body": "Eduard Zingerman says:\n\n====================\nbpf: track changes_pkt_data property for global functions\n\nNick Zavaritsky reported [0] a bug in verifier, where the following\nunsafe program is not rejected:\n\n    __attribute__((__noinline__))\n    long skb_pull_data(struct __sk_buff *sk, __u32 len)\n    {\n        return bpf_skb_pull_data(sk, len);\n    }\n\n    SEC(\"tc\")\n    int test_invalidate_checks(struct __sk_buff *sk)\n    {\n        int *p = (void *)(long)sk->data;\n        if ((void *)(p + 1) > (void *)(long)sk->data_end) return TCX_DROP;\n        skb_pull_data(sk, 0);\n        /* not safe, p is invalid after bpf_skb_pull_data call */\n        *p = 42;\n        return TCX_PASS;\n    }\n\nThis happens because verifier does not track package invalidation\neffect of global sub-programs.\n\nThis patch-set fixes the issue by modifying check_cfg() to compute\nwhether or not each sub-program calls (directly or indirectly)\nhelper invalidating packet pointers.\n\nAs global functions could be replaced with extension programs,\na new field 'changes_pkt_data' is added to struct bpf_prog_aux.\nVerifier only allows replacing functions that do not change packet\ndata with functions that do not change packet data.\n\nIn case if there is a need to a have a global function that does not\nchange packet data, but allow replacing it with function that does,\nthe recommendation is to add a noop call to a helper, e.g.:\n- for skb do 'bpf_skb_change_proto(skb, 0, 0)';\n- for xdp do 'bpf_xdp_adjust_meta(xdp, 0)'.\n\nFunctions also can do tail calls. Effects of the tail call cannot be\nanalyzed before-hand, thus verifier assumes that tail calls always\nchange packet data.\n\nChanges v1 [1] -> v2:\n- added handling of extension programs and tail calls\n  (thanks, Alexei, for all the input).\n\n[0] https://lore.kernel.org/bpf/0498CA22-5779-4767-9C0C-A9515CEA711F@gmail.com/\n[1] https://lore.kernel.org/bpf/20241206040307.568065-1-eddyz87@gmail.com/\n====================\n\nLink: https://patch.msgid.link/20241210041100.1898468-1-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-12-10 10:24:58 -0800",
      "commits": [
        {
          "hash": "27e88bc4df1d80888fe1aaca786a7cc6e69587e2",
          "subject": "bpf: add find_containing_subprog() utility function",
          "message": "Add a utility function, looking for a subprogram containing a given\ninstruction index, rewrite find_subprog() to use this function.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-12-10 10:24:57 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b238e187b4a2d3b54d80aec05a9cab6466b79dde",
          "subject": "bpf: refactor bpf_helper_changes_pkt_data to use helper number",
          "message": "Use BPF helper number instead of function pointer in\nbpf_helper_changes_pkt_data(). This would simplify usage of this\nfunction in verifier.c:check_cfg() (in a follow-up patch),\nwhere only helper number is easily available and there is no real need\nto lookup helper proto.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-3-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-12-10 10:24:57 -0800",
          "modified_files": [
            "include/linux/filter.h",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c",
            "net/core/filter.c"
          ]
        },
        {
          "hash": "51081a3f25c742da5a659d7fc6fd77ebfdd555be",
          "subject": "bpf: track changes_pkt_data property for global functions",
          "message": "When processing calls to certain helpers, verifier invalidates all\npacket pointers in a current state. For example, consider the\nfollowing program:\n\n    __attribute__((__noinline__))\n    long skb_pull_data(struct __sk_buff *sk, __u32 len)\n    {\n        return bpf_skb_pull_data(sk, len);\n    }\n\n    SEC(\"tc\")\n    int test_invalidate_checks(struct __sk_buff *sk)\n    {\n        int *p = (void *)(long)sk->data;\n        if ((void *)(p + 1) > (void *)(long)sk->data_end) return TCX_DROP;\n        skb_pull_data(sk, 0);\n        *p = 42;\n        return TCX_PASS;\n    }\n\nAfter a call to bpf_skb_pull_data() the pointer 'p' can't be used\nsafely. See function filter.c:bpf_helper_changes_pkt_data() for a list\nof such helpers.\n\nAt the moment verifier invalidates packet pointers when processing\nhelper function calls, and does not traverse global sub-programs when\nprocessing calls to global sub-programs. This means that calls to\nhelpers done from global sub-programs do not invalidate pointers in\nthe caller state. E.g. the program above is unsafe, but is not\nrejected by verifier.\n\nThis commit fixes the omission by computing field\nbpf_subprog_info->changes_pkt_data for each sub-program before main\nverification pass.\nchanges_pkt_data should be set if:\n- subprogram calls helper for which bpf_helper_changes_pkt_data\n  returns true;\n- subprogram calls a global function,\n  for which bpf_subprog_info->changes_pkt_data should be set.\n\nThe verifier.c:check_cfg() pass is modified to compute this\ninformation. The commit relies on depth first instruction traversal\ndone by check_cfg() and absence of recursive function calls:\n- check_cfg() would eventually visit every call to subprogram S in a\n  state when S is fully explored;\n- when S is fully explored:\n  - every direct helper call within S is explored\n    (and thus changes_pkt_data is set if needed);\n  - every call to subprogram S1 called by S was visited with S1 fully\n    explored (and thus S inherits changes_pkt_data from S1).\n\nThe downside of such approach is that dead code elimination is not\ntaken into account: if a helper call inside global function is dead\nbecause of current configuration, verifier would conservatively assume\nthat the call occurs for the purpose of the changes_pkt_data\ncomputation.\n\nReported-by: Nick Zavaritsky <mejedi@gmail.com>\nCloses: https://lore.kernel.org/bpf/0498CA22-5779-4767-9C0C-A9515CEA711F@gmail.com/\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-12-10 10:24:57 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "3f23ee5590d9605dbde9a5e1d4b97637a4803329",
          "subject": "selftests/bpf: test for changing packet data from global functions",
          "message": "Check if verifier is aware of packet pointers invalidation done in\nglobal functions. Based on a test shared by Nick Zavaritsky in [0].\n\n[0] https://lore.kernel.org/bpf/0498CA22-5779-4767-9C0C-A9515CEA711F@gmail.com/\n\nSuggested-by: Nick Zavaritsky <mejedi@gmail.com>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-5-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-12-10 10:24:57 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_sock.c"
          ]
        },
        {
          "hash": "81f6d0530ba031b5f038a091619bf2ff29568852",
          "subject": "bpf: check changes_pkt_data property for extension programs",
          "message": "When processing calls to global sub-programs, verifier decides whether\nto invalidate all packet pointers in current state depending on the\nchanges_pkt_data property of the global sub-program.\n\nBecause of this, an extension program replacing a global sub-program\nmust be compatible with changes_pkt_data property of the sub-program\nbeing replaced.\n\nThis commit:\n- adds changes_pkt_data flag to struct bpf_prog_aux:\n  - this flag is set in check_cfg() for main sub-program;\n  - in jit_subprogs() for other sub-programs;\n- modifies bpf_check_attach_btf_id() to check changes_pkt_data flag;\n- moves call to check_attach_btf_id() after the call to check_cfg(),\n  because it needs changes_pkt_data flag to be set:\n\n    bpf_check:\n      ...                             ...\n    - check_attach_btf_id             resolve_pseudo_ldimm64\n      resolve_pseudo_ldimm64   -->    bpf_prog_is_offloaded\n      bpf_prog_is_offloaded           check_cfg\n      check_cfg                     + check_attach_btf_id\n      ...                             ...\n\nThe following fields are set by check_attach_btf_id():\n- env->ops\n- prog->aux->attach_btf_trace\n- prog->aux->attach_func_name\n- prog->aux->attach_func_proto\n- prog->aux->dst_trampoline\n- prog->aux->mod\n- prog->aux->saved_dst_attach_type\n- prog->aux->saved_dst_prog_type\n- prog->expected_attach_type\n\nNeither of these fields are used by resolve_pseudo_ldimm64() or\nbpf_prog_offload_verifier_prep() (for netronome and netdevsim\ndrivers), so the reordering is safe.\n\nSuggested-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-6-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-12-10 10:24:57 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "89ff40890d8f12a7d7e93fb602cc27562f3834f0",
          "subject": "selftests/bpf: freplace tests for tracking of changes_packet_data",
          "message": "Try different combinations of global functions replacement:\n- replace function that changes packet data with one that doesn't;\n- replace function that changes packet data with one that does;\n- replace function that doesn't change packet data with one that does;\n- replace function that doesn't change packet data with one that doesn't;\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-7-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-12-10 10:24:57 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/changes_pkt_data.c",
            "tools/testing/selftests/bpf/progs/changes_pkt_data.c",
            "tools/testing/selftests/bpf/progs/changes_pkt_data_freplace.c"
          ]
        },
        {
          "hash": "1a4607ffba35bf2a630aab299e34dd3f6e658d70",
          "subject": "bpf: consider that tail calls invalidate packet pointers",
          "message": "Tail-called programs could execute any of the helpers that invalidate\npacket pointers. Hence, conservatively assume that each tail call\ninvalidates packet pointers.\n\nMaking the change in bpf_helper_changes_pkt_data() automatically makes\nuse of check_cfg() logic that computes 'changes_pkt_data' effect for\nglobal sub-programs, such that the following program could be\nrejected:\n\n    int tail_call(struct __sk_buff *sk)\n    {\n    \tbpf_tail_call_static(sk, &jmp_table, 0);\n    \treturn 0;\n    }\n\n    SEC(\"tc\")\n    int not_safe(struct __sk_buff *sk)\n    {\n    \tint *p = (void *)(long)sk->data;\n    \t... make p valid ...\n    \ttail_call(sk);\n    \t*p = 42; /* this is unsafe */\n    \t...\n    }\n\nThe tc_bpf2bpf.c:subprog_tc() needs change: mark it as a function that\ncan invalidate packet pointers. Otherwise, it can't be freplaced with\ntailcall_freplace.c:entry_freplace() that does a tail call.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-8-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-12-10 10:24:57 -0800",
          "modified_files": [
            "net/core/filter.c",
            "tools/testing/selftests/bpf/progs/tc_bpf2bpf.c"
          ]
        },
        {
          "hash": "d9706b56e13b7916461ca6b4b731e169ed44ed09",
          "subject": "selftests/bpf: validate that tail call invalidates packet pointers",
          "message": "Add a test case with a tail call done from a global sub-program. Such\ntails calls should be considered as invalidating packet pointers.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-9-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-12-10 10:24:58 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_sock.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "e2cf913314b9543f4479788443c7e9009c6c56d8",
      "merge_subject": "Merge branch 'fixes-for-stack-with-allow_ptr_leaks'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nFixes for stack with allow_ptr_leaks\n\nTwo fixes for usability/correctness gaps when interacting with the stack\nwithout CAP_PERFMON (i.e. with allow_ptr_leaks = false). See the commits\nfor details. I've verified that the tests fail when run without the fixes.\n\nChangelog:\n----------\nv3 -> v4\nv3: https://lore.kernel.org/bpf/20241202083814.1888784-1-memxor@gmail.com\n\n * Address Andrii's comments\n   * Fix bug paperered over by missing CAP_NET_ADMIN in verifier_mtu\n     test\n   * Add warning when undefined CAP_ constant is specified, and fail\n     test\n   * Reorder annotations to be more clear\n   * Verify that fixes fail without patches again\n * Add Acked-by from Andrii\n\nv2 -> v3\nv2: https://lore.kernel.org/bpf/20241127212026.3580542-1-memxor@gmail.com\n\n * Address comments from Eduard\n   * Fix comment for mark_stack_slot_misc\n   * We can simply always return early when stype == STACK_INVALID\n   * Drop allow_ptr_leaks conditionals\n   * Add Eduard's __caps_unpriv patch into the series\n   * Convert test_verifier_mtu to use it\n   * Move existing tests to __caps_unpriv annotation and verifier_spill_fill.c\n   * Add Acked-by from Eduard\n\nv1 -> v2\nv1: https://lore.kernel.org/bpf/20241127185135.2753982-1-memxor@gmail.com\n\n * Fix CI errors in selftest by removing dependence on BPF_ST\n====================\n\nLink: https://patch.msgid.link/20241204044757.1483141-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-12-04 09:19:51 -0800",
      "commits": [
        {
          "hash": "69772f509e084ec6bca12dbcdeeeff41b0103774",
          "subject": "bpf: Don't mark STACK_INVALID as STACK_MISC in mark_stack_slot_misc",
          "message": "Inside mark_stack_slot_misc, we should not upgrade STACK_INVALID to\nSTACK_MISC when allow_ptr_leaks is false, since invalid contents\nshouldn't be read unless the program has the relevant capabilities.\nThe relaxation only makes sense when env->allow_ptr_leaks is true.\n\nHowever, such conversion in privileged mode becomes unnecessary, as\ninvalid slots can be read without being upgraded to STACK_MISC.\n\nCurrently, the condition is inverted (i.e. checking for true instead of\nfalse), simply remove it to restore correct behavior.\n\nFixes: eaf18febd6eb (\"bpf: preserve STACK_ZERO slots on partial reg spills\")\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nReported-by: Tao Lyu <tao.lyu@epfl.ch>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241204044757.1483141-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-12-04 09:19:50 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b0e66977dc072906bb76555fb1a64261d7f63d0f",
          "subject": "bpf: Fix narrow scalar spill onto 64-bit spilled scalar slots",
          "message": "When CAP_PERFMON and CAP_SYS_ADMIN (allow_ptr_leaks) are disabled, the\nverifier aims to reject partial overwrite on an 8-byte stack slot that\ncontains a spilled pointer.\n\nHowever, in such a scenario, it rejects all partial stack overwrites as\nlong as the targeted stack slot is a spilled register, because it does\nnot check if the stack slot is a spilled pointer.\n\nIncomplete checks will result in the rejection of valid programs, which\nspill narrower scalar values onto scalar slots, as shown below.\n\n0: R1=ctx() R10=fp0\n; asm volatile ( @ repro.bpf.c:679\n0: (7a) *(u64 *)(r10 -8) = 1          ; R10=fp0 fp-8_w=1\n1: (62) *(u32 *)(r10 -8) = 1\nattempt to corrupt spilled pointer on stack\nprocessed 2 insns (limit 1000000) max_states_per_insn 0 total_states 0 peak_states 0 mark_read 0.\n\nFix this by expanding the check to not consider spilled scalar registers\nwhen rejecting the write into the stack.\n\nPrevious discussion on this patch is at link [0].\n\n  [0]: https://lore.kernel.org/bpf/20240403202409.2615469-1-tao.lyu@epfl.ch\n\nFixes: ab125ed3ec1c (\"bpf: fix check for attempt to corrupt spilled pointer\")\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Tao Lyu <tao.lyu@epfl.ch>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241204044757.1483141-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Tao Lyu <tao.lyu@epfl.ch>",
          "date": "2024-12-04 09:19:50 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "adfdd9c68566120debc622712888c4d084539081",
          "subject": "selftests/bpf: Introduce __caps_unpriv annotation for tests",
          "message": "Add a __caps_unpriv annotation so that tests requiring specific\ncapabilities while dropping the rest can conveniently specify them\nduring selftest declaration instead of munging with capabilities at\nruntime from the testing binary.\n\nWhile at it, let us convert test_verifier_mtu to use this new support\ninstead.\n\nSince we do not want to include linux/capability.h, we only defined the\nfour main capabilities BPF subsystem deals with in bpf_misc.h for use in\ntests. If the user passes a CAP_SYS_NICE or anything else that's not\ndefined in the header, capability parsing code will return a warning.\n\nAlso reject strtol returning 0. CAP_CHOWN = 0 but we'll never need to\nuse it, and strtol doesn't errno on failed conversion. Fail the test in\nsuch a case.\n\nThe original diff for this idea is available at link [0].\n\n  [0]: https://lore.kernel.org/bpf/a1e48f5d9ae133e19adc6adf27e19d585e06bab4.camel@gmail.com\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\n[ Kartikeya: rebase on bpf-next, add warn to parse_caps, convert test_verifier_mtu ]\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241204044757.1483141-4-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-12-04 09:19:50 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/bpf_misc.h",
            "tools/testing/selftests/bpf/progs/verifier_mtu.c",
            "tools/testing/selftests/bpf/test_loader.c"
          ]
        },
        {
          "hash": "f513c3635078fc184af66b1af9e4f2b2d19b7d79",
          "subject": "selftests/bpf: Add test for reading from STACK_INVALID slots",
          "message": "Ensure that when CAP_PERFMON is dropped, and the verifier sees\nallow_ptr_leaks as false, we are not permitted to read from a\nSTACK_INVALID slot. Without the fix, the test will report unexpected\nsuccess in loading.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241204044757.1483141-5-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-12-04 09:19:50 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
          ]
        },
        {
          "hash": "19b6dbc006ecc1f2c8d7fa2d5afbfb7e7eba7920",
          "subject": "selftests/bpf: Add test for narrow spill into 64-bit spilled scalar",
          "message": "Add a test case to verify that without CAP_PERFMON, the test now\nsucceeds instead of failing due to a verification error.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241204044757.1483141-6-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-12-04 09:19:50 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "d4c44354bcaffed729c4eba8c0f2ecfd61fea743",
      "merge_subject": "Merge branch 'fix-missing-process_iter_arg-type-check'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nFix missing process_iter_arg type check\n\nI am taking over Tao's earlier patch set that can be found at [0], after\nan offline discussion. The bug reported in that thread is that\nprocess_iter_arg missed a reg->type == PTR_TO_STACK check. Fix this by\nadding it in, and also address comments from Andrii on the earlier\nattempt. Include more selftests to ensure the error is caught.\n\n  [0]: https://lore.kernel.org/bpf/20241107214736.347630-1-tao.lyu@epfl.ch\n\nChangelog:\n----------\nv1 -> v2:\nv1: https://lore.kernel.org/bpf/20241127230147.4158201-1-memxor@gmail.com\n====================\n\nLink: https://patch.msgid.link/20241203000238.3602922-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-12-02 17:48:06 -0800",
      "commits": [
        {
          "hash": "12659d28615d606b36e382f4de2dd05550d202af",
          "subject": "bpf: Ensure reg is PTR_TO_STACK in process_iter_arg",
          "message": "Currently, KF_ARG_PTR_TO_ITER handling missed checking the reg->type and\nensuring it is PTR_TO_STACK. Instead of enforcing this in the caller of\nprocess_iter_arg, move the check into it instead so that all callers\nwill gain the check by default. This is similar to process_dynptr_func.\n\nAn existing selftest in verifier_bits_iter.c fails due to this change,\nbut it's because it was passing a NULL pointer into iter_next helper and\ngetting an error further down the checks, but probably meant to pass an\nuninitialized iterator on the stack (as is done in the subsequent test\nbelow it). We will gain coverage for non-PTR_TO_STACK arguments in later\npatches hence just change the declaration to zero-ed stack object.\n\nFixes: 06accc8779c1 (\"bpf: add support for open-coded iterator loops\")\nSuggested-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Tao Lyu <tao.lyu@epfl.ch>\n[ Kartikeya: move check into process_iter_arg, rewrite commit log ]\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241203000238.3602922-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Tao Lyu <tao.lyu@epfl.ch>",
          "date": "2024-12-02 17:47:56 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_bits_iter.c"
          ]
        },
        {
          "hash": "7f71197001e35f46aca97204986edf9301f8dd09",
          "subject": "selftests/bpf: Add tests for iter arg check",
          "message": "Add selftests to cover argument type check for iterator kfuncs, and\ncover all three kinds (new, next, destroy). Without the fix in the\nprevious patch, the selftest would not cause a verifier error.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241203000238.3602922-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-12-02 17:47:56 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/iters.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "c1bc51f85cd6be28a4ec901b358731550a203bb2",
      "merge_subject": "Merge branch 'bpf-support-private-stack-for-bpf-progs'",
      "merge_body": "Yonghong Song says:\n\n====================\nbpf: Support private stack for bpf progs\n\nThe main motivation for private stack comes from nested scheduler in\nsched-ext from Tejun. The basic idea is that\n - each cgroup will its own associated bpf program,\n - bpf program with parent cgroup will call bpf programs\n   in immediate child cgroups.\n\nLet us say we have the following cgroup hierarchy:\n  root_cg (prog0):\n    cg1 (prog1):\n      cg11 (prog11):\n        cg111 (prog111)\n        cg112 (prog112)\n      cg12 (prog12):\n        cg121 (prog121)\n        cg122 (prog122)\n    cg2 (prog2):\n      cg21 (prog21)\n      cg22 (prog22)\n      cg23 (prog23)\n\nIn the above example, prog0 will call a kfunc which will call prog1 and\nprog2 to get sched info for cg1 and cg2 and then the information is\nsummarized and sent back to prog0. Similarly, prog11 and prog12 will be\ninvoked in the kfunc and the result will be summarized and sent back to\nprog1, etc. The following illustrates a possible call sequence:\n   ... -> bpf prog A -> kfunc -> ops.<callback_fn> (bpf prog B) ...\n\nCurrently, for each thread, the x86 kernel allocate 16KB stack. Each\nbpf program (including its subprograms) has maximum 512B stack size to\navoid potential stack overflow. Nested bpf programs further increase the\nrisk of stack overflow. To avoid potential stack overflow caused by bpf\nprograms, this patch set supported private stack and bpf program stack\nspace is allocated during jit time. Using private stack for bpf progs\ncan reduce or avoid potential kernel stack overflow.\n\nCurrently private stack is applied to tracing programs like kprobe/uprobe,\nperf_event, tracepoint, raw tracepoint and struct_ops progs.\nTracing progs enable private stack if any subprog stack size is more\nthan a threshold (i.e. 64B). Struct-ops progs enable private stack\nbased on particular struct op implementation which can enable private\nstack before verification at per-insn level. Struct-ops progs have\nthe same treatment as tracing progs w.r.t when to enable private stack.\n\nFor all these progs, the kernel will do recursion check (no nesting for\nper prog per cpu) to ensure that private stack won't be overwritten.\nThe bpf_prog_aux struct has a callback func recursion_detected() which\ncan be implemented by kernel subsystem to synchronously detect recursion,\nreport error, etc.\n\nOnly x86_64 arch supports private stack now. It can be extended to other\narchs later. Please see each individual patch for details.\n\nChange logs:\n  v11 -> v12:\n    - v11 link: https://lore.kernel.org/bpf/20241109025312.148539-1-yonghong.song@linux.dev/\n    - Fix a bug where allocated percpu space is less than actual private stack.\n    - Add guard memory (before and after actual prog stack) to detect potential\n      underflow/overflow.\n  v10 -> v11:\n    - v10 link: https://lore.kernel.org/bpf/20241107024138.3355687-1-yonghong.song@linux.dev/\n    - Use two bool variables, priv_stack_requested (used by struct-ops only) and\n      jits_use_priv_stack, in order to make code cleaner.\n    - Set env->prog->aux->jits_use_priv_stack to true if any subprog uses private stack.\n      This is for struct-ops use case to kick in recursion protection.\n  v9 -> v10:\n    - v9 link: https://lore.kernel.org/bpf/20241104193455.3241859-1-yonghong.song@linux.dev/\n    - Simplify handling async cbs by making those async cb related progs using normal\n      kernel stack.\n    - Do percpu allocation in jit instead of verifier.\n  v8 -> v9:\n    - v8 link: https://lore.kernel.org/bpf/20241101030950.2677215-1-yonghong.song@linux.dev/\n    - Use enum to express priv stack mode.\n    - Use bits in bpf_subprog_info struct to do subprog recursion check between\n      main/async and async subprogs.\n    - Fix potential memory leak.\n    - Rename recursion detection func from recursion_skipped() to recursion_detected().\n  v7 -> v8:\n    - v7 link: https://lore.kernel.org/bpf/20241029221637.264348-1-yonghong.song@linux.dev/\n    - Add recursion_skipped() callback func to bpf_prog->aux structure such that if\n      a recursion miss happened and bpf_prog->aux->recursion_skipped is not NULL, the\n      callback fn will be called so the subsystem can do proper action based on their\n      respective design.\n  v6 -> v7:\n    - v6 link: https://lore.kernel.org/bpf/20241020191341.2104841-1-yonghong.song@linux.dev/\n    - Going back to do private stack allocation per prog instead per subtree. This can\n      simplify implementation and avoid verifier complexity.\n    - Handle potential nested subprog run if async callback exists.\n    - Use struct_ops->check_member() callback to set whether a particular struct-ops\n      prog wants private stack or not.\n  v5 -> v6:\n    - v5 link: https://lore.kernel.org/bpf/20241017223138.3175885-1-yonghong.song@linux.dev/\n    - Instead of using (or not using) private stack at struct_ops level,\n      each prog in struct_ops can decide whether to use private stack or not.\n  v4 -> v5:\n    - v4 link: https://lore.kernel.org/bpf/20241010175552.1895980-1-yonghong.song@linux.dev/\n    - Remove bpf_prog_call() related implementation.\n    - Allow (opt-in) private stack for sched-ext progs.\n  v3 -> v4:\n    - v3 link: https://lore.kernel.org/bpf/20240926234506.1769256-1-yonghong.song@linux.dev/\n      There is a long discussion in the above v3 link trying to allow private\n      stack to be used by kernel functions in order to simplify implementation.\n      But unfortunately we didn't find a workable solution yet, so we return\n      to the approach where private stack is only used by bpf programs.\n    - Add bpf_prog_call() kfunc.\n  v2 -> v3:\n    - Instead of per-subprog private stack allocation, allocate private\n      stacks at main prog or callback entry prog. Subprogs not main or callback\n      progs will increment the inherited stack pointer to be their\n      frame pointer.\n    - Private stack allows each prog max stack size to be 512 bytes, intead\n      of the whole prog hierarchy to be 512 bytes.\n    - Add some tests.\n====================\n\nLink: https://lore.kernel.org/r/20241112163902.2223011-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-11-12 16:26:25 -0800",
      "commits": [
        {
          "hash": "a76ab5731e32d50ff5b1ae97e9dc4b23f41c23f5",
          "subject": "bpf: Find eligible subprogs for private stack support",
          "message": "Private stack will be allocated with percpu allocator in jit time.\nTo avoid complexity at runtime, only one copy of private stack is\navailable per cpu per prog. So runtime recursion check is necessary\nto avoid stack corruption.\n\nCurrent private stack only supports kprobe/perf_event/tp/raw_tp\nwhich has recursion check in the kernel, and prog types that use\nbpf trampoline recursion check. For trampoline related prog types,\ncurrently only tracing progs have recursion checking.\n\nTo avoid complexity, all async_cb subprogs use normal kernel stack\nincluding those subprogs used by both main prog subtree and async_cb\nsubtree. Any prog having tail call also uses kernel stack.\n\nTo avoid jit penalty with private stack support, a subprog stack\nsize threshold is set such that only if the stack size is no less\nthan the threshold, private stack is supported. The current threshold\nis 64 bytes. This avoids jit penality if the stack usage is small.\n\nA useless 'continue' is also removed from a loop in func\ncheck_max_stack_depth().\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241112163907.2223839-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-11-12 16:26:24 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "include/linux/filter.h",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "e00931c02568dc6ac76f94b1ab471de05e6fdfe8",
          "subject": "bpf: Enable private stack for eligible subprogs",
          "message": "If private stack is used by any subprog, set that subprog\nprog->aux->jits_use_priv_stack to be true so later jit can allocate\nprivate stack for that subprog properly.\n\nAlso set env->prog->aux->jits_use_priv_stack to be true if\nany subprog uses private stack. This is a use case for a\nsingle main prog (no subprogs) to use private stack, and\nalso a use case for later struct-ops progs where\nenv->prog->aux->jits_use_priv_stack will enable recursion\ncheck if any subprog uses private stack.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241112163912.2224007-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-11-12 16:26:24 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "f4b21ed0b9d6c9fe155451a1fb3531fb44b0afa8",
          "subject": "bpf, x86: Avoid repeated usage of bpf_prog->aux->stack_depth",
          "message": "Refactor the code to avoid repeated usage of bpf_prog->aux->stack_depth\nin do_jit() func. If the private stack is used, the stack_depth will be\n0 for that prog. Refactoring make it easy to adjust stack_depth.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241112163917.2224189-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-11-12 16:26:24 -0800",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c"
          ]
        },
        {
          "hash": "7d1cd70d4b16ff0216a5f6c2ae7d0fa9fa978c07",
          "subject": "bpf, x86: Support private stack in jit",
          "message": "Private stack is allocated in function bpf_int_jit_compile() with\nalignment 8. Private stack allocation size includes the stack size\ndetermined by verifier and additional space to protect stack overflow\nand underflow. See below an illustration:\n  ---> memory address increasing\n  [8 bytes to protect overflow] [normal stack] [8 bytes to protect underflow]\nIf overflow/underflow is detected, kernel messages will be\nemited in dmesg like\n  BPF private stack overflow/underflow detected for prog Fx\n  BPF Private stack overflow/underflow detected for prog bpf_prog_a41699c234a1567a_subprog1x\nThose messages are generated when I made some changes to jitted code\nto intentially cause overflow for some progs.\n\nFor the jited prog, The x86 register 9 (X86_REG_R9) is used to replace\nbpf frame register (BPF_REG_10). The private stack is used per\nsubprog per cpu. The X86_REG_R9 is saved and restored around every\nfunc call (not including tailcall) to maintain correctness of\nX86_REG_R9.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241112163922.2224385-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-11-12 16:26:24 -0800",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/bpf.h"
          ]
        },
        {
          "hash": "f4b295ab65980435d7dc8b12d110387d1d1c653c",
          "subject": "selftests/bpf: Add tracing prog private stack tests",
          "message": "Some private stack tests are added including:\n  - main prog only with stack size greater than BPF_PSTACK_MIN_SIZE.\n  - main prog only with stack size smaller than BPF_PSTACK_MIN_SIZE.\n  - prog with one subprog having MAX_BPF_STACK stack size and another\n    subprog having non-zero small stack size.\n  - prog with callback function.\n  - prog with exception in main prog or subprog.\n  - prog with async callback without nesting\n  - prog with async callback with possible nesting\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241112163927.2224750-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-11-12 16:26:25 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_private_stack.c"
          ]
        },
        {
          "hash": "5bd36da1e37e7a78e8b38efd287de6e1394b7d6e",
          "subject": "bpf: Support private stack for struct_ops progs",
          "message": "For struct_ops progs, whether a particular prog uses private stack\ndepends on prog->aux->priv_stack_requested setting before actual\ninsn-level verification for that prog. One particular implementation\nis to piggyback on struct_ops->check_member(). The next patch has\nan example for this. The struct_ops->check_member() sets\nprog->aux->priv_stack_requested to be true which enables private stack\nusage.\n\nThe struct_ops prog follows the same rule as kprobe/tracing progs after\nfunction bpf_enable_priv_stack(). For example, even a struct_ops prog\nrequests private stack, it could still use normal kernel stack if\nthe stack size is small (< 64 bytes).\n\nSimilar to tracing progs, nested same cpu same prog run will be skipped.\nA field (recursion_detected()) is added to bpf_prog_aux structure.\nIf bpf_prog->aux->recursion_detected is implemented by the struct_ops\nsubsystem and nested same cpu/prog happens, the function will be\ntriggered to report an error, collect related info, etc.\n\nAcked-by: Tejun Heo <tj@kernel.org>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241112163933.2224962-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-11-12 16:26:25 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "kernel/bpf/trampoline.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "becfe32b57c7d323fbd94c1a2c6d7eba918ddde8",
          "subject": "selftests/bpf: Add struct_ops prog private stack tests",
          "message": "Add three tests for struct_ops using private stack.\n  ./test_progs -t struct_ops_private_stack\n  #336/1   struct_ops_private_stack/private_stack:OK\n  #336/2   struct_ops_private_stack/private_stack_fail:OK\n  #336/3   struct_ops_private_stack/private_stack_recur:OK\n  #336     struct_ops_private_stack:OK\n\nThe following is a snippet of a struct_ops check_member() implementation:\n\n\tu32 moff = __btf_member_bit_offset(t, member) / 8;\n\tswitch (moff) {\n\tcase offsetof(struct bpf_testmod_ops3, test_1):\n        \tprog->aux->priv_stack_requested = true;\n                prog->aux->recursion_detected = test_1_recursion_detected;\n        \tfallthrough;\n\tdefault:\n        \tbreak;\n\t}\n\treturn 0;\n\nThe first test is with nested two different callback functions where the\nfirst prog has more than 512 byte stack size (including subprogs) with\nprivate stack enabled.\n\nThe second test is a negative test where the second prog has more than 512\nbyte stack size without private stack enabled.\n\nThe third test is the same callback function recursing itself. At run time,\nthe jit trampoline recursion check kicks in to prevent the recursion. The\nrecursion_detected() callback function is implemented by the bpf_testmod,\nthe following message in dmesg\n  bpf_testmod: oh no, recursing into test_1, recursion_misses 1\ndemonstrates the callback function is indeed triggered when recursion miss\nhappens.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241112163938.2225528-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-11-12 16:26:25 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.h",
            "tools/testing/selftests/bpf/prog_tests/struct_ops_private_stack.c",
            "tools/testing/selftests/bpf/progs/struct_ops_private_stack.c",
            "tools/testing/selftests/bpf/progs/struct_ops_private_stack_fail.c",
            "tools/testing/selftests/bpf/progs/struct_ops_private_stack_recur.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "7b6e5bfa2541380b478ea1532880210ea3e39e11",
      "merge_subject": "Merge branch 'refactor-lock-management'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nRefactor lock management\n\nThis set refactors lock management in the verifier in preparation for\nspin locks that can be acquired multiple times. In addition to this,\nunnecessary code special case reference leak logic for callbacks is also\ndropped, that is no longer necessary. See patches for details.\n\nChangelog:\n----------\nv5 -> v6\nv5: https://lore.kernel.org/bpf/20241109225243.2306756-1-memxor@gmail.com\n\n * Move active_locks mutation to {acquire,release}_lock_state (Alexei)\n\nv4 -> v5\nv4: https://lore.kernel.org/bpf/20241109074347.1434011-1-memxor@gmail.com\n\n * Make active_locks part of bpf_func_state (Alexei)\n * Remove unneeded in_callback_fn logic for references\n\nv3 -> v4\nv3: https://lore.kernel.org/bpf/20241104151716.2079893-1-memxor@gmail.com\n\n* Address comments from Alexei\n  * Drop struct bpf_active_lock definition\n  * Name enum type, expand definition to multiple lines\n  * s/REF_TYPE_BPF_LOCK/REF_TYPE_LOCK/g\n  * Change active_lock type to int\n  * Fix type of 'type' in acquire_lock_state\n  * Filter by taking type explicitly in find_lock_state\n  * WARN for default case in refsafe switch statement\n\nv2 -> v3\nv2: https://lore.kernel.org/bpf/20241103212252.547071-1-memxor@gmail.com\n\n  * Rebase on bpf-next to resolve merge conflict\n\nv1 -> v2\nv1: https://lore.kernel.org/bpf/20241103205856.345580-1-memxor@gmail.com\n\n  * Fix refsafe state comparison to check callback_ref and ptr separately.\n====================\n\nLink: https://lore.kernel.org/r/20241109231430.2475236-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-11-11 08:18:59 -0800",
      "commits": [
        {
          "hash": "f6b9a69a9e56b2083aca8a925fc1a28eb698e3ed",
          "subject": "bpf: Refactor active lock management",
          "message": "When bpf_spin_lock was introduced originally, there was deliberation on\nwhether to use an array of lock IDs, but since bpf_spin_lock is limited\nto holding a single lock at any given time, we've been using a single ID\nto identify the held lock.\n\nIn preparation for introducing spin locks that can be taken multiple\ntimes, introduce support for acquiring multiple lock IDs. For this\npurpose, reuse the acquired_refs array and store both lock and pointer\nreferences. We tag the entry with REF_TYPE_PTR or REF_TYPE_LOCK to\ndisambiguate and find the relevant entry. The ptr field is used to track\nthe map_ptr or btf (for bpf_obj_new allocations) to ensure locks can be\nmatched with protected fields within the same \"allocation\", i.e.\nbpf_obj_new object or map value.\n\nThe struct active_lock is changed to an int as the state is part of the\nacquired_refs array, and we only need active_lock as a cheap way of\ndetecting lock presence.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241109231430.2475236-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-11-11 08:18:51 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "ae6e3a273f590a2b64f14a9fab3546c3a8f44ed4",
          "subject": "bpf: Drop special callback reference handling",
          "message": "Logic to prevent callbacks from acquiring new references for the program\n(i.e. leaving acquired references), and releasing caller references\n(i.e. those acquired in parent frames) was introduced in commit\n9d9d00ac29d0 (\"bpf: Fix reference state management for synchronous callbacks\").\n\nThis was necessary because back then, the verifier simulated each\ncallback once (that could potentially be executed N times, where N can\nbe zero). This meant that callbacks that left lingering resources or\ncleared caller resources could do it more than once, operating on\nundefined state or leaking memory.\n\nWith the fixes to callback verification in commit\nab5cfac139ab (\"bpf: verify callbacks as if they are called unknown number of times\"),\nall of this extra logic is no longer necessary. Hence, drop it as part\nof this commit.\n\nCc: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241109231430.2475236-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-11-11 08:18:55 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/prog_tests/cb_refs.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "269e7c97cac8e19117518056e9f4bd3a1dfe9362",
      "merge_subject": "Merge branch 'bpf-add-uprobe-session-support'",
      "merge_body": "Jiri Olsa says:\n\n====================\nbpf: Add uprobe session support\n\nhi,\nthis patchset is adding support for session uprobe attachment and\nusing it through bpf link for bpf programs.\n\nThe session means that the uprobe consumer is executed on entry\nand return of probed function with additional control:\n  - entry callback can control execution of the return callback\n  - entry and return callbacks can share data/cookie\n\nUprobe changes (on top of perf/core [1] are posted in here [2].\nThis patchset is based on bpf-next/master and will be merged once\nwe pull [2] in bpf-next/master.\n\nv9 changes:\n  - rebased on bpf-next/master with perf/core tag merged (thanks Peter!)\n\nthanks,\njirka\n\n[1] git://git.kernel.org/pub/scm/linux/kernel/git/peterz/queue.git perf/core\n[2] https://lore.kernel.org/bpf/20241018202252.693462-1-jolsa@kernel.org/T/#ma43c549c4bf684ca1b17fa638aa5e7cbb46893e9\n---\n====================\n\nLink: https://lore.kernel.org/r/20241108134544.480660-1-jolsa@kernel.org\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2024-11-11 08:18:27 -0800",
      "commits": [
        {
          "hash": "17c4b65a24938c6dd79496cce5df15f70d9c253c",
          "subject": "bpf: Allow return values 0 and 1 for kprobe session",
          "message": "The kprobe session program can return only 0 or 1,\ninstruct verifier to check for that.\n\nFixes: 535a3692ba72 (\"bpf: Add support for kprobe session attach\")\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-2-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:17:57 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "f505005bc7426f4309880da94cfbfc37efa225bd",
          "subject": "bpf: Force uprobe bpf program to always return 0",
          "message": "As suggested by Andrii make uprobe multi bpf programs to always return 0,\nso they can't force uprobe removal.\n\nKeeping the int return type for uprobe_prog_run, because it will be used\nin following session changes.\n\nFixes: 89ae89f53d20 (\"bpf: Add multi uprobe link\")\nSuggested-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-3-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:00 -0800",
          "modified_files": [
            "kernel/trace/bpf_trace.c"
          ]
        },
        {
          "hash": "d920179b3d4842a0e27cae54fdddbe5ef3977e73",
          "subject": "bpf: Add support for uprobe multi session attach",
          "message": "Adding support to attach BPF program for entry and return probe\nof the same function. This is common use case which at the moment\nrequires to create two uprobe multi links.\n\nAdding new BPF_TRACE_UPROBE_SESSION attach type that instructs\nkernel to attach single link program to both entry and exit probe.\n\nIt's possible to control execution of the BPF program on return\nprobe simply by returning zero or non zero from the entry BPF\nprogram execution to execute or not the BPF program on return\nprobe respectively.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-4-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:03 -0800",
          "modified_files": [
            "include/uapi/linux/bpf.h",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c",
            "kernel/trace/bpf_trace.c",
            "tools/include/uapi/linux/bpf.h",
            "tools/lib/bpf/libbpf.c"
          ]
        },
        {
          "hash": "99b403d2060d3e2604958a0ec3a7f37b18256d6b",
          "subject": "bpf: Add support for uprobe multi session context",
          "message": "Placing bpf_session_run_ctx layer in between bpf_run_ctx and\nbpf_uprobe_multi_run_ctx, so the session data can be retrieved\nfrom uprobe_multi link.\n\nPlus granting session kfuncs access to uprobe session programs.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-5-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:04 -0800",
          "modified_files": [
            "kernel/trace/bpf_trace.c"
          ]
        },
        {
          "hash": "022367ec92794a00ad2eb1348c019709faf5c476",
          "subject": "libbpf: Add support for uprobe multi session attach",
          "message": "Adding support to attach program in uprobe session mode\nwith bpf_program__attach_uprobe_multi function.\n\nAdding session bool to bpf_uprobe_multi_opts struct that allows\nto load and attach the bpf program via uprobe session.\nthe attachment to create uprobe multi session.\n\nAlso adding new program loader section that allows:\n  SEC(\"uprobe.session/bpf_fentry_test*\")\n\nand loads/attaches uprobe program as uprobe session.\n\nAdding sleepable hook (uprobe.session.s) as well.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-6-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:06 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/libbpf.c",
            "tools/lib/bpf/libbpf.h"
          ]
        },
        {
          "hash": "4856ecb11524c96bfedbd7dc44d60f394d32bc9f",
          "subject": "selftests/bpf: Add uprobe session test",
          "message": "Adding uprobe session test and testing that the entry program\nreturn value controls execution of the return probe program.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-7-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:08 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/uprobe_multi_test.c",
            "tools/testing/selftests/bpf/progs/uprobe_multi_session.c"
          ]
        },
        {
          "hash": "f6b45e352f0f822bc0bb01b14829ac8f3158d056",
          "subject": "selftests/bpf: Add uprobe session cookie test",
          "message": "Adding uprobe session test that verifies the cookie value\nget properly propagated from entry to return program.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-8-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:10 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/uprobe_multi_test.c",
            "tools/testing/selftests/bpf/progs/uprobe_multi_session_cookie.c"
          ]
        },
        {
          "hash": "8bcb9c62f0689402e90886d3b65fc649d7c600d7",
          "subject": "selftests/bpf: Add uprobe session recursive test",
          "message": "Adding uprobe session test that verifies the cookie value is stored\nproperly when single uprobe-ed function is executed recursively.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-9-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:12 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/uprobe_multi_test.c",
            "tools/testing/selftests/bpf/progs/uprobe_multi_session_recursive.c"
          ]
        },
        {
          "hash": "8c3a48b0d9b41d8c3903a88d35b8f32c260e1a57",
          "subject": "selftests/bpf: Add uprobe session verifier test for return value",
          "message": "Making sure uprobe.session program can return only [0,1] values.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-10-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:14 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/uprobe_multi_test.c",
            "tools/testing/selftests/bpf/progs/uprobe_multi_verifier.c"
          ]
        },
        {
          "hash": "504d21d905002f2b3e2a8703a3d4630a680362e2",
          "subject": "selftests/bpf: Add kprobe session verifier test for return value",
          "message": "Making sure kprobe.session program can return only [0,1] values.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-11-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:16 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/kprobe_multi_test.c",
            "tools/testing/selftests/bpf/progs/kprobe_multi_verifier.c"
          ]
        },
        {
          "hash": "c574bcd6229333c211dbf4ecba2988c3581b0f92",
          "subject": "selftests/bpf: Add uprobe session single consumer test",
          "message": "Testing that the session ret_handler bypass works on single\nuprobe with multiple consumers, each with different session\nignore return value.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-12-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:18 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/uprobe_multi_test.c",
            "tools/testing/selftests/bpf/progs/uprobe_multi_session_single.c"
          ]
        },
        {
          "hash": "b1c570adc7a6f6cbb42926d5313036ed1543f00e",
          "subject": "selftests/bpf: Add uprobe sessions to consumer test",
          "message": "Adding uprobe session consumers to the consumer test,\nso we get the session into the test mix.\n\nIn addition scaling down the test to have just 1 uprobe\nand 1 uretprobe, otherwise the test time grows and is\nunsuitable for CI even with threads.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-13-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:20 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/uprobe_multi_test.c",
            "tools/testing/selftests/bpf/progs/uprobe_multi_consumers.c"
          ]
        },
        {
          "hash": "abaec8341a86e556dff739d093aa30babc498ec5",
          "subject": "selftests/bpf: Add threads to consumer test",
          "message": "With recent uprobe fix [1] the sync time after unregistering uprobe is\nmuch longer and prolongs the consumer test which creates and destroys\nhundreds of uprobes.\n\nThis change adds 16 threads (which fits the test logic) and speeds up\nthe test.\n\nBefore the change:\n\n  # perf stat --null ./test_progs -t uprobe_multi_test/consumers\n  #421/9   uprobe_multi_test/consumers:OK\n  #421     uprobe_multi_test:OK\n  Summary: 1/1 PASSED, 0 SKIPPED, 0 FAILED\n\n   Performance counter stats for './test_progs -t uprobe_multi_test/consumers':\n\n        28.818778973 seconds time elapsed\n\n         0.745518000 seconds user\n         0.919186000 seconds sys\n\nAfter the change:\n\n  # perf stat --null ./test_progs -t uprobe_multi_test/consumers 2>&1\n  #421/9   uprobe_multi_test/consumers:OK\n  #421     uprobe_multi_test:OK\n  Summary: 1/1 PASSED, 0 SKIPPED, 0 FAILED\n\n   Performance counter stats for './test_progs -t uprobe_multi_test/consumers':\n\n         3.504790814 seconds time elapsed\n\n         0.012141000 seconds user\n         0.751760000 seconds sys\n\n[1] commit 87195a1ee332 (\"uprobes: switch to RCU Tasks Trace flavor for better performance\")\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-14-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-11-11 08:18:23 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/uprobe_multi_test.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "1850ce1bddf2e0939651ce1c110bc52c796e0f66",
      "merge_subject": "Merge branch 'handle-possible-null-trusted-raw_tp-arguments'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nHandle possible NULL trusted raw_tp arguments\n\nMore context is available in [0], but the TLDR; is that the verifier\nincorrectly assumes that any raw tracepoint argument will always be\nnon-NULL. This means that even when users correctly check possible NULL\narguments, the verifier can remove the NULL check due to incorrect\nknowledge of the NULL-ness of the pointer. Secondly, kernel helpers or\nkfuncs taking these trusted tracepoint arguments incorrectly assume that\nall arguments will always be valid non-NULL.\n\nIn this set, we mark raw_tp arguments as PTR_MAYBE_NULL on top of\nPTR_TRUSTED, but special case their behavior when dereferencing them or\npointer arithmetic over them is involved. When passing trusted args to\nhelpers or kfuncs, raw_tp programs are permitted to pass possibly NULL\npointers in such cases.\n\nAny loads into such maybe NULL trusted PTR_TO_BTF_ID is promoted to a\nPROBE_MEM load to handle emanating page faults. The verifier will ensure\nNULL checks on such pointers are preserved and do not lead to dead code\nelimination.\n\nThis new behavior is not applied when ref_obj_id is non-zero, as those\npointers do not belong to raw_tp arguments, but instead acquired\nobjects.\n\nSince helpers and kfuncs already require attention for PTR_TO_BTF_ID\n(non-trusted) pointers, we do not implement any protection for such\ncases in this patch set, and leave it as future work for an upcoming\nseries.\n\nA selftest is included with this patch set to verify the new behavior,\nand it crashes the kernel without the first patch.\n\n [0]: https://lore.kernel.org/bpf/CAADnVQLMPPavJQR6JFsi3dtaaLHB816JN4HCV_TFWohJ61D+wQ@mail.gmail.com\n\nChangelog:\n----------\nv2 -> v3\nv2: https://lore.kernel.org/bpf/20241103184144.3765700-1-memxor@gmail.com\n\n * Fix lenient check around check_ptr_to_btf_access allowing any\n   PTR_TO_BTF_ID with PTR_MAYBE_NULL to be deref'd.\n * Add Juri and Jiri's Tested-by, Reviewed-by resp.\n\nv1 -> v2\nv1: https://lore.kernel.org/bpf/20241101000017.3424165-1-memxor@gmail.com\n\n * Add patch to clean up users of gettid (Andrii)\n * Avoid nested blocks in sefltest (Andrii)\n * Prevent code motion optimization in selftest using barrier()\n====================\n\nLink: https://lore.kernel.org/r/20241104171959.2938862-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-11-04 11:37:37 -0800",
      "commits": [
        {
          "hash": "cb4158ce8ec8a5bb528cc1693356a5eb8058094d",
          "subject": "bpf: Mark raw_tp arguments with PTR_MAYBE_NULL",
          "message": "Arguments to a raw tracepoint are tagged as trusted, which carries the\nsemantics that the pointer will be non-NULL.  However, in certain cases,\na raw tracepoint argument may end up being NULL. More context about this\nissue is available in [0].\n\nThus, there is a discrepancy between the reality, that raw_tp arguments\ncan actually be NULL, and the verifier's knowledge, that they are never\nNULL, causing explicit NULL checks to be deleted, and accesses to such\npointers potentially crashing the kernel.\n\nTo fix this, mark raw_tp arguments as PTR_MAYBE_NULL, and then special\ncase the dereference and pointer arithmetic to permit it, and allow\npassing them into helpers/kfuncs; these exceptions are made for raw_tp\nprograms only. Ensure that we don't do this when ref_obj_id > 0, as in\nthat case this is an acquired object and doesn't need such adjustment.\n\nThe reason we do mask_raw_tp_trusted_reg logic is because other will\nrecheck in places whether the register is a trusted_reg, and then\nconsider our register as untrusted when detecting the presence of the\nPTR_MAYBE_NULL flag.\n\nTo allow safe dereference, we enable PROBE_MEM marking when we see loads\ninto trusted pointers with PTR_MAYBE_NULL.\n\nWhile trusted raw_tp arguments can also be passed into helpers or kfuncs\nwhere such broken assumption may cause issues, a future patch set will\ntackle their case separately, as PTR_TO_BTF_ID (without PTR_TRUSTED) can\nalready be passed into helpers and causes similar problems. Thus, they\nare left alone for now.\n\nIt is possible that these checks also permit passing non-raw_tp args\nthat are trusted PTR_TO_BTF_ID with null marking. In such a case,\nallowing dereference when pointer is NULL expands allowed behavior, so\nwon't regress existing programs, and the case of passing these into\nhelpers is the same as above and will be dealt with later.\n\nAlso update the failure case in tp_btf_nullable selftest to capture the\nnew behavior, as the verifier will no longer cause an error when\ndirectly dereference a raw tracepoint argument marked as __nullable.\n\n  [0]: https://lore.kernel.org/bpf/ZrCZS6nisraEqehw@jlelli-thinkpadt14gen4.remote.csb\n\nReviewed-by: Jiri Olsa <jolsa@kernel.org>\nReported-by: Juri Lelli <juri.lelli@redhat.com>\nTested-by: Juri Lelli <juri.lelli@redhat.com>\nFixes: 3f00c5239344 (\"bpf: Allow trusted pointers to be passed to KF_TRUSTED_ARGS kfuncs\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241104171959.2938862-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-11-04 11:37:36 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/test_tp_btf_nullable.c"
          ]
        },
        {
          "hash": "0e2fb011a0ba8e2258ce776fdf89fbd589c2a3a6",
          "subject": "selftests/bpf: Clean up open-coded gettid syscall invocations",
          "message": "Availability of the gettid definition across glibc versions supported by\nBPF selftests is not certain. Currently, all users in the tree open-code\nsyscall to gettid. Convert them to a common macro definition.\n\nReviewed-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241104171959.2938862-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-11-04 11:37:36 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/benchs/bench_trigger.c",
            "tools/testing/selftests/bpf/bpf_util.h",
            "tools/testing/selftests/bpf/map_tests/task_storage_map.c",
            "tools/testing/selftests/bpf/prog_tests/bpf_cookie.c",
            "tools/testing/selftests/bpf/prog_tests/bpf_iter.c",
            "tools/testing/selftests/bpf/prog_tests/cgrp_local_storage.c",
            "tools/testing/selftests/bpf/prog_tests/core_reloc.c",
            "tools/testing/selftests/bpf/prog_tests/linked_funcs.c",
            "tools/testing/selftests/bpf/prog_tests/ns_current_pid_tgid.c",
            "tools/testing/selftests/bpf/prog_tests/rcu_read_lock.c",
            "tools/testing/selftests/bpf/prog_tests/task_local_storage.c",
            "tools/testing/selftests/bpf/prog_tests/uprobe_multi_test.c"
          ]
        },
        {
          "hash": "d798ce3f4cab1b0d886b19ec5cc8e6b3d7e35081",
          "subject": "selftests/bpf: Add tests for raw_tp null handling",
          "message": "Ensure that trusted PTR_TO_BTF_ID accesses perform PROBE_MEM handling in\nraw_tp program. Without the previous fix, this selftest crashes the\nkernel due to a NULL-pointer dereference. Also ensure that dead code\nelimination does not kick in for checks on the pointer.\n\nReviewed-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241104171959.2938862-4-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-11-04 11:37:36 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod-events.h",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/prog_tests/raw_tp_null.c",
            "tools/testing/selftests/bpf/progs/raw_tp_null.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "f2daa5a577e95f4be4e9ffae17b5bbf1ffe7a852",
      "merge_subject": "Merge branch 'fix-resource-leak-checks-for-tail-calls'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nFix resource leak checks for tail calls\n\nThis set contains a fix for detecting unreleased RCU read locks or\nunfinished preempt_disable sections when performing a tail call. Spin\nlocks are prevented by accident since they don't allow any function\ncalls, including tail calls (modelled as call instruction to a helper),\nso we ensure they are checked as well, in preparation for relaxing\nfunction call restricton for critical sections in the future.\n\nThen, in the second patch, all the checks for reference leaks and locks\nare unified into a single function that can be called from different\nplaces. This unification patch is kept separate and placed after the fix\nto allow independent backport of the fix to older kernels without a\ndepdendency on the clean up.\n\nNaturally, this creates a divergence in the disparate error messages,\ntherefore selftests that rely on the exact error strings need to be\nupdated to match the new verifier log message.\n\nA selftest is included to ensure no regressions occur wrt this behavior.\n====================\n\nLink: https://lore.kernel.org/r/20241103225940.1408302-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-11-03 16:52:06 -0800",
      "commits": [
        {
          "hash": "46f7ed32f7a873d6675ea72e1d6317df41a55f81",
          "subject": "bpf: Tighten tail call checks for lingering locks, RCU, preempt_disable",
          "message": "There are three situations when a program logically exits and transfers\ncontrol to the kernel or another program: bpf_throw, BPF_EXIT, and tail\ncalls. The former two check for any lingering locks and references, but\ntail calls currently do not. Expand the checks to check for spin locks,\nRCU read sections and preempt disabled sections.\n\nSpin locks are indirectly preventing tail calls as function calls are\ndisallowed, but the checks for preemption and RCU are more relaxed,\nhence ensure tail calls are prevented in their presence.\n\nFixes: 9bb00b2895cb (\"bpf: Add kfunc bpf_rcu_read_lock/unlock()\")\nFixes: fc7566ad0a82 (\"bpf: Introduce bpf_preempt_[disable,enable] kfuncs\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241103225940.1408302-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-11-03 16:52:06 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "d402755ced2ea8fc1f0513136f074002d509bfa0",
          "subject": "bpf: Unify resource leak checks",
          "message": "There are similar checks for covering locks, references, RCU read\nsections and preempt_disable sections in 3 places in the verifer, i.e.\nfor tail calls, bpf_ld_[abs, ind], and exit path (for BPF_EXIT and\nbpf_throw). Unify all of these into a common check_resource_leak\nfunction to avoid code duplication.\n\nAlso update the error strings in selftests to the new ones in the same\nchange to ensure clean bisection.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241103225940.1408302-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-11-03 16:52:06 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/exceptions_fail.c",
            "tools/testing/selftests/bpf/progs/preempt_lock.c",
            "tools/testing/selftests/bpf/progs/verifier_ref_tracking.c",
            "tools/testing/selftests/bpf/progs/verifier_spin_lock.c"
          ]
        },
        {
          "hash": "711df091dea9b6f9e83ed738967cb0763f4d362c",
          "subject": "selftests/bpf: Add tests for tail calls with locks and refs",
          "message": "Add failure tests to ensure bugs don't slip through for tail calls and\nlingering locks, RCU sections, preemption disabled sections, and\nreferences prevent tail calls.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241103225940.1408302-4-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-11-03 16:52:06 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/tailcalls.c",
            "tools/testing/selftests/bpf/progs/tailcall_fail.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "c6fb8030b4baa01c850f99fc6da051b1017edc46",
      "merge_subject": "Merge branch 'share-user-memory-to-bpf-program-through-task-storage-map'",
      "merge_body": "Martin KaFai Lau says:\n\n====================\nShare user memory to BPF program through task storage map\n\nFrom: Martin KaFai Lau <martin.lau@kernel.org>\n\nIt is the v6 of this series. Starting from v5, it is a continuation work\nof the RFC v4.\n\nChanges in v6:\n1. In patch 1, reject t->size == 0 in btf_check_and_fixup_fields.\n   Reject a uptr pointing to an empty struct.\n\n   A test is added to patch 12 to test this case.\n\n2. In patch 6, when checking if the uptr struct spans across\n   pages, there was an off by one error in calculating the \"end\" such\n   that the uptr will be rejected by error if the object is located\n   exactly at the end of a page.\n\n   This is fixed by adding t->size \"- 1\" to \"start\".\n\n   A test is added to patch 9 to test this case.\n\n3. In patch 6, check for PageHighMem(page) and return -EOPNOTSUPP.\n   The 32 bit arch jit is missing other crucial bpf features (e.g. kfunc).\n   Patch 6 commit message has been updated to include this change.\n\n4. The selftests are cleaned up such that  \"struct user_data *dummy_data\"\n   global ptr is used instead of the whole \"struct user_data  dummy_data\"\n   object. Still a hack to avoid generating fwd btf type for the\n   uptr struct but somewhat lighter than a full blown global object.\n\nChanges in v5:\n1. The original patch 1 and patch 2 are combined.\n2. Patch 3, 4, and 5 are new. They get the bpf_local_storage\n   ready to handle the __uptr in the map_value.\n3. Patch 6 is mostly new, so I reset the sob.\n4. There are some changes in the carry over patch 1 and 2 also. They\n   are mentioned at the individual patch.\n5. More tests are added.\n\nThe following is the original cover letter and the earlier change log.\nThe bpf prog example has been removed. Please find a similar\nexample in the selftests task_ls_uptr.c.\n\n~~~~~~~~\n\nSome of BPF schedulers (sched_ext) need hints from user programs to do\na better job. For example, a scheduler can handle a task in a\ndifferent way if it knows a task is doing GC. So, we need an efficient\nway to share the information between user programs and BPF\nprograms. Sharing memory between user programs and BPF programs is\nwhat this patchset does.\n\n== REQUIREMENT ==\n\nThis patchset enables every task in every process to share a small\nchunk of memory of it's own with a BPF scheduler. So, they can update\nthe hints without expensive overhead of syscalls. It also wants every\ntask sees only the data/memory belong to the task/or the task's\nprocess.\n\n== DESIGN ==\n\nThis patchset enables BPF prorams to embed __uptr; uptr in the values\nof task storage maps. A uptr field can only be set by user programs by\nupdating map element value through a syscall. A uptr points to a block\nof memory allocated by the user program updating the element\nvalue. The memory will be pinned to ensure it staying in the core\nmemory and to avoid a page fault when the BPF program accesses it.\n\nPlease see the selftests task_ls_uptr.c for an example.\n\n== MEMORY ==\n\nIn order to use memory efficiently, we don't want to pin a large\nnumber of pages. To archieve that, user programs should collect the\nmemory blocks pointed by uptrs together to share memory pages if\npossible. It avoid the situation that pin one page for each thread in\na process.  Instead, we can have several threads pointing their uptrs\nto the same page but with different offsets.\n\nAlthough it is not necessary, avoiding the memory pointed by an uptr\ncrossing the boundary of a page can prevent an additional mapping in\nthe kernel address space.\n\n== RESTRICT ==\n\nThe memory pointed by a uptr should reside in one memory\npage. Crossing multi-pages is not supported at the moment.\n\nOnly task storage map have been supported at the moment.\n\nThe values of uptrs can only be updated by user programs through\nsyscalls.\n\nbpf_map_lookup_elem() from userspace returns zeroed values for uptrs\nto prevent leaking information of the kernel.\n---\n\nChanges from v3:\n\n - Merge part 4 and 5 as the new part 4 in order to cease the warning\n    of unused functions from CI.\n\nChanges from v1:\n\n - Rename BPF_KPTR_USER to BPF_UPTR.\n\n - Restrict uptr to one page.\n\n - Mark uptr with PTR_TO_MEM | PTR_MAY_BE_NULL and with the size of\n    the target type.\n\n - Move uptr away from bpf_obj_memcpy() by introducing\n    bpf_obj_uptrcpy() and copy_map_uptr_locked().\n\n - Remove the BPF_FROM_USER flag.\n\n - Align the meory pointed by an uptr in the test case. Remove the\n    uptr of mmapped memory.\n\nKui-Feng Lee (4):\n  bpf: Support __uptr type tag in BTF\n  bpf: Handle BPF_UPTR in verifier\n  libbpf: define __uptr.\n  selftests/bpf: Some basic __uptr tests\n====================\n\nLink: https://lore.kernel.org/r/20241023234759.860539-1-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-10-24 10:26:00 -0700",
      "commits": [
        {
          "hash": "1cb80d9e93f861018fabe81a69ea0ded20f5a2d0",
          "subject": "bpf: Support __uptr type tag in BTF",
          "message": "This patch introduces the \"__uptr\" type tag to BTF. It is to define\na pointer pointing to the user space memory. This patch adds BTF\nlogic to pass the \"__uptr\" type tag.\n\nbtf_find_kptr() is reused for the \"__uptr\" tag. The \"__uptr\" will only\nbe supported in the map_value of the task storage map. However,\nbtf_parse_struct_meta() also uses btf_find_kptr() but it is not\ninterested in \"__uptr\". This patch adds a \"field_mask\" argument\nto btf_find_kptr() which will return BTF_FIELD_IGNORE if the\ncaller is not interested in a \u201c__uptr\u201d field.\n\nbtf_parse_kptr() is also reused to parse the uptr.\nThe btf_check_and_fixup_fields() is changed to do extra\nchecks on the uptr to ensure that its struct size is not larger\nthan PAGE_SIZE. It is not clear how a uptr pointing to a CO-RE\nsupported kernel struct will be used, so it is also not allowed now.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-2-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-10-24 10:25:58 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "99dde42e37497b3062516b1db7231f9dec744a00",
          "subject": "bpf: Handle BPF_UPTR in verifier",
          "message": "This patch adds BPF_UPTR support to the verifier. Not that only the\nmap_value will support the \"__uptr\" type tag.\n\nThis patch enforces only BPF_LDX is allowed to the value of an uptr.\nAfter BPF_LDX, it will mark the dst_reg as PTR_TO_MEM | PTR_MAYBE_NULL\nwith size deduced from the field.kptr.btf_id. This will make the\ndst_reg pointed memory to be readable and writable as scalar.\n\nThere is a redundant \"val_reg = reg_state(env, value_regno);\" statement\nin the check_map_kptr_access(). This patch takes this chance to remove\nit also.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-3-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-10-24 10:25:58 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b9a5a07aeaa2a903fb1306eb422880b2fa5f937f",
          "subject": "bpf: Add \"bool swap_uptrs\" arg to bpf_local_storage_update() and bpf_selem_alloc()",
          "message": "In a later patch, the task local storage will only accept uptr\nfrom the syscall update_elem and will not accept uptr from\nthe bpf prog. The reason is the bpf prog does not have a way\nto provide a valid user space address.\n\nbpf_local_storage_update() and bpf_selem_alloc() are used by\nboth bpf prog bpf_task_storage_get(BPF_LOCAL_STORAGE_GET_F_CREATE)\nand bpf syscall update_elem. \"bool swap_uptrs\" arg is added\nto bpf_local_storage_update() and bpf_selem_alloc() to tell if\nit is called by the bpf prog or by the bpf syscall. When\nswap_uptrs==true, it is called by the syscall.\n\nThe arg is named (swap_)uptrs because the later patch will swap\nthe uptrs between the newly allocated selem and the user space\nprovided map_value. It will make error handling easier in case\nmap->ops->map_update_elem() fails and the caller can decide\nif it needs to unpin the uptr in the user space provided\nmap_value or the bpf_local_storage_update() has already\ntaken the uptr ownership and will take care of unpinning it also.\n\nOnly swap_uptrs==false is passed now. The logic to handle\nthe true case will be added in a later patch.\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-4-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-10-24 10:25:59 -0700",
          "modified_files": [
            "include/linux/bpf_local_storage.h",
            "kernel/bpf/bpf_cgrp_storage.c",
            "kernel/bpf/bpf_inode_storage.c",
            "kernel/bpf/bpf_local_storage.c",
            "kernel/bpf/bpf_task_storage.c",
            "net/core/bpf_sk_storage.c"
          ]
        },
        {
          "hash": "5bd5bab76669b1e1551f03f5fcbc165f3fa8d269",
          "subject": "bpf: Postpone bpf_selem_free() in bpf_selem_unlink_storage_nolock()",
          "message": "In a later patch, bpf_selem_free() will call unpin_user_page()\nthrough bpf_obj_free_fields(). unpin_user_page() may take spin_lock.\nHowever, some bpf_selem_free() call paths have held a raw_spin_lock.\nLike this:\n\nraw_spin_lock_irqsave()\n  bpf_selem_unlink_storage_nolock()\n    bpf_selem_free()\n      unpin_user_page()\n        spin_lock()\n\nTo avoid spinlock nested in raw_spinlock, bpf_selem_free() should be\ndone after releasing the raw_spinlock. The \"bool reuse_now\" arg is\nreplaced with \"struct hlist_head *free_selem_list\" in\nbpf_selem_unlink_storage_nolock(). The bpf_selem_unlink_storage_nolock()\nwill append the to-be-free selem at the free_selem_list. The caller of\nbpf_selem_unlink_storage_nolock() will need to call the new\nbpf_selem_free_list(free_selem_list, reuse_now) to free the selem\nafter releasing the raw_spinlock.\n\nNote that the selem->snode cannot be reused for linking to\nthe free_selem_list because the selem->snode is protected by the\nraw_spinlock that we want to avoid holding. A new\n\"struct hlist_node free_node;\" is union-ized with\nthe rcu_head. Only the first one successfully\nhlist_del_init_rcu(&selem->snode) will be able\nto use the free_node. After succeeding hlist_del_init_rcu(&selem->snode),\nthe free_node and rcu_head usage is serialized such that they\ncan share the 16 bytes in a union.\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-5-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-10-24 10:25:59 -0700",
          "modified_files": [
            "include/linux/bpf_local_storage.h",
            "kernel/bpf/bpf_local_storage.c"
          ]
        },
        {
          "hash": "9bac675e6368b96f448289010caba4ee3320ab24",
          "subject": "bpf: Postpone bpf_obj_free_fields to the rcu callback",
          "message": "A later patch will enable the uptr usage in the task_local_storage map.\nThis will require the unpin_user_page() to be done after the rcu\ntask trace gp for the cases that the uptr may still be used by\na bpf prog. The bpf_obj_free_fields() will be the one doing\nunpin_user_page(), so this patch is to postpone calling\nbpf_obj_free_fields() to the rcu callback.\n\nThe bpf_obj_free_fields() is only required to be done in\nthe rcu callback when bpf->bpf_ma==true and reuse_now==false.\n\nbpf->bpf_ma==true case is because uptr will only be enabled\nin task storage which has already been moved to bpf_mem_alloc.\nThe bpf->bpf_ma==false case can be supported in the future\nalso if there is a need.\n\nreuse_now==false when the selem (aka storage) is deleted\nby bpf prog (bpf_task_storage_delete) or by syscall delete_elem().\nIn both cases, bpf_obj_free_fields() needs to wait for\nrcu gp.\n\nA few words on reuse_now==true. reuse_now==true when the\nstorage's owner (i.e. the task_struct) is destructing or the map\nitself is doing map_free(). In both cases, no bpf prog should\nhave a hold on the selem and its uptrs, so there is no need to\npostpone bpf_obj_free_fields(). reuse_now==true should be the\ncommon case for local storage usage where the storage exists\nthroughout the lifetime of its owner (task_struct).\n\nThe bpf_obj_free_fields() needs to use the map->record. Doing\nbpf_obj_free_fields() in a rcu callback will require the\nbpf_local_storage_map_free() to wait for rcu_barrier. An optimization\ncould be only waiting for rcu_barrier when the map has uptr in\nits map_value. This will require either yet another rcu callback\nfunction or adding a bool in the selem to flag if the SDATA(selem)->smap\nis still valid. This patch chooses to keep it simple and wait for\nrcu_barrier for maps that use bpf_mem_alloc.\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-6-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-10-24 10:25:59 -0700",
          "modified_files": [
            "kernel/bpf/bpf_local_storage.c"
          ]
        },
        {
          "hash": "ba512b00e5efbf7e19cfb7fa9f66ce82669b7077",
          "subject": "bpf: Add uptr support in the map_value of the task local storage.",
          "message": "This patch adds uptr support in the map_value of the task local storage.\n\nstruct map_value {\n\tstruct user_data __uptr *uptr;\n};\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_TASK_STORAGE);\n\t__uint(map_flags, BPF_F_NO_PREALLOC);\n\t__type(key, int);\n\t__type(value, struct value_type);\n} datamap SEC(\".maps\");\n\nA new bpf_obj_pin_uptrs() is added to pin the user page and\nalso stores the kernel address back to the uptr for the\nbpf prog to use later. It currently does not support\nthe uptr pointing to a user struct across two pages.\nIt also excludes PageHighMem support to keep it simple.\nAs of now, the 32bit bpf jit is missing other more crucial bpf\nfeatures. For example, many important bpf features depend on\nbpf kfunc now but so far only one arch (x86-32) supports it\nwhich was added by me as an example when kfunc was first\nintroduced to bpf.\n\nThe uptr can only be stored to the task local storage by the\nsyscall update_elem. Meaning the uptr will not be considered\nif it is provided by the bpf prog through\nbpf_task_storage_get(BPF_LOCAL_STORAGE_GET_F_CREATE).\nThis is enforced by only calling\nbpf_local_storage_update(swap_uptrs==true) in\nbpf_pid_task_storage_update_elem. Everywhere else will\nhave swap_uptrs==false.\n\nThis will pump down to bpf_selem_alloc(swap_uptrs==true). It is\nthe only case that bpf_selem_alloc() will take the uptr value when\nupdating the newly allocated selem. bpf_obj_swap_uptrs() is added\nto swap the uptr between the SDATA(selem)->data and the user provided\nmap_value in \"void *value\". bpf_obj_swap_uptrs() makes the\nSDATA(selem)->data takes the ownership of the uptr and the user space\nprovided map_value will have NULL in the uptr.\n\nThe bpf_obj_unpin_uptrs() is called after map->ops->map_update_elem()\nreturning error. If the map->ops->map_update_elem has reached\na state that the local storage has taken the uptr ownership,\nthe bpf_obj_unpin_uptrs() will be a no op because the uptr\nis NULL. A \"__\"bpf_obj_unpin_uptrs is added to make this\nerror path unpin easier such that it does not have to check\nthe map->record is NULL or not.\n\nBPF_F_LOCK is not supported when the map_value has uptr.\nThis can be revisited later if there is a use case. A similar\nswap_uptrs idea can be considered.\n\nThe final bit is to do unpin_user_page in the bpf_obj_free_fields().\nThe earlier patch has ensured that the bpf_obj_free_fields() has\ngone through the rcu gp when needed.\n\nCc: linux-mm@kvack.org\nCc: Shakeel Butt <shakeel.butt@linux.dev>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\nLink: https://lore.kernel.org/r/20241023234759.860539-7-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-10-24 10:25:59 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/bpf_local_storage.c",
            "kernel/bpf/bpf_task_storage.c",
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "7aa12b8d9f24e9623effa12a3fc330de056d572e",
          "subject": "libbpf: define __uptr.",
          "message": "Make __uptr available to BPF programs to enable them to define uptrs.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-8-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-10-24 10:25:59 -0700",
          "modified_files": [
            "tools/lib/bpf/bpf_helpers.h"
          ]
        },
        {
          "hash": "4579b4a4279ec7df9499943f764da03ae837021c",
          "subject": "selftests/bpf: Some basic __uptr tests",
          "message": "Make sure the memory of uptrs have been mapped to the kernel properly.\nAlso ensure the values of uptrs in the kernel haven't been copied\nto userspace.\n\nIt also has the syscall update_elem/delete_elem test to test the\npin/unpin code paths.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-9-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-10-24 10:25:59 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/task_local_storage.c",
            "tools/testing/selftests/bpf/progs/task_ls_uptr.c",
            "tools/testing/selftests/bpf/uptr_test_common.h"
          ]
        },
        {
          "hash": "51fff4083372381e680724dde7ac3e859f9e3a0a",
          "subject": "selftests/bpf: Test a uptr struct spanning across pages.",
          "message": "This patch tests the case when uptr has a struct spanning across two\npages. It is not supported now and EOPNOTSUPP is expected from the\nsyscall update_elem.\n\nIt also tests the whole uptr struct located exactly at the\nend of a page and ensures that this case is accepted by update_elem.\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-10-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-10-24 10:25:59 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/task_local_storage.c"
          ]
        },
        {
          "hash": "cbf9f849a3e86f1b7c041dfbeeae1c1fff0ddc8d",
          "subject": "selftests/bpf: Add update_elem failure test for task storage uptr",
          "message": "This patch test the following failures in syscall update_elem\n1. The first update_elem(BPF_F_LOCK) should be EOPNOTSUPP. syscall.c takes\n   care of unpinning the uptr.\n2. The second update_elem(BPF_EXIST) fails. syscall.c takes care of\n   unpinning the uptr.\n3. The forth update_elem(BPF_NOEXIST) fails. bpf_local_storage_update\n   takes care of unpinning the uptr.\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-11-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-10-24 10:25:59 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/task_local_storage.c",
            "tools/testing/selftests/bpf/progs/uptr_update_failure.c",
            "tools/testing/selftests/bpf/uptr_test_common.h"
          ]
        },
        {
          "hash": "898cbca4a7579bea3ab746cd8dc33027bff80dac",
          "subject": "selftests/bpf: Add uptr failure verifier tests",
          "message": "Add verifier tests to ensure invalid uptr usages are rejected.\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-12-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-10-24 10:26:00 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/task_local_storage.c",
            "tools/testing/selftests/bpf/progs/uptr_failure.c"
          ]
        },
        {
          "hash": "bd5879a6fe4be407bf36c212cd91ed1e4485a6f9",
          "subject": "selftests/bpf: Create task_local_storage map with invalid uptr's struct",
          "message": "This patch tests the map creation failure when the map_value\nhas unsupported uptr. The three cases are the struct is larger\nthan one page, the struct is empty, and the struct is a kernel struct.\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-13-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-10-24 10:26:00 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/task_local_storage.c",
            "tools/testing/selftests/bpf/progs/uptr_map_failure.c",
            "tools/testing/selftests/bpf/test_progs.h",
            "tools/testing/selftests/bpf/uptr_test_common.h"
          ]
        }
      ]
    },
    {
      "merge_hash": "1477d31b1c9a661a253a644b950f5ce438e4821c",
      "merge_subject": "Merge branch 'bpf-add-kmem_cache-iterator-and-kfunc'",
      "merge_body": "Namhyung Kim says:\n\n====================\nbpf: Add kmem_cache iterator and kfunc\n\nHello,\n\nI'm proposing a new iterator and a kfunc for the slab memory allocator\nto get information of each kmem_cache like in /proc/slabinfo or\n/sys/kernel/slab in more flexible way.\n\nv5 changes\n\n * set PTR_UNTRUSTED for return value of bpf_get_kmem_cache()  (Alexei)\n * add KF_RCU_PROTECTED to bpf_get_kmem_cache().  See below.  (Song)\n * add WARN_ON_ONCE and comment in kmem_cache_iter_seq_next()  (Song)\n * change kmem_cache_iter_seq functions not to call BPF on intermediate stop\n * add a subtest to compare the kmem cache info with /proc/slabinfo  (Alexei)\n\nv4: https://lore.kernel.org/lkml/20241002180956.1781008-1-namhyung@kernel.org\n\n * skip kmem_cache_destroy() in kmem_cache_iter_seq_stop() if possible  (Vlastimil)\n * fix a bug in the kmem_cache_iter_seq_start() for the last entry\n\nv3: https://lore.kernel.org/lkml/20241002065456.1580143-1-namhyung@kernel.org/\n\n * rework kmem_cache_iter not to hold slab_mutex when running BPF  (Alexei)\n * add virt_addr_valid() check  (Alexei)\n * fix random test failure by running test with the current task  (Hyeonggon)\n\nv2: https://lore.kernel.org/lkml/20240927184133.968283-1-namhyung@kernel.org/\n\n * rename it to \"kmem_cache_iter\"\n * fix a build issue\n * add Acked-by's from Roman and Vlastimil (Thanks!)\n * add error codes in the test for debugging\n\nv1: https://lore.kernel.org/lkml/20240925223023.735947-1-namhyung@kernel.org/\n\nMy use case is `perf lock contention` tool which shows contended locks\nbut many of them are not global locks and don't have symbols.  If it\ncan tranlate the address of the lock in a slab object to the name of\nthe slab, it'd be much more useful.\n\nI'm not aware of type information in slab yet, but I was told there's\na work to associate BTF ID with it.  It'd be definitely helpful to my\nuse case.  Probably we need another kfunc to get the start address of\nthe object or the offset in the object from an address if the type\ninfo is available.  But I want to start with a simple thing first.\n\nThe kmem_cache_iter iterates kmem_cache objects under slab_mutex and\nwill be useful for userspace to prepare some work for specific slabs\nlike setting up filters in advance.  And the bpf_get_kmem_cache()\nkfunc will return a pointer to a slab from the address of a lock.\n\nAnd the test code is to read from the iterator and make sure it finds\na slab cache of the task_struct for the current task.\n\nThe code is available at 'bpf/slab-iter-v5' branch in\nhttps://git.kernel.org/pub/scm/linux/kernel/git/namhyung/linux-perf.git\n\nThanks,\nNamhyung\n====================\n\nLink: https://lore.kernel.org/r/20241010232505.1339892-1-namhyung@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-10-16 09:21:18 -0700",
      "commits": [
        {
          "hash": "4971266e1595f76be3f844c834c1f9357a97dbde",
          "subject": "bpf: Add kmem_cache iterator",
          "message": "The new \"kmem_cache\" iterator will traverse the list of slab caches\nand call attached BPF programs for each entry.  It should check the\nargument (ctx.s) if it's NULL before using it.\n\nNow the iteration grabs the slab_mutex only if it traverse the list and\nreleases the mutex when it runs the BPF program.  The kmem_cache entry\nis protected by a refcount during the execution.\n\nSigned-off-by: Namhyung Kim <namhyung@kernel.org>\nAcked-by: Vlastimil Babka <vbabka@suse.cz> #slab\nLink: https://lore.kernel.org/r/20241010232505.1339892-2-namhyung@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Namhyung Kim <namhyung@kernel.org>",
          "date": "2024-10-14 18:33:04 -0700",
          "modified_files": [
            "include/linux/btf_ids.h",
            "kernel/bpf/Makefile",
            "kernel/bpf/kmem_cache_iter.c"
          ]
        },
        {
          "hash": "a992d7a3979120fbd7c13435d27b3da8d9ed095a",
          "subject": "mm/bpf: Add bpf_get_kmem_cache() kfunc",
          "message": "The bpf_get_kmem_cache() is to get a slab cache information from a\nvirtual address like virt_to_cache().  If the address is a pointer\nto a slab object, it'd return a valid kmem_cache pointer, otherwise\nNULL is returned.\n\nIt doesn't grab a reference count of the kmem_cache so the caller is\nresponsible to manage the access.  The returned point is marked as\nPTR_UNTRUSTED.\n\nThe intended use case for now is to symbolize locks in slab objects\nfrom the lock contention tracepoints.\n\nSuggested-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Roman Gushchin <roman.gushchin@linux.dev> (mm/*)\nAcked-by: Vlastimil Babka <vbabka@suse.cz> #mm/slab\nSigned-off-by: Namhyung Kim <namhyung@kernel.org>\nLink: https://lore.kernel.org/r/20241010232505.1339892-3-namhyung@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Namhyung Kim <namhyung@kernel.org>",
          "date": "2024-10-16 09:21:03 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c",
            "mm/slab_common.c"
          ]
        },
        {
          "hash": "a496d0cdc84d81fbfd2026ef41c8ae9385d01fbb",
          "subject": "selftests/bpf: Add a test for kmem_cache_iter",
          "message": "The test traverses all slab caches using the kmem_cache_iter and save\nthe data into slab_result array map.  And check if current task's\npointer is from \"task_struct\" slab cache using bpf_get_kmem_cache().\n\nAlso compare the result array with /proc/slabinfo if available (when\nCONFIG_SLUB_DEBUG is on).  Note that many of the fields in the slabinfo\nare transient, so it only compares the name and objsize fields.\n\nSigned-off-by: Namhyung Kim <namhyung@kernel.org>\nLink: https://lore.kernel.org/r/20241010232505.1339892-4-namhyung@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Namhyung Kim <namhyung@kernel.org>",
          "date": "2024-10-16 09:21:18 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/kmem_cache_iter.c",
            "tools/testing/selftests/bpf/progs/bpf_iter.h",
            "tools/testing/selftests/bpf/progs/kmem_cache_iter.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "ee230090f62fbb1c63c7f305d57289ab753221ef",
      "merge_subject": "Merge branch 'fix-truncation-bug-in-coerce_reg_to_size_sx-and-extend-selftests'",
      "merge_body": "Dimitar Kanaliev says:\n\n====================\nFix truncation bug in coerce_reg_to_size_sx and extend selftests.\n\nThis patch series addresses a truncation bug in the eBPF verifier function\ncoerce_reg_to_size_sx(). The issue was caused by the incorrect ordering\nof assignments between 32-bit and 64-bit min/max values, leading to\nimproper truncation when updating the register state. This issue has been\nreported previously by Zac Ecob[1] , but was not followed up on.\n\nThe first patch fixes the assignment order in coerce_reg_to_size_sx()\nto ensure correct truncation. The subsequent patches add selftests for\ncoerce_{reg,subreg}_to_size_sx.\n\nChangelog:\n\tv1 -> v2:\n\t - Moved selftests inside the conditional check for cpuv4\n\n[1] (https://lore.kernel.org/bpf/h3qKLDEO6m9nhif0eAQX4fVrqdO0D_OPb0y5HfMK9jBePEKK33wQ3K-bqSVnr0hiZdFZtSJOsbNkcEQGpv_yJk61PAAiO8fUkgMRSO-lB50=@protonmail.com/)\n====================\n\nLink: https://lore.kernel.org/r/20241014121155.92887-1-dimitar.kanaliev@siteground.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-10-15 11:16:25 -0700",
      "commits": [
        {
          "hash": "ae67b9fb8c4e981e929a665dcaa070f4b05ebdb4",
          "subject": "bpf: Fix truncation bug in coerce_reg_to_size_sx()",
          "message": "coerce_reg_to_size_sx() updates the register state after a sign-extension\noperation. However, there's a bug in the assignment order of the unsigned\nmin/max values, leading to incorrect truncation:\n\n  0: (85) call bpf_get_prandom_u32#7    ; R0_w=scalar()\n  1: (57) r0 &= 1                       ; R0_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=1,var_off=(0x0; 0x1))\n  2: (07) r0 += 254                     ; R0_w=scalar(smin=umin=smin32=umin32=254,smax=umax=smax32=umax32=255,var_off=(0xfe; 0x1))\n  3: (bf) r0 = (s8)r0                   ; R0_w=scalar(smin=smin32=-2,smax=smax32=-1,umin=umin32=0xfffffffe,umax=0xffffffff,var_off=(0xfffffffffffffffe; 0x1))\n\nIn the current implementation, the unsigned 32-bit min/max values\n(u32_min_value and u32_max_value) are assigned directly from the 64-bit\nsigned min/max values (s64_min and s64_max):\n\n  reg->umin_value = reg->u32_min_value = s64_min;\n  reg->umax_value = reg->u32_max_value = s64_max;\n\nDue to the chain assigmnent, this is equivalent to:\n\n  reg->u32_min_value = s64_min;  // Unintended truncation\n  reg->umin_value = reg->u32_min_value;\n  reg->u32_max_value = s64_max;  // Unintended truncation\n  reg->umax_value = reg->u32_max_value;\n\nFixes: 1f9a1ea821ff (\"bpf: Support new sign-extension load insns\")\nReported-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nReported-by: Zac Ecob <zacecob@protonmail.com>\nSigned-off-by: Dimitar Kanaliev <dimitar.kanaliev@siteground.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nReviewed-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20241014121155.92887-2-dimitar.kanaliev@siteground.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dimitar Kanaliev <dimitar.kanaliev@siteground.com>",
          "date": "2024-10-15 11:16:24 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "61f506eacc77a9dad510fce92477af72be82c89d",
          "subject": "selftests/bpf: Add test for truncation after sign extension in coerce_reg_to_size_sx()",
          "message": "Add test that checks whether unsigned ranges deduced by the verifier for\nsign extension instruction is correct. Without previous patch that\nfixes truncation in coerce_reg_to_size_sx() this test fails.\n\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Dimitar Kanaliev <dimitar.kanaliev@siteground.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241014121155.92887-3-dimitar.kanaliev@siteground.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dimitar Kanaliev <dimitar.kanaliev@siteground.com>",
          "date": "2024-10-15 11:16:25 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_movsx.c"
          ]
        },
        {
          "hash": "35ccd576a23ce495b4064f4a3445626de790cd23",
          "subject": "selftests/bpf: Add test for sign extension in coerce_subreg_to_size_sx()",
          "message": "Add a test for unsigned ranges after signed extension instruction. This\ncase isn't currently covered by existing tests in verifier_movsx.c.\n\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Dimitar Kanaliev <dimitar.kanaliev@siteground.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241014121155.92887-4-dimitar.kanaliev@siteground.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dimitar Kanaliev <dimitar.kanaliev@siteground.com>",
          "date": "2024-10-15 11:16:25 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_movsx.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "3f2ac59c0d7b4d9f0e87371662a6ba8273b07818",
      "merge_subject": "Merge branch 'fix-caching-of-btf-for-kfuncs-in-the-verifier'",
      "merge_body": "Toke H\u00f8iland-J\u00f8rgensen says:\n\n====================\nFix caching of BTF for kfuncs in the verifier\n\nWhen playing around with defining kfuncs in some custom modules, we\nnoticed that if a BPF program calls two functions with the same\nsignature in two different modules, the function from the wrong module\nmay sometimes end up being called. Whether this happens depends on the\norder of the calls in the BPF program, which turns out to be due to the\nuse of sort() inside __find_kfunc_desc_btf() in the verifier code.\n\nThis series contains a fix for the issue (first patch), and a selftest\nto trigger it (last patch). The middle commit is a small refactor to\nexpose the module loading helper functions in testing_helpers.c. See the\nindividual patch descriptions for more details.\n\nChanges in v2:\n- Drop patch that refactors module building in selftests (Alexei)\n- Get rid of expect_val function argument in selftest (Jiri)\n- Collect ACKs\n- Link to v1: https://lore.kernel.org/r/20241008-fix-kfunc-btf-caching-for-modules-v1-0-dfefd9aa4318@redhat.com\n\n====================\n\nLink: https://lore.kernel.org/r/20241010-fix-kfunc-btf-caching-for-modules-v2-0-745af6c1af98@redhat.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-10-10 10:44:28 -0700",
      "commits": [
        {
          "hash": "6cb86a0fdece87e126323ec1bb19deb16a52aedf",
          "subject": "bpf: fix kfunc btf caching for modules",
          "message": "The verifier contains a cache for looking up module BTF objects when\ncalling kfuncs defined in modules. This cache uses a 'struct\nbpf_kfunc_btf_tab', which contains a sorted list of BTF objects that\nwere already seen in the current verifier run, and the BTF objects are\nlooked up by the offset stored in the relocated call instruction using\nbsearch().\n\nThe first time a given offset is seen, the module BTF is loaded from the\nfile descriptor passed in by libbpf, and stored into the cache. However,\nthere's a bug in the code storing the new entry: it stores a pointer to\nthe new cache entry, then calls sort() to keep the cache sorted for the\nnext lookup using bsearch(), and then returns the entry that was just\nstored through the stored pointer. However, because sort() modifies the\nlist of entries in place *by value*, the stored pointer may no longer\npoint to the right entry, in which case the wrong BTF object will be\nreturned.\n\nThe end result of this is an intermittent bug where, if a BPF program\ncalls two functions with the same signature in two different modules,\nthe function from the wrong module may sometimes end up being called.\nWhether this happens depends on the order of the calls in the BPF\nprogram (as that affects whether sort() reorders the array of BTF\nobjects), making it especially hard to track down. Simon, credited as\nreporter below, spent significant effort analysing and creating a\nreproducer for this issue. The reproducer is added as a selftest in a\nsubsequent patch.\n\nThe fix is straight forward: simply don't use the stored pointer after\ncalling sort(). Since we already have an on-stack pointer to the BTF\nobject itself at the point where the function return, just use that, and\npopulate it from the cache entry in the branch where the lookup\nsucceeds.\n\nFixes: 2357672c54c3 (\"bpf: Introduce BPF support for kernel module function calls\")\nReported-by: Simon Sundberg <simon.sundberg@kau.se>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Toke H\u00f8iland-J\u00f8rgensen <toke@redhat.com>\nLink: https://lore.kernel.org/r/20241010-fix-kfunc-btf-caching-for-modules-v2-1-745af6c1af98@redhat.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Toke H\u00f8iland-J\u00f8rgensen <toke@redhat.com>",
          "date": "2024-10-10 10:44:03 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "4192bb294f80928bc2257c7a2ff6c86a27de6807",
          "subject": "selftests/bpf: Provide a generic [un]load_module helper",
          "message": "Generalize the previous [un]load_bpf_testmod() helpers (in\ntesting_helpers.c) to the more generic [un]load_module(), which can\nload an arbitrary kernel module by name. This allows future selftests\nto more easily load custom kernel modules other than bpf_testmod.ko.\nRefactor [un]load_bpf_testmod() to wrap this new helper.\n\nSigned-off-by: Simon Sundberg <simon.sundberg@kau.se>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Toke H\u00f8iland-J\u00f8rgensen <toke@redhat.com>\nLink: https://lore.kernel.org/r/20241010-fix-kfunc-btf-caching-for-modules-v2-2-745af6c1af98@redhat.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Simon Sundberg <simon.sundberg@kau.se>",
          "date": "2024-10-10 10:44:03 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/testing_helpers.c",
            "tools/testing/selftests/bpf/testing_helpers.h"
          ]
        },
        {
          "hash": "f91b256644ea6f7628580029c5a223573f55d98c",
          "subject": "selftests/bpf: Add test for kfunc module order",
          "message": "Add a test case for kfuncs from multiple external modules, checking\nthat the correct kfuncs are called regardless of which order they're\ncalled in. Specifically, check that calling the kfuncs in an order\ndifferent from the one the modules' BTF are loaded in works.\n\nSigned-off-by: Simon Sundberg <simon.sundberg@kau.se>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Toke H\u00f8iland-J\u00f8rgensen <toke@redhat.com>\nLink: https://lore.kernel.org/r/20241010-fix-kfunc-btf-caching-for-modules-v2-3-745af6c1af98@redhat.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Simon Sundberg <simon.sundberg@kau.se>",
          "date": "2024-10-10 10:44:03 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/Makefile",
            "tools/testing/selftests/bpf/bpf_test_modorder_x/Makefile",
            "tools/testing/selftests/bpf/bpf_test_modorder_x/bpf_test_modorder_x.c",
            "tools/testing/selftests/bpf/bpf_test_modorder_y/Makefile",
            "tools/testing/selftests/bpf/bpf_test_modorder_y/bpf_test_modorder_y.c",
            "tools/testing/selftests/bpf/prog_tests/kfunc_module_order.c",
            "tools/testing/selftests/bpf/progs/kfunc_module_order.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "904181b33478a25bbc08f3427f6b25c9001cdbeb",
      "merge_subject": "Merge branch 'bpf_fastcall-attribute-in-vmlinux-h-and-bpf_helper_defs-h'",
      "merge_body": "Eduard Zingerman says:\n\n====================\n'bpf_fastcall' attribute in vmlinux.h and bpf_helper_defs.h\n\nThe goal of this patch-set is to reflect attribute bpf_fastcall\nfor supported helpers and kfuncs in generated header files.\nFor helpers this requires a tweak for scripts/bpf_doc.py and an update\nto uapi/linux/bpf.h doc-comment.\nFor kfuncs this requires:\n- introduction of a new KF_FASTCALL flag;\n- modification to pahole to read kfunc flags and generate\n  DECL_TAG \"bpf_fastcall\" for marked kfuncs;\n- modification to bpftool to scan for DECL_TAG \"bpf_fastcall\"\n  presence.\n\nIn both cases the following helper macro is defined in the generated\nheader:\n\n    #ifndef __bpf_fastcall\n    #if __has_attribute(bpf_fastcall)\n    #define __bpf_fastcall __attribute__((bpf_fastcall))\n    #else\n    #define __bpf_fastcall\n    #endif\n    #endif\n\nAnd is used to mark appropriate function prototypes. More information\nabout bpf_fastcall attribute could be found in [1] and [2].\n\nModifications to pahole are submitted separately.\n\n[1] LLVM source tree commit:\n    64e464349bfc (\"[BPF] introduce __attribute__((bpf_fastcall))\")\n\n[2] Linux kernel tree commit (note: feature was renamed from\n    no_caller_saved_registers to bpf_fastcall after this commit):\n    52839f31cece (\"Merge branch 'no_caller_saved_registers-attribute-for-helper-calls'\")\n====================\n\nLink: https://lore.kernel.org/r/20240916091712.2929279-1-eddyz87@gmail.com\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2024-10-03 17:47:53 -0700",
      "commits": [
        {
          "hash": "48b13cab1e7cb77def27cb89711fb5e3b04db972",
          "subject": "bpf: Allow specifying bpf_fastcall attribute for BPF helpers",
          "message": "Allow a new optional 'Attributes' section to be specified for helper\nfunctions description, e.g.:\n\n * u32 bpf_get_smp_processor_id(void)\n * \t\t...\n * \tReturn\n * \t\t...\n * \tAttributes\n * \t\t__bpf_fastcall\n *\n\nGenerated header for the example above:\n\n  #ifndef __bpf_fastcall\n  #if __has_attribute(__bpf_fastcall)\n  #define __bpf_fastcall __attribute__((bpf_fastcall))\n  #else\n  #define __bpf_fastcall\n  #endif\n  #endif\n  ...\n  __bpf_fastcall\n  static __u32 (* const bpf_get_smp_processor_id)(void) = (void *) 8;\n\nThe following rules apply:\n- when present, section must follow 'Return' section;\n- attribute names are specified on the line following 'Attribute'\n  keyword;\n- attribute names are separated by spaces;\n- section ends with an \"empty\" line (\" *\\n\").\n\nValid attribute names are recorded in the ATTRS map.\nATTRS maps shortcut attribute name to correct C syntax.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240916091712.2929279-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-10-03 17:47:53 -0700",
          "modified_files": [
            "scripts/bpf_doc.py"
          ]
        },
        {
          "hash": "4f647a780f3606acbd2116248d51eadb4d865615",
          "subject": "bpf: __bpf_fastcall for bpf_get_smp_processor_id in uapi",
          "message": "Since [1] kernel supports __bpf_fastcall attribute for helper function\nbpf_get_smp_processor_id(). Update uapi definition for this helper in\norder to have this attribute in the generated bpf_helper_defs.h\n\n[1] commit 91b7fbf3936f (\"bpf, x86, riscv, arm: no_caller_saved_registers for bpf_get_smp_processor_id()\")\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240916091712.2929279-3-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-10-03 17:47:53 -0700",
          "modified_files": [
            "include/uapi/linux/bpf.h",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "da7d71bcb0637b7aa18934628fdb5a55f2db49a6",
          "subject": "bpf: Use KF_FASTCALL to mark kfuncs supporting fastcall contract",
          "message": "In order to allow pahole add btf_decl_tag(\"bpf_fastcall\") for kfuncs\nsupporting bpf_fastcall, mark such functions with KF_FASTCALL in\nid_set8 objects.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240916091712.2929279-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-10-03 17:47:53 -0700",
          "modified_files": [
            "include/linux/btf.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "bf7ce5416f68db058ac7105902adf497b3ce4e8c",
          "subject": "bpftool: __bpf_fastcall for kfuncs marked with special decl_tag",
          "message": "Generate __attribute__((bpf_fastcall)) for kfuncs marked with\n\"bpf_fastcall\" decl tag. E.g. for the following BTF:\n\n    $ bpftool btf dump file vmlinux\n    ...\n    [A] FUNC 'bpf_rdonly_cast' type_id=...\n    ...\n    [B] DECL_TAG 'bpf_kfunc' type_id=A component_idx=-1\n    [C] DECL_TAG 'bpf_fastcall' type_id=A component_idx=-1\n\nGenerate the following vmlinux.h:\n\n    #ifndef __VMLINUX_H__\n    #define __VMLINUX_H__\n    ...\n    #ifndef __bpf_fastcall\n    #if __has_attribute(bpf_fastcall)\n    #define __bpf_fastcall __attribute__((bpf_fastcall))\n    #else\n    #define __bpf_fastcall\n    #endif\n    #endif\n    ...\n    __bpf_fastcall extern void *bpf_rdonly_cast(...) ...;\n\nThe \"bpf_fastcall\" / \"bpf_kfunc\" tags pair would generated by pahole\nwhen constructing vmlinux BTF.\n\nWhile at it, sort printed kfuncs by name for better vmlinux.h\nstability.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240916091712.2929279-5-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-10-03 17:47:53 -0700",
          "modified_files": [
            "tools/bpf/bpftool/btf.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "fdfd9d82a43a7a50b9d0989a0440d12a3d68ea15",
      "merge_subject": "Merge branch 'bpf: Allow skb dynptr for tp_btf'",
      "merge_body": "Philo Lu says:\n\n====================\nThis makes bpf_dynptr_from_skb usable for tp_btf, so that we can easily\nparse skb in tracepoints. This has been discussed in [0], and Martin\nsuggested to use dynptr (instead of helpers like bpf_skb_load_bytes).\n\nFor safety, skb dynptr shouldn't be used in fentry/fexit. This is achieved\nby add KF_TRUSTED_ARGS flag in bpf_dynptr_from_skb defination, because\npointers passed by tracepoint are trusted (PTR_TRUSTED) while those of\nfentry/fexit are not.\n\nAnother problem raises that NULL pointers could be passed to tracepoint,\nsuch as trace_tcp_send_reset, and we need to recognize them. This is done\nby add a \"__nullable\" suffix in the func_proto of the tracepoint,\ndiscussed in [1].\n\n2 Test cases are added, one for \"__nullable\" suffix, and the other for\nusing skb dynptr in tp_btf.\n\nchangelog\nv2 -> v3 (Andrii Nakryiko):\n Patch 1:\n  - Remove prog type check in prog_arg_maybe_null()\n  - Add bpf_put_raw_tracepoint() after get()\n  - Use kallsyms_lookup() instead of sprintf(\"%ps\")\n Patch 2: Add separate test \"tp_btf_nullable\", and use full failure msg\nv1 -> v2:\n - Add \"__nullable\" suffix support (Alexei Starovoitov)\n - Replace \"struct __sk_buff*\" with \"void*\" in test (Martin KaFai Lau)\n\n[0]\nhttps://lore.kernel.org/all/20240205121038.41344-1-lulie@linux.alibaba.com/T/\n[1]\nhttps://lore.kernel.org/all/20240430121805.104618-1-lulie@linux.alibaba.com/T/\n====================\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_date": "2024-09-11 08:57:59 -0700",
      "commits": [
        {
          "hash": "8aeaed21befc90f27f4fca6dd190850d97d2e9e3",
          "subject": "bpf: Support __nullable argument suffix for tp_btf",
          "message": "Pointers passed to tp_btf were trusted to be valid, but some tracepoints\ndo take NULL pointer as input, such as trace_tcp_send_reset(). Then the\ninvalid memory access cannot be detected by verifier.\n\nThis patch fix it by add a suffix \"__nullable\" to the unreliable\nargument. The suffix is shown in btf, and PTR_MAYBE_NULL will be added\nto nullable arguments. Then users must check the pointer before use it.\n\nA problem here is that we use \"btf_trace_##call\" to search func_proto.\nAs it is a typedef, argument names as well as the suffix are not\nrecorded. To solve this, I use bpf_raw_event_map to find\n\"__bpf_trace##template\" from \"btf_trace_##call\", and then we can see the\nsuffix.\n\nSuggested-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Philo Lu <lulie@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20240911033719.91468-2-lulie@linux.alibaba.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Philo Lu <lulie@linux.alibaba.com>",
          "date": "2024-09-11 08:56:37 -0700",
          "modified_files": [
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "2060f07f861a237345922023e9347a204c0795af",
          "subject": "selftests/bpf: Add test for __nullable suffix in tp_btf",
          "message": "Add a tracepoint with __nullable suffix in bpf_testmod, and add cases\nfor it:\n\n$ ./test_progs -t \"tp_btf_nullable\"\n #406/1   tp_btf_nullable/handle_tp_btf_nullable_bare1:OK\n #406/2   tp_btf_nullable/handle_tp_btf_nullable_bare2:OK\n #406     tp_btf_nullable:OK\n Summary: 1/2 PASSED, 0 SKIPPED, 0 FAILED\n\nSigned-off-by: Philo Lu <lulie@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20240911033719.91468-3-lulie@linux.alibaba.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Philo Lu <lulie@linux.alibaba.com>",
          "date": "2024-09-11 08:56:42 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod-events.h",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/prog_tests/tp_btf_nullable.c",
            "tools/testing/selftests/bpf/progs/test_tp_btf_nullable.c"
          ]
        },
        {
          "hash": "edd3f6f7588c713477e1299c38c84dcd91a7f148",
          "subject": "tcp: Use skb__nullable in trace_tcp_send_reset",
          "message": "Replace skb with skb__nullable as the argument name. The suffix tells\nbpf verifier through btf that the arg could be NULL and should be\nchecked in tp_btf prog.\n\nFor now, this is the only nullable argument in tcp tracepoints.\n\nSigned-off-by: Philo Lu <lulie@linux.alibaba.com>\nAcked-by: Jakub Kicinski <kuba@kernel.org>\nLink: https://lore.kernel.org/r/20240911033719.91468-4-lulie@linux.alibaba.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Philo Lu <lulie@linux.alibaba.com>",
          "date": "2024-09-11 08:56:42 -0700",
          "modified_files": [
            "include/trace/events/tcp.h"
          ]
        },
        {
          "hash": "ffc83860d8c09705d8e83474b8c6ec4d1d3dca41",
          "subject": "bpf: Allow bpf_dynptr_from_skb() for tp_btf",
          "message": "Making tp_btf able to use bpf_dynptr_from_skb(), which is useful for skb\nparsing, especially for non-linear paged skb data. This is achieved by\nadding KF_TRUSTED_ARGS flag to bpf_dynptr_from_skb and registering it\nfor TRACING progs. With KF_TRUSTED_ARGS, args from fentry/fexit are\nexcluded, so that unsafe progs like fexit/__kfree_skb are not allowed.\n\nWe also need the skb dynptr to be read-only in tp_btf. Because\nmay_access_direct_pkt_data() returns false by default when checking\nbpf_dynptr_from_skb, there is no need to add BPF_PROG_TYPE_TRACING to it\nexplicitly.\n\nSuggested-by: Martin KaFai Lau <martin.lau@linux.dev>\nSigned-off-by: Philo Lu <lulie@linux.alibaba.com>\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240911033719.91468-5-lulie@linux.alibaba.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Philo Lu <lulie@linux.alibaba.com>",
          "date": "2024-09-11 08:56:42 -0700",
          "modified_files": [
            "net/core/filter.c"
          ]
        },
        {
          "hash": "83dff601715bdc086dc1fc470ee3aaff42215e65",
          "subject": "selftests/bpf: Expand skb dynptr selftests for tp_btf",
          "message": "Add 3 test cases for skb dynptr used in tp_btf:\n- test_dynptr_skb_tp_btf: use skb dynptr in tp_btf and make sure it is\n  read-only.\n- skb_invalid_ctx_fentry/skb_invalid_ctx_fexit: bpf_dynptr_from_skb\n  should fail in fentry/fexit.\n\nIn test_dynptr_skb_tp_btf, to trigger the tracepoint in kfree_skb,\ntest_pkt_access is used for its test_run, as in kfree_skb.c. Because the\ntest process is different from others, a new setup type is defined,\ni.e., SETUP_SKB_PROG_TP.\n\nThe result is like:\n$ ./test_progs -t 'dynptr/test_dynptr_skb_tp_btf'\n  #84/14   dynptr/test_dynptr_skb_tp_btf:OK\n  #84      dynptr:OK\n  #127     kfunc_dynptr_param:OK\n  Summary: 2/1 PASSED, 0 SKIPPED, 0 FAILED\n\n$ ./test_progs -t 'dynptr/skb_invalid_ctx_f'\n  #84/85   dynptr/skb_invalid_ctx_fentry:OK\n  #84/86   dynptr/skb_invalid_ctx_fexit:OK\n  #84      dynptr:OK\n  #127     kfunc_dynptr_param:OK\n  Summary: 2/2 PASSED, 0 SKIPPED, 0 FAILED\n\nAlso fix two coding style nits (change spaces to tabs).\n\nSigned-off-by: Philo Lu <lulie@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20240911033719.91468-6-lulie@linux.alibaba.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Philo Lu <lulie@linux.alibaba.com>",
          "date": "2024-09-11 08:57:54 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/dynptr.c",
            "tools/testing/selftests/bpf/progs/dynptr_fail.c",
            "tools/testing/selftests/bpf/progs/dynptr_success.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "6fee7a7e9ad8613251d74472fb5bd2c6464f234a",
      "merge_subject": "Merge branch 'bpf-follow-up-on-gen_epilogue'",
      "merge_body": "Martin KaFai Lau says:\n\n====================\nbpf: Follow up on gen_epilogue\n\nFrom: Martin KaFai Lau <martin.lau@kernel.org>\n\nThe set addresses some follow ups on the earlier gen_epilogue\npatch set.\n====================\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240904180847.56947-1-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-09-04 12:45:26 -0700",
      "commits": [
        {
          "hash": "940ce73bdec5a020fec058ba947d2bf627462c53",
          "subject": "bpf: Remove the insn_buf array stack usage from the inline_bpf_loop()",
          "message": "This patch removes the insn_buf array stack usage from the\ninline_bpf_loop(). Instead, the env->insn_buf is used. The\nusage in inline_bpf_loop() needs more than 16 insn, so the\nINSN_BUF_SIZE needs to be increased from 16 to 32.\nThe compiler stack size warning on the verifier is gone\nafter this change.\n\nCc: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240904180847.56947-2-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-09-04 12:45:18 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "00750788dfc6b5167a294915e2690d8af182c496",
          "subject": "bpf: Fix indentation issue in epilogue_idx",
          "message": "There is a report on new indentation issue in epilogue_idx.\nThis patch fixed it.\n\nFixes: 169c31761c8d (\"bpf: Add gen_epilogue to bpf_verifier_ops\")\nReported-by: kernel test robot <lkp@intel.com>\nCloses: https://lore.kernel.org/oe-kbuild-all/202408311622.4GzlzN33-lkp@intel.com/\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240904180847.56947-3-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-09-04 12:45:18 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "f6284563acc9be8f1d389431f265061fa9791b0c",
      "merge_subject": "Merge branch 'bpf-add-gen_epilogue-to-bpf_verifier_ops'",
      "merge_body": "Martin KaFai Lau says:\n\n====================\nbpf: Add gen_epilogue to bpf_verifier_ops\n\nFrom: Martin KaFai Lau <martin.lau@kernel.org>\n\nThis set allows the subsystem to patch codes before BPF_EXIT.\nThe verifier ops, .gen_epilogue, is added for this purpose.\nOne of the use case will be in the bpf qdisc, the bpf qdisc\nsubsystem can ensure the skb->dev is in the correct value.\nThe bpf qdisc subsystem can either inline fixing it in the\nepilogue or call another kernel function to handle it (e.g. drop)\nin the epilogue. Another use case could be in bpf_tcp_ca.c to\nenforce snd_cwnd has valid value (e.g. positive value).\n\nv5:\n * Removed the skip_cnt argument from adjust_jmp_off() in patch 2.\n   Instead, reuse the delta argument and skip\n   the [tgt_idx, tgt_idx + delta) instructions.\n * Added a BPF_JMP32_A macro in patch 3.\n * Removed pro_epilogue_subprog.c in patch 6.\n   The pro_epilogue_kfunc.c has covered the subprog case.\n   Renamed the file pro_epilogue_kfunc.c to pro_epilogue.c.\n   Some of the SEC names and function names are changed\n   accordingly (mainly shorten them by removing the _kfunc suffix).\n * Added comments to explain the tail_call result in patch 7.\n * Fixed the following bpf CI breakages. I ran it in CI\n   manually to confirm:\n   https://github.com/kernel-patches/bpf/actions/runs/10590714532\n * s390 zext added \"w3 = w3\". Adjusted the test to\n   use all ALU64 and BPF_DW to avoid zext.\n   Also changed the \"int a\" in the \"struct st_ops_args\" to \"u64 a\".\n * llvm17 does not take:\n       *(u64 *)(r1 +0) = 0;\n   so it is changed to:\n       r3 = 0;\n       *(u64 *)(r1 +0) = r3;\n\nv4:\n * Fixed a bug in the memcpy in patch 3\n   The size in the memcpy should be\n   epilogue_cnt * sizeof(*epilogue_buf)\n\nv3:\n * Moved epilogue_buf[16] to env.\n   Patch 1 is added to move the existing insn_buf[16] to env.\n * Fixed a case that the bpf prog has a BPF_JMP that goes back\n   to the first instruction of the main prog.\n   The jump back to 1st insn case also applies to the prologue.\n   Patch 2 is added to handle it.\n * If the bpf main prog has multiple BPF_EXIT, use a BPF_JA\n   to goto the earlier patched epilogue.\n   Note that there are (BPF_JMP32 | BPF_JA) vs (BPF_JMP | BPF_JA)\n   details in the patch 3 commit message.\n * There are subtle changes in patch 3, so I reset the Reviewed-by.\n * Added patch 8 and patch 9 to cover the changes in patch 2 and patch 3.\n * Dropped the kfunc call from pro/epilogue and its selftests.\n\nv2:\n * Remove the RFC tag. Keep the ordering at where .gen_epilogue is\n   called in the verifier relative to the check_max_stack_depth().\n   This will be consistent with the other extra stack_depth\n   usage like optimize_bpf_loop().\n * Use __xlated check provided by the test_loader to\n   check the patched instructions after gen_pro/epilogue (Eduard).\n * Added Patch 3 by Eduard (Thanks!).\n====================\n\nLink: https://lore.kernel.org/r/20240829210833.388152-1-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-08-29 18:15:46 -0700",
      "commits": [
        {
          "hash": "6f606ffd6dd7583d8194ee3d858ba4da2eff26a3",
          "subject": "bpf: Move insn_buf[16] to bpf_verifier_env",
          "message": "This patch moves the 'struct bpf_insn insn_buf[16]' stack usage\nto the bpf_verifier_env. A '#define INSN_BUF_SIZE 16' is also added\nto replace the ARRAY_SIZE(insn_buf) usages.\n\nBoth convert_ctx_accesses() and do_misc_fixup() are changed\nto use the env->insn_buf.\n\nIt is a refactoring work for adding the epilogue_buf[16] in a later patch.\n\nWith this patch, the stack size usage decreased.\n\nBefore:\n./kernel/bpf/verifier.c:22133:5: warning: stack frame size (2584)\n\nAfter:\n./kernel/bpf/verifier.c:22184:5: warning: stack frame size (2264)\n\nReviewed-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-2-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-08-29 18:15:44 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "d5c47719f24438838e60bcbf97008179d6f737a8",
          "subject": "bpf: Adjust BPF_JMP that jumps to the 1st insn of the prologue",
          "message": "The next patch will add a ctx ptr saving instruction\n\"(r1 = *(u64 *)(r10 -8)\" at the beginning for the main prog\nwhen there is an epilogue patch (by the .gen_epilogue() verifier\nops added in the next patch).\n\nThere is one corner case if the bpf prog has a BPF_JMP that jumps\nto the 1st instruction. It needs an adjustment such that\nthose BPF_JMP instructions won't jump to the newly added\nctx saving instruction.\nThe commit 5337ac4c9b80 (\"bpf: Fix the corner case with may_goto and jump to the 1st insn.\")\nhas the details on this case.\n\nNote that the jump back to 1st instruction is not limited to the\nctx ptr saving instruction. The same also applies to the prologue.\nA later test, pro_epilogue_goto_start.c, has a test for the prologue\nonly case.\n\nThus, this patch does one adjustment after gen_prologue and\nthe future ctx ptr saving. It is done by\nadjust_jmp_off(env->prog, 0, delta) where delta has the total\nnumber of instructions in the prologue and\nthe future ctx ptr saving instruction.\n\nThe adjust_jmp_off(env->prog, 0, delta) assumes that the\nprologue does not have a goto 1st instruction itself.\nTo accommodate the prologue might have a goto 1st insn itself,\nthis patch changes the adjust_jmp_off() to skip considering\nthe instructions between [tgt_idx, tgt_idx + delta).\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-3-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-08-29 18:15:44 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "169c31761c8d7f606f3ee628829c27998626c4f0",
          "subject": "bpf: Add gen_epilogue to bpf_verifier_ops",
          "message": "This patch adds a .gen_epilogue to the bpf_verifier_ops. It is similar\nto the existing .gen_prologue. Instead of allowing a subsystem\nto run code at the beginning of a bpf prog, it allows the subsystem\nto run code just before the bpf prog exit.\n\nOne of the use case is to allow the upcoming bpf qdisc to ensure that\nthe skb->dev is the same as the qdisc->dev_queue->dev. The bpf qdisc\nstruct_ops implementation could either fix it up or drop the skb.\nAnother use case could be in bpf_tcp_ca.c to enforce snd_cwnd\nhas sane value (e.g. non zero).\n\nThe epilogue can do the useful thing (like checking skb->dev) if it\ncan access the bpf prog's ctx. Unlike prologue, r1 may not hold the\nctx pointer. This patch saves the r1 in the stack if the .gen_epilogue\nhas returned some instructions in the \"epilogue_buf\".\n\nThe existing .gen_prologue is done in convert_ctx_accesses().\nThe new .gen_epilogue is done in the convert_ctx_accesses() also.\nWhen it sees the (BPF_JMP | BPF_EXIT) instruction, it will be patched\nwith the earlier generated \"epilogue_buf\". The epilogue patching is\nonly done for the main prog.\n\nOnly one epilogue will be patched to the main program. When the\nbpf prog has multiple BPF_EXIT instructions, a BPF_JA is used\nto goto the earlier patched epilogue. Majority of the archs\nsupport (BPF_JMP32 | BPF_JA): x86, arm, s390, risv64, loongarch,\npowerpc and arc. This patch keeps it simple and always\nuse (BPF_JMP32 | BPF_JA). A new macro BPF_JMP32_A is added to\ngenerate the (BPF_JMP32 | BPF_JA) insn.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-4-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-08-29 18:15:45 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "include/linux/filter.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "866d571e6201cb8ccb18cb8407ab3ad3adb474b8",
          "subject": "bpf: Export bpf_base_func_proto",
          "message": "The bpf_testmod needs to use the bpf_tail_call helper in\na later selftest patch. This patch is to EXPORT_GPL_SYMBOL\nthe bpf_base_func_proto.\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-5-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-08-29 18:15:45 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "a0dbf6d0b21e197bf919591081ff2eb7a34ef982",
          "subject": "selftests/bpf: attach struct_ops maps before test prog runs",
          "message": "In test_loader based tests to bpf_map__attach_struct_ops()\nbefore call to bpf_prog_test_run_opts() in order to trigger\nbpf_struct_ops->reg() callbacks on kernel side.\nThis allows to use __retval macro for struct_ops tests.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-6-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-08-29 18:15:45 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/test_loader.c"
          ]
        },
        {
          "hash": "47e69431b57aee6c3c134014e7fca490ca8ca9d1",
          "subject": "selftests/bpf: Test gen_prologue and gen_epilogue",
          "message": "This test adds a new struct_ops \"bpf_testmod_st_ops\" in bpf_testmod.\nThe ops of the bpf_testmod_st_ops is triggered by new kfunc calls\n\"bpf_kfunc_st_ops_test_*logue\". These new kfunc calls are\nprimarily used by the SEC(\"syscall\") program. The test triggering\nsequence is like:\n    SEC(\"syscall\")\n    syscall_prologue(struct st_ops_args *args)\n        bpf_kfunc_st_op_test_prologue(args)\n\t    st_ops->test_prologue(args)\n\n.gen_prologue adds 1000 to args->a\n.gen_epilogue adds 10000 to args->a\n.gen_epilogue will also set the r0 to 2 * args->a.\n\nThe .gen_prologue and .gen_epilogue of the bpf_testmod_st_ops\nwill test the prog->aux->attach_func_name to decide if\nit needs to generate codes.\n\nThe main programs of the pro_epilogue.c will call a\nnew kfunc bpf_kfunc_st_ops_inc10 which does \"args->a += 10\".\nIt will also call a subprog() which does \"args->a += 1\".\n\nThis patch uses the test_loader infra to check the __xlated\ninstructions patched after gen_prologue and/or gen_epilogue.\nThe __xlated check is based on Eduard's example (Thanks!) in v1.\n\nargs->a is returned by the struct_ops prog (either the main prog\nor the epilogue). Thus, the __retval of the SEC(\"syscall\") prog\nis checked. For example, when triggering the ops in the\n'SEC(\"struct_ops/test_epilogue\") int test_epilogue'\nThe expected args->a is +1 (subprog call) + 10 (kfunc call)\n    \t     \t     \t+ 10000 (.gen_epilogue) = 10011.\nThe expected return value is 2 * 10011 (.gen_epilogue).\n\nSuggested-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-7-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-08-29 18:15:45 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.h",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod_kfunc.h",
            "tools/testing/selftests/bpf/prog_tests/pro_epilogue.c",
            "tools/testing/selftests/bpf/progs/pro_epilogue.c"
          ]
        },
        {
          "hash": "b191b0fd740062ede672693671c6e6e942fb02f4",
          "subject": "selftests/bpf: Add tailcall epilogue test",
          "message": "This patch adds a gen_epilogue test to test a main prog\nusing a bpf_tail_call.\n\nA non test_loader test is used. The tailcall target program,\n\"test_epilogue_subprog\", needs to be used in a struct_ops map\nbefore it can be loaded. Another struct_ops map is also needed\nto host the actual \"test_epilogue_tailcall\" struct_ops program\nthat does the bpf_tail_call. The earlier test_loader patch\nwill attach all struct_ops maps but the bpf_testmod.c does\nnot support >1 attached struct_ops.\n\nThe earlier patch used the test_loader which has already covered\nchecking for the patched pro/epilogue instructions. This is done\nby the __xlated tag.\n\nThis patch goes for the regular skel load and syscall test to do\nthe tailcall test that can also allow to directly pass the\nthe \"struct st_ops_args *args\" as ctx_in to the\nSEC(\"syscall\") program.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-8-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-08-29 18:15:45 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/pro_epilogue.c",
            "tools/testing/selftests/bpf/progs/epilogue_tailcall.c"
          ]
        },
        {
          "hash": "42fdbbde6cf4159f77de13a40f8c0be6ef48bcc1",
          "subject": "selftests/bpf: A pro/epilogue test when the main prog jumps back to the 1st insn",
          "message": "This patch adds a pro/epilogue test when the main prog has a goto insn\nthat goes back to the very first instruction of the prog. It is\nto test the correctness of the adjust_jmp_off(prog, 0, delta)\nafter the verifier has applied the prologue and/or epilogue patch.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-9-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-08-29 18:15:45 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/pro_epilogue.c",
            "tools/testing/selftests/bpf/progs/pro_epilogue_goto_start.c"
          ]
        },
        {
          "hash": "cada0bdcc471443dbd41bd63286f48be3dae0d89",
          "subject": "selftests/bpf: Test epilogue patching when the main prog has multiple BPF_EXIT",
          "message": "This patch tests the epilogue patching when the main prog has\nmultiple BPF_EXIT. The verifier should have patched the 2nd (and\nlater) BPF_EXIT with a BPF_JA that goes back to the earlier\npatched epilogue instructions.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-10-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-08-29 18:15:45 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/pro_epilogue.c",
            "tools/testing/selftests/bpf/progs/epilogue_exit.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "5cd0aea0b16a74769533ca605bc43634a4485dd9",
      "merge_subject": "Merge branch 'support-bpf_kptr_xchg-into-local-kptr'",
      "merge_body": "Amery Hung says:\n\n====================\nSupport bpf_kptr_xchg into local kptr\n\nThis revision adds substaintial changes to patch 2 to support structures\nwith kptr as the only special btf type. The test is split into\nlocal_kptr_stash and task_kfunc_success to remove dependencies on\nbpf_testmod that would break veristat results.\n\nThis series allows stashing kptr into local kptr. Currently, kptrs are\nonly allowed to be stashed into map value with bpf_kptr_xchg(). A\nmotivating use case of this series is to enable adding referenced kptr to\nbpf_rbtree or bpf_list by using allocated object as graph node and the\nstorage of referenced kptr. For example, a bpf qdisc [0] enqueuing a\nreferenced kptr to a struct sk_buff* to a bpf_list serving as a fifo:\n\n    struct skb_node {\n            struct sk_buff __kptr *skb;\n            struct bpf_list_node node;\n    };\n\n    private(A) struct bpf_spin_lock fifo_lock;\n    private(A) struct bpf_list_head fifo __contains(skb_node, node);\n\n    /* In Qdisc_ops.enqueue */\n    struct skb_node *skbn;\n\n    skbn = bpf_obj_new(typeof(*skbn));\n    if (!skbn)\n        goto drop;\n\n    /* skb is a referenced kptr to struct sk_buff acquired earilier\n     * but not shown in this code snippet.\n     */\n    skb = bpf_kptr_xchg(&skbn->skb, skb);\n    if (skb)\n        /* should not happen; do something below releasing skb to\n         * satisfy the verifier */\n    \t...\n\n    bpf_spin_lock(&fifo_lock);\n    bpf_list_push_back(&fifo, &skbn->node);\n    bpf_spin_unlock(&fifo_lock);\n\nThe implementation first searches for BPF_KPTR when generating program\nBTF. Then, we teach the verifier that the detination argument of\nbpf_kptr_xchg() can be local kptr, and use the btf_record in program BTF\nto check against the source argument.\n\nThis series is mostly developed by Dave, who kindly helped and sent me\nthe patchset. The selftests in bpf qdisc (WIP) relies on this series to\nwork.\n\n[0] https://lore.kernel.org/netdev/20240714175130.4051012-10-amery.hung@bytedance.com/\n---\nv3 -> v4\n  - Allow struct in prog btf w/ kptr as the only special field type\n  - Split tests of stashing referenced kptr and local kptr\n  - v3: https://lore.kernel.org/bpf/20240809005131.3916464-1-amery.hung@bytedance.com/\n\nv2 -> v3\n  - Fix prog btf memory leak\n  - Test stashing kptr in prog btf\n  - Test unstashing kptrs after stashing into local kptrs\n  - v2: https://lore.kernel.org/bpf/20240803001145.635887-1-amery.hung@bytedance.com/\n\nv1 -> v2\n  - Fix the document for bpf_kptr_xchg()\n  - Add a comment explaining changes in the verifier\n  - v1: https://lore.kernel.org/bpf/20240728030115.3970543-1-amery.hung@bytedance.com/\n====================\n\nLink: https://lore.kernel.org/r/20240813212424.2871455-1-amery.hung@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-08-23 11:39:34 -0700",
      "commits": [
        {
          "hash": "c5ef53420f46c9ca6badca4f4cabacd76de8091e",
          "subject": "bpf: Let callers of btf_parse_kptr() track life cycle of prog btf",
          "message": "btf_parse_kptr() and btf_record_free() do btf_get() and btf_put()\nrespectively when working on btf_record in program and map if there are\nkptr fields. If the kptr is from program BTF, since both callers has\nalready tracked the life cycle of program BTF, it is safe to remove the\nbtf_get() and btf_put().\n\nThis change prevents memory leak of program BTF later when we start\nsearching for kptr fields when building btf_record for program. It can\nhappen when the btf fd is closed. The btf_put() corresponding to the\nbtf_get() in btf_parse_kptr() was supposed to be called by\nbtf_record_free() in btf_free_struct_meta_tab() in btf_free(). However,\nit will never happen since the invocation of btf_free() depends on the\nrefcount of the btf to become 0 in the first place.\n\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nAcked-by: Hou Tao <houtao1@huawei.com>\nSigned-off-by: Amery Hung <amery.hung@bytedance.com>\nLink: https://lore.kernel.org/r/20240813212424.2871455-2-amery.hung@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Amery Hung <amery.hung@bytedance.com>",
          "date": "2024-08-23 11:39:33 -0700",
          "modified_files": [
            "kernel/bpf/btf.c",
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "7a851ecb180686f9e50110247b0fcd537e772e63",
          "subject": "bpf: Search for kptrs in prog BTF structs",
          "message": "Currently btf_parse_fields is used in two places to create struct\nbtf_record's for structs: when looking at mapval type, and when looking\nat any struct in program BTF. The former looks for kptr fields while the\nlatter does not. This patch modifies the btf_parse_fields call made when\nlooking at prog BTF struct types to search for kptrs as well.\n\nBefore this series there was no reason to search for kptrs in non-mapval\ntypes: a referenced kptr needs some owner to guarantee resource cleanup,\nand map values were the only owner that supported this. If a struct with\na kptr field were to have some non-kptr-aware owner, the kptr field\nmight not be properly cleaned up and result in resources leaking. Only\nsearching for kptr fields in mapval was a simple way to avoid this\nproblem.\n\nIn practice, though, searching for BPF_KPTR when populating\nstruct_meta_tab does not expose us to this risk, as struct_meta_tab is\nonly accessed through btf_find_struct_meta helper, and that helper is\nonly called in contexts where recognizing the kptr field is safe:\n\n  * PTR_TO_BTF_ID reg w/ MEM_ALLOC flag\n    * Such a reg is a local kptr and must be free'd via bpf_obj_drop,\n      which will correctly handle kptr field\n\n  * When handling specific kfuncs which either expect MEM_ALLOC input or\n    return MEM_ALLOC output (obj_{new,drop}, percpu_obj_{new,drop},\n    list+rbtree funcs, refcount_acquire)\n     * Will correctly handle kptr field for same reasons as above\n\n  * When looking at kptr pointee type\n     * Called by functions which implement \"correct kptr resource\n       handling\"\n\n  * In btf_check_and_fixup_fields\n     * Helper that ensures no ownership loops for lists and rbtrees,\n       doesn't care about kptr field existence\n\nSo we should be able to find BPF_KPTR fields in all prog BTF structs\nwithout leaking resources.\n\nFurther patches in the series will build on this change to support\nkptr_xchg into non-mapval local kptr. Without this change there would be\nno kptr field found in such a type.\n\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nAcked-by: Hou Tao <houtao1@huawei.com>\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nSigned-off-by: Amery Hung <amery.hung@bytedance.com>\nLink: https://lore.kernel.org/r/20240813212424.2871455-3-amery.hung@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2024-08-23 11:39:33 -0700",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "d59232afb0344e33e9399f308d9b4a03876e7676",
          "subject": "bpf: Rename ARG_PTR_TO_KPTR -> ARG_KPTR_XCHG_DEST",
          "message": "ARG_PTR_TO_KPTR is currently only used by the bpf_kptr_xchg helper.\nAlthough it limits reg types for that helper's first arg to\nPTR_TO_MAP_VALUE, any arbitrary mapval won't do: further custom\nverification logic ensures that the mapval reg being xchgd-into is\npointing to a kptr field. If this is not the case, it's not safe to xchg\ninto that reg's pointee.\n\nLet's rename the bpf_arg_type to more accurately describe the fairly\nspecific expectations that this arg type encodes.\n\nThis is a nonfunctional change.\n\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nSigned-off-by: Amery Hung <amery.hung@bytedance.com>\nLink: https://lore.kernel.org/r/20240813212424.2871455-4-amery.hung@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2024-08-23 11:39:33 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b0966c724584a5a9fd7fb529de19807c31f27a45",
          "subject": "bpf: Support bpf_kptr_xchg into local kptr",
          "message": "Currently, users can only stash kptr into map values with bpf_kptr_xchg().\nThis patch further supports stashing kptr into local kptr by adding local\nkptr as a valid destination type.\n\nWhen stashing into local kptr, btf_record in program BTF is used instead\nof btf_record in map to search for the btf_field of the local kptr.\n\nThe local kptr specific checks in check_reg_type() only apply when the\nsource argument of bpf_kptr_xchg() is local kptr. Therefore, we make the\nscope of the check explicit as the destination now can also be local kptr.\n\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nSigned-off-by: Amery Hung <amery.hung@bytedance.com>\nLink: https://lore.kernel.org/r/20240813212424.2871455-5-amery.hung@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2024-08-23 11:39:33 -0700",
          "modified_files": [
            "include/uapi/linux/bpf.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "91c96842ab1e9159c8129ab5ddfeb7dd97bf840e",
          "subject": "selftests/bpf: Test bpf_kptr_xchg stashing into local kptr",
          "message": "Test stashing both referenced kptr and local kptr into local kptrs. Then,\ntest unstashing them.\n\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nAcked-by: Hou Tao <houtao1@huawei.com>\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nSigned-off-by: Amery Hung <amery.hung@bytedance.com>\nLink: https://lore.kernel.org/r/20240813212424.2871455-6-amery.hung@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2024-08-23 11:39:33 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/local_kptr_stash.c",
            "tools/testing/selftests/bpf/progs/task_kfunc_success.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "d352eca2662734cdd5ef90df1f8bc28b9505e36f",
      "merge_subject": "Merge branch 'support-bpf_fastcall-patterns-for-calls-to-kfuncs'",
      "merge_body": "Eduard Zingerman says:\n\n====================\nsupport bpf_fastcall patterns for calls to kfuncs\n\nAs an extension of [1], allow bpf_fastcall patterns for kfuncs:\n- pattern rules are the same as for helpers;\n- spill/fill removal is allowed only for kfuncs listed in the\n  is_fastcall_kfunc_call (under assumption that such kfuncs would\n  always be members of special_kfunc_list).\n\nAllow bpf_fastcall rewrite for bpf_cast_to_kern_ctx() and\nbpf_rdonly_cast() in order to conjure selftests for this feature.\n\nAfter this patch-set verifier would rewrite the program below:\n\n  r2 = 1\n  *(u64 *)(r10 - 32) = r2\n  call %[bpf_cast_to_kern_ctx]\n  r2 = *(u64 *)(r10 - 32)\n  r0 = r2;\"\n\nAs follows:\n\n  r2 = 1   /* spill/fill at r10[-32] is removed */\n  r0 = r1  /* replacement for bpf_cast_to_kern_ctx() */\n  r0 = r2\n  exit\n\nAlso, attribute used by LLVM implementation of the feature had been\nchanged from no_caller_saved_registers to bpf_fastcall (see [2]).\nThis patch-set replaces references to nocsr by references to\nbpf_fastcall to keep LLVM and Kernel parts in sync.\n\n[1] no_caller_saved_registers attribute for helper calls\n    https://lore.kernel.org/bpf/20240722233844.1406874-1-eddyz87@gmail.com/\n[2] [BPF] introduce __attribute__((bpf_fastcall))\n    https://github.com/llvm/llvm-project/pull/105417\n\nChanges v2->v3:\n- added a patch fixing arch_mask handling in test_loader,\n  otherwise newly added tests for the feature were skipped\n  (a fix for regression introduced by a recent commit);\n- fixed warning regarding unused 'params' variable;\n- applied stylistical fixes suggested by Yonghong;\n- added acks from Yonghong;\n\nChanges v1->v2:\n- added two patches replacing all mentions of nocsr by bpf_fastcall\n  (suggested by Andrii);\n- removed KF_NOCSR flag (suggested by Yonghong).\n\nv1: https://lore.kernel.org/bpf/20240812234356.2089263-1-eddyz87@gmail.com/\nv2: https://lore.kernel.org/bpf/20240817015140.1039351-1-eddyz87@gmail.com/\n====================\n\nLink: https://lore.kernel.org/r/20240822084112.3257995-1-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-08-22 08:35:22 -0700",
      "commits": [
        {
          "hash": "ae010757a55b57c8b82628e8ea9b7da2269131d9",
          "subject": "bpf: rename nocsr -> bpf_fastcall in verifier",
          "message": "Attribute used by LLVM implementation of the feature had been changed\nfrom no_caller_saved_registers to bpf_fastcall (see [1]).\nThis commit replaces references to nocsr by references to bpf_fastcall\nto keep LLVM and Kernel parts in sync.\n\n[1] https://github.com/llvm/llvm-project/pull/105417\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240822084112.3257995-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-08-22 08:35:20 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "adec67d372fecd97585d76dbebb610d62ec9d2cc",
          "subject": "selftests/bpf: rename nocsr -> bpf_fastcall in selftests",
          "message": "Attribute used by LLVM implementation of the feature had been changed\nfrom no_caller_saved_registers to bpf_fastcall (see [1]).\nThis commit replaces references to nocsr by references to bpf_fastcall\nto keep LLVM and selftests parts in sync.\n\n[1] https://github.com/llvm/llvm-project/pull/105417\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240822084112.3257995-3-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-08-22 08:35:20 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_bpf_fastcall.c"
          ]
        },
        {
          "hash": "b2ee6d27e9c6be0409e96591dcee62032a8e0156",
          "subject": "bpf: support bpf_fastcall patterns for kfuncs",
          "message": "Recognize bpf_fastcall patterns around kfunc calls.\nFor example, suppose bpf_cast_to_kern_ctx() follows bpf_fastcall\ncontract (which it does), in such a case allow verifier to rewrite BPF\nprogram below:\n\n  r2 = 1;\n  *(u64 *)(r10 - 32) = r2;\n  call %[bpf_cast_to_kern_ctx];\n  r2 = *(u64 *)(r10 - 32);\n  r0 = r2;\n\nBy removing the spill/fill pair:\n\n  r2 = 1;\n  call %[bpf_cast_to_kern_ctx];\n  r0 = r2;\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240822084112.3257995-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-08-22 08:35:21 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "40609093247b586ee6f8ca8f2b30c7d583c6fd25",
          "subject": "bpf: allow bpf_fastcall for bpf_cast_to_kern_ctx and bpf_rdonly_cast",
          "message": "do_misc_fixups() relaces bpf_cast_to_kern_ctx() and bpf_rdonly_cast()\nby a single instruction \"r0 = r1\". This follows bpf_fastcall contract.\nThis commit allows bpf_fastcall pattern rewrite for these two\nfunctions in order to use them in bpf_fastcall selftests.\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240822084112.3257995-5-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-08-22 08:35:21 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "f406026fefa745ff9a3153507cc3b9a87371d954",
          "subject": "selftests/bpf: by default use arch mask allowing all archs",
          "message": "If test case does not specify architecture via __arch_* macro consider\nthat it should be run for all architectures.\n\nFixes: 7d743e4c759c (\"selftests/bpf: __jited test tag to check disassembly after jit\")\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240822084112.3257995-6-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-08-22 08:35:21 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/test_loader.c"
          ]
        },
        {
          "hash": "8c2e043daadad021fc501ac64cce131f48c3ca46",
          "subject": "selftests/bpf: check if bpf_fastcall is recognized for kfuncs",
          "message": "Use kfunc_bpf_cast_to_kern_ctx() and kfunc_bpf_rdonly_cast() to verify\nthat bpf_fastcall pattern is recognized for kfunc calls.\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240822084112.3257995-7-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-08-22 08:35:21 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_bpf_fastcall.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "ffc41ce5cf09f8d7113aca98e68f6e11956c0eab",
      "merge_subject": "Merge branch 'support-passing-bpf-iterator-to-kfuncs'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nSupport passing BPF iterator to kfuncs\n\nAdd support for passing BPF iterator state to any kfunc. Such kfunc has to\ndeclare such argument with valid `struct bpf_iter_<type> *` type and should\nuse \"__iter\" suffix in argument name, following the established suffix-based\nconvention. We add a simple test/demo iterator getter in bpf_testmod.\n====================\n\nLink: https://lore.kernel.org/r/20240808232230.2848712-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-08-21 10:37:52 -0700",
      "commits": [
        {
          "hash": "496ddd19a0fad22f250fc7a7b7a8000155418934",
          "subject": "bpf: extract iterator argument type and name validation logic",
          "message": "Verifier enforces that all iterator structs are named `bpf_iter_<name>`\nand that whenever iterator is passed to a kfunc it's passed as a valid PTR ->\nSTRUCT chain (with potentially const modifiers in between).\n\nWe'll need this check for upcoming changes, so instead of duplicating\nthe logic, extract it into a helper function.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240808232230.2848712-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-08-21 10:37:52 -0700",
          "modified_files": [
            "include/linux/btf.h",
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "baebe9aaba1e59e34cd1fe6455bb4c3029ad3bc1",
          "subject": "bpf: allow passing struct bpf_iter_<type> as kfunc arguments",
          "message": "There are potentially useful cases where a specific iterator type might\nneed to be passed into some kfunc. So, in addition to existing\nbpf_iter_<type>_{new,next,destroy}() kfuncs, allow to pass iterator\npointer to any kfunc.\n\nWe employ \"__iter\" naming suffix for arguments that are meant to accept\niterators. We also enforce that they accept PTR -> STRUCT btf_iter_<type>\ntype chain and point to a valid initialized on-the-stack iterator state.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240808232230.2848712-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-08-21 10:37:52 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b0cd726f9a8279b33faa5b4b0011df14f331fb33",
          "subject": "selftests/bpf: test passing iterator to a kfunc",
          "message": "Define BPF iterator \"getter\" kfunc, which accepts iterator pointer as\none of the arguments. Make sure that argument passed doesn't have to be\nthe very first argument (unlike new-next-destroy combo).\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240808232230.2848712-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-08-21 10:37:52 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/progs/iters_testmod_seq.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "52839f31cecec2efe8cd50f432b7dd153d485c30",
      "merge_subject": "Merge branch 'no_caller_saved_registers-attribute-for-helper-calls'",
      "merge_body": "Eduard Zingerman says:\n\n====================\nno_caller_saved_registers attribute for helper calls\n\nThis patch-set seeks to allow using no_caller_saved_registers gcc/clang\nattribute with some BPF helper functions (and kfuncs in the future).\n\nAs documented in [1], this attribute means that function scratches\nonly some of the caller saved registers defined by ABI.\nFor BPF the set of such registers could be defined as follows:\n- R0 is scratched only if function is non-void;\n- R1-R5 are scratched only if corresponding parameter type is defined\n  in the function prototype.\n\nThe goal of the patch-set is to implement no_caller_saved_registers\n(nocsr for short) in a backwards compatible manner:\n- for kernels that support the feature, gain some performance boost\n  from better register allocation;\n- for kernels that don't support the feature, allow programs execution\n  with minor performance losses.\n\nTo achieve this, use a scheme suggested by Alexei Starovoitov:\n- for nocsr calls clang allocates registers as-if relevant r0-r5\n  registers are not scratched by the call;\n- as a post-processing step, clang visits each nocsr call and adds\n  spill/fill for every live r0-r5;\n- stack offsets used for spills/fills are allocated as lowest\n  stack offsets in whole function and are not used for any other\n  purpose;\n- when kernel loads a program, it looks for such patterns\n  (nocsr function surrounded by spills/fills) and checks if\n  spill/fill stack offsets are used exclusively in nocsr patterns;\n- if so, and if current JIT inlines the call to the nocsr function\n  (e.g. a helper call), kernel removes unnecessary spill/fill pairs;\n- when old kernel loads a program, presence of spill/fill pairs\n  keeps BPF program valid, albeit slightly less efficient.\n\nCorresponding clang/llvm changes are available in [2].\n\nThe patch-set uses bpf_get_smp_processor_id() function as a canary,\nmaking it the first helper with nocsr attribute.\n\nFor example, consider the following program:\n\n  #define __no_csr __attribute__((no_caller_saved_registers))\n  #define SEC(name) __attribute__((section(name), used))\n  #define bpf_printk(fmt, ...) bpf_trace_printk((fmt), sizeof(fmt), __VA_ARGS__)\n\n  typedef unsigned int __u32;\n\n  static long (* const bpf_trace_printk)(const char *fmt, __u32 fmt_size, ...) = (void *) 6;\n  static __u32 (*const bpf_get_smp_processor_id)(void) __no_csr = (void *)8;\n\n  SEC(\"raw_tp\")\n  int test(void *ctx)\n  {\n          __u32 task = bpf_get_smp_processor_id();\n  \tbpf_printk(\"ctx=%p, smp=%d\", ctx, task);\n  \treturn 0;\n  }\n\n  char _license[] SEC(\"license\") = \"GPL\";\n\nCompiled (using [2]) as follows:\n\n  $ clang --target=bpf -O2 -g -c -o nocsr.bpf.o nocsr.bpf.c\n  $ llvm-objdump --no-show-raw-insn -Sd nocsr.bpf.o\n    ...\n  3rd parameter for printk call     removable spill/fill pair\n  .--- 0:       r3 = r1                             |\n; |       __u32 task = bpf_get_smp_processor_id();  |\n  |    1:       *(u64 *)(r10 - 0x8) = r3 <----------|\n  |    2:       call 0x8                            |\n  |    3:       r3 = *(u64 *)(r10 - 0x8) <----------'\n; |     bpf_printk(\"ctx=%p, smp=%d\", ctx, task);\n  |    4:       r1 = 0x0 ll\n  |    6:       r2 = 0xf\n  |    7:       r4 = r0\n  '--> 8:       call 0x6\n;       return 0;\n       9:       r0 = 0x0\n      10:       exit\n\nHere is how the program looks after verifier processing:\n\n  # bpftool prog load ./nocsr.bpf.o /sys/fs/bpf/nocsr-test\n  # bpftool prog dump xlated pinned /sys/fs/bpf/nocsr-test\n\n  int test(void * ctx):\n     0: (bf) r3 = r1                         <--- 3rd printk parameter\n  ; __u32 task = bpf_get_smp_processor_id();\n     1: (b4) w0 = 197324                     <--. inlined helper call,\n     2: (bf) r0 = &(void __percpu *)(r0)     <--- spill/fill\n     3: (61) r0 = *(u32 *)(r0 +0)            <--' pair removed\n  ; bpf_printk(\"ctx=%p, smp=%d\", ctx, task);\n     4: (18) r1 = map[id:5][0]+0\n     6: (b7) r2 = 15\n     7: (bf) r4 = r0\n     8: (85) call bpf_trace_printk#-125920\n  ; return 0;\n     9: (b7) r0 = 0\n    10: (95) exit\n\n[1] https://clang.llvm.org/docs/AttributeReference.html#no-caller-saved-registers\n[2] https://github.com/eddyz87/llvm-project/tree/bpf-no-caller-saved-registers\n\nChange list:\n- v3 -> v4:\n  - When nocsr spills/fills are removed in the subprogram, allow these\n    spills/fills to reside in [-MAX_BPF_STACK-48..MAX_BPF_STACK) range\n    (suggested by Alexei);\n  - Dropped patches with special handling for bpf_probe_read_kernel()\n    (requested by Alexei);\n  - Reset aux .nocsr_pattern and .nocsr_spills_num fields in\n    check_nocsr_stack_contract() (requested by Andrii).\n    Andrii, I have not added an additional flag to\n    struct bpf_subprog_info, it currently does not have holes\n    and I really don't like adding a bool field there just as an\n    alternative indicator that nocsr is disabled.\n    Indicator at the moment:\n    - nocsr_stack_off >= S16_MIN means that nocsr rewrite is enabled;\n    - nocsr_stack_off == S16_MIN means that nocsr rewrite is disabled.\n- v2 -> v3:\n  - As suggested by Andrii, 'nocsr_stack_off' is no longer checked at\n    rewrite time, instead mark_nocsr_patterns() now does two passes\n    over BPF program:\n    - on a first pass it computes the lowest stack spill offset for\n      the subprogram;\n    - on a second pass this offset is used to recognize nocsr pattern.\n  - As suggested by Alexei, a new mechanic is added to work around a\n    situation mentioned by Andrii, when more helper functions are\n    marked as nocsr at compile time than current kernel supports:\n    - all {spill*,helper call,fill*} patterns are now marked as\n      insn_aux_data[*].nocsr_pattern, thus relaxing failure condition\n      for check_nocsr_stack_contract();\n    - spill/fill pairs are not removed for patterns where helper can't\n      be inlined;\n    - see mark_nocsr_pattern_for_call() for details an example.\n  - As suggested by Alexei, subprogram stack depth is now adjusted\n    if all spill/fill pairs could be removed. This adjustment has\n    to take place before optimize_bpf_loop(), hence the rewrite\n    is moved from do_misc_fixups() to remove_nocsr_spills_fills()\n    (again).\n  - As suggested by Andrii, special measures are taken to work around\n    bpf_probe_read_kernel() access to BPF stack, see patches 11, 12.\n    Patch #11 is very simplistic, a more comprehensive solution would\n    be to change the type of the third parameter of the\n    bpf_probe_read_kernel() from ARG_ANYTHING to something else and\n    not only check nocsr contract, but also propagate stack slot\n    liveness information. However, such change would require update in\n    struct bpf_call_arg_meta processing, which currently implies that\n    every memory parameter is followed by a size parameter.\n    I can work on these changes, please comment.\n  - Stylistic changes suggested by Andrii.\n  - Added acks from Andrii.\n  - Dropped RFC tag.\n- v1 -> v2:\n  - assume that functions inlined by either jit or verifier\n    conform to no_caller_saved_registers contract (Andrii, Puranjay);\n  - allow nocsr rewrite for bpf_get_smp_processor_id()\n    on arm64 and riscv64 architectures (Puranjay);\n  - __arch_{x86_64,arm64,riscv64} macro for test_loader;\n  - moved remove_nocsr_spills_fills() inside do_misc_fixups() (Andrii);\n  - moved nocsr pattern detection from check_cfg() to a separate pass\n    (Andrii);\n  - various stylistic/correctness changes according to Andrii's\n    comments.\n\nRevisions:\n- v1 https://lore.kernel.org/bpf/20240629094733.3863850-1-eddyz87@gmail.com/\n- v2 https://lore.kernel.org/bpf/20240704102402.1644916-1-eddyz87@gmail.com/\n- v3 https://lore.kernel.org/bpf/20240715230201.3901423-1-eddyz87@gmail.com/\n====================\n\nLink: https://lore.kernel.org/r/20240722233844.1406874-1-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-07-29 15:05:06 -0700",
      "commits": [
        {
          "hash": "45cbc7a5e004cf08528ef83633c62120cca3a5ee",
          "subject": "bpf: add a get_helper_proto() utility function",
          "message": "Extract the part of check_helper_call() as a utility function allowing\nto query 'struct bpf_func_proto' for a specific helper function id.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 15:05:05 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "5b5f51bff1b66cedb62b5ba74a1878341204e057",
          "subject": "bpf: no_caller_saved_registers attribute for helper calls",
          "message": "GCC and LLVM define a no_caller_saved_registers function attribute.\nThis attribute means that function scratches only some of\nthe caller saved registers defined by ABI.\nFor BPF the set of such registers could be defined as follows:\n- R0 is scratched only if function is non-void;\n- R1-R5 are scratched only if corresponding parameter type is defined\n  in the function prototype.\n\nThis commit introduces flag bpf_func_prot->allow_nocsr.\nIf this flag is set for some helper function, verifier assumes that\nit follows no_caller_saved_registers calling convention.\n\nThe contract between kernel and clang allows to simultaneously use\nsuch functions and maintain backwards compatibility with old\nkernels that don't understand no_caller_saved_registers calls\n(nocsr for short):\n\n- clang generates a simple pattern for nocsr calls, e.g.:\n\n    r1 = 1;\n    r2 = 2;\n    *(u64 *)(r10 - 8)  = r1;\n    *(u64 *)(r10 - 16) = r2;\n    call %[to_be_inlined]\n    r2 = *(u64 *)(r10 - 16);\n    r1 = *(u64 *)(r10 - 8);\n    r0 = r1;\n    r0 += r2;\n    exit;\n\n- kernel removes unnecessary spills and fills, if called function is\n  inlined by verifier or current JIT (with assumption that patch\n  inserted by verifier or JIT honors nocsr contract, e.g. does not\n  scratch r3-r5 for the example above), e.g. the code above would be\n  transformed to:\n\n    r1 = 1;\n    r2 = 2;\n    call %[to_be_inlined]\n    r0 = r1;\n    r0 += r2;\n    exit;\n\nTechnically, the transformation is split into the following phases:\n- function mark_nocsr_patterns(), called from bpf_check()\n  searches and marks potential patterns in instruction auxiliary data;\n- upon stack read or write access,\n  function check_nocsr_stack_contract() is used to verify if\n  stack offsets, presumably reserved for nocsr patterns, are used\n  only from those patterns;\n- function remove_nocsr_spills_fills(), called from bpf_check(),\n  applies the rewrite for valid patterns.\n\nSee comment in mark_nocsr_pattern_for_call() for more details.\n\nSuggested-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-3-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 15:05:05 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "91b7fbf3936f5c27d1673264dc24a713290e2165",
          "subject": "bpf, x86, riscv, arm: no_caller_saved_registers for bpf_get_smp_processor_id()",
          "message": "The function bpf_get_smp_processor_id() is processed in a different\nway, depending on the arch:\n- on x86 verifier replaces call to bpf_get_smp_processor_id() with a\n  sequence of instructions that modify only r0;\n- on riscv64 jit replaces call to bpf_get_smp_processor_id() with a\n  sequence of instructions that modify only r0;\n- on arm64 jit replaces call to bpf_get_smp_processor_id() with a\n  sequence of instructions that modify only r0 and tmp registers.\n\nThese rewrites satisfy attribute no_caller_saved_registers contract.\nAllow rewrite of no_caller_saved_registers patterns for\nbpf_get_smp_processor_id() in order to use this function as a canary\nfor no_caller_saved_registers tests.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 15:05:05 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "424ebaa3678b0d7f653dffb08eae90424740e0f4",
          "subject": "selftests/bpf: extract utility function for BPF disassembly",
          "message": "struct bpf_insn *disasm_insn(struct bpf_insn *insn, char *buf, size_t buf_sz);\n\n  Disassembles instruction 'insn' to a text buffer 'buf'.\n  Removes insn->code hex prefix added by kernel disassembly routine.\n  Returns a pointer to the next instruction\n  (increments insn by either 1 or 2).\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-5-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 15:05:05 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/Makefile",
            "tools/testing/selftests/bpf/disasm_helpers.c",
            "tools/testing/selftests/bpf/disasm_helpers.h",
            "tools/testing/selftests/bpf/prog_tests/ctx_rewrite.c",
            "tools/testing/selftests/bpf/testing_helpers.c"
          ]
        },
        {
          "hash": "203e6aba7692bca18fd97251b1354da0f5e2ba30",
          "subject": "selftests/bpf: print correct offset for pseudo calls in disasm_insn()",
          "message": "Adjust disasm_helpers.c:disasm_insn() to account for the following\npart of the verifier.c:jit_subprogs:\n\n  for (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n        /* ... */\n        if (!bpf_pseudo_call(insn))\n                continue;\n        insn->off = env->insn_aux_data[i].call_imm;\n        subprog = find_subprog(env, i + insn->off + 1);\n        insn->imm = subprog;\n  }\n\nWhere verifier moves offset of the subprogram to the insn->off field.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-6-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 15:05:06 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/disasm_helpers.c"
          ]
        },
        {
          "hash": "4ef5d6af493558124b7a6c13cace58b938fe27d4",
          "subject": "selftests/bpf: no need to track next_match_pos in struct test_loader",
          "message": "The call stack for validate_case() function looks as follows:\n- test_loader__run_subtests()\n  - process_subtest()\n    - run_subtest()\n      - prepare_case(), which does 'tester->next_match_pos = 0';\n      - validate_case(), which increments tester->next_match_pos.\n\nHence, each subtest is run with next_match_pos freshly set to zero.\nMeaning that there is no need to persist this variable in the\nstruct test_loader, use local variable instead.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-7-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 15:05:06 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/test_loader.c",
            "tools/testing/selftests/bpf/test_progs.h"
          ]
        },
        {
          "hash": "64f01e935ddb26f48baec71883c27878ac4231dc",
          "subject": "selftests/bpf: extract test_loader->expect_msgs as a data structure",
          "message": "Non-functional change: use a separate data structure to represented\nexpected messages in test_loader.\nThis would allow to use the same functionality for expected set of\ndisassembled instructions in the follow-up commit.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-8-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 15:05:06 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/test_loader.c"
          ]
        },
        {
          "hash": "9c9f7339131030949a8ef111080427ff1a8085b5",
          "subject": "selftests/bpf: allow checking xlated programs in verifier_* tests",
          "message": "Add a macro __xlated(\"...\") for use with test_loader tests.\n\nWhen such annotations are present for the test case:\n- bpf_prog_get_info_by_fd() is used to get BPF program after all\n  rewrites are applied by verifier.\n- the program is disassembled and patterns specified in __xlated are\n  searched for in the disassembly text.\n\n__xlated matching follows the same mechanics as __msg:\neach subsequent pattern is matched from the point where\nprevious pattern ended.\n\nThis allows to write tests like below, where the goal is to verify the\nbehavior of one of the of the transformations applied by verifier:\n\n    SEC(\"raw_tp\")\n    __xlated(\"1: w0 = \")\n    __xlated(\"2: r0 = &(void __percpu *)(r0)\")\n    __xlated(\"3: r0 = *(u32 *)(r0 +0)\")\n    __xlated(\"4: exit\")\n    __success __naked void simple(void)\n    {\n            asm volatile (\n            \"call %[bpf_get_smp_processor_id];\"\n            \"exit;\"\n            :\n            : __imm(bpf_get_smp_processor_id)\n            : __clobber_all);\n    }\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-9-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 15:05:06 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/bpf_misc.h",
            "tools/testing/selftests/bpf/test_loader.c"
          ]
        },
        {
          "hash": "ee7fe84468b1732fe65c5af3836437d54ac4c419",
          "subject": "selftests/bpf: __arch_* macro to limit test cases to specific archs",
          "message": "Add annotations __arch_x86_64, __arch_arm64, __arch_riscv64\nto specify on which architecture the test case should be tested.\nSeveral __arch_* annotations could be specified at once.\nWhen test case is not run on current arch it is marked as skipped.\n\nFor example, the following would be tested only on arm64 and riscv64:\n\n  SEC(\"raw_tp\")\n  __arch_arm64\n  __arch_riscv64\n  __xlated(\"1: *(u64 *)(r10 - 16) = r1\")\n  __xlated(\"2: call\")\n  __xlated(\"3: r1 = *(u64 *)(r10 - 16);\")\n  __success\n  __naked void canary_arm64_riscv64(void)\n  {\n  \tasm volatile (\n  \t\"r1 = 1;\"\n  \t\"*(u64 *)(r10 - 16) = r1;\"\n  \t\"call %[bpf_get_smp_processor_id];\"\n  \t\"r1 = *(u64 *)(r10 - 16);\"\n  \t\"exit;\"\n  \t:\n  \t: __imm(bpf_get_smp_processor_id)\n  \t: __clobber_all);\n  }\n\nOn x86 it would be skipped:\n\n  #467/2   verifier_nocsr/canary_arm64_riscv64:SKIP\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-10-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 15:05:06 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/bpf_misc.h",
            "tools/testing/selftests/bpf/test_loader.c"
          ]
        },
        {
          "hash": "d0ad1f8f8846cffebca55abdd1ed275e276a6754",
          "subject": "selftests/bpf: test no_caller_saved_registers spill/fill removal",
          "message": "Tests for no_caller_saved_registers processing logic\n(see verifier.c:match_and_mark_nocsr_pattern()):\n- a canary positive test case;\n- a canary test case for arm64 and riscv64;\n- various tests with broken patterns;\n- tests with read/write fixed/varying stack access that violate nocsr\n  stack access contract;\n- tests with multiple subprograms;\n- tests using nocsr in combination with may_goto/bpf_loop,\n  as all of these features affect stack depth;\n- tests for nocsr stack spills below max stack depth.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-11-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 15:05:06 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_nocsr.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "02d9fe1c4d987c4b3d272b5217da53280ffa71f5",
      "merge_subject": "Merge branch 'add-bpf-lsm-return-value-range-check-bpf-part'",
      "merge_body": "Xu Kuohai says:\n\n====================\nAdd BPF LSM return value range check, BPF part\n\nFrom: Xu Kuohai <xukuohai@huawei.com>\n\nLSM BPF prog may make kernel panic when returning an unexpected value,\nsuch as returning positive value on hook file_alloc_security.\n\nTo fix it, series [1] refactored LSM hook return values and added\nBPF return value check on top of that. Since the refactoring of LSM\nhooks and checking BPF prog return value patches is not closely related,\nthis series separates BPF-related patches from [1].\n\nv2:\n- Update Shung-Hsi's patch with [3]\n\nv1: https://lore.kernel.org/bpf/20240719081749.769748-1-xukuohai@huaweicloud.com/\n\nChanges to [1]:\n\n1. Extend LSM disabled list to include hooks refactored in [1] to avoid\n   dependency on the hooks return value refactoring patches.\n\n2. Replace the special case patch for bitwise AND on [-1, 0] with Shung-Hsi's\n   general bitwise AND improvement patch [2].\n\n3. Remove unused patches.\n\n[1] https://lore.kernel.org/bpf/20240711111908.3817636-1-xukuohai@huaweicloud.com\n    https://lore.kernel.org/bpf/20240711113828.3818398-1-xukuohai@huaweicloud.com\n\n[2] https://lore.kernel.org/bpf/ykuhustu7vt2ilwhl32kj655xfdgdlm2xkl5rff6tw2ycksovp@ss2n4gpjysnw\n\n[3] https://lore.kernel.org/bpf/20240719081702.137173-1-shung-hsi.yu@suse.com/\n\nShung-Hsi Yu (1):\n  bpf, verifier: improve signed ranges inference for BPF_AND\n====================\n\nLink: https://lore.kernel.org/r/20240719110059.797546-1-xukuohai@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-07-29 13:09:48 -0700",
      "commits": [
        {
          "hash": "21c7063f6d08ab9afa088584939791bee0c177e5",
          "subject": "bpf, lsm: Add disabled BPF LSM hook list",
          "message": "Add a disabled hooks list for BPF LSM. progs being attached to the\nlisted hooks will be rejected by the verifier.\n\nSuggested-by: KP Singh <kpsingh@kernel.org>\nSigned-off-by: Xu Kuohai <xukuohai@huawei.com>\nLink: https://lore.kernel.org/r/20240719110059.797546-2-xukuohai@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Xu Kuohai <xukuohai@huawei.com>",
          "date": "2024-07-29 13:09:18 -0700",
          "modified_files": [
            "kernel/bpf/bpf_lsm.c"
          ]
        },
        {
          "hash": "5d99e198be279045e6ecefe220f5c52f8ce9bfd5",
          "subject": "bpf, lsm: Add check for BPF LSM return value",
          "message": "A bpf prog returning a positive number attached to file_alloc_security\nhook makes kernel panic.\n\nThis happens because file system can not filter out the positive number\nreturned by the LSM prog using IS_ERR, and misinterprets this positive\nnumber as a file pointer.\n\nGiven that hook file_alloc_security never returned positive number\nbefore the introduction of BPF LSM, and other BPF LSM hooks may\nencounter similar issues, this patch adds LSM return value check\nin verifier, to ensure no unexpected value is returned.\n\nFixes: 520b7aa00d8c (\"bpf: lsm: Initialize the BPF LSM hooks\")\nReported-by: Xin Liu <liuxin350@huawei.com>\nSigned-off-by: Xu Kuohai <xukuohai@huawei.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240719110059.797546-3-xukuohai@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Xu Kuohai <xukuohai@huawei.com>",
          "date": "2024-07-29 13:09:22 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_lsm.h",
            "kernel/bpf/bpf_lsm.c",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "28ead3eaabc16ecc907cfb71876da028080f6356",
          "subject": "bpf: Prevent tail call between progs attached to different hooks",
          "message": "bpf progs can be attached to kernel functions, and the attached functions\ncan take different parameters or return different return values. If\nprog attached to one kernel function tail calls prog attached to another\nkernel function, the ctx access or return value verification could be\nbypassed.\n\nFor example, if prog1 is attached to func1 which takes only 1 parameter\nand prog2 is attached to func2 which takes two parameters. Since verifier\nassumes the bpf ctx passed to prog2 is constructed based on func2's\nprototype, verifier allows prog2 to access the second parameter from\nthe bpf ctx passed to it. The problem is that verifier does not prevent\nprog1 from passing its bpf ctx to prog2 via tail call. In this case,\nthe bpf ctx passed to prog2 is constructed from func1 instead of func2,\nthat is, the assumption for ctx access verification is bypassed.\n\nAnother example, if BPF LSM prog1 is attached to hook file_alloc_security,\nand BPF LSM prog2 is attached to hook bpf_lsm_audit_rule_known. Verifier\nknows the return value rules for these two hooks, e.g. it is legal for\nbpf_lsm_audit_rule_known to return positive number 1, and it is illegal\nfor file_alloc_security to return positive number. So verifier allows\nprog2 to return positive number 1, but does not allow prog1 to return\npositive number. The problem is that verifier does not prevent prog1\nfrom calling prog2 via tail call. In this case, prog2's return value 1\nwill be used as the return value for prog1's hook file_alloc_security.\nThat is, the return value rule is bypassed.\n\nThis patch adds restriction for tail call to prevent such bypasses.\n\nSigned-off-by: Xu Kuohai <xukuohai@huawei.com>\nLink: https://lore.kernel.org/r/20240719110059.797546-4-xukuohai@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Xu Kuohai <xukuohai@huawei.com>",
          "date": "2024-07-29 13:09:26 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/core.c"
          ]
        },
        {
          "hash": "763aa759d3b2c4f95b11855e3d37b860860107e2",
          "subject": "bpf: Fix compare error in function retval_range_within",
          "message": "After checking lsm hook return range in verifier, the test case\n\"test_progs -t test_lsm\" failed, and the failure log says:\n\nlibbpf: prog 'test_int_hook': BPF program load failed: Invalid argument\nlibbpf: prog 'test_int_hook': -- BEGIN PROG LOAD LOG --\n0: R1=ctx() R10=fp0\n; int BPF_PROG(test_int_hook, struct vm_area_struct *vma, @ lsm.c:89\n0: (79) r0 = *(u64 *)(r1 +24)         ; R0_w=scalar(smin=smin32=-4095,smax=smax32=0) R1=ctx()\n\n[...]\n\n24: (b4) w0 = -1                      ; R0_w=0xffffffff\n; int BPF_PROG(test_int_hook, struct vm_area_struct *vma, @ lsm.c:89\n25: (95) exit\nAt program exit the register R0 has smin=4294967295 smax=4294967295 should have been in [-4095, 0]\n\nIt can be seen that instruction \"w0 = -1\" zero extended -1 to 64-bit\nregister r0, setting both smin and smax values of r0 to 4294967295.\nThis resulted in a false reject when r0 was checked with range [-4095, 0].\n\nGiven bpf lsm does not return 64-bit values, this patch fixes it by changing\nthe compare between r0 and return range from 64-bit operation to 32-bit\noperation for bpf lsm.\n\nFixes: 8fa4ecd49b81 (\"bpf: enforce exact retval range on subprog/callback exit\")\nSigned-off-by: Xu Kuohai <xukuohai@huawei.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20240719110059.797546-5-xukuohai@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Xu Kuohai <xukuohai@huawei.com>",
          "date": "2024-07-29 13:09:29 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "4dc7556490d77d5046844e7731d5d0a3055a3448",
          "subject": "selftests/bpf: Avoid load failure for token_lsm.c",
          "message": "The compiler optimized the two bpf progs in token_lsm.c to make return\nvalue from the bool variable in the \"return -1\" path, causing an\nunexpected rejection:\n\n0: R1=ctx() R10=fp0\n; int BPF_PROG(bpf_token_capable, struct bpf_token *token, int cap) @ bpf_lsm.c:17\n0: (b7) r6 = 0                        ; R6_w=0\n; if (my_pid == 0 || my_pid != (bpf_get_current_pid_tgid() >> 32)) @ bpf_lsm.c:19\n1: (18) r1 = 0xffffc9000102a000       ; R1_w=map_value(map=bpf_lsm.bss,ks=4,vs=5)\n3: (61) r7 = *(u32 *)(r1 +0)          ; R1_w=map_value(map=bpf_lsm.bss,ks=4,vs=5) R7_w=scalar(smin=0,smax=umax=0xffffffff,var_off=(0x0; 0xffffffff))\n4: (15) if r7 == 0x0 goto pc+11       ; R7_w=scalar(smin=umin=umin32=1,smax=umax=0xffffffff,var_off=(0x0; 0xffffffff))\n5: (67) r7 <<= 32                     ; R7_w=scalar(smax=0x7fffffff00000000,umax=0xffffffff00000000,smin32=0,smax32=umax32=0,var_off=(0x0; 0xffffffff00000000))\n6: (c7) r7 s>>= 32                    ; R7_w=scalar(smin=0xffffffff80000000,smax=0x7fffffff)\n7: (85) call bpf_get_current_pid_tgid#14      ; R0=scalar()\n8: (77) r0 >>= 32                     ; R0_w=scalar(smin=0,smax=umax=0xffffffff,var_off=(0x0; 0xffffffff))\n9: (5d) if r0 != r7 goto pc+6         ; R0_w=scalar(smin=smin32=0,smax=umax=umax32=0x7fffffff,var_off=(0x0; 0x7fffffff)) R7=scalar(smin=smin32=0,smax=umax=umax32=0x7fffffff,var_off=(0x0; 0x7fffffff))\n; if (reject_capable) @ bpf_lsm.c:21\n10: (18) r1 = 0xffffc9000102a004      ; R1_w=map_value(map=bpf_lsm.bss,ks=4,vs=5,off=4)\n12: (71) r6 = *(u8 *)(r1 +0)          ; R1_w=map_value(map=bpf_lsm.bss,ks=4,vs=5,off=4) R6_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=255,var_off=(0x0; 0xff))\n;  @ bpf_lsm.c:0\n13: (87) r6 = -r6                     ; R6_w=scalar()\n14: (67) r6 <<= 56                    ; R6_w=scalar(smax=0x7f00000000000000,umax=0xff00000000000000,smin32=0,smax32=umax32=0,var_off=(0x0; 0xff00000000000000))\n15: (c7) r6 s>>= 56                   ; R6_w=scalar(smin=smin32=-128,smax=smax32=127)\n; int BPF_PROG(bpf_token_capable, struct bpf_token *token, int cap) @ bpf_lsm.c:17\n16: (bf) r0 = r6                      ; R0_w=scalar(id=1,smin=smin32=-128,smax=smax32=127) R6_w=scalar(id=1,smin=smin32=-128,smax=smax32=127)\n17: (95) exit\nAt program exit the register R0 has smin=-128 smax=127 should have been in [-4095, 0]\n\nTo avoid this failure, change the variable type from bool to int.\n\nSigned-off-by: Xu Kuohai <xukuohai@huawei.com>\nLink: https://lore.kernel.org/r/20240719110059.797546-7-xukuohai@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Xu Kuohai <xukuohai@huawei.com>",
          "date": "2024-07-29 13:09:34 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/token_lsm.c"
          ]
        },
        {
          "hash": "2b23b6c0f03c94dbacff2abdfd45490eca9d7f6e",
          "subject": "selftests/bpf: Add return value checks for failed tests",
          "message": "The return ranges of some bpf lsm test progs can not be deduced by\nthe verifier accurately. To avoid erroneous rejections, add explicit\nreturn value checks for these progs.\n\nSigned-off-by: Xu Kuohai <xukuohai@huawei.com>\nLink: https://lore.kernel.org/r/20240719110059.797546-8-xukuohai@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Xu Kuohai <xukuohai@huawei.com>",
          "date": "2024-07-29 13:09:37 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/err.h",
            "tools/testing/selftests/bpf/progs/test_sig_in_xattr.c",
            "tools/testing/selftests/bpf/progs/test_verify_pkcs7_sig.c",
            "tools/testing/selftests/bpf/progs/verifier_global_subprogs.c"
          ]
        },
        {
          "hash": "d463dd9c9aa24b17ccb8ed76bdd7768baf857b48",
          "subject": "selftests/bpf: Add test for lsm tail call",
          "message": "Add test for lsm tail call to ensure tail call can only be used between\nbpf lsm progs attached to the same hook.\n\nSigned-off-by: Xu Kuohai <xukuohai@huawei.com>\nLink: https://lore.kernel.org/r/20240719110059.797546-9-xukuohai@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Xu Kuohai <xukuohai@huawei.com>",
          "date": "2024-07-29 13:09:41 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/test_lsm.c",
            "tools/testing/selftests/bpf/progs/lsm_tailcall.c"
          ]
        },
        {
          "hash": "04d8243b1f83251bd599fdd3ea91e89039b6a557",
          "subject": "selftests/bpf: Add verifier tests for bpf lsm",
          "message": "Add verifier tests to check bpf lsm return values and disabled hooks.\n\nSigned-off-by: Xu Kuohai <xukuohai@huawei.com>\nLink: https://lore.kernel.org/r/20240719110059.797546-10-xukuohai@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Xu Kuohai <xukuohai@huawei.com>",
          "date": "2024-07-29 13:09:45 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_lsm.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "e2854bc37344f40a47c6e04a222e65d2669e64dd",
      "merge_subject": "Merge branch 'bpf-retire-the-unsupported_ops-usage-in-struct_ops'",
      "merge_body": "Martin KaFai Lau says:\n\n====================\nbpf: Retire the unsupported_ops usage in struct_ops\n\nFrom: Martin KaFai Lau <martin.lau@kernel.org>\n\nThis series retires the unsupported_ops usage and depends on the\nnull-ness check on the cfi_stubs instead.\n\nPlease see individual patches for details.\n\nv2:\n- Fixed a gcc compiler warning on Patch 1.\n====================\n\nLink: https://lore.kernel.org/r/20240722183049.2254692-1-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-07-29 13:09:14 -0700",
      "commits": [
        {
          "hash": "e42ac14180554fa23a3312d4f921dc4ea7972fb7",
          "subject": "bpf: Check unsupported ops from the bpf_struct_ops's cfi_stubs",
          "message": "The bpf_tcp_ca struct_ops currently uses a \"u32 unsupported_ops[]\"\narray to track which ops is not supported.\n\nAfter cfi_stubs had been added, the function pointer in cfi_stubs is\nalso NULL for the unsupported ops. Thus, the \"u32 unsupported_ops[]\"\nbecomes redundant. This observation was originally brought up in the\nbpf/cfi discussion:\nhttps://lore.kernel.org/bpf/CAADnVQJoEkdjyCEJRPASjBw1QGsKYrF33QdMGc1RZa9b88bAEA@mail.gmail.com/\n\nThe recent bpf qdisc patch (https://lore.kernel.org/bpf/20240714175130.4051012-6-amery.hung@bytedance.com/)\nalso needs to specify quite many unsupported ops. It is a good time\nto clean it up.\n\nThis patch removes the need of \"u32 unsupported_ops[]\" and tests for null-ness\nin the cfi_stubs instead.\n\nTesting the cfi_stubs is done in a new function bpf_struct_ops_supported().\nThe verifier will call bpf_struct_ops_supported() when loading the\nstruct_ops program. The \".check_member\" is removed from the bpf_tcp_ca\nin this patch. \".check_member\" could still be useful for other subsytems\nto enforce other restrictions (e.g. sched_ext checks for prog->sleepable).\n\nTo keep the same error return, ENOTSUPP is used.\n\nCc: Amery Hung <ameryhung@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240722183049.2254692-2-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-07-29 12:54:13 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/bpf_struct_ops.c",
            "kernel/bpf/verifier.c",
            "net/ipv4/bpf_tcp_ca.c"
          ]
        },
        {
          "hash": "e44b4fc40cb429f6dedc4518a16221115035f654",
          "subject": "selftests/bpf: Fix the missing tramp_1 to tramp_40 ops in cfi_stubs",
          "message": "The tramp_1 to tramp_40 ops is not set in the cfi_stubs in the\nbpf_testmod_ops. It fails the struct_ops_multi_pages test after\nretiring the unsupported_ops in the earlier patch.\n\nThis patch initializes them in a loop during the bpf_testmod_init().\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240722183049.2254692-3-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-07-29 13:08:56 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c"
          ]
        },
        {
          "hash": "4009c95fede6b783802ad01f264a7a0541f5ea60",
          "subject": "selftests/bpf: Ensure the unsupported struct_ops prog cannot be loaded",
          "message": "There is an existing \"bpf_tcp_ca/unsupp_cong_op\" test to ensure\nthe unsupported tcp-cc \"get_info\" struct_ops prog cannot be loaded.\n\nThis patch adds a new test in the bpf_testmod such that the\nunsupported ops test does not depend on other kernel subsystem\nwhere its supporting ops may be changed in the future.\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240722183049.2254692-4-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
          "author": "Martin KaFai Lau <martin.lau@kernel.org>",
          "date": "2024-07-29 13:09:10 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.h",
            "tools/testing/selftests/bpf/prog_tests/test_struct_ops_module.c",
            "tools/testing/selftests/bpf/progs/unsupported_ops.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "bde0c5a7375699c80a96392800744edf93093a07",
      "merge_subject": "Merge branch 'bpf-track-find_equal_scalars-history-on-per-instruction-level'",
      "merge_body": "Eduard Zingerman says:\n\n====================\nbpf: track find_equal_scalars history on per-instruction level\n\nThis is a fix for precision tracking bug reported in [0].\nIt supersedes my previous attempt to fix similar issue in commit [1].\nHere is a minimized test case from [0]:\n\n    0:  call bpf_get_prandom_u32;\n    1:  r7 = r0;\n    2:  r8 = r0;\n    3:  call bpf_get_prandom_u32;\n    4:  if r0 > 1 goto +0;\n    /* --- checkpoint #1: r7.id=1, r8.id=1 --- */\n    5:  if r8 >= r0 goto 9f;\n    6:  r8 += r8;\n    /* --- checkpoint #2: r7.id=1, r8.id=0 --- */\n    7:  if r7 == 0 goto 9f;\n    8:  r0 /= 0;\n    /* --- checkpoint #3 --- */\n    9:  r0 = 42;\n    10: exit;\n\nW/o this fix verifier incorrectly assumes that instruction at label\n(8) is unreachable. The issue is caused by failure to infer\nprecision mark for r0 at checkpoint #1:\n- first verification path is:\n  - (0-4): r0 range [0,1];\n  - (5): r8 range [0,0], propagated to r7;\n  - (6): r8.id is reset;\n  - (7): jump is predicted to happen;\n  - (9-10): safe exit.\n- when jump at (7) is predicted mark_chain_precision() for r7 is\n  called and backtrack_insn() proceeds as follows:\n  - at (7) r7 is marked as precise;\n  - at (5) r8 is not currently tracked and thus r0 is not marked;\n  - at (4-5) boundary logic from [1] is triggered and r7,r8 are marked\n    as precise;\n  - => r0 precision mark is missed.\n- when second branch of (4) is considered, verifier prunes the state\n  because r0 is not marked as precise in the visited state.\n\nBasically, backtracking logic fails to notice that at (5)\nrange information is gained for both r7 and r8, and thus both\nr8 and r0 have to be marked as precise.\nThis happens because [1] can only account for such range\ntransfers at parent/child state boundaries.\n\nThe solution suggested by Andrii Nakryiko in [0] is to use jump\nhistory to remember which registers gained range as a result of\nfind_equal_scalars() [renamed to sync_linked_regs()] and use\nthis information in backtrack_insn().\nWhich is what this patch-set does.\n\nThe patch-set uses u64 value as a vector of 10-bit values that\nidentify registers gaining range in find_equal_scalars().\nThis amounts to maximum of 6 possible values.\nTo check if such capacity is sufficient I've instrumented kernel\nto track a histogram for maximal amount of registers that gain range\nin find_equal_scalars per program verification [2].\nMeasurements done for verifier selftests and Cilium bpf object files\nfrom [3] show that number of such registers is *always* <= 4 and\nin 98% of cases it is <= 2.\n\nWhen tested on a subset of selftests identified by\nselftests/bpf/veristat.cfg and Cilium bpf object files from [3]\nthis patch-set has minimal verification performance impact:\n\nFile                      Program                   Insns   (DIFF)  States (DIFF)\n------------------------  ------------------------  --------------  -------------\nbpf_host.o                tail_handle_nat_fwd_ipv4    -75 (-0.61%)    -3 (-0.39%)\npyperf600_nounroll.bpf.o  on_event                  +1673 (+0.33%)    +3 (+0.01%)\n\n[0] https://lore.kernel.org/bpf/CAEf4BzZ0xidVCqB47XnkXcNhkPWF6_nTV7yt+_Lf0kcFEut2Mg@mail.gmail.com/\n[1] commit 904e6ddf4133 (\"bpf: Use scalar ids in mark_chain_precision()\")\n[2] https://github.com/eddyz87/bpf/tree/find-equal-scalars-in-jump-history-with-stats\n[3] https://github.com/anakryiko/cilium\n\nChanges:\n- v2 -> v3:\n  A number of stylistic changes suggested by Andrii:\n  - renamings:\n    - struct reg_or_spill   -> linked_reg;\n    - find_equal_scalars()  -> collect_linked_regs;\n    - copy_known_reg()      -> sync_linked_regs;\n  - collect_linked_regs() now returns linked regs set of\n    size 2 or larger;\n  - dropped usage of bit fields in struct linked_reg;\n  - added a patch changing references to find_equal_scalars() in\n    selftests comments.\n- v1 -> v2:\n  - patch \"bpf: replace env->cur_hist_ent with a getter function\" is\n    dropped (Andrii);\n  - added structure linked_regs and helper functions to [de]serialize\n    u64 value as such structure (Andrii);\n  - bt_set_equal_scalars() renamed to bt_sync_linked_regs(), moved to\n    start and end of backtrack_insn() in order to untie linked\n    register logic from conditional jumps backtracking.\n    Andrii requested a more radical change of moving linked registers\n    processing to bt_set_xxx() functions, I did an experiment in this\n    direction:\n    https://github.com/eddyz87/bpf/tree/find-equal-scalars-in-jump-history--linked-regs-in-bt-set-reg\n    the end result of the experiment seems much uglier than version\n    presented in v2.\n\nRevisions:\n- v1: https://lore.kernel.org/bpf/20240222005005.31784-1-eddyz87@gmail.com/\n- v2: https://lore.kernel.org/bpf/20240705205851.2635794-1-eddyz87@gmail.com/\n====================\n\nLink: https://lore.kernel.org/r/20240718202357.1746514-1-eddyz87@gmail.com\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2024-07-29 12:53:27 -0700",
      "commits": [
        {
          "hash": "4bf79f9be434e000c8e12fe83b2f4402480f1460",
          "subject": "bpf: Track equal scalars history on per-instruction level",
          "message": "Use bpf_verifier_state->jmp_history to track which registers were\nupdated by find_equal_scalars() (renamed to collect_linked_regs())\nwhen conditional jump was verified. Use recorded information in\nbacktrack_insn() to propagate precision.\n\nE.g. for the following program:\n\n            while verifying instructions\n  1: r1 = r0              |\n  2: if r1 < 8  goto ...  | push r0,r1 as linked registers in jmp_history\n  3: if r0 > 16 goto ...  | push r0,r1 as linked registers in jmp_history\n  4: r2 = r10             |\n  5: r2 += r0             v mark_chain_precision(r0)\n\n            while doing mark_chain_precision(r0)\n  5: r2 += r0             | mark r0 precise\n  4: r2 = r10             |\n  3: if r0 > 16 goto ...  | mark r0,r1 as precise\n  2: if r1 < 8  goto ...  | mark r0,r1 as precise\n  1: r1 = r0              v\n\nTechnically, do this as follows:\n- Use 10 bits to identify each register that gains range because of\n  sync_linked_regs():\n  - 3 bits for frame number;\n  - 6 bits for register or stack slot number;\n  - 1 bit to indicate if register is spilled.\n- Use u64 as a vector of 6 such records + 4 bits for vector length.\n- Augment struct bpf_jmp_history_entry with a field 'linked_regs'\n  representing such vector.\n- When doing check_cond_jmp_op() remember up to 6 registers that\n  gain range because of sync_linked_regs() in such a vector.\n- Don't propagate range information and reset IDs for registers that\n  don't fit in 6-value vector.\n- Push a pair {instruction index, linked registers vector}\n  to bpf_verifier_state->jmp_history.\n- When doing backtrack_insn() check if any of recorded linked\n  registers is currently marked precise, if so mark all linked\n  registers as precise.\n\nThis also requires fixes for two test_verifier tests:\n- precise: test 1\n- precise: test 2\n\nBoth tests contain the following instruction sequence:\n\n19: (bf) r2 = r9                      ; R2=scalar(id=3) R9=scalar(id=3)\n20: (a5) if r2 < 0x8 goto pc+1        ; R2=scalar(id=3,umin=8)\n21: (95) exit\n22: (07) r2 += 1                      ; R2_w=scalar(id=3+1,...)\n23: (bf) r1 = r10                     ; R1_w=fp0 R10=fp0\n24: (07) r1 += -8                     ; R1_w=fp-8\n25: (b7) r3 = 0                       ; R3_w=0\n26: (85) call bpf_probe_read_kernel#113\n\nThe call to bpf_probe_read_kernel() at (26) forces r2 to be precise.\nPreviously, this forced all registers with same id to become precise\nimmediately when mark_chain_precision() is called.\nAfter this change, the precision is propagated to registers sharing\nsame id only when 'if' instruction is backtracked.\nHence verification log for both tests is changed:\nregs=r2,r9 -> regs=r2 for instructions 25..20.\n\nFixes: 904e6ddf4133 (\"bpf: Use scalar ids in mark_chain_precision()\")\nReported-by: Hao Sun <sunhao.th@gmail.com>\nSuggested-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240718202357.1746514-2-eddyz87@gmail.com\n\nCloses: https://lore.kernel.org/bpf/CAEf4BzZ0xidVCqB47XnkXcNhkPWF6_nTV7yt+_Lf0kcFEut2Mg@mail.gmail.com/",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 12:53:10 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c",
            "tools/testing/selftests/bpf/verifier/precise.c"
          ]
        },
        {
          "hash": "842edb5507a1038e009d27e69d13b94b6f085763",
          "subject": "bpf: Remove mark_precise_scalar_ids()",
          "message": "Function mark_precise_scalar_ids() is superseded by\nbt_sync_linked_regs() and equal scalars tracking in jump history.\nmark_precise_scalar_ids() propagates precision over registers sharing\nsame ID on parent/child state boundaries, while jump history records\nallow bt_sync_linked_regs() to propagate same information with\ninstruction level granularity, which is strictly more precise.\n\nThis commit removes mark_precise_scalar_ids() and updates test cases\nin progs/verifier_scalar_ids to reflect new verifier behavior.\n\nThe tests are updated in the following manner:\n- mark_precise_scalar_ids() propagated precision regardless of\n  presence of conditional jumps, while new jump history based logic\n  only kicks in when conditional jumps are present.\n  Hence test cases are augmented with conditional jumps to still\n  trigger precision propagation.\n- As equal scalars tracking no longer relies on parent/child state\n  boundaries some test cases are no longer interesting,\n  such test cases are removed, namely:\n  - precision_same_state and precision_cross_state are superseded by\n    linked_regs_bpf_k;\n  - precision_same_state_broken_link and equal_scalars_broken_link\n    are superseded by linked_regs_broken_link.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240718202357.1746514-3-eddyz87@gmail.com",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 12:53:14 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_scalar_ids.c",
            "tools/testing/selftests/bpf/verifier/precise.c"
          ]
        },
        {
          "hash": "bebc17b1c03b224a0b4aec6a171815e39f8ba9bc",
          "subject": "selftests/bpf: Tests for per-insn sync_linked_regs() precision tracking",
          "message": "Add a few test cases to verify precision tracking for scalars gaining\nrange because of sync_linked_regs():\n- check what happens when more than 6 registers might gain range in\n  sync_linked_regs();\n- check if precision is propagated correctly when operand of\n  conditional jump gained range in sync_linked_regs() and one of\n  linked registers is marked precise;\n- check if precision is propagated correctly when operand of\n  conditional jump gained range in sync_linked_regs() and a\n  other-linked operand of the conditional jump is marked precise;\n- add a minimized reproducer for precision tracking bug reported in [0];\n- Check that mark_chain_precision() for one of the conditional jump\n  operands does not trigger equal scalars precision propagation.\n\n[0] https://lore.kernel.org/bpf/CAEf4BzZ0xidVCqB47XnkXcNhkPWF6_nTV7yt+_Lf0kcFEut2Mg@mail.gmail.com/\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240718202357.1746514-4-eddyz87@gmail.com",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 12:53:17 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_scalar_ids.c"
          ]
        },
        {
          "hash": "cfbf25481d6dec0089c99c9d33a2ea634fe8f008",
          "subject": "selftests/bpf: Update comments find_equal_scalars->sync_linked_regs",
          "message": "find_equal_scalars() is renamed to sync_linked_regs(),\nthis commit updates existing references in the selftests comments.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240718202357.1746514-5-eddyz87@gmail.com",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-07-29 12:53:24 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "a1010fce1c0c2ce3b305aa6e8ff70e86f99e3226",
      "merge_subject": "Merge branch 'use-overflow-h-helpers-to-check-for-overflows'",
      "merge_body": "Shung-Hsi Yu says:\n\n====================\nUse overflow.h helpers to check for overflows\n\nThis patch set refactors kernel/bpf/verifier.c to use type-agnostic, generic\noverflow-check helpers defined in include/linux/overflow.h to check for addition\nand subtraction overflow, and drop the signed_*_overflows() helpers we currently\nhave in kernel/bpf/verifier.c; with a fix for overflow check in adjust_jmp_off()\nin patch 1.\n\nThere should be no functional change in how the verifier works and  the main\nmotivation is to make future refactoring[1] easier.\n\nWhile check_mul_overflow() also exists and could potentially replace what\nwe have in scalar*_min_max_mul(), it does not help with refactoring and\nwould either change how the verifier works (e.g. lifting restriction on\numax<=U32_MAX and u32_max<=U16_MAX) or make the code slightly harder to\nread, so it is left for future endeavour.\n\nChanges from v2 <https://lore.kernel.org/r/20240701055907.82481-1-shung-hsi.yu@suse.com>\n- add fix for 5337ac4c9b80 (\"bpf: Fix the corner case with may_goto and jump to\n  the 1st insn.\") to correct the overflow check for general jump instructions\n- adapt to changes in commit 5337ac4c9b80 (\"bpf: Fix the corner case with\n  may_goto and jump to the 1st insn.\")\n  - refactor in adjust_jmp_off() as well and remove signed_add16_overflow()\n\nChanges from v1 <https://lore.kernel.org/r/20240623070324.12634-1-shung-hsi.yu@suse.com>:\n- use pointers to values in dst_reg directly as the sum/diff pointer and\n  remove the else branch (Jiri)\n- change local variables to be dst_reg pointers instead of src_reg values\n- include comparison of generated assembly before & after the change\n  (Alexei)\n\n1: https://github.com/kernel-patches/bpf/pull/7205/commits\n====================\n\nLink: https://lore.kernel.org/r/20240712080127.136608-1-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-07-12 08:54:17 -0700",
      "commits": [
        {
          "hash": "4a04b4f0de59dd5c621e78f15803ee0b0544eeb8",
          "subject": "bpf: fix overflow check in adjust_jmp_off()",
          "message": "adjust_jmp_off() incorrectly used the insn->imm field for all overflow check,\nwhich is incorrect as that should only be done or the BPF_JMP32 | BPF_JA case,\nnot the general jump instruction case. Fix it by using insn->off for overflow\ncheck in the general case.\n\nFixes: 5337ac4c9b80 (\"bpf: Fix the corner case with may_goto and jump to the 1st insn.\")\nSigned-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20240712080127.136608-2-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Shung-Hsi Yu <shung-hsi.yu@suse.com>",
          "date": "2024-07-12 08:54:07 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "28a4411076b254c67842348e3b25c2fb41a94cad",
          "subject": "bpf: use check_add_overflow() to check for addition overflows",
          "message": "signed_add*_overflows() was added back when there was no overflow-check\nhelper. With the introduction of such helpers in commit f0907827a8a91\n(\"compiler.h: enable builtin overflow checkers and add fallback code\"), we\ncan drop signed_add*_overflows() in kernel/bpf/verifier.c and use the\ngeneric check_add_overflow() instead.\n\nThis will make future refactoring easier, and takes advantage of\ncompiler-emitted hardware instructions that efficiently implement these\nchecks.\n\nAfter the change GCC 13.3.0 generates cleaner assembly on x86_64:\n\n\terr = adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n   13625:\tmov    0x28(%rbx),%r9  /*  r9 = src_reg->smin_value */\n   13629:\tmov    0x30(%rbx),%rcx /* rcx = src_reg->smax_value */\n   ...\n\tif (check_add_overflow(*dst_smin, src_reg->smin_value, dst_smin) ||\n   141c1:\tmov    %r9,%rax\n   141c4:\tadd    0x28(%r12),%rax\n   141c9:\tmov    %rax,0x28(%r12)\n   141ce:\tjo     146e4 <adjust_reg_min_max_vals+0x1294>\n\t    check_add_overflow(*dst_smax, src_reg->smax_value, dst_smax)) {\n   141d4:\tadd    0x30(%r12),%rcx\n   141d9:\tmov    %rcx,0x30(%r12)\n\tif (check_add_overflow(*dst_smin, src_reg->smin_value, dst_smin) ||\n   141de:\tjo     146e4 <adjust_reg_min_max_vals+0x1294>\n   ...\n\t\t*dst_smin = S64_MIN;\n   146e4:\tmovabs $0x8000000000000000,%rax\n   146ee:\tmov    %rax,0x28(%r12)\n\t\t*dst_smax = S64_MAX;\n   146f3:\tsub    $0x1,%rax\n   146f7:\tmov    %rax,0x30(%r12)\n\nBefore the change it gives:\n\n\ts64 smin_val = src_reg->smin_value;\n     675:\tmov    0x28(%rsi),%r8\n\ts64 smax_val = src_reg->smax_value;\n\tu64 umin_val = src_reg->umin_value;\n\tu64 umax_val = src_reg->umax_value;\n     679:\tmov    %rdi,%rax /* rax = dst_reg */\n\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n     67c:\tmov    0x28(%rdi),%rdi /* rdi = dst_reg->smin_value */\n\tu64 umin_val = src_reg->umin_value;\n     680:\tmov    0x38(%rsi),%rdx\n\tu64 umax_val = src_reg->umax_value;\n     684:\tmov    0x40(%rsi),%rcx\n\ts64 res = (s64)((u64)a + (u64)b);\n     688:\tlea    (%r8,%rdi,1),%r9 /* r9 = dst_reg->smin_value + src_reg->smin_value */\n\treturn res < a;\n     68c:\tcmp    %r9,%rdi\n     68f:\tsetg   %r10b /* r10b = (dst_reg->smin_value + src_reg->smin_value) > dst_reg->smin_value */\n\tif (b < 0)\n     693:\ttest   %r8,%r8\n     696:\tjs     72b <scalar_min_max_add+0xbb>\n\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n     69c:\tmovabs $0x7fffffffffffffff,%rdi\n\ts64 smax_val = src_reg->smax_value;\n     6a6:\tmov    0x30(%rsi),%r8\n\t\tdst_reg->smin_value = S64_MIN;\n     6aa:\t00 00 00 \tmovabs $0x8000000000000000,%rsi\n\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n     6b4:\ttest   %r10b,%r10b /* (dst_reg->smin_value + src_reg->smin_value) > dst_reg->smin_value ? goto 6cb */\n     6b7:\tjne    6cb <scalar_min_max_add+0x5b>\n\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n     6b9:\tmov    0x30(%rax),%r10   /* r10 = dst_reg->smax_value */\n\ts64 res = (s64)((u64)a + (u64)b);\n     6bd:\tlea    (%r10,%r8,1),%r11 /* r11 = dst_reg->smax_value + src_reg->smax_value */\n\tif (b < 0)\n     6c1:\ttest   %r8,%r8\n     6c4:\tjs     71e <scalar_min_max_add+0xae>\n\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n     6c6:\tcmp    %r11,%r10 /* (dst_reg->smax_value + src_reg->smax_value) <= dst_reg->smax_value ? goto 723 */\n     6c9:\tjle    723 <scalar_min_max_add+0xb3>\n\t} else {\n\t\tdst_reg->smin_value += smin_val;\n\t\tdst_reg->smax_value += smax_val;\n\t}\n     6cb:\tmov    %rsi,0x28(%rax)\n     ...\n     6d5:\tmov    %rdi,0x30(%rax)\n     ...\n\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n     71e:\tcmp    %r11,%r10\n     721:\tjl     6cb <scalar_min_max_add+0x5b>\n\t\tdst_reg->smin_value += smin_val;\n     723:\tmov    %r9,%rsi\n\t\tdst_reg->smax_value += smax_val;\n     726:\tmov    %r11,%rdi\n     729:\tjmp    6cb <scalar_min_max_add+0x5b>\n\t\treturn res > a;\n     72b:\tcmp    %r9,%rdi\n     72e:\tsetl   %r10b\n     732:\tjmp    69c <scalar_min_max_add+0x2c>\n     737:\tnopw   0x0(%rax,%rax,1)\n\nNote: unlike adjust_ptr_min_max_vals() and scalar*_min_max_add(), it is\nnecessary to introduce intermediate variable in adjust_jmp_off() to keep\nthe functional behavior unchanged. Without an intermediate variable\nimm/off will be altered even on overflow.\n\nSuggested-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20240712080127.136608-3-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Shung-Hsi Yu <shung-hsi.yu@suse.com>",
          "date": "2024-07-12 08:54:08 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "deac5871eb0751454cb80b3ff6b69e42a6c1bab2",
          "subject": "bpf: use check_sub_overflow() to check for subtraction overflows",
          "message": "Similar to previous patch that drops signed_add*_overflows() and uses\n(compiler) builtin-based check_add_overflow(), do the same for\nsigned_sub*_overflows() and replace them with the generic\ncheck_sub_overflow() to make future refactoring easier and have the\nchecks implemented more efficiently.\n\nUnsigned overflow check for subtraction does not use helpers and are\nsimple enough already, so they're left untouched.\n\nAfter the change GCC 13.3.0 generates cleaner assembly on x86_64:\n\n\tif (check_sub_overflow(*dst_smin, src_reg->smax_value, dst_smin) ||\n   139bf:\tmov    0x28(%r12),%rax\n   139c4:\tmov    %edx,0x54(%r12)\n   139c9:\tsub    %r11,%rax\n   139cc:\tmov    %rax,0x28(%r12)\n   139d1:\tjo     14627 <adjust_reg_min_max_vals+0x1237>\n\t    check_sub_overflow(*dst_smax, src_reg->smin_value, dst_smax)) {\n   139d7:\tmov    0x30(%r12),%rax\n   139dc:\tsub    %r9,%rax\n   139df:\tmov    %rax,0x30(%r12)\n\tif (check_sub_overflow(*dst_smin, src_reg->smax_value, dst_smin) ||\n   139e4:\tjo     14627 <adjust_reg_min_max_vals+0x1237>\n   ...\n\t\t*dst_smin = S64_MIN;\n   14627:\tmovabs $0x8000000000000000,%rax\n   14631:\tmov    %rax,0x28(%r12)\n\t\t*dst_smax = S64_MAX;\n   14636:\tsub    $0x1,%rax\n   1463a:\tmov    %rax,0x30(%r12)\n\nBefore the change it gives:\n\n\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n   13a50:\tmov    0x28(%r12),%rdi\n   13a55:\tmov    %edx,0x54(%r12)\n\t\tdst_reg->smax_value = S64_MAX;\n   13a5a:\tmovabs $0x7fffffffffffffff,%rdx\n   13a64:\tmov    %eax,0x50(%r12)\n\t\tdst_reg->smin_value = S64_MIN;\n   13a69:\tmovabs $0x8000000000000000,%rax\n\ts64 res = (s64)((u64)a - (u64)b);\n   13a73:\tmov    %rdi,%rsi\n   13a76:\tsub    %rcx,%rsi\n\tif (b < 0)\n   13a79:\ttest   %rcx,%rcx\n   13a7c:\tjs     145ea <adjust_reg_min_max_vals+0x119a>\n\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n   13a82:\tcmp    %rsi,%rdi\n   13a85:\tjl     13ac7 <adjust_reg_min_max_vals+0x677>\n\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n   13a87:\tmov    0x30(%r12),%r8\n\ts64 res = (s64)((u64)a - (u64)b);\n   13a8c:\tmov    %r8,%rax\n   13a8f:\tsub    %r9,%rax\n\treturn res > a;\n   13a92:\tcmp    %rax,%r8\n   13a95:\tsetl   %sil\n\tif (b < 0)\n   13a99:\ttest   %r9,%r9\n   13a9c:\tjs     147d1 <adjust_reg_min_max_vals+0x1381>\n\t\tdst_reg->smax_value = S64_MAX;\n   13aa2:\tmovabs $0x7fffffffffffffff,%rdx\n\t\tdst_reg->smin_value = S64_MIN;\n   13aac:\tmovabs $0x8000000000000000,%rax\n\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n   13ab6:\ttest   %sil,%sil\n   13ab9:\tjne    13ac7 <adjust_reg_min_max_vals+0x677>\n\t\tdst_reg->smin_value -= smax_val;\n   13abb:\tmov    %rdi,%rax\n\t\tdst_reg->smax_value -= smin_val;\n   13abe:\tmov    %r8,%rdx\n\t\tdst_reg->smin_value -= smax_val;\n   13ac1:\tsub    %rcx,%rax\n\t\tdst_reg->smax_value -= smin_val;\n   13ac4:\tsub    %r9,%rdx\n   13ac7:\tmov    %rax,0x28(%r12)\n   ...\n   13ad1:\tmov    %rdx,0x30(%r12)\n   ...\n\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n   145ea:\tcmp    %rsi,%rdi\n   145ed:\tjg     13ac7 <adjust_reg_min_max_vals+0x677>\n   145f3:\tjmp    13a87 <adjust_reg_min_max_vals+0x637>\n\nSuggested-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nLink: https://lore.kernel.org/r/20240712080127.136608-4-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Shung-Hsi Yu <shung-hsi.yu@suse.com>",
          "date": "2024-07-12 08:54:08 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "bf977ee4a9e2ad8a41b3a2497aada5e7eb09eaea",
      "merge_subject": "Merge branch 'fix-compiler-warnings-looking-for-suggestions'",
      "merge_body": "Rafael Passos says:\n\n====================\nFix compiler warnings, looking for suggestions\n\nHi,\nThis patchset has a few fixes to compiler warnings.\nI am studying the BPF subsystem and wish to bring more tangible contributions.\nI would appreciate receiving suggestions on things to investigate.\nI also documented a bit in my blog. I could help with docs here, too.\nhttps://rcpassos.me/post/linux-ebpf-understanding-kernel-level-mechanics\nThanks!\n\nChangelog V1 -> V2:\n- rebased all commits to updated for-next base\n- removes new cases of the extra parameter for bpf_jit_binary_pack_finalize\n- built and tested for ARM64\n- sent the series for the test workflow:\n  https://github.com/kernel-patches/bpf/pull/7198\n====================\n\nAcked-by: Puranjay Mohan <puranjay@kernel.org>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nLink: https://lore.kernel.org/r/20240615022641.210320-1-rafael@rcpassos.me\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-06-20 19:50:33 -0700",
      "commits": [
        {
          "hash": "9919c5c98cb25dbf7e76aadb9beab55a2a25f830",
          "subject": "bpf: remove unused parameter in bpf_jit_binary_pack_finalize",
          "message": "Fixes a compiler warning. the bpf_jit_binary_pack_finalize function\nwas taking an extra bpf_prog parameter that went unused.\nThis removves it and updates the callers accordingly.\n\nSigned-off-by: Rafael Passos <rafael@rcpassos.me>\nLink: https://lore.kernel.org/r/20240615022641.210320-2-rafael@rcpassos.me\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Rafael Passos <rafael@rcpassos.me>",
          "date": "2024-06-20 19:50:26 -0700",
          "modified_files": [
            "arch/arm64/net/bpf_jit_comp.c",
            "arch/powerpc/net/bpf_jit_comp.c",
            "arch/riscv/net/bpf_jit_core.c",
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/filter.h",
            "kernel/bpf/core.c"
          ]
        },
        {
          "hash": "ab224b9ef7c4eaa752752455ea79bd7022209d5d",
          "subject": "bpf: remove unused parameter in __bpf_free_used_btfs",
          "message": "Fixes a compiler warning. The __bpf_free_used_btfs function\nwas taking an extra unused struct bpf_prog_aux *aux param\n\nSigned-off-by: Rafael Passos <rafael@rcpassos.me>\nLink: https://lore.kernel.org/r/20240615022641.210320-3-rafael@rcpassos.me\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Rafael Passos <rafael@rcpassos.me>",
          "date": "2024-06-20 19:50:26 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "21ab4980e02d495174bc64c00ceb4d3cf87fadb1",
          "subject": "bpf: remove redeclaration of new_n in bpf_verifier_vlog",
          "message": "This new_n is defined in the start of this function.\nIts value is overwritten by `new_n = min(n, log->len_total);`\na couple lines before my change,\nrendering the shadow declaration unnecessary.\n\nSigned-off-by: Rafael Passos <rafael@rcpassos.me>\nLink: https://lore.kernel.org/r/20240615022641.210320-4-rafael@rcpassos.me\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Rafael Passos <rafael@rcpassos.me>",
          "date": "2024-06-20 19:50:26 -0700",
          "modified_files": [
            "kernel/bpf/log.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "3b06304370931f90cd6f50ea9dd55603429b13dc",
      "merge_subject": "Merge branch 'bpf-verifier-correct-tail_call_reachable-for-bpf-prog'",
      "merge_body": "Leon Hwang says:\n\n====================\nbpf, verifier: Correct tail_call_reachable for bpf prog\n\nIt's confusing to inspect 'prog->aux->tail_call_reachable' with drgn[0],\nwhen bpf prog has tail call but 'tail_call_reachable' is false.\n\nThis patch corrects 'tail_call_reachable' when bpf prog has tail call.\n\nTherefore, it's unnecessary to detect tail call in x86 jit. Let's remove\nit.\n\nChanges:\nv1 -> v2:\n* Address comment from Yonghong:\n  * Remove unnecessary tail call detection in x86 jit.\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\n---\n\nLinks:\n[0] https://github.com/osandov/drgn\n====================\n\nLink: https://lore.kernel.org/r/20240610124224.34673-1-hffilwlqm@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-06-20 19:48:29 -0700",
      "commits": [
        {
          "hash": "01793ed86b5d7df1e956520b5474940743eb7ed8",
          "subject": "bpf, verifier: Correct tail_call_reachable for bpf prog",
          "message": "It's confusing to inspect 'prog->aux->tail_call_reachable' with drgn[0],\nwhen bpf prog has tail call but 'tail_call_reachable' is false.\n\nThis patch corrects 'tail_call_reachable' when bpf prog has tail call.\n\nSigned-off-by: Leon Hwang <hffilwlqm@gmail.com>\nLink: https://lore.kernel.org/r/20240610124224.34673-2-hffilwlqm@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Leon Hwang <hffilwlqm@gmail.com>",
          "date": "2024-06-20 19:48:29 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "f663a03c8e35c5156bad073a4a8f5e673d656e3f",
          "subject": "bpf, x64: Remove tail call detection",
          "message": "As 'prog->aux->tail_call_reachable' is correct for tail call present,\nit's unnecessary to detect tail call in x86 jit.\n\nTherefore, let's remove it.\n\nSigned-off-by: Leon Hwang <hffilwlqm@gmail.com>\nLink: https://lore.kernel.org/r/20240610124224.34673-3-hffilwlqm@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Leon Hwang <hffilwlqm@gmail.com>",
          "date": "2024-06-20 19:48:29 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "bfbcb2c9d2978a28e9f0a77100170dc14fcf7c79",
      "merge_subject": "Merge branch 'bpf-fix-missed-var_off-related-to-movsx-in-verifier'",
      "merge_body": "Yonghong Song says:\n\n====================\nbpf: Fix missed var_off related to movsx in verifier\n\nZac reported a verification issue ([1]) where verification unexpectedly succeeded.\nThis is due to missing proper var_off setting in verifier related to\nmovsx insn. I found another similar issue as well. This patch set fixed\nboth problems and added three inline asm tests to test these fixes.\n\n  [1] https://lore.kernel.org/bpf/CAADnVQLPU0Shz7dWV4bn2BgtGdxN3uFHPeobGBA72tpg5Xoykw@mail.gmail.com/\n====================\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240615174621.3994321-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-06-17 10:45:56 -0700",
      "commits": [
        {
          "hash": "380d5f89a4815ff88461a45de2fb6f28533df708",
          "subject": "bpf: Add missed var_off setting in set_sext32_default_val()",
          "message": "Zac reported a verification failure and Alexei reproduced the issue\nwith a simple reproducer ([1]). The verification failure is due to missed\nsetting for var_off.\n\nThe following is the reproducer in [1]:\n  0: R1=ctx() R10=fp0\n  0: (71) r3 = *(u8 *)(r10 -387)        ;\n     R3_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=255,var_off=(0x0; 0xff)) R10=fp0\n  1: (bc) w7 = (s8)w3                   ;\n     R3_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=255,var_off=(0x0; 0xff))\n     R7_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=127,var_off=(0x0; 0x7f))\n  2: (36) if w7 >= 0x2533823b goto pc-3\n     mark_precise: frame0: last_idx 2 first_idx 0 subseq_idx -1\n     mark_precise: frame0: regs=r7 stack= before 1: (bc) w7 = (s8)w3\n     mark_precise: frame0: regs=r3 stack= before 0: (71) r3 = *(u8 *)(r10 -387)\n  2: R7_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=127,var_off=(0x0; 0x7f))\n  3: (b4) w0 = 0                        ; R0_w=0\n  4: (95) exit\n\nNote that after insn 1, the var_off for R7 is (0x0; 0x7f). This is not correct\nsince upper 24 bits of w7 could be 0 or 1. So correct var_off should be\n(0x0; 0xffffffff). Missing var_off setting in set_sext32_default_val() caused later\nincorrect analysis in zext_32_to_64(dst_reg) and reg_bounds_sync(dst_reg).\n\nTo fix the issue, set var_off correctly in set_sext32_default_val(). The correct\nreg state after insn 1 becomes:\n  1: (bc) w7 = (s8)w3                   ;\n     R3_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=255,var_off=(0x0; 0xff))\n     R7_w=scalar(smin=0,smax=umax=0xffffffff,smin32=-128,smax32=127,var_off=(0x0; 0xffffffff))\nand at insn 2, the verifier correctly determines either branch is possible.\n\n  [1] https://lore.kernel.org/bpf/CAADnVQLPU0Shz7dWV4bn2BgtGdxN3uFHPeobGBA72tpg5Xoykw@mail.gmail.com/\n\nFixes: 8100928c8814 (\"bpf: Support new sign-extension mov insns\")\nReported-by: Zac Ecob <zacecob@protonmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240615174626.3994813-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-06-17 10:45:46 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "44b7f7151dfc2e0947f39ed4b9bc4b0c2ccd46fc",
          "subject": "bpf: Add missed var_off setting in coerce_subreg_to_size_sx()",
          "message": "In coerce_subreg_to_size_sx(), for the case where upper\nsign extension bits are the same for smax32 and smin32\nvalues, we missed to setup properly. This is especially\nproblematic if both smax32 and smin32's sign extension\nbits are 1.\n\nThe following is a simple example illustrating the inconsistent\nverifier states due to missed var_off:\n\n  0: (85) call bpf_get_prandom_u32#7    ; R0_w=scalar()\n  1: (bf) r3 = r0                       ; R0_w=scalar(id=1) R3_w=scalar(id=1)\n  2: (57) r3 &= 15                      ; R3_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=15,var_off=(0x0; 0xf))\n  3: (47) r3 |= 128                     ; R3_w=scalar(smin=umin=smin32=umin32=128,smax=umax=smax32=umax32=143,var_off=(0x80; 0xf))\n  4: (bc) w7 = (s8)w3\n  REG INVARIANTS VIOLATION (alu): range bounds violation u64=[0xffffff80, 0x8f] s64=[0xffffff80, 0x8f]\n    u32=[0xffffff80, 0x8f] s32=[0x80, 0xffffff8f] var_off=(0x80, 0xf)\n\nThe var_off=(0x80, 0xf) is not correct, and the correct one should\nbe var_off=(0xffffff80; 0xf) since from insn 3, we know that at\ninsn 4, the sign extension bits will be 1. This patch fixed this\nissue by setting var_off properly.\n\nFixes: 8100928c8814 (\"bpf: Support new sign-extension mov insns\")\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240615174632.3995278-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-06-17 10:45:46 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "a62293c33b058415237c55058a6d20de313a2e61",
          "subject": "selftests/bpf: Add a few tests to cover",
          "message": "Add three unit tests in verifier_movsx.c to cover\ncases where missed var_off setting can cause\nunexpected verification success or failure.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240615174637.3995589-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-06-17 10:45:47 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_movsx.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "cdbde084d163835ef41cabb59be2292bb0421c51",
      "merge_subject": "Merge branch 'bpf-make-trusted-args-nullable'",
      "merge_body": "Vadim Fedorenko says:\n\n====================\nbpf: make trusted args nullable\n\nCurrent verifier checks for the arg to be nullable after checking for\ncertain pointer types. It prevents programs to pass NULL to kfunc args\neven if they are marked as nullable. This patchset adjusts verifier and\nchanges bpf crypto kfuncs to allow null for IV parameter which is\noptional for some ciphers. Benchmark shows ~4% improvements when there\nis no need to initialise 0-sized dynptr.\n\nv3:\n- add special selftest for nullable parameters\nv2:\n- adjust kdoc accordingly\n====================\n\nLink: https://lore.kernel.org/r/20240613211817.1551967-1-vadfed@meta.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-06-13 16:33:05 -0700",
      "commits": [
        {
          "hash": "a90797993afcb0eaf6bf47a062ff47eb3810a6d5",
          "subject": "bpf: verifier: make kfuncs args nullalble",
          "message": "Some arguments to kfuncs might be NULL in some cases. But currently it's\nnot possible to pass NULL to any BTF structures because the check for\nthe suffix is located after all type checks. Move it to earlier place\nto allow nullable args.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Vadim Fedorenko <vadfed@meta.com>\nLink: https://lore.kernel.org/r/20240613211817.1551967-2-vadfed@meta.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Vadim Fedorenko <vadfed@meta.com>",
          "date": "2024-06-13 16:33:04 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "65d6d61d25968d1f13a478a6f303ed8d6b978a77",
          "subject": "bpf: crypto: make state and IV dynptr nullable",
          "message": "Some ciphers do not require state and IV buffer, but with current\nimplementation 0-sized dynptr is always needed. With adjustment to\nverifier we can provide NULL instead of 0-sized dynptr. Make crypto\nkfuncs ready for this.\n\nReviewed-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Vadim Fedorenko <vadfed@meta.com>\nLink: https://lore.kernel.org/r/20240613211817.1551967-3-vadfed@meta.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Vadim Fedorenko <vadfed@meta.com>",
          "date": "2024-06-13 16:33:04 -0700",
          "modified_files": [
            "kernel/bpf/crypto.c"
          ]
        },
        {
          "hash": "9363dc8ddc4e222c4259013ae5428070712910b9",
          "subject": "selftests: bpf: crypto: use NULL instead of 0-sized dynptr",
          "message": "Adjust selftests to use nullable option for state and IV arg.\n\nReviewed-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Vadim Fedorenko <vadfed@meta.com>\nLink: https://lore.kernel.org/r/20240613211817.1551967-4-vadfed@meta.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Vadim Fedorenko <vadfed@meta.com>",
          "date": "2024-06-13 16:33:04 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/crypto_sanity.c"
          ]
        },
        {
          "hash": "9b560751f75f7b2484fa22c781be68f4f9fec2b0",
          "subject": "selftests: bpf: crypto: adjust bench to use nullable IV",
          "message": "The bench shows some improvements, around 4% faster on decrypt.\n\nBefore:\n\nBenchmark 'crypto-decrypt' started.\nIter   0 (325.719us): hits    5.105M/s (  5.105M/prod), drops 0.000M/s, total operations    5.105M/s\nIter   1 (-17.295us): hits    5.224M/s (  5.224M/prod), drops 0.000M/s, total operations    5.224M/s\nIter   2 (  5.504us): hits    4.630M/s (  4.630M/prod), drops 0.000M/s, total operations    4.630M/s\nIter   3 (  9.239us): hits    5.148M/s (  5.148M/prod), drops 0.000M/s, total operations    5.148M/s\nIter   4 ( 37.885us): hits    5.198M/s (  5.198M/prod), drops 0.000M/s, total operations    5.198M/s\nIter   5 (-53.282us): hits    5.167M/s (  5.167M/prod), drops 0.000M/s, total operations    5.167M/s\nIter   6 (-17.809us): hits    5.186M/s (  5.186M/prod), drops 0.000M/s, total operations    5.186M/s\nSummary: hits    5.092 \u00b1 0.228M/s (  5.092M/prod), drops    0.000 \u00b10.000M/s, total operations    5.092 \u00b1 0.228M/s\n\nAfter:\n\nBenchmark 'crypto-decrypt' started.\nIter   0 (268.912us): hits    5.312M/s (  5.312M/prod), drops 0.000M/s, total operations    5.312M/s\nIter   1 (124.869us): hits    5.354M/s (  5.354M/prod), drops 0.000M/s, total operations    5.354M/s\nIter   2 (-36.801us): hits    5.334M/s (  5.334M/prod), drops 0.000M/s, total operations    5.334M/s\nIter   3 (254.628us): hits    5.334M/s (  5.334M/prod), drops 0.000M/s, total operations    5.334M/s\nIter   4 (-77.691us): hits    5.275M/s (  5.275M/prod), drops 0.000M/s, total operations    5.275M/s\nIter   5 (-164.510us): hits    5.313M/s (  5.313M/prod), drops 0.000M/s, total operations    5.313M/s\nIter   6 (-81.376us): hits    5.346M/s (  5.346M/prod), drops 0.000M/s, total operations    5.346M/s\nSummary: hits    5.326 \u00b1 0.029M/s (  5.326M/prod), drops    0.000 \u00b10.000M/s, total operations    5.326 \u00b1 0.029M/s\n\nReviewed-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Vadim Fedorenko <vadfed@meta.com>\nLink: https://lore.kernel.org/r/20240613211817.1551967-5-vadfed@meta.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Vadim Fedorenko <vadfed@meta.com>",
          "date": "2024-06-13 16:33:04 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/crypto_bench.c"
          ]
        },
        {
          "hash": "2d45ab1eda469c802728d0a74e1601de5e71c098",
          "subject": "selftests: bpf: add testmod kfunc for nullable params",
          "message": "Add special test to be sure that only __nullable BTF params can be\nreplaced by NULL. This patch adds fake kfuncs in bpf_testmod to\nproperly test different params.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Vadim Fedorenko <vadfed@meta.com>\nLink: https://lore.kernel.org/r/20240613211817.1551967-6-vadfed@meta.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Vadim Fedorenko <vadfed@meta.com>",
          "date": "2024-06-13 16:33:04 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod_kfunc.h",
            "tools/testing/selftests/bpf/prog_tests/kfunc_param_nullable.c",
            "tools/testing/selftests/bpf/progs/test_kfunc_param_nullable.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "4ff5747158f323939e2ce8881ca61f3c646948c4",
      "merge_subject": "Merge branch 'bpf-support-dumping-kfunc-prototypes-from-btf'",
      "merge_body": "Daniel Xu says:\n\n====================\nbpf: Support dumping kfunc prototypes from BTF\n\nThis patchset enables both detecting as well as dumping compilable\nprototypes for kfuncs.\n\nThe first commit instructs pahole to DECL_TAG kfuncs when available.\nThis requires v1.27 which was released on 6/11/24. With it, users will\nbe able to look at BTF inside vmlinux (or modules) and check if the\nkfunc they want is available.\n\nThe final commit teaches bpftool how to dump kfunc prototypes. This\nis done for developer convenience.\n\nThe rest of the commits are fixups to enable selftests to use the\nnewly dumped kfunc prototypes. With these, selftests will regularly\nexercise the newly added codepaths.\n\nTested with and without the required pahole changes:\n\n  * https://github.com/kernel-patches/bpf/pull/7186\n  * https://github.com/kernel-patches/bpf/pull/7187\n\n=== Changelog ===\nFrom v4:\n* Change bpf_session_cookie() return type\n* Only fixup used fentry test kfunc prototypes\n* Extract out projection detection into shared btf_is_projection_of()\n* Fix kernel test robot build warnings about doc comments\n\nFrom v3:\n* Teach selftests to use dumped prototypes\n\nFrom v2:\n* Update Makefile.btf with pahole flag\n* More error checking\n* Output formatting changes\n* Drop already-merged commit\n\nFrom v1:\n* Add __weak annotation\n* Use btf_dump for kfunc prototypes\n* Update kernel bpf_rdonly_cast() signature\n====================\n\nLink: https://lore.kernel.org/r/cover.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-06-12 11:01:32 -0700",
      "commits": [
        {
          "hash": "ebb79e96f1ea454fbcc8fe27dfe44e751bd74b4b",
          "subject": "kbuild: bpf: Tell pahole to DECL_TAG kfuncs",
          "message": "With [0], pahole can now discover kfuncs and inject DECL_TAG\ninto BTF. With this commit, we will start shipping said DECL_TAGs\nto downstream consumers if pahole supports it.\n\nThis is useful for feature probing kfuncs as well as generating\ncompilable prototypes. This is particularly important as kfuncs\ndo not have stable ABI.\n\n[0]: https://git.kernel.org/pub/scm/devel/pahole/pahole.git/commit/?id=72e88f29c6f7e14201756e65bd66157427a61aaf\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/324aac5c627bddb80d9968c30df6382846994cc8.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:30 -0700",
          "modified_files": [
            "scripts/Makefile.btf"
          ]
        },
        {
          "hash": "718135f5bd24ec10ff38aa0294a7da0a7b99fa89",
          "subject": "bpf: selftests: Fix bpf_iter_task_vma_new() prototype",
          "message": "bpf_iter_task_vma_new() is defined as taking a u64 as its 3rd argument.\nu64 is a unsigned long long. bpf_experimental.h was defining the\nprototype as unsigned long.\n\nFix by using __u64.\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/fab4509bfee914f539166a91c3ff41e949f3df30.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:30 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "dff96e4f5078c6c61fc6c36dddf27b124c4318fc",
          "subject": "bpf: selftests: Fix fentry test kfunc prototypes",
          "message": "Some prototypes in progs/get_func_ip_test.c were not in line with how the\nactual kfuncs are defined in net/bpf/test_run.c. This causes compilation\nerrors when kfunc prototypes are generated from BTF.\n\nFix by aligning with actual kfunc definitions.\n\nAlso remove two unused prototypes.\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/1e68870e7626b7b9c6420e65076b307fc404a2f0.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:30 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/get_func_ip_test.c"
          ]
        },
        {
          "hash": "89f0b1abac497c47d0851b780abecc756c1e8734",
          "subject": "bpf: selftests: Fix bpf_cpumask_first_zero() kfunc prototype",
          "message": "The prototype in progs/nested_trust_common.h is not in line with how the\nactual kfuncs are defined in kernel/bpf/cpumask.c. This causes compilation\nerrors when kfunc prototypes are generated from BTF.\n\nFix by aligning with actual kfunc definitions.\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/437936a4e554b02e04566dd6e3f0a5d08370cc8c.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:31 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/nested_trust_common.h"
          ]
        },
        {
          "hash": "ac42f636dc11b2e8d6dea9dd5bb10a39c7bec342",
          "subject": "bpf: selftests: Fix bpf_map_sum_elem_count() kfunc prototype",
          "message": "The prototype in progs/map_percpu_stats.c is not in line with how the\nactual kfuncs are defined in kernel/bpf/map_iter.c. This causes\ncompilation errors when kfunc prototypes are generated from BTF.\n\nFix by aligning with actual kfunc definitions.\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/0497e11a71472dcb71ada7c90ad691523ae87c3b.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:31 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/map_percpu_stats.c"
          ]
        },
        {
          "hash": "2b8dd87332cd2782b5b3f0c423bd6693e487ed30",
          "subject": "bpf: Make bpf_session_cookie() kfunc return long *",
          "message": "We will soon be generating kfunc prototypes from BTF. As part of that,\nwe need to align the manual signatures in bpf_kfuncs.h with the actual\nkfunc definitions. There is currently a conflicting signature for\nbpf_session_cookie() w.r.t. return type.\n\nThe original intent was to return long * and not __u64 *. You can see\nevidence of that intent in a3a5113393cc (\"selftests/bpf: Add kprobe\nsession cookie test\").\n\nFix conflict by changing kfunc definition.\n\nFixes: 5c919acef851 (\"bpf: Add support for kprobe session cookie\")\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/7043e1c251ab33151d6e3830f8ea1902ed2604ac.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:31 -0700",
          "modified_files": [
            "kernel/trace/bpf_trace.c"
          ]
        },
        {
          "hash": "0ce089cbdc6a393bf9ad04964427852800503a58",
          "subject": "bpf: selftests: Namespace struct_opt callbacks in bpf_dctcp",
          "message": "With generated kfunc prototypes, the existing callback names will\nconflict. Fix by namespacing with a bpf_ prefix.\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/efe7aadad8a054e5aeeba94b1d2e4502eee09d7a.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:31 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/bpf_dctcp.c"
          ]
        },
        {
          "hash": "ec209ad86324de84ef66990f0e9df0851e45e054",
          "subject": "bpf: verifier: Relax caller requirements for kfunc projection type args",
          "message": "Currently, if a kfunc accepts a projection type as an argument (eg\nstruct __sk_buff *), the caller must exactly provide exactly the same\ntype with provable provenance.\n\nHowever in practice, kfuncs that accept projection types _must_ cast to\nthe underlying type before use b/c projection type layouts are\ncompletely made up. Thus, it is ok to relax the verifier rules around\nimplicit conversions.\n\nWe will use this functionality in the next commit when we align kfuncs\nto user-facing types.\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/e2c025cb09ccfd4af1ec9e18284dc3cecff7514d.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:31 -0700",
          "modified_files": [
            "include/linux/btf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "cce4c40b960673f9e020835def310f1e89d3a940",
          "subject": "bpf: treewide: Align kfunc signatures to prog point-of-view",
          "message": "Previously, kfunc declarations in bpf_kfuncs.h (and others) used \"user\nfacing\" types for kfuncs prototypes while the actual kfunc definitions\nused \"kernel facing\" types. More specifically: bpf_dynptr vs\nbpf_dynptr_kern, __sk_buff vs sk_buff, and xdp_md vs xdp_buff.\n\nIt wasn't an issue before, as the verifier allows aliased types.\nHowever, since we are now generating kfunc prototypes in vmlinux.h (in\naddition to keeping bpf_kfuncs.h around), this conflict creates\ncompilation errors.\n\nFix this conflict by using \"user facing\" types in kfunc definitions.\nThis results in more casts, but otherwise has no additional runtime\ncost.\n\nNote, similar to 5b268d1ebcdc (\"bpf: Have bpf_rdonly_cast() take a const\npointer\"), we also make kfuncs take const arguments where appropriate in\norder to make the kfunc more permissive.\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/b58346a63a0e66bc9b7504da751b526b0b189a67.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:31 -0700",
          "modified_files": [
            "fs/verity/measure.c",
            "include/linux/bpf.h",
            "kernel/bpf/crypto.c",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c",
            "kernel/trace/bpf_trace.c",
            "net/core/filter.c",
            "tools/testing/selftests/bpf/progs/ip_check_defrag.c",
            "tools/testing/selftests/bpf/progs/verifier_netfilter_ctx.c"
          ]
        },
        {
          "hash": "f709124dd72fe7a3f6ba7764b2ed145c55c33e47",
          "subject": "bpf: selftests: nf: Opt out of using generated kfunc prototypes",
          "message": "The bpf-nf selftests play various games with aliased types such that\nfolks with CONFIG_NF_CONNTRACK=m/n configs can still build the\nselftests. See commits:\n\n1058b6a78db2 (\"selftests/bpf: Do not fail build if CONFIG_NF_CONNTRACK=m/n\")\n92afc5329a5b (\"selftests/bpf: Fix build errors if CONFIG_NF_CONNTRACK=m\")\n\nThus, it is simpler if these selftests opt out of using generated kfunc\nprototypes. The preprocessor macro this commit uses will be introduced\nin the final commit.\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/044a5b10cb3abd0d71cb1c818ee0bfc4a2239332.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:31 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/test_bpf_nf.c",
            "tools/testing/selftests/bpf/progs/test_bpf_nf_fail.c",
            "tools/testing/selftests/bpf/progs/xdp_synproxy_kern.c"
          ]
        },
        {
          "hash": "c567cba34585514f82600a10587c8813c50e3a7c",
          "subject": "bpf: selftests: xfrm: Opt out of using generated kfunc prototypes",
          "message": "The xfrm_info selftest locally defines an aliased type such that folks\nwith CONFIG_XFRM_INTERFACE=m/n configs can still build the selftests.\nSee commit aa67961f3243 (\"selftests/bpf: Allow building bpf tests with CONFIG_XFRM_INTERFACE=[m|n]\").\n\nThus, it is simpler if this selftest opts out of using enerated kfunc\nprototypes. The preprocessor macro this commit uses will be introduced\nin the final commit.\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/afe0bb1c50487f52542cdd5230c4aef9e36ce250.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:31 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/xfrm_info.c"
          ]
        },
        {
          "hash": "770abbb5a25a5b767f1c60ba366aea503728e957",
          "subject": "bpftool: Support dumping kfunc prototypes from BTF",
          "message": "This patch enables dumping kfunc prototypes from bpftool. This is useful\nb/c with this patch, end users will no longer have to manually define\nkfunc prototypes. For the kernel tree, this also means we can optionally\ndrop kfunc prototypes from:\n\n        tools/testing/selftests/bpf/bpf_kfuncs.h\n        tools/testing/selftests/bpf/bpf_experimental.h\n\nExample usage:\n\n        $ make PAHOLE=/home/dxu/dev/pahole/build/pahole -j30 vmlinux\n\n        $ ./tools/bpf/bpftool/bpftool btf dump file ./vmlinux format c | rg \"__ksym;\" | head -3\n        extern void cgroup_rstat_updated(struct cgroup *cgrp, int cpu) __weak __ksym;\n        extern void cgroup_rstat_flush(struct cgroup *cgrp) __weak __ksym;\n        extern struct bpf_key *bpf_lookup_user_key(u32 serial, u64 flags) __weak __ksym;\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/bf6c08f9263c4bd9d10a717de95199d766a13f61.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Xu <dxu@dxuuu.xyz>",
          "date": "2024-06-12 11:01:32 -0700",
          "modified_files": [
            "tools/bpf/bpftool/btf.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "49df0019f36798d414e6b913bec30a3a0cd47c70",
      "merge_subject": "Merge branch 'enable-bpf-programs-to-declare-arrays-of-kptr-bpf_rb_root-and-bpf_list_head'",
      "merge_body": "Kui-Feng Lee says:\n\n====================\nEnable BPF programs to declare arrays of kptr, bpf_rb_root, and bpf_list_head.\n\nSome types, such as type kptr, bpf_rb_root, and bpf_list_head, are\ntreated in a special way. Previously, these types could not be the\ntype of a field in a struct type that is used as the type of a global\nvariable. They could not be the type of a field in a struct type that\nis used as the type of a field in the value type of a map either. They\ncould not even be the type of array elements. This means that they can\nonly be the type of global variables or of direct fields in the value\ntype of a map.\n\nThe patch set aims to enable the use of these specific types in arrays\nand struct fields, providing flexibility. It examines the types of\nglobal variables or the value types of maps, such as arrays and struct\ntypes, recursively to identify these special types and generate field\ninformation for them.\n\nFor example,\n\n  ...\n  struct task_struct __kptr *ptr[3];\n  ...\n\nit will create 3 instances of \"struct btf_field\" in the \"btf_record\" of\nthe data section.\n\n [...,\n  btf_field(offset=0x100, type=BPF_KPTR_REF),\n  btf_field(offset=0x108, type=BPF_KPTR_REF),\n  btf_field(offset=0x110, type=BPF_KPTR_REF),\n  ...\n ]\n\nIt creates a record of each of three elements. These three records are\nalmost identical except their offsets.\n\nAnother example is\n\n  ...\n  struct A {\n    ...\n    struct task_struct __kptr *task;\n    struct bpf_rb_root root;\n    ...\n  }\n\n  struct A foo[2];\n\nit will create 4 records.\n\n [...,\n  btf_field(offset=0x7100, type=BPF_KPTR_REF),\n  btf_field(offset=0x7108, type=BPF_RB_ROOT:),\n  btf_field(offset=0x7200, type=BPF_KPTR_REF),\n  btf_field(offset=0x7208, type=BPF_RB_ROOT:),\n  ...\n ]\n\nAssuming that the size of an element/struct A is 0x100 and \"foo\"\nstarts at 0x7000, it includes two kptr records at 0x7100 and 0x7200,\nand two rbtree root records at 0x7108 and 0x7208.\n\nAll these field information will be flatten, for struct types, and\nrepeated, for arrays.\n---\nChanges from v6:\n\n - Return BPF_KPTR_REF from btf_get_field_type() only if var_type is a\n   struct type.\n\n   - Pass btf and type to btf_get_field_type().\n\nChanges from v5:\n\n - Ensure field->offset values of kptrs are advanced correctly from\n   one nested struct/or array to another.\n\nChanges from v4:\n\n - Return -E2BIG for i == MAX_RESOLVE_DEPTH.\n\nChanges from v3:\n\n - Refactor the common code of btf_find_struct_field() and\n   btf_find_datasec_var().\n\n - Limit the number of levels looking into a struct types.\n\nChanges from v2:\n\n - Support fields in nested struct type.\n\n - Remove nelems and duplicate field information with offset\n   adjustments for arrays.\n\nChanges from v1:\n\n - Move the check of element alignment out of btf_field_cmp() to\n   btf_record_find().\n\n - Change the order of the previous patch 4 \"bpf:\n   check_map_kptr_access() compute the offset from the reg state\" as\n   the patch 7 now.\n\n - Reject BPF_RB_NODE and BPF_LIST_NODE with nelems > 1.\n\n - Rephrase the commit log of the patch \"bpf: check_map_access() with\n   the knowledge of arrays\" to clarify the alignment on elements.\n\nv6: https://lore.kernel.org/all/20240520204018.884515-1-thinker.li@gmail.com/\nv5: https://lore.kernel.org/all/20240510011312.1488046-1-thinker.li@gmail.com/\nv4: https://lore.kernel.org/all/20240508063218.2806447-1-thinker.li@gmail.com/\nv3: https://lore.kernel.org/all/20240501204729.484085-1-thinker.li@gmail.com/\nv2: https://lore.kernel.org/all/20240412210814.603377-1-thinker.li@gmail.com/\nv1: https://lore.kernel.org/bpf/20240410004150.2917641-1-thinker.li@gmail.com/\n\nKui-Feng Lee (9):\n  bpf: Remove unnecessary checks on the offset of btf_field.\n  bpf: Remove unnecessary call to btf_field_type_size().\n  bpf: refactor btf_find_struct_field() and btf_find_datasec_var().\n  bpf: create repeated fields for arrays.\n  bpf: look into the types of the fields of a struct type recursively.\n  bpf: limit the number of levels of a nested struct type.\n  selftests/bpf: Test kptr arrays and kptrs in nested struct fields.\n  selftests/bpf: Test global bpf_rb_root arrays and fields in nested\n    struct types.\n  selftests/bpf: Test global bpf_list_head arrays.\n\n kernel/bpf/btf.c                              | 310 ++++++++++++------\n kernel/bpf/verifier.c                         |   4 +-\n .../selftests/bpf/prog_tests/cpumask.c        |   5 +\n .../selftests/bpf/prog_tests/linked_list.c    |  12 +\n .../testing/selftests/bpf/prog_tests/rbtree.c |  47 +++\n .../selftests/bpf/progs/cpumask_success.c     | 171 ++++++++++\n .../testing/selftests/bpf/progs/linked_list.c |  42 +++\n tools/testing/selftests/bpf/progs/rbtree.c    |  77 +++++\n 8 files changed, 558 insertions(+), 110 deletions(-)\n====================\n\nLink: https://lore.kernel.org/r/20240523174202.461236-1-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-06-03 20:52:43 -0700",
      "commits": [
        {
          "hash": "c95a3be45ad22ee8925d6d1ab531d5ba98216311",
          "subject": "bpf: Remove unnecessary checks on the offset of btf_field.",
          "message": "reg_find_field_offset() always return a btf_field with a matching offset\nvalue. Checking the offset of the returned btf_field is unnecessary.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240523174202.461236-2-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-06-03 20:52:42 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "482f7133791e894b94a57ab3251e03d4c98ea42b",
          "subject": "bpf: Remove unnecessary call to btf_field_type_size().",
          "message": "field->size has been initialized by bpf_parse_fields() with the value\nreturned by btf_field_type_size(). Use it instead of calling\nbtf_field_type_size() again.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240523174202.461236-3-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-06-03 20:52:42 -0700",
          "modified_files": [
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "a7db0d4f872a869feb7c0201c0fa736c309192d5",
          "subject": "bpf: refactor btf_find_struct_field() and btf_find_datasec_var().",
          "message": "Move common code of the two functions to btf_find_field_one().\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240523174202.461236-4-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-06-03 20:52:42 -0700",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "994796c0256c4001633488fd24c3d54691949f8d",
          "subject": "bpf: create repeated fields for arrays.",
          "message": "The verifier uses field information for certain special types, such as\nkptr, rbtree root, and list head. These types are treated\ndifferently. However, we did not previously support these types in\narrays. This update examines arrays and duplicates field information the\nsame number of times as the length of the array if the element type is one\nof the special types.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240523174202.461236-5-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-06-03 20:52:42 -0700",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "64e8ee814819f21beeeda00d4119221443d77992",
          "subject": "bpf: look into the types of the fields of a struct type recursively.",
          "message": "The verifier has field information for specific special types, such as\nkptr, rbtree root, and list head. These types are handled\ndifferently. However, we did not previously examine the types of fields of\na struct type variable. Field information records were not generated for\nthe kptrs, rbtree roots, and linked_list heads that are not located at the\noutermost struct type of a variable.\n\nFor example,\n\n  struct A {\n    struct task_struct __kptr * task;\n  };\n\n  struct B {\n    struct A mem_a;\n  }\n\n  struct B var_b;\n\nIt did not examine \"struct A\" so as not to generate field information for\nthe kptr in \"struct A\" for \"var_b\".\n\nThis patch enables BPF programs to define fields of these special types in\na struct type other than the direct type of a variable or in a struct type\nthat is the type of a field in the value type of a map.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240523174202.461236-6-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-06-03 20:52:42 -0700",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "f19caf57d80f4432acea61d858d45ce194444389",
          "subject": "bpf: limit the number of levels of a nested struct type.",
          "message": "Limit the number of levels looking into struct types to avoid running out\nof stack space.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240523174202.461236-7-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-06-03 20:52:42 -0700",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "c4c6c3b785a0b1426add15d078da61f899abeaac",
          "subject": "selftests/bpf: Test kptr arrays and kptrs in nested struct fields.",
          "message": "Make sure that BPF programs can declare global kptr arrays and kptr fields\nin struct types that is the type of a global variable or the type of a\nnested descendant field in a global variable.\n\nAn array with only one element is special case, that it treats the element\nlike a non-array kptr field. Nested arrays are also tested to ensure they\nare handled properly.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240523174202.461236-8-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-06-03 20:52:42 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/cpumask.c",
            "tools/testing/selftests/bpf/progs/cpumask_success.c"
          ]
        },
        {
          "hash": "d55c765a9b2d54b53ef86a62d6209e2e5eb62585",
          "subject": "selftests/bpf: Test global bpf_rb_root arrays and fields in nested struct types.",
          "message": "Make sure global arrays of bpf_rb_root and fields of bpf_rb_root in nested\nstruct types work correctly.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240523174202.461236-9-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-06-03 20:52:42 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/rbtree.c",
            "tools/testing/selftests/bpf/progs/rbtree.c"
          ]
        },
        {
          "hash": "43d50ffb1f7e32865cdd343224659614d8b558b9",
          "subject": "selftests/bpf: Test global bpf_list_head arrays.",
          "message": "Make sure global arrays of bpf_list_heads and fields of bpf_list_heads in\nnested struct types work correctly.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240523174202.461236-10-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-06-03 20:52:43 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/linked_list.c",
            "tools/testing/selftests/bpf/progs/linked_list.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "55302bc1ca64160fec4dfa25e52142691ecb5dcd",
      "merge_subject": "Merge branch 'bpf-inline-helpers-in-arm64-and-riscv-jits'",
      "merge_body": "Puranjay Mohan says:\n\n====================\nbpf: Inline helpers in arm64 and riscv JITs\n\nChanges in v5 -> v6:\narm64 v5: https://lore.kernel.org/all/20240430234739.79185-1-puranjay@kernel.org/\nriscv v2: https://lore.kernel.org/all/20240430175834.33152-1-puranjay@kernel.org/\n- Combine riscv and arm64 changes in single series\n- Some coding style fixes\n\nChanges in v4 -> v5:\nv4: https://lore.kernel.org/all/20240429131647.50165-1-puranjay@kernel.org/\n- Implement the inlining of the bpf_get_smp_processor_id() in the JIT.\n\nNOTE: This needs to be based on:\nhttps://lore.kernel.org/all/20240430175834.33152-1-puranjay@kernel.org/\nto be built.\n\nManual run of bpf-ci with this series rebased on above:\nhttps://github.com/kernel-patches/bpf/pull/6929\n\nChanges in v3 -> v4:\nv3: https://lore.kernel.org/all/20240426121349.97651-1-puranjay@kernel.org/\n- Fix coding style issue related to C89 standards.\n\nChanges in v2 -> v3:\nv2: https://lore.kernel.org/all/20240424173550.16359-1-puranjay@kernel.org/\n- Fixed the xlated dump of percpu mov to \"r0 = &(void __percpu *)(r0)\"\n- Made ARM64 and x86-64 use the same code for inlining. The only difference\n  that remains is the per-cpu address of the cpu_number.\n\nChanges in v1 -> v2:\nv1: https://lore.kernel.org/all/20240405091707.66675-1-puranjay12@gmail.com/\n- Add a patch to inline bpf_get_smp_processor_id()\n- Fix an issue in MRS instruction encoding as pointed out by Will\n- Remove CONFIG_SMP check because arm64 kernel always compiles with CONFIG_SMP\n\nThis series adds the support of internal only per-CPU instructions and inlines\nthe bpf_get_smp_processor_id() helper call for ARM64 and RISC-V BPF JITs.\n\nHere is an example of calls to bpf_get_smp_processor_id() and\npercpu_array_map_lookup_elem() before and after this series on ARM64.\n\n                                         BPF\n                                        =====\n              BEFORE                                       AFTER\n             --------                                     -------\n\nint cpu = bpf_get_smp_processor_id();           int cpu = bpf_get_smp_processor_id();\n(85) call bpf_get_smp_processor_id#229032       (85) call bpf_get_smp_processor_id#8\n\np = bpf_map_lookup_elem(map, &zero);            p = bpf_map_lookup_elem(map, &zero);\n(18) r1 = map[id:78]                            (18) r1 = map[id:153]\n(18) r2 = map[id:82][0]+65536                   (18) r2 = map[id:157][0]+65536\n(85) call percpu_array_map_lookup_elem#313512   (07) r1 += 496\n                                                (61) r0 = *(u32 *)(r2 +0)\n                                                (35) if r0 >= 0x1 goto pc+5\n                                                (67) r0 <<= 3\n                                                (0f) r0 += r1\n                                                (79) r0 = *(u64 *)(r0 +0)\n                                                (bf) r0 = &(void __percpu *)(r0)\n                                                (05) goto pc+1\n                                                (b7) r0 = 0\n\n                                      ARM64 JIT\n                                     ===========\n\n              BEFORE                                       AFTER\n             --------                                     -------\n\nint cpu = bpf_get_smp_processor_id();           int cpu = bpf_get_smp_processor_id();\nmov     x10, #0xfffffffffffff4d0                mrs     x10, sp_el0\nmovk    x10, #0x802b, lsl #16                   ldr     w7, [x10, #24]\nmovk    x10, #0x8000, lsl #32\nblr     x10\nadd     x7, x0, #0x0\n\np = bpf_map_lookup_elem(map, &zero);            p = bpf_map_lookup_elem(map, &zero);\nmov     x0, #0xffff0003ffffffff                 mov     x0, #0xffff0003ffffffff\nmovk    x0, #0xce5c, lsl #16                    movk    x0, #0xe0f3, lsl #16\nmovk    x0, #0xca00                             movk    x0, #0x7c00\nmov     x1, #0xffff8000ffffffff                 mov     x1, #0xffff8000ffffffff\nmovk    x1, #0x8bdb, lsl #16                    movk    x1, #0xb0c7, lsl #16\nmovk    x1, #0x6000                             movk    x1, #0xe000\nmov     x10, #0xffffffffffff3ed0                add     x0, x0, #0x1f0\nmovk    x10, #0x802d, lsl #16                   ldr     w7, [x1]\nmovk    x10, #0x8000, lsl #32                   cmp     x7, #0x1\nblr     x10                                     b.cs    0x0000000000000090\nadd     x7, x0, #0x0                            lsl     x7, x7, #3\n                                                add     x7, x7, x0\n                                                ldr     x7, [x7]\n                                                mrs     x10, tpidr_el1\n                                                add     x7, x7, x10\n                                                b       0x0000000000000094\n                                                mov     x7, #0x0\n\n              Performance improvement found using benchmark[1]\n\n./benchs/run_bench_trigger.sh glob-arr-inc arr-inc hash-inc\n\n  +---------------+-------------------+-------------------+--------------+\n  |      Name     |      Before       |        After      |   % change   |\n  |---------------+-------------------+-------------------+--------------|\n  | glob-arr-inc  | 23.380 \u00b1 1.675M/s | 25.893 \u00b1 0.026M/s |   + 10.74%   |\n  | arr-inc       | 23.928 \u00b1 0.034M/s | 25.213 \u00b1 0.063M/s |   + 5.37%    |\n  | hash-inc      | 12.352 \u00b1 0.005M/s | 12.609 \u00b1 0.013M/s |   + 2.08%    |\n  +---------------+-------------------+-------------------+--------------+\n\n[1] https://github.com/anakryiko/linux/commit/8dec900975ef\n\n             RISCV64 JIT output for `call bpf_get_smp_processor_id`\n            =======================================================\n\n                  Before                           After\n                 --------                         -------\n\n           auipc   t1,0x848c                  ld    a5,32(tp)\n           jalr    604(t1)\n           mv      a5,a0\n\n  Benchmark using [1] on Qemu.\n\n  ./benchs/run_bench_trigger.sh glob-arr-inc arr-inc hash-inc\n\n  +---------------+------------------+------------------+--------------+\n  |      Name     |     Before       |       After      |   % change   |\n  |---------------+------------------+------------------+--------------|\n  | glob-arr-inc  | 1.077 \u00b1 0.006M/s | 1.336 \u00b1 0.010M/s |   + 24.04%   |\n  | arr-inc       | 1.078 \u00b1 0.002M/s | 1.332 \u00b1 0.015M/s |   + 23.56%   |\n  | hash-inc      | 0.494 \u00b1 0.004M/s | 0.653 \u00b1 0.001M/s |   + 32.18%   |\n  +---------------+------------------+------------------+--------------+\n====================\n\nLink: https://lore.kernel.org/r/20240502151854.9810-1-puranjay@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-05-12 16:54:35 -0700",
      "commits": [
        {
          "hash": "19c56d4e5be102cd118162b9f72d9c6d353e76fc",
          "subject": "riscv, bpf: add internal-only MOV instruction to resolve per-CPU addrs",
          "message": "Support an instruction for resolving absolute addresses of per-CPU\ndata from their per-CPU offsets. This instruction is internal-only and\nusers are not allowed to use them directly. They will only be used for\ninternal inlining optimizations for now between BPF verifier and BPF\nJITs.\n\nRISC-V uses generic per-cpu implementation where the offsets for CPUs\nare kept in an array called __per_cpu_offset[cpu_number]. RISCV stores\nthe address of the task_struct in TP register. The first element in\ntask_struct is struct thread_info, and we can get the cpu number by\nreading from the TP register + offsetof(struct thread_info, cpu).\n\nOnce we have the cpu number in a register we read the offset for that\ncpu from address: &__per_cpu_offset + cpu_number << 3. Then we add this\noffset to the destination register.\n\nTo measure the improvement from this change, the benchmark in [1] was\nused on Qemu:\n\nBefore:\nglob-arr-inc   :    1.127 \u00b1 0.013M/s\narr-inc        :    1.121 \u00b1 0.004M/s\nhash-inc       :    0.681 \u00b1 0.052M/s\n\nAfter:\nglob-arr-inc   :    1.138 \u00b1 0.011M/s\narr-inc        :    1.366 \u00b1 0.006M/s\nhash-inc       :    0.676 \u00b1 0.001M/s\n\n[1] https://github.com/anakryiko/linux/commit/8dec900975ef\n\nSigned-off-by: Puranjay Mohan <puranjay@kernel.org>\nAcked-by: Bj\u00f6rn T\u00f6pel <bjorn@kernel.org>\nLink: https://lore.kernel.org/r/20240502151854.9810-2-puranjay@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Puranjay Mohan <puranjay@kernel.org>",
          "date": "2024-05-12 16:54:34 -0700",
          "modified_files": [
            "arch/riscv/net/bpf_jit_comp64.c"
          ]
        },
        {
          "hash": "2ddec2c80b4402c293c7e6e0881cecaaf77e8cec",
          "subject": "riscv, bpf: inline bpf_get_smp_processor_id()",
          "message": "Inline the calls to bpf_get_smp_processor_id() in the riscv bpf jit.\n\nRISCV saves the pointer to the CPU's task_struct in the TP (thread\npointer) register. This makes it trivial to get the CPU's processor id.\nAs thread_info is the first member of task_struct, we can read the\nprocessor id from TP + offsetof(struct thread_info, cpu).\n\n          RISCV64 JIT output for `call bpf_get_smp_processor_id`\n\t  ======================================================\n\n                Before                           After\n               --------                         -------\n\n         auipc   t1,0x848c                  ld    a5,32(tp)\n         jalr    604(t1)\n         mv      a5,a0\n\nBenchmark using [1] on Qemu.\n\n./benchs/run_bench_trigger.sh glob-arr-inc arr-inc hash-inc\n\n+---------------+------------------+------------------+--------------+\n|      Name     |     Before       |       After      |   % change   |\n|---------------+------------------+------------------+--------------|\n| glob-arr-inc  | 1.077 \u00b1 0.006M/s | 1.336 \u00b1 0.010M/s |   + 24.04%   |\n| arr-inc       | 1.078 \u00b1 0.002M/s | 1.332 \u00b1 0.015M/s |   + 23.56%   |\n| hash-inc      | 0.494 \u00b1 0.004M/s | 0.653 \u00b1 0.001M/s |   + 32.18%   |\n+---------------+------------------+------------------+--------------+\n\nNOTE: This benchmark includes changes from this patch and the previous\n      patch that implemented the per-cpu insn.\n\n[1] https://github.com/anakryiko/linux/commit/8dec900975ef\n\nSigned-off-by: Puranjay Mohan <puranjay@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Bj\u00f6rn T\u00f6pel <bjorn@kernel.org>\nLink: https://lore.kernel.org/r/20240502151854.9810-3-puranjay@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Puranjay Mohan <puranjay@kernel.org>",
          "date": "2024-05-12 16:54:34 -0700",
          "modified_files": [
            "arch/riscv/net/bpf_jit_comp64.c",
            "include/linux/filter.h",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "7a4c32222b0e14349a6311e72bf6ebd3e1d1064b",
          "subject": "arm64, bpf: add internal-only MOV instruction to resolve per-CPU addrs",
          "message": "Support an instruction for resolving absolute addresses of per-CPU\ndata from their per-CPU offsets. This instruction is internal-only and\nusers are not allowed to use them directly. They will only be used for\ninternal inlining optimizations for now between BPF verifier and BPF\nJITs.\n\nSince commit 7158627686f0 (\"arm64: percpu: implement optimised pcpu\naccess using tpidr_el1\"), the per-cpu offset for the CPU is stored in\nthe tpidr_el1/2 register of that CPU.\n\nTo support this BPF instruction in the ARM64 JIT, the following ARM64\ninstructions are emitted:\n\nmov dst, src\t\t// Move src to dst, if src != dst\nmrs tmp, tpidr_el1/2\t// Move per-cpu offset of the current cpu in tmp.\nadd dst, dst, tmp\t// Add the per cpu offset to the dst.\n\nTo measure the performance improvement provided by this change, the\nbenchmark in [1] was used:\n\nBefore:\nglob-arr-inc   :   23.597 \u00b1 0.012M/s\narr-inc        :   23.173 \u00b1 0.019M/s\nhash-inc       :   12.186 \u00b1 0.028M/s\n\nAfter:\nglob-arr-inc   :   23.819 \u00b1 0.034M/s\narr-inc        :   23.285 \u00b1 0.017M/s\nhash-inc       :   12.419 \u00b1 0.011M/s\n\n[1] https://github.com/anakryiko/linux/commit/8dec900975ef\n\nSigned-off-by: Puranjay Mohan <puranjay12@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240502151854.9810-4-puranjay@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Puranjay Mohan <puranjay12@gmail.com>",
          "date": "2024-05-12 16:54:34 -0700",
          "modified_files": [
            "arch/arm64/include/asm/insn.h",
            "arch/arm64/lib/insn.c",
            "arch/arm64/net/bpf_jit.h",
            "arch/arm64/net/bpf_jit_comp.c"
          ]
        },
        {
          "hash": "75fe4c0b3e181f5e3b990128013ac192fdfd4012",
          "subject": "bpf, arm64: inline bpf_get_smp_processor_id() helper",
          "message": "Inline calls to bpf_get_smp_processor_id() helper in the JIT by emitting\na read from struct thread_info. The SP_EL0 system register holds the\npointer to the task_struct and thread_info is the first member of this\nstruct. We can read the cpu number from the thread_info.\n\nHere is how the ARM64 JITed assembly changes after this commit:\n\n                                      ARM64 JIT\n                                     ===========\n\n              BEFORE                                    AFTER\n             --------                                  -------\n\nint cpu = bpf_get_smp_processor_id();        int cpu = bpf_get_smp_processor_id();\n\nmov     x10, #0xfffffffffffff4d0             mrs     x10, sp_el0\nmovk    x10, #0x802b, lsl #16                ldr     w7, [x10, #24]\nmovk    x10, #0x8000, lsl #32\nblr     x10\nadd     x7, x0, #0x0\n\n               Performance improvement using benchmark[1]\n\n./benchs/run_bench_trigger.sh glob-arr-inc arr-inc hash-inc\n\n+---------------+-------------------+-------------------+--------------+\n|      Name     |      Before       |        After      |   % change   |\n|---------------+-------------------+-------------------+--------------|\n| glob-arr-inc  | 23.380 \u00b1 1.675M/s | 25.893 \u00b1 0.026M/s |   + 10.74%   |\n| arr-inc       | 23.928 \u00b1 0.034M/s | 25.213 \u00b1 0.063M/s |   + 5.37%    |\n| hash-inc      | 12.352 \u00b1 0.005M/s | 12.609 \u00b1 0.013M/s |   + 2.08%    |\n+---------------+-------------------+-------------------+--------------+\n\n[1] https://github.com/anakryiko/linux/commit/8dec900975ef\n\nSigned-off-by: Puranjay Mohan <puranjay@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240502151854.9810-5-puranjay@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Puranjay Mohan <puranjay@kernel.org>",
          "date": "2024-05-12 16:54:34 -0700",
          "modified_files": [
            "arch/arm64/include/asm/insn.h",
            "arch/arm64/net/bpf_jit.h",
            "arch/arm64/net/bpf_jit_comp.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "329a6720a3ebbc041983b267981ab2cac102de93",
      "merge_subject": "Merge branch 'bpf-verifier-range-computation-improvements'",
      "merge_body": "Cupertino Miranda says:\n\n====================\nbpf/verifier: range computation improvements\n\nHi everyone,\n\nThis is what I hope to be the last version. :)\n\nRegards,\nCupertino\n\nChanges from v1:\n - Reordered patches in the series.\n - Fix refactor to be acurate with original code.\n - Fixed other mentioned small problems.\n\nChanges from v2:\n - Added a patch to replace mark_reg_unknowon for __mark_reg_unknown in\n   the context of range computation.\n - Reverted implementation of refactor to v1 which used a simpler\n   boolean return value in check function.\n - Further relaxed MUL to allow it to still compute a range when neither\n   of its registers is a known value.\n - Simplified tests based on Eduards example.\n - Added messages in selftest commits.\n\nChanges from v3:\n - Improved commit message of patch nr 1.\n - Coding style fixes.\n - Improve XOR and OR tests.\n - Made function calls to pass struct bpf_reg_state pointer instead.\n - Improved final code as a last patch.\n\nChanges from v4:\n - Merged patch nr 7 in 2.\n\n====================\n\nLink: https://lore.kernel.org/r/20240506141849.185293-1-cupertino.miranda@oracle.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-05-06 17:09:20 -0700",
      "commits": [
        {
          "hash": "d786957ebd3fb4cfd9147dbcccd1e8f3871b45ce",
          "subject": "bpf/verifier: replace calls to mark_reg_unknown.",
          "message": "In order to further simplify the code in adjust_scalar_min_max_vals all\nthe calls to mark_reg_unknown are replaced by __mark_reg_unknown.\n\nstatic void mark_reg_unknown(struct bpf_verifier_env *env,\n  \t\t\t     struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\t... mark all regs not init ...\n\t\treturn;\n    }\n\t__mark_reg_unknown(env, regs + regno);\n}\n\nThe 'regno >= MAX_BPF_REG' does not apply to\nadjust_scalar_min_max_vals(), because it is only called from the\nfollowing stack:\n  - check_alu_op\n    - adjust_reg_min_max_vals\n      - adjust_scalar_min_max_vals\n\nThe check_alu_op() does check_reg_arg() which verifies that both src and\ndst register numbers are within bounds.\n\nSigned-off-by: Cupertino Miranda <cupertino.miranda@oracle.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nCc: Yonghong Song <yonghong.song@linux.dev>\nCc: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nCc: David Faust <david.faust@oracle.com>\nCc: Jose Marchesi <jose.marchesi@oracle.com>\nCc: Elena Zannoni <elena.zannoni@oracle.com>\nCc: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nLink: https://lore.kernel.org/r/20240506141849.185293-2-cupertino.miranda@oracle.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Cupertino Miranda <cupertino.miranda@oracle.com>",
          "date": "2024-05-06 17:09:11 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "0922c78f592c60e5a8fe6ab968479def124d4ff3",
          "subject": "bpf/verifier: refactor checks for range computation",
          "message": "Split range computation checks in its own function, isolating pessimitic\nrange set for dst_reg and failing return to a single point.\n\nSigned-off-by: Cupertino Miranda <cupertino.miranda@oracle.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nCc: Yonghong Song <yonghong.song@linux.dev>\nCc: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nCc: David Faust <david.faust@oracle.com>\nCc: Jose Marchesi <jose.marchesi@oracle.com>\nCc: Elena Zannoni <elena.zannoni@oracle.com>\nCc: Andrii Nakryiko <andrii.nakryiko@gmail.com>\n\nbpf/verifier: improve code after range computation recent changes.\nLink: https://lore.kernel.org/r/20240506141849.185293-3-cupertino.miranda@oracle.com\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Cupertino Miranda <cupertino.miranda@oracle.com>",
          "date": "2024-05-06 17:09:11 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "138cc42c05d11fd5ee82ee1606d2c9823373a926",
          "subject": "bpf/verifier: improve XOR and OR range computation",
          "message": "Range for XOR and OR operators would not be attempted unless src_reg\nwould resolve to a single value, i.e. a known constant value.\nThis condition is unnecessary, and the following XOR/OR operator\nhandling could compute a possible better range.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\n\nSigned-off-by: Cupertino Miranda <cupertino.miranda@oracle.com\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nCc: Yonghong Song <yonghong.song@linux.dev>\nCc: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nCc: David Faust <david.faust@oracle.com>\nCc: Jose Marchesi <jose.marchesi@oracle.com>\nCc: Elena Zannoni <elena.zannoni@oracle.com>\nCc: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nLink: https://lore.kernel.org/r/20240506141849.185293-4-cupertino.miranda@oracle.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Cupertino Miranda <cupertino.miranda@oracle.com>",
          "date": "2024-05-06 17:09:11 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "5ec9a7d13f49b9c1c5ba854244d1f2ba414cf139",
          "subject": "selftests/bpf: XOR and OR range computation tests.",
          "message": "Added a test for bound computation in XOR and OR when non constant\nvalues are used and both registers have bounded ranges.\n\nSigned-off-by: Cupertino Miranda <cupertino.miranda@oracle.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nCc: Yonghong Song <yonghong.song@linux.dev>\nCc: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nCc: David Faust <david.faust@oracle.com>\nCc: Jose Marchesi <jose.marchesi@oracle.com>\nCc: Elena Zannoni <elena.zannoni@oracle.com>\nCc: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nLink: https://lore.kernel.org/r/20240506141849.185293-5-cupertino.miranda@oracle.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Cupertino Miranda <cupertino.miranda@oracle.com>",
          "date": "2024-05-06 17:09:11 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_bounds.c"
          ]
        },
        {
          "hash": "41d047a871062f1a4d1871a1908d380c14e75428",
          "subject": "bpf/verifier: relax MUL range computation check",
          "message": "MUL instruction required that src_reg would be a known value (i.e.\nsrc_reg would be a const value). The condition in this case can be\nrelaxed, since the range computation algorithm used in current code\nalready supports a proper range computation for any valid range value on\nits operands.\n\nSigned-off-by: Cupertino Miranda <cupertino.miranda@oracle.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nCc: Yonghong Song <yonghong.song@linux.dev>\nCc: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nCc: David Faust <david.faust@oracle.com>\nCc: Jose Marchesi <jose.marchesi@oracle.com>\nCc: Elena Zannoni <elena.zannoni@oracle.com>\nLink: https://lore.kernel.org/r/20240506141849.185293-6-cupertino.miranda@oracle.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Cupertino Miranda <cupertino.miranda@oracle.com>",
          "date": "2024-05-06 17:09:12 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "92956786b4e26ea22e5b3c1c86cc71f5c9b3b9d8",
          "subject": "selftests/bpf: MUL range computation tests.",
          "message": "Added a test for bound computation in MUL when non constant\nvalues are used and both registers have bounded ranges.\n\nSigned-off-by: Cupertino Miranda <cupertino.miranda@oracle.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nCc: Yonghong Song <yonghong.song@linux.dev>\nCc: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nCc: David Faust <david.faust@oracle.com>\nCc: Jose Marchesi <jose.marchesi@oracle.com>\nCc: Elena Zannoni <elena.zannoni@oracle.com>\nLink: https://lore.kernel.org/r/20240506141849.185293-7-cupertino.miranda@oracle.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Cupertino Miranda <cupertino.miranda@oracle.com>",
          "date": "2024-05-06 17:09:12 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_bounds.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "06ebfd11678ad63cfd7021580e13d1582ee6c782",
      "merge_subject": "Merge branch 'bpf-introduce-kprobe_multi-session-attach'",
      "merge_body": "Jiri Olsa says:\n\n====================\nbpf: Introduce kprobe_multi session attach\n\nhi,\nadding support to attach kprobe program through kprobe_multi link\nin a session mode, which means:\n  - program is attached to both function entry and return\n  - entry program can decided if the return program gets executed\n  - entry program can share u64 cookie value with return program\n\nThe initial RFC for this was posted in [0] and later discussed more\nand which ended up with the session idea [1]\n\nHaving entry together with return probe for given function is common\nuse case for tetragon, bpftrace and most likely for others.\n\nAt the moment if we want both entry and return probe to execute bpf\nprogram we need to create two (entry and return probe) links. The link\nfor return probe creates extra entry probe to setup the return probe.\nThe extra entry probe execution could be omitted if we had a way to\nuse just single link for both entry and exit probe.\n\nIn addition the possibility to control the return program execution\nand sharing data within entry and return probe allows for other use\ncases.\n\nv2 changes:\n  - renamed BPF_TRACE_KPROBE_MULTI_SESSION to BPF_TRACE_KPROBE_SESSION\n    [Andrii]\n  - use arrays for results in selftest [Andrii]\n  - various small selftests and libbpf changes [Andrii]\n  - moved the verifier cookie setup earlier in check_kfunc_call [Andrii]\n  - added acks\n\nAlso available at:\n  https://git.kernel.org/pub/scm/linux/kernel/git/jolsa/perf.git\n  bpf/session_data\n\nthanks,\njirka\n\n[0] https://lore.kernel.org/bpf/20240207153550.856536-1-jolsa@kernel.org/\n[1] https://lore.kernel.org/bpf/20240228090242.4040210-1-jolsa@kernel.org/\n---\n====================\n\nLink: https://lore.kernel.org/r/20240430112830.1184228-1-jolsa@kernel.org\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2024-04-30 10:23:27 -0700",
      "commits": [
        {
          "hash": "535a3692ba7245792e6f23654507865d4293c850",
          "subject": "bpf: Add support for kprobe session attach",
          "message": "Adding support to attach bpf program for entry and return probe\nof the same function. This is common use case which at the moment\nrequires to create two kprobe multi links.\n\nAdding new BPF_TRACE_KPROBE_SESSION attach type that instructs\nkernel to attach single link program to both entry and exit probe.\n\nIt's possible to control execution of the bpf program on return\nprobe simply by returning zero or non zero from the entry bpf\nprogram execution to execute or not the bpf program on return\nprobe respectively.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240430112830.1184228-2-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-04-30 09:45:53 -0700",
          "modified_files": [
            "include/uapi/linux/bpf.h",
            "kernel/bpf/syscall.c",
            "kernel/trace/bpf_trace.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "adf46d88ae4b2557f7e2e02547a25fb866935711",
          "subject": "bpf: Add support for kprobe session context",
          "message": "Adding struct bpf_session_run_ctx object to hold session related\ndata, which is atm is_return bool and data pointer coming in\nfollowing changes.\n\nPlacing bpf_session_run_ctx layer in between bpf_run_ctx and\nbpf_kprobe_multi_run_ctx so the session data can be retrieved\nregardless of if it's kprobe_multi or uprobe_multi link, which\nsupport is coming in future. This way both kprobe_multi and\nuprobe_multi can use same kfuncs to access the session data.\n\nAdding bpf_session_is_return kfunc that returns true if the\nbpf program is executed from the exit probe of the kprobe multi\nlink attached in wrapper mode. It returns false otherwise.\n\nAdding new kprobe hook for kprobe program type.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240430112830.1184228-3-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-04-30 09:45:53 -0700",
          "modified_files": [
            "kernel/bpf/btf.c",
            "kernel/trace/bpf_trace.c"
          ]
        },
        {
          "hash": "5c919acef85147886eb2abf86fb147f94680a8b0",
          "subject": "bpf: Add support for kprobe session cookie",
          "message": "Adding support for cookie within the session of kprobe multi\nentry and return program.\n\nThe session cookie is u64 value and can be retrieved be new\nkfunc bpf_session_cookie, which returns pointer to the cookie\nvalue. The bpf program can use the pointer to store (on entry)\nand load (on return) the value.\n\nThe cookie value is implemented via fprobe feature that allows\nto share values between entry and return ftrace fprobe callbacks.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240430112830.1184228-4-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-04-30 09:45:53 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "kernel/trace/bpf_trace.c"
          ]
        },
        {
          "hash": "2ca178f02b2f4e523e970894def16282e4adbc39",
          "subject": "libbpf: Add support for kprobe session attach",
          "message": "Adding support to attach program in kprobe session mode\nwith bpf_program__attach_kprobe_multi_opts function.\n\nAdding session bool to bpf_kprobe_multi_opts struct that allows\nto load and attach the bpf program via kprobe session.\nthe attachment to create kprobe multi session.\n\nAlso adding new program loader section that allows:\n SEC(\"kprobe.session/bpf_fentry_test*\")\n\nand loads/attaches kprobe program as kprobe session.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240430112830.1184228-5-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-04-30 09:45:53 -0700",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/libbpf.c",
            "tools/lib/bpf/libbpf.h"
          ]
        },
        {
          "hash": "7b94965429f2fa32a83e1275c6bf6ed0add08603",
          "subject": "libbpf: Add kprobe session attach type name to attach_type_name",
          "message": "Adding kprobe session attach type name to attach_type_name,\nso libbpf_bpf_attach_type_str returns proper string name for\nBPF_TRACE_KPROBE_SESSION attach type.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240430112830.1184228-6-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-04-30 09:45:53 -0700",
          "modified_files": [
            "tools/lib/bpf/libbpf.c"
          ]
        },
        {
          "hash": "0983b1697aefbf69f465f907b934b89bbce467ea",
          "subject": "selftests/bpf: Add kprobe session test",
          "message": "Adding kprobe session test and testing that the entry program\nreturn value controls execution of the return probe program.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240430112830.1184228-7-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-04-30 10:23:01 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_kfuncs.h",
            "tools/testing/selftests/bpf/prog_tests/kprobe_multi_test.c",
            "tools/testing/selftests/bpf/progs/kprobe_multi_session.c"
          ]
        },
        {
          "hash": "a3a5113393ccfad2eb23ca091aa6e55b5bd67eb4",
          "subject": "selftests/bpf: Add kprobe session cookie test",
          "message": "Adding kprobe session test that verifies the cookie value\nget properly propagated from entry to return program.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240430112830.1184228-8-jolsa@kernel.org",
          "author": "Jiri Olsa <jolsa@kernel.org>",
          "date": "2024-04-30 10:23:25 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_kfuncs.h",
            "tools/testing/selftests/bpf/prog_tests/kprobe_multi_test.c",
            "tools/testing/selftests/bpf/progs/kprobe_multi_session_cookie.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "a86538a2efb826b9a62c7b41e0499948b04aec7d",
      "merge_subject": "Merge branch 'bpf-prevent-userspace-memory-access'",
      "merge_body": "Puranjay Mohan says:\n\n====================\nbpf: prevent userspace memory access\n\nV5: https://lore.kernel.org/bpf/20240324185356.59111-1-puranjay12@gmail.com/\nChanges in V6:\n- Disable the verifier's instrumentation in x86-64 and update the JIT to\n  take care of vsyscall page in addition to userspace addresses.\n- Update bpf_testmod to test for vsyscall addresses.\n\nV4: https://lore.kernel.org/bpf/20240321124640.8870-1-puranjay12@gmail.com/\nChanges in V5:\n- Use TASK_SIZE_MAX + PAGE_SIZE, VSYSCALL_ADDR as userspace boundary in\n  x86-64 JIT.\n- Added Acked-by: Ilya Leoshkevich <iii@linux.ibm.com>\n\nV3: https://lore.kernel.org/bpf/20240321120842.78983-1-puranjay12@gmail.com/\nChanges in V4:\n- Disable this feature on architectures that don't define\n  CONFIG_ARCH_HAS_NON_OVERLAPPING_ADDRESS_SPACE.\n- By doing the above, we don't need anything explicitly for s390x.\n\nV2: https://lore.kernel.org/bpf/20240321101058.68530-1-puranjay12@gmail.com/\nChanges in V3:\n- Return 0 from bpf_arch_uaddress_limit() in disabled case because it\n  returns u64.\n- Modify the check in verifier to no do instrumentation when uaddress_limit\n  is 0.\n\nV1: https://lore.kernel.org/bpf/20240320105436.4781-1-puranjay12@gmail.com/\nChanges in V2:\n- Disable this feature on s390x.\n\nWith BPF_PROBE_MEM, BPF allows de-referencing an untrusted pointer. To\nthwart invalid memory accesses, the JITs add an exception table entry for\nall such accesses. But in case the src_reg + offset is a userspace address,\nthe BPF program might read that memory if the user has mapped it.\n\nx86-64 JIT already instruments the BPF_PROBE_MEM based loads with checks to\nskip loads from userspace addresses, but is doesn't check for vsyscall page\nbecause it falls in the kernel address space but is considered a userspace\npage. The second patch in this series fixes the x86-64 JIT to also skip\nloads from the vsyscall page. The last patch updates the bpf_testmod so\nthis address can be checked as part of the selftests.\n\nOther architectures don't have the complexity of the vsyscall address and\njust need to skip loads from the userspace. To make this more scalable and\nrobust, the verifier is updated in the first patch to instrument\nBPF_PROBE_MEM to skip loads from the userspace addresses.\n====================\n\nLink: https://lore.kernel.org/r/20240424100210.11982-1-puranjay@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-04-26 09:45:19 -0700",
      "commits": [
        {
          "hash": "66e13b615a0ce76b785d780ecc9776ba71983629",
          "subject": "bpf: verifier: prevent userspace memory access",
          "message": "With BPF_PROBE_MEM, BPF allows de-referencing an untrusted pointer. To\nthwart invalid memory accesses, the JITs add an exception table entry\nfor all such accesses. But in case the src_reg + offset is a userspace\naddress, the BPF program might read that memory if the user has\nmapped it.\n\nMake the verifier add guard instructions around such memory accesses and\nskip the load if the address falls into the userspace region.\n\nThe JITs need to implement bpf_arch_uaddress_limit() to define where\nthe userspace addresses end for that architecture or TASK_SIZE is taken\nas default.\n\nThe implementation is as follows:\n\nREG_AX =  SRC_REG\nif(offset)\n\tREG_AX += offset;\nREG_AX >>= 32;\nif (REG_AX <= (uaddress_limit >> 32))\n\tDST_REG = 0;\nelse\n\tDST_REG = *(size *)(SRC_REG + offset);\n\nComparing just the upper 32 bits of the load address with the upper\n32 bits of uaddress_limit implies that the values are being aligned down\nto a 4GB boundary before comparison.\n\nThe above means that all loads with address <= uaddress_limit + 4GB are\nskipped. This is acceptable because there is a large hole (much larger\nthan 4GB) between userspace and kernel space memory, therefore a\ncorrectly functioning BPF program should not access this 4GB memory\nabove the userspace.\n\nLet's analyze what this patch does to the following fentry program\ndereferencing an untrusted pointer:\n\n  SEC(\"fentry/tcp_v4_connect\")\n  int BPF_PROG(fentry_tcp_v4_connect, struct sock *sk)\n  {\n                *(volatile long *)sk;\n                return 0;\n  }\n\n    BPF Program before              |           BPF Program after\n    ------------------              |           -----------------\n\n  0: (79) r1 = *(u64 *)(r1 +0)          0: (79) r1 = *(u64 *)(r1 +0)\n  -----------------------------------------------------------------------\n  1: (79) r1 = *(u64 *)(r1 +0) --\\      1: (bf) r11 = r1\n  ----------------------------\\   \\     2: (77) r11 >>= 32\n  2: (b7) r0 = 0               \\   \\    3: (b5) if r11 <= 0x8000 goto pc+2\n  3: (95) exit                  \\   \\-> 4: (79) r1 = *(u64 *)(r1 +0)\n                                 \\      5: (05) goto pc+1\n                                  \\     6: (b7) r1 = 0\n                                   \\--------------------------------------\n                                        7: (b7) r0 = 0\n                                        8: (95) exit\n\nAs you can see from above, in the best case (off=0), 5 extra instructions\nare emitted.\n\nNow, we analyze the same program after it has gone through the JITs of\nARM64 and RISC-V architectures. We follow the single load instruction\nthat has the untrusted pointer and see what instrumentation has been\nadded around it.\n\n                                x86-64 JIT\n                                ==========\n     JIT's Instrumentation\n          (upstream)\n     ---------------------\n\n   0:   nopl   0x0(%rax,%rax,1)\n   5:   xchg   %ax,%ax\n   7:   push   %rbp\n   8:   mov    %rsp,%rbp\n   b:   mov    0x0(%rdi),%rdi\n  ---------------------------------\n   f:   movabs $0x800000000000,%r11\n  19:   cmp    %r11,%rdi\n  1c:   jb     0x000000000000002a\n  1e:   mov    %rdi,%r11\n  21:   add    $0x0,%r11\n  28:   jae    0x000000000000002e\n  2a:   xor    %edi,%edi\n  2c:   jmp    0x0000000000000032\n  2e:   mov    0x0(%rdi),%rdi\n  ---------------------------------\n  32:   xor    %eax,%eax\n  34:   leave\n  35:   ret\n\nThe x86-64 JIT already emits some instructions to protect against user\nmemory access. This patch doesn't make any changes for the x86-64 JIT.\n\n                                  ARM64 JIT\n                                  =========\n\n        No Intrumentation                       Verifier's Instrumentation\n           (upstream)                                  (This patch)\n        -----------------                       --------------------------\n\n   0:   add     x9, x30, #0x0                0:   add     x9, x30, #0x0\n   4:   nop                                  4:   nop\n   8:   paciasp                              8:   paciasp\n   c:   stp     x29, x30, [sp, #-16]!        c:   stp     x29, x30, [sp, #-16]!\n  10:   mov     x29, sp                     10:   mov     x29, sp\n  14:   stp     x19, x20, [sp, #-16]!       14:   stp     x19, x20, [sp, #-16]!\n  18:   stp     x21, x22, [sp, #-16]!       18:   stp     x21, x22, [sp, #-16]!\n  1c:   stp     x25, x26, [sp, #-16]!       1c:   stp     x25, x26, [sp, #-16]!\n  20:   stp     x27, x28, [sp, #-16]!       20:   stp     x27, x28, [sp, #-16]!\n  24:   mov     x25, sp                     24:   mov     x25, sp\n  28:   mov     x26, #0x0                   28:   mov     x26, #0x0\n  2c:   sub     x27, x25, #0x0              2c:   sub     x27, x25, #0x0\n  30:   sub     sp, sp, #0x0                30:   sub     sp, sp, #0x0\n  34:   ldr     x0, [x0]                    34:   ldr     x0, [x0]\n--------------------------------------------------------------------------------\n  38:   ldr     x0, [x0] ----------\\        38:   add     x9, x0, #0x0\n-----------------------------------\\\\       3c:   lsr     x9, x9, #32\n  3c:   mov     x7, #0x0            \\\\      40:   cmp     x9, #0x10, lsl #12\n  40:   mov     sp, sp               \\\\     44:   b.ls    0x0000000000000050\n  44:   ldp     x27, x28, [sp], #16   \\\\--> 48:   ldr     x0, [x0]\n  48:   ldp     x25, x26, [sp], #16    \\    4c:   b       0x0000000000000054\n  4c:   ldp     x21, x22, [sp], #16     \\   50:   mov     x0, #0x0\n  50:   ldp     x19, x20, [sp], #16      \\---------------------------------------\n  54:   ldp     x29, x30, [sp], #16         54:   mov     x7, #0x0\n  58:   add     x0, x7, #0x0                58:   mov     sp, sp\n  5c:   autiasp                             5c:   ldp     x27, x28, [sp], #16\n  60:   ret                                 60:   ldp     x25, x26, [sp], #16\n  64:   nop                                 64:   ldp     x21, x22, [sp], #16\n  68:   ldr     x10, 0x0000000000000070     68:   ldp     x19, x20, [sp], #16\n  6c:   br      x10                         6c:   ldp     x29, x30, [sp], #16\n                                            70:   add     x0, x7, #0x0\n                                            74:   autiasp\n                                            78:   ret\n                                            7c:   nop\n                                            80:   ldr     x10, 0x0000000000000088\n                                            84:   br      x10\n\nThere are 6 extra instructions added in ARM64 in the best case. This will\nbecome 7 in the worst case (off != 0).\n\n                           RISC-V JIT (RISCV_ISA_C Disabled)\n                           ==========\n\n        No Intrumentation           Verifier's Instrumentation\n           (upstream)                      (This patch)\n        -----------------           --------------------------\n\n   0:   nop                            0:   nop\n   4:   nop                            4:   nop\n   8:   li      a6, 33                 8:   li      a6, 33\n   c:   addi    sp, sp, -16            c:   addi    sp, sp, -16\n  10:   sd      s0, 8(sp)             10:   sd      s0, 8(sp)\n  14:   addi    s0, sp, 16            14:   addi    s0, sp, 16\n  18:   ld      a0, 0(a0)             18:   ld      a0, 0(a0)\n---------------------------------------------------------------\n  1c:   ld      a0, 0(a0) --\\         1c:   mv      t0, a0\n--------------------------\\  \\        20:   srli    t0, t0, 32\n  20:   li      a5, 0      \\  \\       24:   lui     t1, 4096\n  24:   ld      s0, 8(sp)   \\  \\      28:   sext.w  t1, t1\n  28:   addi    sp, sp, 16   \\  \\     2c:   bgeu    t1, t0, 12\n  2c:   sext.w  a0, a5        \\  \\--> 30:   ld      a0, 0(a0)\n  30:   ret                    \\      34:   j       8\n                                \\     38:   li      a0, 0\n                                 \\------------------------------\n                                      3c:   li      a5, 0\n                                      40:   ld      s0, 8(sp)\n                                      44:   addi    sp, sp, 16\n                                      48:   sext.w  a0, a5\n                                      4c:   ret\n\nThere are 7 extra instructions added in RISC-V.\n\nFixes: 800834285361 (\"bpf, arm64: Add BPF exception tables\")\nReported-by: Breno Leitao <leitao@debian.org>\nSuggested-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: Ilya Leoshkevich <iii@linux.ibm.com>\nSigned-off-by: Puranjay Mohan <puranjay12@gmail.com>\nLink: https://lore.kernel.org/r/20240424100210.11982-2-puranjay@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Puranjay Mohan <puranjay12@gmail.com>",
          "date": "2024-04-26 09:45:18 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/filter.h",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b599d7d26d6ad1fc9975218574bc2ca6d0293cfd",
          "subject": "bpf, x86: Fix PROBE_MEM runtime load check",
          "message": "When a load is marked PROBE_MEM - e.g. due to PTR_UNTRUSTED access - the\naddress being loaded from is not necessarily valid. The BPF jit sets up\nexception handlers for each such load which catch page faults and 0 out\nthe destination register.\n\nIf the address for the load is outside kernel address space, the load\nwill escape the exception handling and crash the kernel. To prevent this\nfrom happening, the emits some instruction to verify that addr is > end\nof userspace addresses.\n\nx86 has a legacy vsyscall ABI where a page at address 0xffffffffff600000\nis mapped with user accessible permissions. The addresses in this page\nare considered userspace addresses by the fault handler. Therefore, a\nBPF program accessing this page will crash the kernel.\n\nThis patch fixes the runtime checks to also check that the PROBE_MEM\naddress is below VSYSCALL_ADDR.\n\nExample BPF program:\n\n SEC(\"fentry/tcp_v4_connect\")\n int BPF_PROG(fentry_tcp_v4_connect, struct sock *sk)\n {\n\t*(volatile unsigned long *)&sk->sk_tsq_flags;\n\treturn 0;\n }\n\nBPF Assembly:\n\n 0: (79) r1 = *(u64 *)(r1 +0)\n 1: (79) r1 = *(u64 *)(r1 +344)\n 2: (b7) r0 = 0\n 3: (95) exit\n\n\t\t\t       x86-64 JIT\n\t\t\t       ==========\n\n            BEFORE                                    AFTER\n\t    ------                                    -----\n\n 0:   nopl   0x0(%rax,%rax,1)             0:   nopl   0x0(%rax,%rax,1)\n 5:   xchg   %ax,%ax                      5:   xchg   %ax,%ax\n 7:   push   %rbp                         7:   push   %rbp\n 8:   mov    %rsp,%rbp                    8:   mov    %rsp,%rbp\n b:   mov    0x0(%rdi),%rdi               b:   mov    0x0(%rdi),%rdi\n-------------------------------------------------------------------------------\n f:   movabs $0x100000000000000,%r11      f:   movabs $0xffffffffff600000,%r10\n19:   add    $0x2a0,%rdi                 19:   mov    %rdi,%r11\n20:   cmp    %r11,%rdi                   1c:   add    $0x2a0,%r11\n23:   jae    0x0000000000000029          23:   sub    %r10,%r11\n25:   xor    %edi,%edi                   26:   movabs $0x100000000a00000,%r10\n27:   jmp    0x000000000000002d          30:   cmp    %r10,%r11\n29:   mov    0x0(%rdi),%rdi              33:   ja     0x0000000000000039\n--------------------------------\\        35:   xor    %edi,%edi\n2d:   xor    %eax,%eax           \\       37:   jmp    0x0000000000000040\n2f:   leave                       \\      39:   mov    0x2a0(%rdi),%rdi\n30:   ret                          \\--------------------------------------------\n                                         40:   xor    %eax,%eax\n                                         42:   leave\n                                         43:   ret\n\nSigned-off-by: Puranjay Mohan <puranjay@kernel.org>\nLink: https://lore.kernel.org/r/20240424100210.11982-3-puranjay@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Puranjay Mohan <puranjay@kernel.org>",
          "date": "2024-04-26 09:45:18 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c"
          ]
        },
        {
          "hash": "7cd6750d9a560fa69bb640a7280479d6a67999ad",
          "subject": "selftests/bpf: Test PROBE_MEM of VSYSCALL_ADDR on x86-64",
          "message": "The vsyscall is a legacy API for fast execution of system calls. It maps\na page at address VSYSCALL_ADDR into the userspace program. This address\nis in the top 10MB of the address space:\n\nffffffffff600000 - ffffffffff600fff |    4 kB | legacy vsyscall ABI\n\nThe last commit fixes the x86-64 BPF JIT to skip accessing addresses in\nthis memory region. Add this address to bpf_testmod_return_ptr() so we\ncan make sure that it is fixed.\n\nAfter this change and without the previous commit, subprogs_extable\nselftest will crash the kernel.\n\nSigned-off-by: Puranjay Mohan <puranjay@kernel.org>\nLink: https://lore.kernel.org/r/20240424100210.11982-4-puranjay@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Puranjay Mohan <puranjay@kernel.org>",
          "date": "2024-04-26 09:45:18 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "52578f7f53ff8fe3a8f6f3bc8b5956615c07a16e",
      "merge_subject": "Merge branch 'BPF crypto API framework'",
      "merge_body": "Vadim Fedorenko says:\n\n====================\nThis series introduces crypto kfuncs to make BPF programs able to\nutilize kernel crypto subsystem. Crypto operations made pluggable to\navoid extensive growth of kernel when it's not needed. Only skcipher is\nadded within this series, but it can be easily extended to other types\nof operations. No hardware offload supported as it needs sleepable\ncontext which is not available for TX or XDP programs. At the same time\ncrypto context initialization kfunc can only run in sleepable context,\nthat's why it should be run separately and store the result in the map.\n\nSelftests show the common way to implement crypto actions in BPF\nprograms. Benchmark is also added to have a baseline.\n====================\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_date": "2024-04-24 16:01:10 -0700",
      "commits": [
        {
          "hash": "3e1c6f35409f9e447bf37f64840f5b65576bfb78",
          "subject": "bpf: make common crypto API for TC/XDP programs",
          "message": "Add crypto API support to BPF to be able to decrypt or encrypt packets\nin TC/XDP BPF programs. Special care should be taken for initialization\npart of crypto algo because crypto alloc) doesn't work with preemtion\ndisabled, it can be run only in sleepable BPF program. Also async crypto\nis not supported because of the very same issue - TC/XDP BPF programs\nare not sleepable.\n\nSigned-off-by: Vadim Fedorenko <vadfed@meta.com>\nLink: https://lore.kernel.org/r/20240422225024.2847039-2-vadfed@meta.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Vadim Fedorenko <vadfed@meta.com>",
          "date": "2024-04-24 16:01:10 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_crypto.h",
            "kernel/bpf/Makefile",
            "kernel/bpf/crypto.c",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "fda4f71282b21a8cf230b656781efb0a41371fd4",
          "subject": "bpf: crypto: add skcipher to bpf crypto",
          "message": "Implement skcipher crypto in BPF crypto framework.\n\nSigned-off-by: Vadim Fedorenko <vadfed@meta.com>\nAcked-by: Herbert Xu <herbert@gondor.apana.org.au>\nLink: https://lore.kernel.org/r/20240422225024.2847039-3-vadfed@meta.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Vadim Fedorenko <vadfed@meta.com>",
          "date": "2024-04-24 16:01:10 -0700",
          "modified_files": [
            "MAINTAINERS",
            "crypto/Makefile",
            "crypto/bpf_crypto_skcipher.c"
          ]
        },
        {
          "hash": "91541ab192fc7f573e6c711ba9c2ae22a299c408",
          "subject": "selftests: bpf: crypto skcipher algo selftests",
          "message": "Add simple tc hook selftests to show the way to work with new crypto\nBPF API. Some tricky dynptr initialization is used to provide empty iv\ndynptr. Simple AES-ECB algo is used to demonstrate encryption and\ndecryption of fixed size buffers.\n\nSigned-off-by: Vadim Fedorenko <vadfed@meta.com>\nLink: https://lore.kernel.org/r/20240422225024.2847039-4-vadfed@meta.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Vadim Fedorenko <vadfed@meta.com>",
          "date": "2024-04-24 16:01:10 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/config",
            "tools/testing/selftests/bpf/prog_tests/crypto_sanity.c",
            "tools/testing/selftests/bpf/progs/crypto_basic.c",
            "tools/testing/selftests/bpf/progs/crypto_common.h",
            "tools/testing/selftests/bpf/progs/crypto_sanity.c"
          ]
        },
        {
          "hash": "8000e627dc98efc44658af6150fd81c62d936b1b",
          "subject": "selftests: bpf: crypto: add benchmark for crypto functions",
          "message": "Some simple benchmarks are added to understand the baseline of\nperformance.\n\nSigned-off-by: Vadim Fedorenko <vadfed@meta.com>\nLink: https://lore.kernel.org/r/20240422225024.2847039-5-vadfed@meta.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Vadim Fedorenko <vadfed@meta.com>",
          "date": "2024-04-24 16:01:10 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/Makefile",
            "tools/testing/selftests/bpf/bench.c",
            "tools/testing/selftests/bpf/benchs/bench_bpf_crypto.c",
            "tools/testing/selftests/bpf/progs/crypto_bench.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "55d30cc90fd42587594345a025b34399585e6e19",
      "merge_subject": "Merge branch 'introduce-bpf_preempt_-disable-enable'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nIntroduce bpf_preempt_{disable,enable}\n\nThis set introduces two kfuncs, bpf_preempt_disable and\nbpf_preempt_enable, which are wrappers around preempt_disable and\npreempt_enable in the kernel. These functions allow a BPF program to\nhave code sections where preemption is disabled. There are multiple use\ncases that are served by such a feature, a few are listed below:\n\n1. Writing safe per-CPU alogrithms/data structures that work correctly\n   across different contexts.\n2. Writing safe per-CPU allocators similar to bpf_memalloc on top of\n   array/arena memory blobs.\n3. Writing locking algorithms in BPF programs natively.\n\nNote that local_irq_disable/enable equivalent is also needed for proper\nIRQ context protection, but that is a more involved change and will be\nsent later.\n\nWhile bpf_preempt_{disable,enable} is not sufficient for all of these\nusage scenarios on its own, it is still necessary.\n\nThe same effect as these kfuncs can in some sense be already achieved\nusing the bpf_spin_lock or rcu_read_lock APIs, therefore from the\nstandpoint of kernel functionality exposure in the verifier, this is\nwell understood territory.\n\nNote that these helpers do allow calling kernel helpers and kfuncs from\nwithin the non-preemptible region (unless sleepable). Otherwise, any\nlocks built using the preemption helpers will be as limited as\nexisting bpf_spin_lock.\n\nNesting is allowed by keeping a counter for tracking remaining enables\nrequired to be performed. Similar approach can be applied to\nrcu_read_locks in a follow up.\n\nChangelog\n=========\nv1: https://lore.kernel.org/bpf/20240423061922.2295517-1-memxor@gmail.com\n\n * Move kfunc BTF ID declerations above css task kfunc for\n   !CONFIG_CGROUPS config (Alexei)\n * Add test case for global function call in non-preemptible region\n   (Jiri)\n====================\n\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nLink: https://lore.kernel.org/r/20240424031315.2757363-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-04-24 09:48:05 -0700",
      "commits": [
        {
          "hash": "fc7566ad0a826cdc8886c5dbbb39ce72a0dc6333",
          "subject": "bpf: Introduce bpf_preempt_[disable,enable] kfuncs",
          "message": "Introduce two new BPF kfuncs, bpf_preempt_disable and\nbpf_preempt_enable. These kfuncs allow disabling preemption in BPF\nprograms. Nesting is allowed, since the intended use cases includes\nbuilding native BPF spin locks without kernel helper involvement. Apart\nfrom that, this can be used to per-CPU data structures for cases where\nprograms (or userspace) may preempt one or the other. Currently, while\nper-CPU access is stable, whether it will be consistent is not\nguaranteed, as only migration is disabled for BPF programs.\n\nGlobal functions are disallowed from being called, but support for them\nwill be added as a follow up not just preempt kfuncs, but rcu_read_lock\nkfuncs as well. Static subprog calls are permitted. Sleepable helpers\nand kfuncs are disallowed in non-preemptible regions.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20240424031315.2757363-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-04-24 09:47:49 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "3134396f1cba939783b87c63dae3a54708285a9a",
          "subject": "selftests/bpf: Add tests for preempt kfuncs",
          "message": "Add tests for nested cases, nested count preservation upon different\nsubprog calls that disable/enable preemption, and test sleepable helper\ncall in non-preemptible regions.\n\n182/1   preempt_lock/preempt_lock_missing_1:OK\n182/2   preempt_lock/preempt_lock_missing_2:OK\n182/3   preempt_lock/preempt_lock_missing_3:OK\n182/4   preempt_lock/preempt_lock_missing_3_minus_2:OK\n182/5   preempt_lock/preempt_lock_missing_1_subprog:OK\n182/6   preempt_lock/preempt_lock_missing_2_subprog:OK\n182/7   preempt_lock/preempt_lock_missing_2_minus_1_subprog:OK\n182/8   preempt_lock/preempt_balance:OK\n182/9   preempt_lock/preempt_balance_subprog_test:OK\n182/10  preempt_lock/preempt_global_subprog_test:OK\n182/11  preempt_lock/preempt_sleepable_helper:OK\n182     preempt_lock:OK\nSummary: 1/11 PASSED, 0 SKIPPED, 0 FAILED\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20240424031315.2757363-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-04-24 09:47:49 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/preempt_lock.c",
            "tools/testing/selftests/bpf/progs/preempt_lock.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "6e10b6350a67d398c795ac0b93a7bb7103633fe4",
      "merge_subject": "Merge branch 'introduce-bpf_wq'",
      "merge_body": "Benjamin Tissoires says:\n\n====================\nIntroduce bpf_wq\n\nThis is a followup of sleepable bpf_timer[0].\n\nWhen discussing sleepable bpf_timer, it was thought that we should give\na try to bpf_wq, as the 2 APIs are similar but distinct enough to\njustify a new one.\n\nSo here it is.\n\nI tried to keep as much as possible common code in kernel/bpf/helpers.c\nbut I couldn't get away with code duplication in kernel/bpf/verifier.c.\n\nThis series introduces a basic bpf_wq support:\n- creation is supported\n- assignment is supported\n- running a simple bpf_wq is also supported.\n\nWe will probably need to extend the API further with:\n- a full delayed_work API (can be piggy backed on top with a correct\n  flag)\n- bpf_wq_cancel() <- apparently not, this is shooting ourself in the\n  foot\n- bpf_wq_cancel_sync() (for sleepable programs)\n- documentation\n---\n\nFor reference, the use cases I have in mind:\n\n---\n\nBasically, I need to be able to defer a HID-BPF program for the\nfollowing reasons (from the aforementioned patch):\n1. defer an event:\n   Sometimes we receive an out of proximity event, but the device can not\n   be trusted enough, and we need to ensure that we won't receive another\n   one in the following n milliseconds. So we need to wait those n\n   milliseconds, and eventually re-inject that event in the stack.\n\n2. inject new events in reaction to one given event:\n   We might want to transform one given event into several. This is the\n   case for macro keys where a single key press is supposed to send\n   a sequence of key presses. But this could also be used to patch a\n   faulty behavior, if a device forgets to send a release event.\n\n3. communicate with the device in reaction to one event:\n   We might want to communicate back to the device after a given event.\n   For example a device might send us an event saying that it came back\n   from sleeping state and needs to be re-initialized.\n\nCurrently we can achieve that by keeping a userspace program around,\nraise a bpf event, and let that userspace program inject the events and\ncommands.\nHowever, we are just keeping that program alive as a daemon for just\nscheduling commands. There is no logic in it, so it doesn't really justify\nan actual userspace wakeup. So a kernel workqueue seems simpler to handle.\n\nbpf_timers are currently running in a soft IRQ context, this patch\nseries implements a sleppable context for them.\n\nCheers,\nBenjamin\n\n[0] https://lore.kernel.org/all/20240408-hid-bpf-sleepable-v6-0-0499ddd91b94@kernel.org/\n\nChanges in v2:\n- took previous review into account\n- mainly dropped BPF_F_WQ_SLEEPABLE\n- Link to v1: https://lore.kernel.org/r/20240416-bpf_wq-v1-0-c9e66092f842@kernel.org\n\n====================\n\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-0-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-04-23 19:46:57 -0700",
      "commits": [
        {
          "hash": "be2749beff62e0d63cf97fe63cabc79a68443139",
          "subject": "bpf: make timer data struct more generic",
          "message": "To be able to add workqueues and reuse most of the timer code, we need\nto make bpf_hrtimer more generic.\n\nThere is no code change except that the new struct gets a new u64 flags\nattribute. We are still below 2 cache lines, so this shouldn't impact\nthe current running codes.\n\nThe ordering is also changed. Everything related to async callback\nis now on top of bpf_hrtimer.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-1-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 18:31:24 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "56b4a177ae6322173360a93ea828ad18570a5a14",
          "subject": "bpf: replace bpf_timer_init with a generic helper",
          "message": "No code change except for the new flags argument being stored in the\nlocal data struct.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-2-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 18:31:24 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "073f11b0264310b85754b6a0946afee753790c66",
          "subject": "bpf: replace bpf_timer_set_callback with a generic helper",
          "message": "In the same way we have a generic __bpf_async_init(), we also need\nto share code between timer and workqueue for the set_callback call.\n\nWe just add an unused flags parameter, as it will be used for workqueues.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-3-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 18:31:24 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "fc22d9495f0b32d75b5d25a17b300b7aad05c55d",
          "subject": "bpf: replace bpf_timer_cancel_and_free with a generic helper",
          "message": "Same reason than most bpf_timer* functions, we need almost the same for\nworkqueues.\nSo extract the generic part out of it so bpf_wq_cancel_and_free can reuse\nit.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-4-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 18:31:24 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "d56b63cf0c0f71e1b2e04dd8220b408f049e67ff",
          "subject": "bpf: add support for bpf_wq user type",
          "message": "Mostly a copy/paste from the bpf_timer API, without the initialization\nand free, as they will be done in a separate patch.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-5-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 18:31:24 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "f1d0a2fbb0088675cceeab73d1f4b4308b289984",
          "subject": "tools: sync include/uapi/linux/bpf.h",
          "message": "cp include/uapi/linux/bpf.h tools/include/uapi/linux/bpf.h\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-6-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 18:31:24 -0700",
          "modified_files": [
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "ad2c03e691be3268eefc75ff1d892db3f0e79f62",
          "subject": "bpf: verifier: bail out if the argument is not a map",
          "message": "When a kfunc is declared with a KF_ARG_PTR_TO_MAP, we should have\nreg->map_ptr set to a non NULL value, otherwise, that means that the\nunderlying type is not a map.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-7-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 18:31:24 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "d940c9b94d7e6d9cff288623e3e8bf5fdea98b79",
          "subject": "bpf: add support for KF_ARG_PTR_TO_WORKQUEUE",
          "message": "Introduce support for KF_ARG_PTR_TO_WORKQUEUE. The kfuncs will use bpf_wq\nas argument and that will be recognized as workqueue argument by verifier.\nbpf_wq_kern casting can happen inside kfunc, but using bpf_wq in\nargument makes life easier for users who work with non-kern type in BPF\nprogs.\n\nDuplicate process_timer_func into process_wq_func.\nmeta argument is only needed to ensure bpf_wq_init's workqueue and map\narguments are coming from the same map (map_uid logic is necessary for\ncorrect inner-map handling), so also amend check_kfunc_args() to\nmatch what helpers functions check is doing.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-8-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 18:31:25 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "246331e3f1eac905170a923f0ec76725c2558232",
          "subject": "bpf: allow struct bpf_wq to be embedded in arraymaps and hashmaps",
          "message": "Currently bpf_wq_cancel_and_free() is just a placeholder as there is\nno memory allocation for bpf_wq just yet.\n\nAgain, duplication of the bpf_timer approach\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-9-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 18:31:25 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/arraymap.c",
            "kernel/bpf/hashtab.c",
            "kernel/bpf/helpers.c",
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "b4abee7c1ae3d59440e7915da28c6d2cd394738a",
          "subject": "selftests/bpf: add bpf_wq tests",
          "message": "We simply try in all supported map types if we can store/load a bpf_wq.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-10-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 19:46:53 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/wq.c",
            "tools/testing/selftests/bpf/progs/wq.c"
          ]
        },
        {
          "hash": "eb48f6cd41a0f7803770a76bbffb6bd5b1b2ae2f",
          "subject": "bpf: wq: add bpf_wq_init",
          "message": "We need to teach the verifier about the second argument which is declared\nas void * but which is of type KF_ARG_PTR_TO_MAP. We could have dropped\nthis extra case if we declared the second argument as struct bpf_map *,\nbut that means users will have to do extra casting to have their program\ncompile.\n\nWe also need to duplicate the timer code for the checking if the map\nargument is matching the provided workqueue.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-11-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 19:46:57 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "e3d9eac99afd94980475833479332fefd74c5c2b",
          "subject": "selftests/bpf: wq: add bpf_wq_init() checks",
          "message": "Allows to test if allocation/free works\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-12-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 19:46:57 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_experimental.h",
            "tools/testing/selftests/bpf/prog_tests/wq.c",
            "tools/testing/selftests/bpf/progs/wq.c",
            "tools/testing/selftests/bpf/progs/wq_failures.c"
          ]
        },
        {
          "hash": "81f1d7a583fa1fa14f0c4e6140d34b5e3d08d227",
          "subject": "bpf: wq: add bpf_wq_set_callback_impl",
          "message": "To support sleepable async callbacks, we need to tell push_async_cb()\nwhether the cb is sleepable or not.\n\nThe verifier now detects that we are in bpf_wq_set_callback_impl and\ncan allow a sleepable callback to happen.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-13-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 19:46:57 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "01b7b1c5f3cc029bdd2652eba61e953ccd286c0e",
          "subject": "selftests/bpf: add checks for bpf_wq_set_callback()",
          "message": "We assign the callback and set everything up.\nThe actual tests of these callbacks will be done when bpf_wq_start() is\navailable.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-14-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 19:46:57 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_experimental.h",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod_kfunc.h",
            "tools/testing/selftests/bpf/progs/wq.c",
            "tools/testing/selftests/bpf/progs/wq_failures.c"
          ]
        },
        {
          "hash": "8e83da9732d91c60fdc651b2486c8e5935eb0ca2",
          "subject": "bpf: add bpf_wq_start",
          "message": "again, copy/paste from bpf_timer_start().\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-15-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 19:46:57 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "8290dba51910d36721ced6ccf03049ed6b7ea2ce",
          "subject": "selftests/bpf: wq: add bpf_wq_start() checks",
          "message": "Allows to test if allocation/free works\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-16-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Benjamin Tissoires <bentiss@kernel.org>",
          "date": "2024-04-23 19:46:57 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_experimental.h",
            "tools/testing/selftests/bpf/prog_tests/wq.c",
            "tools/testing/selftests/bpf/progs/wq.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "270954791c706b133a03b01e4b2d063dc870f704",
      "merge_subject": "Merge branch 'bpf-allow-bpf_for_each_map_elem-helper-with-different-input-maps'",
      "merge_body": "Philo Lu says:\n\n====================\nbpf: allow bpf_for_each_map_elem() helper with different input maps\n\nCurrently, taking different maps within a single bpf_for_each_map_elem\ncall is not allowed. For example the following codes cannot pass the\nverifier (with error \"tail_call abusing map_ptr\"):\n```\nstatic void test_by_pid(int pid)\n{\n\tif (pid <= 100)\n\t\tbpf_for_each_map_elem(&map1, map_elem_cb, NULL, 0);\n\telse\n\t\tbpf_for_each_map_elem(&map2, map_elem_cb, NULL, 0);\n}\n```\n\nThis is because during bpf_for_each_map_elem verifying,\nbpf_insn_aux_data->map_ptr_state is expected as map_ptr (instead of poison\nstate), which is then needed by set_map_elem_callback_state. However, as\nthere are two different map ptr input, map_ptr_state is marked as\nBPF_MAP_PTR_POISON, and thus the second map_ptr would be lost.\nBPF_MAP_PTR_POISON is also needed by bpf_for_each_map_elem to skip\nretpoline optimization in do_misc_fixups(). Therefore, map_ptr_state and\nmap_ptr are both needed for bpf_for_each_map_elem.\n\nThis patchset solves it by transform bpf_insn_aux_data->map_ptr_state as a\nnew struct, storing poison/unpriv state and map pointer together without\nadditional memory overhead. Then bpf_for_each_map_elem works well with\ndifferent input maps. It also makes map_ptr_state logic clearer.\n\nA test case is added to selftest, which would fail to load without this\npatchset.\n\nChangelogs\n-> v1:\n- PATCH 1/3:\n  - make the commit log clearer\n  - change poison and unpriv to bool in struct bpf_map_ptr_state, also the\n    return value in bpf_map_ptr_poisoned() and bpf_map_ptr_unpriv()\n- PATCH 2/3:\n  - change the comments in set_map_elem_callback_state()\n- PATCH 3/3:\n  - remove the \"skipping the last element\" logic during map updating\n  - change if() to ASSERT_OK()\n\nPlease review, thanks.\n====================\n\nLink: https://lore.kernel.org/r/20240405025536.18113-1-lulie@linux.alibaba.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-04-05 10:31:18 -0700",
      "commits": [
        {
          "hash": "0a525621b7e5b49202b19d8f75382c6778fdd0c1",
          "subject": "bpf: store both map ptr and state in bpf_insn_aux_data",
          "message": "Currently, bpf_insn_aux_data->map_ptr_state is used to store either\nmap_ptr or its poison state (i.e., BPF_MAP_PTR_POISON). Thus\nBPF_MAP_PTR_POISON must be checked before reading map_ptr. In certain\ncases, we may need valid map_ptr even in case of poison state.\nThis will be explained in next patch with bpf_for_each_map_elem()\nhelper.\n\nThis patch changes map_ptr_state into a new struct including both map\npointer and its state (poison/unpriv). It's in the same union with\nstruct bpf_loop_inline_state, so there is no extra memory overhead.\nBesides, macros BPF_MAP_PTR_UNPRIV/BPF_MAP_PTR_POISON/BPF_MAP_PTR are no\nlonger needed.\n\nThis patch does not change any existing functionality.\n\nSigned-off-by: Philo Lu <lulie@linux.alibaba.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240405025536.18113-2-lulie@linux.alibaba.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Philo Lu <lulie@linux.alibaba.com>",
          "date": "2024-04-05 10:31:17 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "9d482da9e17a4ddd5563428f74302a36b2610306",
          "subject": "bpf: allow invoking bpf_for_each_map_elem with different maps",
          "message": "Taking different maps within a single bpf_for_each_map_elem call is not\nallowed before, because from the second map,\nbpf_insn_aux_data->map_ptr_state will be marked as *poison*. In fact\nboth map_ptr and state are needed to support this use case: map_ptr is\nused by set_map_elem_callback_state() while poison state is needed to\ndetermine whether to use direct call.\n\nSigned-off-by: Philo Lu <lulie@linux.alibaba.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240405025536.18113-3-lulie@linux.alibaba.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Philo Lu <lulie@linux.alibaba.com>",
          "date": "2024-04-05 10:31:17 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "fecb1597cc11a23f32faa90d70a199533871686a",
          "subject": "selftests/bpf: add test for bpf_for_each_map_elem() with different maps",
          "message": "A test is added for bpf_for_each_map_elem() with either an arraymap or a\nhashmap.\n$ tools/testing/selftests/bpf/test_progs -t for_each\n #93/1    for_each/hash_map:OK\n #93/2    for_each/array_map:OK\n #93/3    for_each/write_map_key:OK\n #93/4    for_each/multi_maps:OK\n #93      for_each:OK\nSummary: 1/4 PASSED, 0 SKIPPED, 0 FAILED\n\nSigned-off-by: Philo Lu <lulie@linux.alibaba.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240405025536.18113-4-lulie@linux.alibaba.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Philo Lu <lulie@linux.alibaba.com>",
          "date": "2024-04-05 10:31:18 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/for_each.c",
            "tools/testing/selftests/bpf/progs/for_each_multi_maps.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "d82c045f9dfde6b9ea220d7f8310c98210dfc8cb",
      "merge_subject": "Merge branch 'inline-bpf_get_branch_snapshot-bpf-helper'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nInline bpf_get_branch_snapshot() BPF helper\n\nImplement inlining of bpf_get_branch_snapshot() BPF helper using generic BPF\nassembly approach. This allows to reduce LBR record usage right before LBR\nrecords are captured from inside BPF program.\n\nSee v1 cover letter ([0]) for some visual examples. I dropped them from v2\nbecause there are multiple independent changes landing and being reviewed, all\nof which remove different parts of LBR record waste, so presenting final state\nof LBR \"waste\" gets more complicated until all of the pieces land.\n\n  [0] https://lore.kernel.org/bpf/20240321180501.734779-1-andrii@kernel.org/\n\nv2->v3:\n  - fix BPF_MUL instruction definition;\nv1->v2:\n  - inlining of bpf_get_smp_processor_id() split out into a separate patch set\n    implementing internal per-CPU BPF instruction;\n  - add efficient divide-by-24 through multiplication logic, and leave\n    comments to explain the idea behind it; this way inlined version of\n    bpf_get_branch_snapshot() has no compromises compared to non-inlined\n    version of the helper  (Alexei).\n====================\n\nLink: https://lore.kernel.org/r/20240404002640.1774210-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-04-04 13:08:01 -0700",
      "commits": [
        {
          "hash": "5e6a3c1ee693da1793739bb378b224bcf33d7f14",
          "subject": "bpf: make bpf_get_branch_snapshot() architecture-agnostic",
          "message": "perf_snapshot_branch_stack is set up in an architecture-agnostic way, so\nthere is no reason for BPF subsystem to keep track of which\narchitectures do support LBR or not. E.g., it looks like ARM64 might soon\nget support for BRBE ([0]), which (with proper integration) should be\npossible to utilize using this BPF helper.\n\nperf_snapshot_branch_stack static call will point to\n__static_call_return0() by default, which just returns zero, which will\nlead to -ENOENT, as expected. So no need to guard anything here.\n\n  [0] https://lore.kernel.org/linux-arm-kernel/20240125094119.2542332-1-anshuman.khandual@arm.com/\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240404002640.1774210-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-04-04 13:08:01 -0700",
          "modified_files": [
            "kernel/trace/bpf_trace.c"
          ]
        },
        {
          "hash": "314a53623cd4e62e1b88126e5ed2bc87073d90ee",
          "subject": "bpf: inline bpf_get_branch_snapshot() helper",
          "message": "Inline bpf_get_branch_snapshot() helper using architecture-agnostic\ninline BPF code which calls directly into underlying callback of\nperf_snapshot_branch_stack static call. This callback is set early\nduring kernel initialization and is never updated or reset, so it's ok\nto fetch actual implementation using static_call_query() and call\ndirectly into it.\n\nThis change eliminates a full function call and saves one LBR entry\nin PERF_SAMPLE_BRANCH_ANY LBR mode.\n\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240404002640.1774210-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-04-04 13:08:01 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "519e1de94b719f741e0de42b085b9a4551c5b15c",
      "merge_subject": "Merge branch 'add-internal-only-bpf-per-cpu-instruction'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nAdd internal-only BPF per-CPU instruction\n\nAdd a new BPF instruction for resolving per-CPU memory addresses.\n\nNew instruction is a special form of BPF_ALU64 | BPF_MOV | BPF_X, with\ninsns->off set to BPF_ADDR_PERCPU (== -1). It resolves provided per-CPU offset\nto an absolute address where per-CPU data resides for \"this\" CPU.\n\nThis patch set implements support for it in x86-64 BPF JIT only.\n\nUsing the new instruction, we also implement inlining for three cases:\n  - bpf_get_smp_processor_id(), which allows to avoid unnecessary trivial\n    function call, saving a bit of performance and also not polluting LBR\n    records with unnecessary function call/return records;\n  - PERCPU_ARRAY's bpf_map_lookup_elem() is completely inlined, bringing its\n    performance to implementing per-CPU data structures using global variables\n    in BPF (which is an awesome improvement, see benchmarks below);\n  - PERCPU_HASH's bpf_map_lookup_elem() is partially inlined, just like the\n    same for non-PERCPU HASH map; this still saves a bit of overhead.\n\nTo validate performance benefits, I hacked together a tiny benchmark doing\nonly bpf_map_lookup_elem() and incrementing the value by 1 for PERCPU_ARRAY\n(arr-inc benchmark below) and PERCPU_HASH (hash-inc benchmark below) maps. To\nestablish a baseline, I also implemented logic similar to PERCPU_ARRAY based\non global variable array using bpf_get_smp_processor_id() to index array for\ncurrent CPU (glob-arr-inc benchmark below).\n\nBEFORE\n======\nglob-arr-inc   :  163.685 \u00b1 0.092M/s\narr-inc        :  138.096 \u00b1 0.160M/s\nhash-inc       :   66.855 \u00b1 0.123M/s\n\nAFTER\n=====\nglob-arr-inc   :  173.921 \u00b1 0.039M/s (+6%)\narr-inc        :  170.729 \u00b1 0.210M/s (+23.7%)\nhash-inc       :   68.673 \u00b1 0.070M/s (+2.7%)\n\nAs can be seen, PERCPU_HASH gets a modest +2.7% improvement, while global\narray-based gets a nice +6% due to inlining of bpf_get_smp_processor_id().\n\nBut what's really important is that arr-inc benchmark basically catches up\nwith glob-arr-inc, resulting in +23.7% improvement. This means that in\npractice it won't be necessary to avoid PERCPU_ARRAY anymore if performance is\ncritical (e.g., high-frequent stats collection, which is often a practical use\nfor PERCPU_ARRAY today).\n\nv1->v2:\n  - use BPF_ALU64 | BPF_MOV instruction instead of LDX (Alexei);\n  - dropped the direct per-CPU memory read instruction, it can always be added\n    back, if necessary;\n  - guarded bpf_get_smp_processor_id() behind x86-64 check (Alexei);\n  - switched all per-cpu addr casts to (unsigned long) to avoid sparse\n    warnings.\n====================\n\nLink: https://lore.kernel.org/r/20240402021307.1012571-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-04-03 10:30:13 -0700",
      "commits": [
        {
          "hash": "7bdbf7446305cb65c510c16d57cde82bc76b234a",
          "subject": "bpf: add special internal-only MOV instruction to resolve per-CPU addrs",
          "message": "Add a new BPF instruction for resolving absolute addresses of per-CPU\ndata from their per-CPU offsets. This instruction is internal-only and\nusers are not allowed to use them directly. They will only be used for\ninternal inlining optimizations for now between BPF verifier and BPF JITs.\n\nWe use a special BPF_MOV | BPF_ALU64 | BPF_X form with insn->off field\nset to BPF_ADDR_PERCPU = -1. I used negative offset value to distinguish\nthem from positive ones used by user-exposed instructions.\n\nSuch instruction performs a resolution of a per-CPU offset stored in\na register to a valid kernel address which can be dereferenced. It is\nuseful in any use case where absolute address of a per-CPU data has to\nbe resolved (e.g., in inlining bpf_map_lookup_elem()).\n\nBPF disassembler is also taught to recognize them to support dumping\nfinal BPF assembly code (non-JIT'ed version).\n\nAdd arch-specific way for BPF JITs to mark support for this instructions.\n\nThis patch also adds support for these instructions in x86-64 BPF JIT.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/r/20240402021307.1012571-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-04-03 10:29:55 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/filter.h",
            "kernel/bpf/core.c",
            "kernel/bpf/disasm.c"
          ]
        },
        {
          "hash": "1ae6921009e5d72787e07ccc04754514ccf6bc99",
          "subject": "bpf: inline bpf_get_smp_processor_id() helper",
          "message": "If BPF JIT supports per-CPU MOV instruction, inline bpf_get_smp_processor_id()\nto eliminate unnecessary function calls.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/r/20240402021307.1012571-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-04-03 10:29:56 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "db69718b8efac802c7cc20d5a6c7dfc913f99c43",
          "subject": "bpf: inline bpf_map_lookup_elem() for PERCPU_ARRAY maps",
          "message": "Using new per-CPU BPF instruction implement inlining for per-CPU ARRAY\nmap lookup helper, if BPF JIT support is present.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/r/20240402021307.1012571-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-04-03 10:29:56 -0700",
          "modified_files": [
            "kernel/bpf/arraymap.c"
          ]
        },
        {
          "hash": "0b56e637f705836b9aa51e2b1058c3c814c121a8",
          "subject": "bpf: inline bpf_map_lookup_elem() helper for PERCPU_HASH map",
          "message": "Using new per-CPU BPF instruction, partially inline\nbpf_map_lookup_elem() helper for per-CPU hashmap BPF map. Just like for\nnormal HASH map, we still generate a call into __htab_map_lookup_elem(),\nbut after that we resolve per-CPU element address using a new\ninstruction, saving on extra functions calls.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/r/20240402021307.1012571-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-04-03 10:29:56 -0700",
          "modified_files": [
            "kernel/bpf/hashtab.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "a4e02d6b91c5e57f820032ec6ad794694c86f327",
      "merge_subject": "Merge branch 'check-bloom-filter-map-value-size'",
      "merge_body": "Andrei Matei says:\n\n====================\nCheck bloom filter map value size\n\nv1->v2:\n- prepend a patch addressing the bloom map specifically\n- change low-level rejection error to EFAULT, to indicate a bug\n====================\n\nLink: https://lore.kernel.org/r/20240327024245.318299-1-andreimatei1@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-03-27 09:56:43 -0700",
      "commits": [
        {
          "hash": "a8d89feba7e54e691ca7c4efc2a6264fa83f3687",
          "subject": "bpf: Check bloom filter map value size",
          "message": "This patch adds a missing check to bloom filter creating, rejecting\nvalues above KMALLOC_MAX_SIZE. This brings the bloom map in line with\nmany other map types.\n\nThe lack of this protection can cause kernel crashes for value sizes\nthat overflow int's. Such a crash was caught by syzkaller. The next\npatch adds more guard-rails at a lower level.\n\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240327024245.318299-2-andreimatei1@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrei Matei <andreimatei1@gmail.com>",
          "date": "2024-03-27 09:56:17 -0700",
          "modified_files": [
            "kernel/bpf/bloom_filter.c",
            "tools/testing/selftests/bpf/prog_tests/bloom_filter_map.c"
          ]
        },
        {
          "hash": "ecc6a2101840177e57c925c102d2d29f260d37c8",
          "subject": "bpf: Protect against int overflow for stack access size",
          "message": "This patch re-introduces protection against the size of access to stack\nmemory being negative; the access size can appear negative as a result\nof overflowing its signed int representation. This should not actually\nhappen, as there are other protections along the way, but we should\nprotect against it anyway. One code path was missing such protections\n(fixed in the previous patch in the series), causing out-of-bounds array\naccesses in check_stack_range_initialized(). This patch causes the\nverification of a program with such a non-sensical access size to fail.\n\nThis check used to exist in a more indirect way, but was inadvertendly\nremoved in a833a17aeac7.\n\nFixes: a833a17aeac7 (\"bpf: Fix verification of indirect var-off stack access\")\nReported-by: syzbot+33f4297b5f927648741a@syzkaller.appspotmail.com\nReported-by: syzbot+aafd0513053a1cbf52ef@syzkaller.appspotmail.com\nCloses: https://lore.kernel.org/bpf/CAADnVQLORV5PT0iTAhRER+iLBTkByCYNBYyvBSgjN1T31K+gOw@mail.gmail.com/\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nLink: https://lore.kernel.org/r/20240327024245.318299-3-andreimatei1@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrei Matei <andreimatei1@gmail.com>",
          "date": "2024-03-27 09:56:36 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "08701e306e480c56b68c1fa35f2c5b27204083e2",
      "merge_subject": "Merge branch 'bpf-introduce-bpf-arena'",
      "merge_body": "Alexei Starovoitov says:\n\n====================\nbpf: Introduce BPF arena.\n\nFrom: Alexei Starovoitov <ast@kernel.org>\n\nv2->v3:\n- contains bpf bits only, but cc-ing past audience for continuity\n- since prerequisite patches landed, this series focus on the main\n  functionality of bpf_arena.\n- adopted Andrii's approach to support arena in libbpf.\n- simplified LLVM support. Instead of two instructions it's now only one.\n- switched to cond_break (instead of open coded iters) in selftests\n- implemented several follow-ups that will be sent after this set\n  . remember first IP and bpf insn that faulted in arena.\n    report to user space via bpftool\n  . copy paste and tweak glob_match() aka mini-regex as a selftests/bpf\n- see patch 1 for detailed description of bpf_arena\n\nv1->v2:\n- Improved commit log with reasons for using vmap_pages_range() in arena.\n  Thanks to Johannes\n- Added support for __arena global variables in bpf programs\n- Fixed race conditions spotted by Barret\n- Fixed wrap32 issue spotted by Barret\n- Fixed bpf_map_mmap_sz() the way Andrii suggested\n\nThe work on bpf_arena was inspired by Barret's work:\nhttps://github.com/google/ghost-userspace/blob/main/lib/queue.bpf.h\nthat implements queues, lists and AVL trees completely as bpf programs\nusing giant bpf array map and integer indices instead of pointers.\nbpf_arena is a sparse array that allows to use normal C pointers to\nbuild such data structures. Last few patches implement page_frag\nallocator, link list and hash table as bpf programs.\n\nv1:\nbpf programs have multiple options to communicate with user space:\n- Various ring buffers (perf, ftrace, bpf): The data is streamed\n  unidirectionally from bpf to user space.\n- Hash map: The bpf program populates elements, and user space consumes\n  them via bpf syscall.\n- mmap()-ed array map: Libbpf creates an array map that is directly\n  accessed by the bpf program and mmap-ed to user space. It's the fastest\n  way. Its disadvantage is that memory for the whole array is reserved at\n  the start.\n====================\n\nLink: https://lore.kernel.org/r/20240308010812.89848-1-alexei.starovoitov@gmail.com\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2024-03-11 15:43:43 -0700",
      "commits": [
        {
          "hash": "317460317a02a1af512697e6e964298dedd8a163",
          "subject": "bpf: Introduce bpf_arena.",
          "message": "Introduce bpf_arena, which is a sparse shared memory region between the bpf\nprogram and user space.\n\nUse cases:\n1. User space mmap-s bpf_arena and uses it as a traditional mmap-ed\n   anonymous region, like memcached or any key/value storage. The bpf\n   program implements an in-kernel accelerator. XDP prog can search for\n   a key in bpf_arena and return a value without going to user space.\n2. The bpf program builds arbitrary data structures in bpf_arena (hash\n   tables, rb-trees, sparse arrays), while user space consumes it.\n3. bpf_arena is a \"heap\" of memory from the bpf program's point of view.\n   The user space may mmap it, but bpf program will not convert pointers\n   to user base at run-time to improve bpf program speed.\n\nInitially, the kernel vm_area and user vma are not populated. User space\ncan fault in pages within the range. While servicing a page fault,\nbpf_arena logic will insert a new page into the kernel and user vmas. The\nbpf program can allocate pages from that region via\nbpf_arena_alloc_pages(). This kernel function will insert pages into the\nkernel vm_area. The subsequent fault-in from user space will populate that\npage into the user vma. The BPF_F_SEGV_ON_FAULT flag at arena creation time\ncan be used to prevent fault-in from user space. In such a case, if a page\nis not allocated by the bpf program and not present in the kernel vm_area,\nthe user process will segfault. This is useful for use cases 2 and 3 above.\n\nbpf_arena_alloc_pages() is similar to user space mmap(). It allocates pages\neither at a specific address within the arena or allocates a range with the\nmaple tree. bpf_arena_free_pages() is analogous to munmap(), which frees\npages and removes the range from the kernel vm_area and from user process\nvmas.\n\nbpf_arena can be used as a bpf program \"heap\" of up to 4GB. The speed of\nbpf program is more important than ease of sharing with user space. This is\nuse case 3. In such a case, the BPF_F_NO_USER_CONV flag is recommended.\nIt will tell the verifier to treat the rX = bpf_arena_cast_user(rY)\ninstruction as a 32-bit move wX = wY, which will improve bpf prog\nperformance. Otherwise, bpf_arena_cast_user is translated by JIT to\nconditionally add the upper 32 bits of user vm_start (if the pointer is not\nNULL) to arena pointers before they are stored into memory. This way, user\nspace sees them as valid 64-bit pointers.\n\nDiff https://github.com/llvm/llvm-project/pull/84410 enables LLVM BPF\nbackend generate the bpf_addr_space_cast() instruction to cast pointers\nbetween address_space(1) which is reserved for bpf_arena pointers and\ndefault address space zero. All arena pointers in a bpf program written in\nC language are tagged as __attribute__((address_space(1))). Hence, clang\nprovides helpful diagnostics when pointers cross address space. Libbpf and\nthe kernel support only address_space == 1. All other address space\nidentifiers are reserved.\n\nrX = bpf_addr_space_cast(rY, /* dst_as */ 1, /* src_as */ 0) tells the\nverifier that rX->type = PTR_TO_ARENA. Any further operations on\nPTR_TO_ARENA register have to be in the 32-bit domain. The verifier will\nmark load/store through PTR_TO_ARENA with PROBE_MEM32. JIT will generate\nthem as kern_vm_start + 32bit_addr memory accesses. The behavior is similar\nto copy_from_kernel_nofault() except that no address checks are necessary.\nThe address is guaranteed to be in the 4GB range. If the page is not\npresent, the destination register is zeroed on read, and the operation is\nignored on write.\n\nrX = bpf_addr_space_cast(rY, 0, 1) tells the verifier that rX->type =\nunknown scalar. If arena->map_flags has BPF_F_NO_USER_CONV set, then the\nverifier converts such cast instructions to mov32. Otherwise, JIT will emit\nnative code equivalent to:\nrX = (u32)rY;\nif (rY)\n  rX |= clear_lo32_bits(arena->user_vm_start); /* replace hi32 bits in rX */\n\nAfter such conversion, the pointer becomes a valid user pointer within\nbpf_arena range. The user process can access data structures created in\nbpf_arena without any additional computations. For example, a linked list\nbuilt by a bpf program can be walked natively by user space.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nReviewed-by: Barret Rhoden <brho@google.com>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-2-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:37:23 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_types.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/Makefile",
            "kernel/bpf/arena.c",
            "kernel/bpf/core.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "667a86ad9b71d934c444eec193cf3508016f35c5",
          "subject": "bpf: Disasm support for addr_space_cast instruction.",
          "message": "LLVM generates rX = addr_space_cast(rY, dst_addr_space, src_addr_space)\ninstruction when pointers in non-zero address space are used by the bpf\nprogram. Recognize this insn in uapi and in bpf disassembler.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-3-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:37:24 -0700",
          "modified_files": [
            "include/uapi/linux/bpf.h",
            "kernel/bpf/disasm.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "2fe99eb0ccf2bb73df65ebcbbf2f2ff70e63547b",
          "subject": "bpf: Add x86-64 JIT support for PROBE_MEM32 pseudo instructions.",
          "message": "Add support for [LDX | STX | ST], PROBE_MEM32, [B | H | W | DW] instructions.\nThey are similar to PROBE_MEM instructions with the following differences:\n- PROBE_MEM has to check that the address is in the kernel range with\n  src_reg + insn->off >= TASK_SIZE_MAX + PAGE_SIZE check\n- PROBE_MEM doesn't support store\n- PROBE_MEM32 relies on the verifier to clear upper 32-bit in the register\n- PROBE_MEM32 adds 64-bit kern_vm_start address (which is stored in %r12 in the prologue)\n  Due to bpf_arena constructions such %r12 + %reg + off16 access is guaranteed\n  to be within arena virtual range, so no address check at run-time.\n- PROBE_MEM32 allows STX and ST. If they fault the store is a nop.\n  When LDX faults the destination register is zeroed.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-4-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:37:24 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/bpf.h",
            "include/linux/filter.h"
          ]
        },
        {
          "hash": "142fd4d2dcf58b1720a6af644f31de1a5551f219",
          "subject": "bpf: Add x86-64 JIT support for bpf_addr_space_cast instruction.",
          "message": "LLVM generates bpf_addr_space_cast instruction while translating\npointers between native (zero) address space and\n__attribute__((address_space(N))).\nThe addr_space=1 is reserved as bpf_arena address space.\n\nrY = addr_space_cast(rX, 0, 1) is processed by the verifier and\nconverted to normal 32-bit move: wX = wY\n\nrY = addr_space_cast(rX, 1, 0) has to be converted by JIT:\n\naux_reg = upper_32_bits of arena->user_vm_start\naux_reg <<= 32\nwX = wY // clear upper 32 bits of dst register\nif (wX) // if not zero add upper bits of user_vm_start\n  wX |= aux_reg\n\nJIT can do it more efficiently:\n\nmov dst_reg32, src_reg32  // 32-bit move\nshl dst_reg, 32\nor dst_reg, user_vm_start\nrol dst_reg, 32\nxor r11, r11\ntest dst_reg32, dst_reg32 // check if lower 32-bit are zero\ncmove r11, dst_reg\t  // if so, set dst_reg to zero\n\t\t\t  // Intel swapped src/dst register encoding in CMOVcc\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-5-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:37:24 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/filter.h",
            "kernel/bpf/core.c"
          ]
        },
        {
          "hash": "6082b6c328b5486da2b356eae94b8b83c98b5565",
          "subject": "bpf: Recognize addr_space_cast instruction in the verifier.",
          "message": "rY = addr_space_cast(rX, 0, 1) tells the verifier that rY->type = PTR_TO_ARENA.\nAny further operations on PTR_TO_ARENA register have to be in 32-bit domain.\n\nThe verifier will mark load/store through PTR_TO_ARENA with PROBE_MEM32.\nJIT will generate them as kern_vm_start + 32bit_addr memory accesses.\n\nrY = addr_space_cast(rX, 1, 0) tells the verifier that rY->type = unknown scalar.\nIf arena->map_flags has BPF_F_NO_USER_CONV set then convert cast_user to mov32 as well.\nOtherwise JIT will convert it to:\n  rY = (u32)rX;\n  if (rY)\n     rY |= arena->user_vm_start & ~(u64)~0U;\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-6-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:37:24 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "kernel/bpf/log.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "2edc3de6fb650924a87fffebebc3b7572cbf6e38",
          "subject": "bpf: Recognize btf_decl_tag(\"arg: Arena\") as PTR_TO_ARENA.",
          "message": "In global bpf functions recognize btf_decl_tag(\"arg:arena\") as PTR_TO_ARENA.\n\nNote, when the verifier sees:\n\n__weak void foo(struct bar *p)\n\nit recognizes 'p' as PTR_TO_MEM and 'struct bar' has to be a struct with scalars.\nHence the only way to use arena pointers in global functions is to tag them with \"arg:arena\".\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-7-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:37:24 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "4d2b56081c32cb33364745da434b88eeaa9d8d8d",
          "subject": "libbpf: Add __arg_arena to bpf_helpers.h",
          "message": "Add __arg_arena to bpf_helpers.h\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-8-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:37:24 -0700",
          "modified_files": [
            "tools/lib/bpf/bpf_helpers.h"
          ]
        },
        {
          "hash": "79ff13e99169ddb0e2277e046dbfb112f77dfac5",
          "subject": "libbpf: Add support for bpf_arena.",
          "message": "mmap() bpf_arena right after creation, since the kernel needs to\nremember the address returned from mmap. This is user_vm_start.\nLLVM will generate bpf_arena_cast_user() instructions where\nnecessary and JIT will add upper 32-bit of user_vm_start\nto such pointers.\n\nFix up bpf_map_mmap_sz() to compute mmap size as\nmap->value_size * map->max_entries for arrays and\nPAGE_SIZE * map->max_entries for arena.\n\nDon't set BTF at arena creation time, since it doesn't support it.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-9-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:37:24 -0700",
          "modified_files": [
            "tools/lib/bpf/libbpf.c",
            "tools/lib/bpf/libbpf_probes.c"
          ]
        },
        {
          "hash": "eed512e8ac64339cfc69da1a6a4b60982cb502ca",
          "subject": "bpftool: Recognize arena map type",
          "message": "Teach bpftool to recognize arena map type.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Quentin Monnet <quentin@isovalent.com>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-10-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:37:24 -0700",
          "modified_files": [
            "tools/bpf/bpftool/Documentation/bpftool-map.rst",
            "tools/bpf/bpftool/map.c"
          ]
        },
        {
          "hash": "2e7ba4f8fd1fa879b37db0b738c23ba2af8292ee",
          "subject": "libbpf: Recognize __arena global variables.",
          "message": "LLVM automatically places __arena variables into \".arena.1\" ELF section.\nIn order to use such global variables bpf program must include definition\nof arena map in \".maps\" section, like:\nstruct {\n       __uint(type, BPF_MAP_TYPE_ARENA);\n       __uint(map_flags, BPF_F_MMAPABLE);\n       __uint(max_entries, 1000);         /* number of pages */\n       __ulong(map_extra, 2ull << 44);    /* start of mmap() region */\n} arena SEC(\".maps\");\n\nlibbpf recognizes both uses of arena and creates single `struct bpf_map *`\ninstance in libbpf APIs.\n\".arena.1\" ELF section data is used as initial data image, which is exposed\nthrough skeleton and bpf_map__initial_value() to the user, if they need to tune\nit before the load phase. During load phase, this initial image is copied over\ninto mmap()'ed region corresponding to arena, and discarded.\n\nFew small checks here and there had to be added to make sure this\napproach works with bpf_map__initial_value(), mostly due to hard-coded\nassumption that map->mmaped is set up with mmap() syscall and should be\nmunmap()'ed. For arena, .arena.1 can be (much) smaller than maximum\narena size, so this smaller data size has to be tracked separately.\nGiven it is enforced that there is only one arena for entire bpf_object\ninstance, we just keep it in a separate field. This can be generalized\nif necessary later.\n\nAll global variables from \".arena.1\" section are accessible from user space\nvia skel->arena->name_of_var.\n\nFor bss/data/rodata the skeleton/libbpf perform the following sequence:\n1. addr = mmap(MAP_ANONYMOUS)\n2. user space optionally modifies global vars\n3. map_fd = bpf_create_map()\n4. bpf_update_map_elem(map_fd, addr) // to store values into the kernel\n5. mmap(addr, MAP_FIXED, map_fd)\nafter step 5 user spaces see the values it wrote at step 2 at the same addresses\n\narena doesn't support update_map_elem. Hence skeleton/libbpf do:\n1. addr = malloc(sizeof SEC \".arena.1\")\n2. user space optionally modifies global vars\n3. map_fd = bpf_create_map(MAP_TYPE_ARENA)\n4. real_addr = mmap(map->map_extra, MAP_SHARED | MAP_FIXED, map_fd)\n5. memcpy(real_addr, addr) // this will fault-in and allocate pages\n\nAt the end look and feel of global data vs __arena global data is the same from\nbpf prog pov.\n\nAnother complication is:\nstruct {\n  __uint(type, BPF_MAP_TYPE_ARENA);\n} arena SEC(\".maps\");\n\nint __arena foo;\nint bar;\n\n  ptr1 = &foo;   // relocation against \".arena.1\" section\n  ptr2 = &arena; // relocation against \".maps\" section\n  ptr3 = &bar;   // relocation against \".bss\" section\n\nFo the kernel ptr1 and ptr2 has point to the same arena's map_fd\nwhile ptr3 points to a different global array's map_fd.\nFor the verifier:\nptr1->type == unknown_scalar\nptr2->type == const_ptr_to_map\nptr3->type == ptr_to_map_value\n\nAfter verification, from JIT pov all 3 ptr-s are normal ld_imm64 insns.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Quentin Monnet <quentin@isovalent.com>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-11-alexei.starovoitov@gmail.com",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-03-11 15:43:35 -0700",
          "modified_files": [
            "tools/bpf/bpftool/gen.c",
            "tools/lib/bpf/libbpf.c",
            "tools/lib/bpf/libbpf.h"
          ]
        },
        {
          "hash": "204c628730c62de5a0b593008549a9b95aa96b01",
          "subject": "bpf: Add helper macro bpf_addr_space_cast()",
          "message": "Introduce helper macro bpf_addr_space_cast() that emits:\nrX = rX\ninstruction with off = BPF_ADDR_SPACE_CAST\nand encodes dest and src address_space-s into imm32.\n\nIt's useful with older LLVM that doesn't emit this insn automatically.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-12-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:43:42 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "80a4129fcf20da3c6941411155a9b3b45caa5b8d",
          "subject": "selftests/bpf: Add unit tests for bpf_arena_alloc/free_pages",
          "message": "Add unit tests for bpf_arena_alloc/free_pages() functionality\nand bpf_arena_common.h with a set of common helpers and macros that\nis used in this test and the following patches.\n\nAlso modify test_loader that didn't support running bpf_prog_type_syscall\nprograms.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-13-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:43:43 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/DENYLIST.aarch64",
            "tools/testing/selftests/bpf/DENYLIST.s390x",
            "tools/testing/selftests/bpf/bpf_arena_common.h",
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_arena.c",
            "tools/testing/selftests/bpf/test_loader.c"
          ]
        },
        {
          "hash": "9f2c156f90a422b4897a8c2831076a96a31413d1",
          "subject": "selftests/bpf: Add bpf_arena_list test.",
          "message": "bpf_arena_alloc.h - implements page_frag allocator as a bpf program.\nbpf_arena_list.h - doubly linked link list as a bpf program.\n\nCompiled as a bpf program and as native C code.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-14-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:43:43 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_arena_alloc.h",
            "tools/testing/selftests/bpf/bpf_arena_list.h",
            "tools/testing/selftests/bpf/prog_tests/arena_list.c",
            "tools/testing/selftests/bpf/progs/arena_list.c"
          ]
        },
        {
          "hash": "8df839ae23b8c581bdac4b6970d029d65a415852",
          "subject": "selftests/bpf: Add bpf_arena_htab test.",
          "message": "bpf_arena_htab.h - hash table implemented as bpf program\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-15-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-11 15:43:43 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/DENYLIST.aarch64",
            "tools/testing/selftests/bpf/DENYLIST.s390x",
            "tools/testing/selftests/bpf/bpf_arena_htab.h",
            "tools/testing/selftests/bpf/prog_tests/arena_htab.c",
            "tools/testing/selftests/bpf/progs/arena_htab.c",
            "tools/testing/selftests/bpf/progs/arena_htab_asm.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "c7d4274e90a1e7aa43d11d2a16066cbbe610070e",
      "merge_subject": "Merge branch 'bpf: arena prerequisites'",
      "merge_body": "Alexei Starovoitov says:\n\n====================\nThese are bpf_arena prerequisite patches.\nUseful on its own.\n\nAlexei Starovoitov (5):\n  bpf: Allow kfuncs return 'void *'\n  bpf: Recognize '__map' suffix in kfunc arguments\n  bpf: Plumb get_unmapped_area() callback into bpf_map_ops\n  libbpf: Allow specifying 64-bit integers in map BTF.\n  bpf: Tell bpf programs kernel's PAGE_SIZE\n====================\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_date": "2024-03-07 15:01:57 -0800",
      "commits": [
        {
          "hash": "88d1d4a7eebea2836859246d91fe9d141789dfc3",
          "subject": "bpf: Allow kfuncs return 'void *'",
          "message": "Recognize return of 'void *' from kfunc as returning unknown scalar.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/r/20240307031228.42896-2-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-07 14:58:48 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "8d94f1357c00d7706c1f3d0bb568e054cef6aea1",
          "subject": "bpf: Recognize '__map' suffix in kfunc arguments",
          "message": "Recognize 'void *p__map' kfunc argument as 'struct bpf_map *p__map'.\nIt allows kfunc to have 'void *' argument for maps, since bpf progs\nwill call them as:\nstruct {\n        __uint(type, BPF_MAP_TYPE_ARENA);\n\t...\n} arena SEC(\".maps\");\n\nbpf_kfunc_with_map(... &arena ...);\n\nUnderneath libbpf will load CONST_PTR_TO_MAP into the register via ld_imm64\ninsn. If kfunc was defined with 'struct bpf_map *' it would pass the\nverifier as well, but bpf prog would need to type cast the argument\n(void *)&arena, which is not clean.\n\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/r/20240307031228.42896-3-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-07 14:58:48 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "cf2c2e4a3d910270903d50462aaa75140cdb2c96",
          "subject": "bpf: Plumb get_unmapped_area() callback into bpf_map_ops",
          "message": "Subsequent patches introduce bpf_arena that imposes special alignment\nrequirements on address selection.\n\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/r/20240307031228.42896-4-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-07 14:58:48 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "d147357e2e5977c5fe9218457a1e359fd1d36609",
          "subject": "libbpf: Allow specifying 64-bit integers in map BTF.",
          "message": "__uint() macro that is used to specify map attributes like:\n  __uint(type, BPF_MAP_TYPE_ARRAY);\n  __uint(map_flags, BPF_F_MMAPABLE);\nIt is limited to 32-bit, since BTF_KIND_ARRAY has u32 \"number of elements\"\nfield in \"struct btf_array\".\n\nIntroduce __ulong() macro that allows specifying values bigger than 32-bit.\nIn map definition \"map_extra\" is the only u64 field, so far.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/r/20240307031228.42896-5-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-07 14:58:48 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf_helpers.h",
            "tools/lib/bpf/libbpf.c"
          ]
        },
        {
          "hash": "1576b07961971d4eeb0e269c7133e9a6d430daf8",
          "subject": "bpftool: rename is_internal_mmapable_map into is_mmapable_map",
          "message": "It's not restricted to working with \"internal\" maps, it cares about any\nmap that can be mmap'ed. Reflect that in more succinct and generic name.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: Quentin Monnet <quentin@isovalent.com>\nLink: https://lore.kernel.org/r/20240307031228.42896-6-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-03-07 14:58:48 -0800",
          "modified_files": [
            "tools/bpf/bpftool/gen.c"
          ]
        },
        {
          "hash": "fe5064158c561b807af5708c868f6c7cb5144e01",
          "subject": "bpf: Tell bpf programs kernel's PAGE_SIZE",
          "message": "vmlinux BTF includes all kernel enums.\nAdd __PAGE_SIZE = PAGE_SIZE enum, so that bpf programs\nthat include vmlinux.h can easily access it.\n\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/r/20240307031228.42896-7-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-07 14:58:48 -0800",
          "modified_files": [
            "kernel/bpf/core.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "0f79bb8987a5c483362dc12d58b221a1a1c45578",
      "merge_subject": "Merge branch 'bpf-introduce-may_goto-and-cond_break'",
      "merge_body": "Alexei Starovoitov says:\n\n====================\nbpf: Introduce may_goto and cond_break\n\nFrom: Alexei Starovoitov <ast@kernel.org>\n\nv5 -> v6:\n- Rename BPF_JMA to BPF_JCOND\n- Addressed Andrii's review comments\n\nv4 -> v5:\n- rewrote patch 1 to avoid fake may_goto_reg and use 'u32 may_goto_cnt' instead.\n  This way may_goto handling is similar to bpf_loop() processing.\n- fixed bug in patch 2 that RANGE_WITHIN should not use\n  rold->type == NOT_INIT as a safe signal.\n- patch 3 fixed negative offset computation in cond_break macro\n- using bpf_arena and cond_break recompiled lib/glob.c as bpf prog\n  and it works! It will be added as a selftest to arena series.\n\nv3 -> v4:\n- fix drained issue reported by John.\n  may_goto insn could be implemented with sticky state (once\n  reaches 0 it stays 0), but the verifier shouldn't assume that.\n  It has to explore both branches.\n  Arguably drained iterator state shouldn't be there at all.\n  bpf_iter_css_next() is not sticky. Can be fixed, but auditing all\n  iterators for stickiness. That's an orthogonal discussion.\n- explained JMA name reasons in patch 1\n- fixed test_progs-no_alu32 issue and added another test\n\nv2 -> v3: Major change\n- drop bpf_can_loop() kfunc and introduce may_goto instruction instead\n  kfunc is a function call while may_goto doesn't consume any registers\n  and LLVM can produce much better code due to less register pressure.\n- instead of counting from zero to BPF_MAX_LOOPS start from it instead\n  and break out of the loop when count reaches zero\n- use may_goto instruction in cond_break macro\n- recognize that 'exact' state comparison doesn't need to be truly exact.\n  regsafe() should ignore precision and liveness marks, but range_within\n  logic is safe to use while evaluating open coded iterators.\n====================\n\nLink: https://lore.kernel.org/r/20240306031929.42666-1-alexei.starovoitov@gmail.com\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2024-03-06 15:18:14 -0800",
      "commits": [
        {
          "hash": "011832b97b311bb9e3c27945bc0d1089a14209c9",
          "subject": "bpf: Introduce may_goto instruction",
          "message": "Introduce may_goto instruction that from the verifier pov is similar to\nopen coded iterators bpf_for()/bpf_repeat() and bpf_loop() helper, but it\ndoesn't iterate any objects.\nIn assembly 'may_goto' is a nop most of the time until bpf runtime has to\nterminate the program for whatever reason. In the current implementation\nmay_goto has a hidden counter, but other mechanisms can be used.\nFor programs written in C the later patch introduces 'cond_break' macro\nthat combines 'may_goto' with 'break' statement and has similar semantics:\ncond_break is a nop until bpf runtime has to break out of this loop.\nIt can be used in any normal \"for\" or \"while\" loop, like\n\n  for (i = zero; i < cnt; cond_break, i++) {\n\nThe verifier recognizes that may_goto is used in the program, reserves\nadditional 8 bytes of stack, initializes them in subprog prologue, and\nreplaces may_goto instruction with:\naux_reg = *(u64 *)(fp - 40)\nif aux_reg == 0 goto pc+off\naux_reg -= 1\n*(u64 *)(fp - 40) = aux_reg\n\nmay_goto instruction can be used by LLVM to implement __builtin_memcpy,\n__builtin_strcmp.\n\nmay_goto is not a full substitute for bpf_for() macro.\nbpf_for() doesn't have induction variable that verifiers sees,\nso 'i' in bpf_for(i, 0, 100) is seen as imprecise and bounded.\n\nBut when the code is written as:\nfor (i = 0; i < 100; cond_break, i++)\nthe verifier see 'i' as precise constant zero,\nhence cond_break (aka may_goto) doesn't help to converge the loop.\nA static or global variable can be used as a workaround:\nstatic int zero = 0;\nfor (i = zero; i < 100; cond_break, i++) // works!\n\nmay_goto works well with arena pointers that don't need to be bounds\nchecked on access. Load/store from arena returns imprecise unbounded\nscalar and loops with may_goto pass the verifier.\n\nReserve new opcode BPF_JMP | BPF_JCOND for may_goto insn.\nJCOND stands for conditional pseudo jump.\nSince goto_or_nop insn was proposed, it may use the same opcode.\nmay_goto vs goto_or_nop can be distinguished by src_reg:\ncode = BPF_JMP | BPF_JCOND\nsrc_reg = 0 - may_goto\nsrc_reg = 1 - goto_or_nop\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nTested-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240306031929.42666-2-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-06 15:17:31 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/core.c",
            "kernel/bpf/disasm.c",
            "kernel/bpf/verifier.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "4f81c16f50baf6d5d8bfa6eef3250dcfa22cbc08",
          "subject": "bpf: Recognize that two registers are safe when their ranges match",
          "message": "When open code iterators, bpf_loop or may_goto are used the following two\nstates are equivalent and safe to prune the search:\n\ncur state: fp-8_w=scalar(id=3,smin=umin=smin32=umin32=2,smax=umax=smax32=umax32=11,var_off=(0x0; 0xf))\nold state: fp-8_rw=scalar(id=2,smin=umin=smin32=umin32=1,smax=umax=smax32=umax32=11,var_off=(0x0; 0xf))\n\nIn other words \"exact\" state match should ignore liveness and precision\nmarks, since open coded iterator logic didn't complete their propagation,\nreg_old->type == NOT_INIT && reg_cur->type != NOT_INIT is also not safe to\nprune while looping, but range_within logic that applies to scalars,\nptr_to_mem, map_value, pkt_ptr is safe to rely on.\n\nAvoid doing such comparison when regular infinite loop detection logic is\nused, otherwise bounded loop logic will declare such \"infinite loop\" as\nfalse positive. Such example is in progs/verifier_loops1.c\nnot_an_inifinite_loop().\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nTested-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240306031929.42666-3-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-06 15:18:00 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "06375801525717173aee790310b7d959bb77879b",
          "subject": "bpf: Add cond_break macro",
          "message": "Use may_goto instruction to implement cond_break macro.\nIdeally the macro should be written as:\n  asm volatile goto(\".byte 0xe5;\n                     .byte 0;\n                     .short %l[l_break] ...\n                     .long 0;\nbut LLVM doesn't recognize fixup of 2 byte PC relative yet.\nHence use\n  asm volatile goto(\".byte 0xe5;\n                     .byte 0;\n                     .long %l[l_break] ...\n                     .short 0;\nthat produces correct asm on little endian.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nTested-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240306031929.42666-4-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-06 15:18:04 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "0c8bbf990bddef1a4f32889b18a4a016d9bd2cfd",
          "subject": "selftests/bpf: Test may_goto",
          "message": "Add tests for may_goto instruction via cond_break macro.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nTested-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240306031929.42666-5-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2024-03-06 15:18:10 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/DENYLIST.s390x",
            "tools/testing/selftests/bpf/progs/verifier_iterating_callbacks.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "399eca1bd4fc14645dcdf19ee10adf5cde85aecf",
      "merge_subject": "Merge branch 'check-bpf_func_state-callback_depth-when-pruning-states'",
      "merge_body": "Eduard Zingerman says:\n\n====================\ncheck bpf_func_state->callback_depth when pruning states\n\nThis patch-set fixes bug in states pruning logic hit in mailing list\ndiscussion [0]. The details of the fix are in patch #1.\n\nThe main idea for the fix belongs to Yonghong Song,\nmine contribution is merely in review and test cases.\n\nThere are some changes in verification performance:\n\nFile                       Program        Insns    (DIFF)  States  (DIFF)\n-------------------------  -------------  ---------------  --------------\npyperf600_bpf_loop.bpf.o   on_event          +15 (+0.42%)     +0 (+0.00%)\nstrobemeta_bpf_loop.bpf.o  on_event        +857 (+37.95%)   +60 (+38.96%)\nxdp_synproxy_kern.bpf.o    syncookie_tc   +2892 (+30.39%)  +109 (+36.33%)\nxdp_synproxy_kern.bpf.o    syncookie_xdp  +2892 (+30.01%)  +109 (+36.09%)\n\n(when tested on a subset of selftests identified by\n selftests/bpf/veristat.cfg and Cilium bpf object files from [4])\n\nChangelog:\nv2 [2] -> v3:\n- fixes for verifier.c commit message as suggested by Yonghong;\n- patch-set re-rerouted to 'bpf' tree as suggested in [2];\n- patch for test_tcp_custom_syncookie is sent separately to 'bpf-next' [3].\n- veristat results updated using 'bpf' tree as baseline and clang 16.\n\nv1 [1] -> v2:\n- patch #2 commit message updated to better reflect verifier behavior\n  with regards to checkpoints tree (suggested by Yonghong);\n- veristat results added (suggested by Andrii).\n\n[0] https://lore.kernel.org/bpf/9b251840-7cb8-4d17-bd23-1fc8071d8eef@linux.dev/\n[1] https://lore.kernel.org/bpf/20240212143832.28838-1-eddyz87@gmail.com/\n[2] https://lore.kernel.org/bpf/20240216150334.31937-1-eddyz87@gmail.com/\n[3] https://lore.kernel.org/bpf/20240222150300.14909-1-eddyz87@gmail.com/\n[4] https://github.com/anakryiko/cilium\n====================\n\nLink: https://lore.kernel.org/r/20240222154121.6991-1-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-03-05 16:15:57 -0800",
      "commits": [
        {
          "hash": "e9a8e5a587ca55fec6c58e4881742705d45bee54",
          "subject": "bpf: check bpf_func_state->callback_depth when pruning states",
          "message": "When comparing current and cached states verifier should consider\nbpf_func_state->callback_depth. Current state cannot be pruned against\ncached state, when current states has more iterations left compared to\ncached state. Current state has more iterations left when it's\ncallback_depth is smaller.\n\nBelow is an example illustrating this bug, minimized from mailing list\ndiscussion [0] (assume that BPF_F_TEST_STATE_FREQ is set).\nThe example is not a safe program: if loop_cb point (1) is followed by\nloop_cb point (2), then division by zero is possible at point (4).\n\n    struct ctx {\n    \t__u64 a;\n    \t__u64 b;\n    \t__u64 c;\n    };\n\n    static void loop_cb(int i, struct ctx *ctx)\n    {\n    \t/* assume that generated code is \"fallthrough-first\":\n    \t * if ... == 1 goto\n    \t * if ... == 2 goto\n    \t * <default>\n    \t */\n    \tswitch (bpf_get_prandom_u32()) {\n    \tcase 1:  /* 1 */ ctx->a = 42; return 0; break;\n    \tcase 2:  /* 2 */ ctx->b = 42; return 0; break;\n    \tdefault: /* 3 */ ctx->c = 42; return 0; break;\n    \t}\n    }\n\n    SEC(\"tc\")\n    __failure\n    __flag(BPF_F_TEST_STATE_FREQ)\n    int test(struct __sk_buff *skb)\n    {\n    \tstruct ctx ctx = { 7, 7, 7 };\n\n    \tbpf_loop(2, loop_cb, &ctx, 0);              /* 0 */\n    \t/* assume generated checks are in-order: .a first */\n    \tif (ctx.a == 42 && ctx.b == 42 && ctx.c == 7)\n    \t\tasm volatile(\"r0 /= 0;\":::\"r0\");    /* 4 */\n    \treturn 0;\n    }\n\nPrior to this commit verifier built the following checkpoint tree for\nthis example:\n\n .------------------------------------- Checkpoint / State name\n |    .-------------------------------- Code point number\n |    |   .---------------------------- Stack state {ctx.a,ctx.b,ctx.c}\n |    |   |        .------------------- Callback depth in frame #0\n v    v   v        v\n   - (0) {7P,7P,7},depth=0\n     - (3) {7P,7P,7},depth=1\n       - (0) {7P,7P,42},depth=1\n         - (3) {7P,7,42},depth=2\n           - (0) {7P,7,42},depth=2      loop terminates because of depth limit\n             - (4) {7P,7,42},depth=0    predicted false, ctx.a marked precise\n             - (6) exit\n(a)      - (2) {7P,7,42},depth=2\n           - (0) {7P,42,42},depth=2     loop terminates because of depth limit\n             - (4) {7P,42,42},depth=0   predicted false, ctx.a marked precise\n             - (6) exit\n(b)      - (1) {7P,7P,42},depth=2\n           - (0) {42P,7P,42},depth=2    loop terminates because of depth limit\n             - (4) {42P,7P,42},depth=0  predicted false, ctx.{a,b} marked precise\n             - (6) exit\n     - (2) {7P,7,7},depth=1             considered safe, pruned using checkpoint (a)\n(c)  - (1) {7P,7P,7},depth=1            considered safe, pruned using checkpoint (b)\n\nHere checkpoint (b) has callback_depth of 2, meaning that it would\nnever reach state {42,42,7}.\nWhile checkpoint (c) has callback_depth of 1, and thus\ncould yet explore the state {42,42,7} if not pruned prematurely.\nThis commit makes forbids such premature pruning,\nallowing verifier to explore states sub-tree starting at (c):\n\n(c)  - (1) {7,7,7P},depth=1\n       - (0) {42P,7,7P},depth=1\n         ...\n         - (2) {42,7,7},depth=2\n           - (0) {42,42,7},depth=2      loop terminates because of depth limit\n             - (4) {42,42,7},depth=0    predicted true, ctx.{a,b,c} marked precise\n               - (5) division by zero\n\n[0] https://lore.kernel.org/bpf/9b251840-7cb8-4d17-bd23-1fc8071d8eef@linux.dev/\n\nFixes: bb124da69c47 (\"bpf: keep track of max number of bpf_loop callback iterations\")\nSuggested-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240222154121.6991-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-03-05 16:15:56 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "5c2bc5e2f81d3344095ae241032dde20a4ea2b48",
          "subject": "selftests/bpf: test case for callback_depth states pruning logic",
          "message": "The test case was minimized from mailing list discussion [0].\nIt is equivalent to the following C program:\n\n    struct iter_limit_bug_ctx { __u64 a; __u64 b; __u64 c; };\n\n    static __naked void iter_limit_bug_cb(void)\n    {\n    \tswitch (bpf_get_prandom_u32()) {\n    \tcase 1:  ctx->a = 42; break;\n    \tcase 2:  ctx->b = 42; break;\n    \tdefault: ctx->c = 42; break;\n    \t}\n    }\n\n    int iter_limit_bug(struct __sk_buff *skb)\n    {\n    \tstruct iter_limit_bug_ctx ctx = { 7, 7, 7 };\n\n    \tbpf_loop(2, iter_limit_bug_cb, &ctx, 0);\n    \tif (ctx.a == 42 && ctx.b == 42 && ctx.c == 7)\n    \t  asm volatile(\"r1 /= 0;\":::\"r1\");\n    \treturn 0;\n    }\n\nThe main idea is that each loop iteration changes one of the state\nvariables in a non-deterministic manner. Hence it is premature to\nprune the states that have two iterations left comparing them to\nstates with one iteration left.\nE.g. {{7,7,7}, callback_depth=0} can reach state {42,42,7},\nwhile {{7,7,7}, callback_depth=1} can't.\n\n[0] https://lore.kernel.org/bpf/9b251840-7cb8-4d17-bd23-1fc8071d8eef@linux.dev/\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240222154121.6991-3-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-03-05 16:15:56 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_iterating_callbacks.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "96adbf7125e49687e5c1dbd8a241c68e2441da98",
      "merge_subject": "Merge branch 'fix-global-subprog-ptr_to_ctx-arg-handling'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nFix global subprog PTR_TO_CTX arg handling\n\nFix confusing and incorrect inference of PTR_TO_CTX argument type in BPF\nglobal subprogs. For some program types (iters, tracepoint, any program type\nthat doesn't have fixed named \"canonical\" context type) when user uses (in\na correct and valid way) a pointer argument to user-defined anonymous struct\ntype, verifier will incorrectly assume that it has to be PTR_TO_CTX argument.\nWhile it should be just a PTR_TO_MEM argument with allowed size calculated\nfrom user-provided (even if anonymous) struct.\n\nThis did come up in practice and was very confusing to users, so let's prevent\nthis going forward. We had to do a slight refactoring of\nbtf_get_prog_ctx_type() to make it easy to support a special s390x KPROBE use\ncases. See details in respective patches.\n\nv1->v2:\n  - special-case typedef bpf_user_pt_regs_t handling for KPROBE programs,\n    fixing s390x after changes in patch #2.\n====================\n\nLink: https://lore.kernel.org/r/20240212233221.2575350-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-02-13 18:46:47 -0800",
      "commits": [
        {
          "hash": "fb5b86cfd4ef21ea18966718f6bf6c8f1b9df12e",
          "subject": "bpf: simplify btf_get_prog_ctx_type() into btf_is_prog_ctx_type()",
          "message": "Return result of btf_get_prog_ctx_type() is never used and callers only\ncheck NULL vs non-NULL case to determine if given type matches expected\nPTR_TO_CTX type. So rename function to `btf_is_prog_ctx_type()` and\nreturn a simple true/false. We'll use this simpler interface to handle\nkprobe program type's special typedef case in the next patch.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240212233221.2575350-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-02-13 18:46:46 -0800",
          "modified_files": [
            "include/linux/btf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "824c58fb1090ae5e502284400682e30841280a87",
          "subject": "bpf: handle bpf_user_pt_regs_t typedef explicitly for PTR_TO_CTX global arg",
          "message": "Expected canonical argument type for global function arguments\nrepresenting PTR_TO_CTX is `bpf_user_pt_regs_t *ctx`. This currently\nworks on s390x by accident because kernel resolves such typedef to\nunderlying struct (which is anonymous on s390x), and erroneously\naccepting it as expected context type. We are fixing this problem next,\nwhich would break s390x arch, so we need to handle `bpf_user_pt_regs_t`\ncase explicitly for KPROBE programs.\n\nFixes: 91cc1a99740e (\"bpf: Annotate context types\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240212233221.2575350-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-02-13 18:46:47 -0800",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "879bbe7aa4afa80acf72a1cad7f52416ea78c52d",
          "subject": "bpf: don't infer PTR_TO_CTX for programs with unnamed context type",
          "message": "For program types that don't have named context type name (e.g., BPF\niterator programs or tracepoint programs), ctx_tname will be a non-NULL\nempty string. For such programs it shouldn't be possible to have\nPTR_TO_CTX argument for global subprogs based on type name alone.\narg:ctx tag is the only way to have PTR_TO_CTX passed into global\nsubprog for such program types.\n\nFix this loophole, which currently would assume PTR_TO_CTX whenever\nuser uses a pointer to anonymous struct as an argument to their global\nsubprogs. This happens in practice with the following (quite common, in\npractice) approach:\n\ntypedef struct { /* anonymous */\n    int x;\n} my_type_t;\n\nint my_subprog(my_type_t *arg) { ... }\n\nUser's intent is to have PTR_TO_MEM argument for `arg`, but verifier\nwill complain about expecting PTR_TO_CTX.\n\nThis fix also closes unintended s390x-specific KPROBE handling of\nPTR_TO_CTX case. Selftest change is necessary to accommodate this.\n\nFixes: 91cc1a99740e (\"bpf: Annotate context types\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240212233221.2575350-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-02-13 18:46:47 -0800",
          "modified_files": [
            "kernel/bpf/btf.c",
            "tools/testing/selftests/bpf/progs/test_global_func_ctx_args.c"
          ]
        },
        {
          "hash": "63d5a33fb4ec2a4ed6907c8ac144b6f10f6dba47",
          "subject": "selftests/bpf: add anonymous user struct as global subprog arg test",
          "message": "Add tests validating that kernel handles pointer to anonymous struct\nargument as PTR_TO_MEM case, not as PTR_TO_CTX case.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240212233221.2575350-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-02-13 18:46:47 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_global_subprogs.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "2c21a0f67c8ce334b8a58332e8c2d71694bef0ab",
      "merge_subject": "Merge branch 'Support PTR_MAYBE_NULL for struct_ops arguments.'",
      "merge_body": "Kui-Feng Lee says:\n\n====================\nAllow passing null pointers to the operators provided by a struct_ops\nobject. This is an RFC to collect feedbacks/opinions.\n\nThe function pointers that are passed to struct_ops operators (the function\npointers) are always considered reliable until now. They cannot be\nnull. However, in certain scenarios, it should be possible to pass null\npointers to these operators. For instance, sched_ext may pass a null\npointer in the struct task type to an operator that is provided by its\nstruct_ops objects.\n\nThe proposed solution here is to add PTR_MAYBE_NULL annotations to\narguments and create instances of struct bpf_ctx_arg_aux (arg_info) for\nthese arguments. These arg_infos will be installed at\nprog->aux->ctx_arg_info and will be checked by the BPF verifier when\nloading the programs. When a struct_ops program accesses arguments in the\nctx, the verifier will call btf_ctx_access() (through\nbpf_verifier_ops->is_valid_access) to verify the access. btf_ctx_access()\nwill check arg_info and use the information of the matched arg_info to\nproperly set reg_type.\n\nFor nullable arguments, this patch sets an arg_info to label them with\nPTR_TO_BTF_ID | PTR_TRUSTED | PTR_MAYBE_NULL. This enforces the verifier to\ncheck programs and ensure that they properly check the pointer. The\nprograms should check if the pointer is null before reading/writing the\npointed memory.\n\nThe implementer of a struct_ops should annotate the arguments that can\nbe null. The implementer should define a stub function (empty) as a\nplaceholder for each defined operator. The name of a stub function\nshould be in the pattern \"<st_op_type>__<operator name>\". For example,\nfor test_maybe_null of struct bpf_testmod_ops, it's stub function name\nshould be \"bpf_testmod_ops__test_maybe_null\". You mark an argument\nnullable by suffixing the argument name with \"__nullable\" at the stub\nfunction.  Here is the example in bpf_testmod.c.\n\n  static int bpf_testmod_ops__test_maybe_null(int dummy,\n                                              struct task_struct *task__nullable)\n  {\n          return 0;\n  }\n\nThis means that the argument 1 (2nd) of bpf_testmod_ops->test_maybe_null,\nwhich is a function pointer that can be null. With this annotation, the\nverifier will understand how to check programs using this arguments.  A BPF\nprogram that implement test_maybe_null should check the pointer to make\nsure it is not null before using it. For example,\n\n  if (task__nullable)\n      save_tgid = task__nullable->tgid\n\nWithout the check, the verifier will reject the program.\n\nSince we already has stub functions for kCFI, we just reuse these stub\nfunctions with the naming convention mentioned earlier. These stub\nfunctions with the naming convention is only required if there are nullable\narguments to annotate. For functions without nullable arguments, stub\nfunctions are not necessary for the purpose of this patch.\n---\nMajor changes from v7:\n\n - Update a comment that is out of date.\n\nMajor changes from v6:\n\n - Remove \"len\" from bpf_struct_ops_desc_release().\n\n - Rename arg_info(s) to info, and rename all_arg_info to arg_info in\n   prepare_arg_info().\n\n - Rename arg_info to info in struct bpf_struct_ops_arg_info.\n\nMajor changes from v5:\n\n - Rename all member_arg_info variables.\n\n - Refactor to bpf_struct_ops_desc_release() to share code\n   between btf_free_struct_ops_tab() and bpf_struct_ops_desc_init().\n\n - Refactor to btf_param_match_suffix(). (Add a new patch as the part 2.)\n\n - Clean up the commit log and remaining code in the patch of test cases.\n\n - Update a comment in struct_ops_maybe_null.c.\n\nMajor changes from v4:\n\n - Remove the support of pointers to types other than struct\n   types. That would be a separate patchset.\n\n   - Remove the patch about extending PTR_TO_BTF_ID.\n\n   - Remove the test against various pointer types from selftests.\n\n - Remove the patch \"bpf: Remove an unnecessary check\" and send that\n   patch separately.\n\n - Remove member_arg_info_cnt from struct bpf_struct_ops_desc.\n\n - Use btf_id from FUNC_PROTO of a function pointer instead of a stub\n   function.\n\nMajor changes from v3:\n\n - Move the code collecting argument information to prepare_arg_info()\n   called in the loop in bpf_struct_ops_desc_init().\n\n - Simplify the memory allocation by having separated arg_info for\n   each member of a struct_ops type.\n\n - Extend PTR_TO_BTF_ID to pointers to scalar types and array types,\n   not only to struct types.\n\nMajor changes from v2:\n\n - Remove dead code.\n\n - Add comments to explain the code itself.\n\nMajor changes from v1:\n\n - Annotate arguments by suffixing argument names with \"__nullable\" at\n   stub functions.\n\nv7: https://lore.kernel.org/all/20240209020053.1132710-1-thinker.li@gmail.com/\nv6: https://lore.kernel.org/all/20240208065103.2154768-1-thinker.li@gmail.com/\nv5: https://lore.kernel.org/all/20240206063833.2520479-1-thinker.li@gmail.com/\nv4: https://lore.kernel.org/all/20240202220516.1165466-1-thinker.li@gmail.com/\nv3: https://lore.kernel.org/all/20240122212217.1391878-1-thinker.li@gmail.com/\nv2: https://lore.kernel.org/all/20240118224922.336006-1-thinker.li@gmail.com/\n====================\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_date": "2024-02-13 15:16:44 -0800",
      "commits": [
        {
          "hash": "77c0208e199ccb0986fb3612f2409c8cdcb036ad",
          "subject": "bpf: add btf pointer to struct bpf_ctx_arg_aux.",
          "message": "Enable the providers to use types defined in a module instead of in the\nkernel (btf_vmlinux).\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240209023750.1153905-2-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-02-13 15:16:44 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "6115a0aeef01aef152ad7738393aad11422bfb82",
          "subject": "bpf: Move __kfunc_param_match_suffix() to btf.c.",
          "message": "Move __kfunc_param_match_suffix() to btf.c and rename it as\nbtf_param_match_suffix(). It can be reused by bpf_struct_ops later.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240209023750.1153905-3-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-02-13 15:16:44 -0800",
          "modified_files": [
            "include/linux/btf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "1611603537a4b88cec7993f32b70c03113801a46",
          "subject": "bpf: Create argument information for nullable arguments.",
          "message": "Collect argument information from the type information of stub functions to\nmark arguments of BPF struct_ops programs with PTR_MAYBE_NULL if they are\nnullable.  A nullable argument is annotated by suffixing \"__nullable\" at\nthe argument name of stub function.\n\nFor nullable arguments, this patch sets a struct bpf_ctx_arg_aux to label\ntheir reg_type with PTR_TO_BTF_ID | PTR_TRUSTED | PTR_MAYBE_NULL. This\nmakes the verifier to check programs and ensure that they properly check\nthe pointer. The programs should check if the pointer is null before\naccessing the pointed memory.\n\nThe implementer of a struct_ops type should annotate the arguments that can\nbe null. The implementer should define a stub function (empty) as a\nplaceholder for each defined operator. The name of a stub function should\nbe in the pattern \"<st_op_type>__<operator name>\". For example, for\ntest_maybe_null of struct bpf_testmod_ops, it's stub function name should\nbe \"bpf_testmod_ops__test_maybe_null\". You mark an argument nullable by\nsuffixing the argument name with \"__nullable\" at the stub function.\n\nSince we already has stub functions for kCFI, we just reuse these stub\nfunctions with the naming convention mentioned earlier. These stub\nfunctions with the naming convention is only required if there are nullable\narguments to annotate. For functions having not nullable arguments, stub\nfunctions are not necessary for the purpose of this patch.\n\nThis patch will prepare a list of struct bpf_ctx_arg_aux, aka arg_info, for\neach member field of a struct_ops type.  \"arg_info\" will be assigned to\n\"prog->aux->ctx_arg_info\" of BPF struct_ops programs in\ncheck_struct_ops_btf_id() so that it can be used by btf_ctx_access() later\nto set reg_type properly for the verifier.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240209023750.1153905-4-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-02-13 15:16:44 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/btf.h",
            "kernel/bpf/bpf_struct_ops.c",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "00f239eccf461a6403b3c16e767d04f3954cae98",
          "subject": "selftests/bpf: Test PTR_MAYBE_NULL arguments of struct_ops operators.",
          "message": "Test if the verifier verifies nullable pointer arguments correctly for BPF\nstruct_ops programs.\n\n\"test_maybe_null\" in struct bpf_testmod_ops is the operator defined for the\ntest cases here.\n\nA BPF program should check a pointer for NULL beforehand to access the\nvalue pointed by the nullable pointer arguments, or the verifier should\nreject the programs. The test here includes two parts; the programs\nchecking pointers properly and the programs not checking pointers\nbeforehand. The test checks if the verifier accepts the programs checking\nproperly and rejects the programs not checking at all.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240209023750.1153905-5-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-02-13 15:16:44 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.h",
            "tools/testing/selftests/bpf/prog_tests/test_struct_ops_maybe_null.c",
            "tools/testing/selftests/bpf/progs/struct_ops_maybe_null.c",
            "tools/testing/selftests/bpf/progs/struct_ops_maybe_null_fail.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "20a286c1a35ba4dc2fca5d4c1fb2e7ced101e576",
      "merge_subject": "Merge branch 'transfer-rcu-lock-state-across-subprog-calls'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nTransfer RCU lock state across subprog calls\n\nDavid suggested during the discussion in [0] that we should handle RCU\nlocks in a similar fashion to spin locks where the verifier understands\nwhen a lock held in a caller is released in callee, or lock taken in\ncallee is released in a caller, or the callee is called within a lock\ncritical section. This set extends the same semantics to RCU read locks\nand adds a few selftests to verify correct behavior. This issue has also\ncome up for sched-ext programs.\n\nThis would now allow static subprog calls to be made without errors\nwithin RCU read sections, for subprogs to release RCU locks of callers\nand return to them, or for subprogs to take RCU lock which is later\nreleased in the caller.\n\n  [0]: https://lore.kernel.org/bpf/20240204120206.796412-1-memxor@gmail.com\n\nChangelog:\n----------\nv1 -> v2:\nv1: https://lore.kernel.org/bpf/20240204230231.1013964-1-memxor@gmail.com\n\n * Add tests for global subprog behaviour (Yafang)\n * Add Acks, Tested-by (Yonghong, Yafang)\n====================\n\nLink: https://lore.kernel.org/r/20240205055646.1112186-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-02-05 20:00:14 -0800",
      "commits": [
        {
          "hash": "6fceea0fa59f6786a2847a4cae409117624e8b58",
          "subject": "bpf: Transfer RCU lock state between subprog calls",
          "message": "Allow transferring an imbalanced RCU lock state between subprog calls\nduring verification. This allows patterns where a subprog call returns\nwith an RCU lock held, or a subprog call releases an RCU lock held by\nthe caller. Currently, the verifier would end up complaining if the RCU\nlock is not released when processing an exit from a subprog, which is\nnon-ideal if its execution is supposed to be enclosed in an RCU read\nsection of the caller.\n\nInstead, simply only check whether we are processing exit for frame#0\nand do not complain on an active RCU lock otherwise. We only need to\nupdate the check when processing BPF_EXIT insn, as copy_verifier_state\nis already set up to do the right thing.\n\nSuggested-by: David Vernet <void@manifault.com>\nTested-by: Yafang Shao <laoar.shao@gmail.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20240205055646.1112186-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-02-05 20:00:14 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "8be6a0147af314fd60db9da2158cd737dc6394a7",
          "subject": "selftests/bpf: Add tests for RCU lock transfer between subprogs",
          "message": "Add selftests covering the following cases:\n- A static or global subprog called from within a RCU read section works\n- A static subprog taking an RCU read lock which is released in caller works\n- A static subprog releasing the caller's RCU read lock works\n\nGlobal subprogs that leave the lock in an imbalanced state will not\nwork, as they are verified separately, so ensure those cases fail as\nwell.\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20240205055646.1112186-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-02-05 20:00:14 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/rcu_read_lock.c",
            "tools/testing/selftests/bpf/progs/rcu_read_lock.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "8244ab509f89d63941d5ee207967c5a3e00bb493",
      "merge_subject": "Merge branch 'enable-static-subprog-calls-in-spin-lock-critical-sections'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nEnable static subprog calls in spin lock critical sections\n\nThis set allows a BPF program to make a call to a static subprog within\na bpf_spin_lock critical section. This problem has been hit in sched-ext\nand ghOSt [0] as well, and is mostly an annoyance which is worked around\nby inling the static subprog into the critical section.\n\nIn case of sched-ext, there are a lot of other helper/kfunc calls that\nneed to be allow listed for the support to be complete, but a separate\nfollow up will deal with that.\n\nUnlike static subprogs, global subprogs cannot be allowed yet as the\nverifier will not explore their body when encountering a call\ninstruction for them. Therefore, we would need an alternative approach\n(some sort of function summarization to ensure a lock is never taken\nfrom a global subprog and all its callees).\n\n [0]: https://lore.kernel.org/bpf/bd173bf2-dea6-3e0e-4176-4a9256a9a056@google.com\n\nChangelog:\n----------\nv1 -> v2\nv1: https://lore.kernel.org/bpf/20240204120206.796412-1-memxor@gmail.com\n\n * Indicate global function call in verifier error string (Yonghong, David)\n * Add Acks from Yonghong, David\n====================\n\nLink: https://lore.kernel.org/r/20240204222349.938118-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-02-05 19:58:47 -0800",
      "commits": [
        {
          "hash": "a44b1334aadd82203f661adb9adb41e53ad0e8d1",
          "subject": "bpf: Allow calling static subprogs while holding a bpf_spin_lock",
          "message": "Currently, calling any helpers, kfuncs, or subprogs except the graph\ndata structure (lists, rbtrees) API kfuncs while holding a bpf_spin_lock\nis not allowed. One of the original motivations of this decision was to\nforce the BPF programmer's hand into keeping the bpf_spin_lock critical\nsection small, and to ensure the execution time of the program does not\nincrease due to lock waiting times. In addition to this, some of the\nhelpers and kfuncs may be unsafe to call while holding a bpf_spin_lock.\n\nHowever, when it comes to subprog calls, atleast for static subprogs,\nthe verifier is able to explore their instructions during verification.\nTherefore, it is similar in effect to having the same code inlined into\nthe critical section. Hence, not allowing static subprog calls in the\nbpf_spin_lock critical section is mostly an annoyance that needs to be\nworked around, without providing any tangible benefit.\n\nUnlike static subprog calls, global subprog calls are not safe to permit\nwithin the critical section, as the verifier does not explore them\nduring verification, therefore whether the same lock will be taken\nagain, or unlocked, cannot be ascertained.\n\nTherefore, allow calling static subprogs within a bpf_spin_lock critical\nsection, and only reject it in case the subprog linkage is global.\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: David Vernet <void@manifault.com>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20240204222349.938118-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-02-05 19:58:47 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_spin_lock.c"
          ]
        },
        {
          "hash": "e8699c4ff85baedcf40f33db816cc487cee39397",
          "subject": "selftests/bpf: Add test for static subprog call in lock cs",
          "message": "Add selftests for static subprog calls within bpf_spin_lock critical\nsection, and ensure we still reject global subprog calls. Also test the\ncase where a subprog call will unlock the caller's held lock, or the\ncaller will unlock a lock taken by a subprog call, ensuring correct\ntransfer of lock state across frames on exit.\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: David Vernet <void@manifault.com>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20240204222349.938118-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2024-02-05 19:58:47 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/spin_lock.c",
            "tools/testing/selftests/bpf/progs/test_spin_lock.c",
            "tools/testing/selftests/bpf/progs/test_spin_lock_fail.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "2a79690eae953daaac232f93e6c5ac47ac539f2d",
      "merge_subject": "Merge branch 'two-small-fixes-for-global-subprog-tagging'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nTwo small fixes for global subprog tagging\n\nFix a bug with passing trusted PTR_TO_BTF_ID_OR_NULL register into global\nsubprog that expects `__arg_trusted __arg_nullable` arguments, which was\ndiscovered when adopting production BPF application.\n\nAlso fix annoying warnings that are irrelevant for static subprogs, which are\njust an artifact of using btf_prepare_func_args() for both static and global\nsubprogs.\n====================\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240202190529.2374377-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-02-02 18:09:06 -0800",
      "commits": [
        {
          "hash": "8f13c34087d3eb64329529b8517e5a6251653176",
          "subject": "bpf: handle trusted PTR_TO_BTF_ID_OR_NULL in argument check logic",
          "message": "Add PTR_TRUSTED | PTR_MAYBE_NULL modifiers for PTR_TO_BTF_ID to\ncheck_reg_type() to support passing trusted nullable PTR_TO_BTF_ID\nregisters into global functions accepting `__arg_trusted __arg_nullable`\narguments. This hasn't been caught earlier because tests were either\npassing known non-NULL PTR_TO_BTF_ID registers or known NULL (SCALAR)\nregisters.\n\nWhen utilizing this functionality in complicated real-world BPF\napplication that passes around PTR_TO_BTF_ID_OR_NULL, it became apparent\nthat verifier rejects valid case because check_reg_type() doesn't handle\nthis case explicitly. Existing check_reg_type() logic is already\nanticipating this combination, so we just need to explicitly list this\ncombo in the switch statement.\n\nFixes: e2b3c4ff5d18 (\"bpf: add __arg_trusted global func arg tag\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240202190529.2374377-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-02-02 18:08:58 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "e2e70535dd76c6f17bdc9009ffca3d26cfd35ea4",
          "subject": "selftests/bpf: add more cases for __arg_trusted __arg_nullable args",
          "message": "Add extra layer of global functions to ensure that passing around\n(trusted) PTR_TO_BTF_ID_OR_NULL registers works as expected. We also\nextend trusted_task_arg_nullable subtest to check three possible valid\nargumements: known NULL, known non-NULL, and maybe NULL cases.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240202190529.2374377-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-02-02 18:08:58 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_global_ptr_args.c"
          ]
        },
        {
          "hash": "1eb986746a67952df86eb2c50a36450ef103d01b",
          "subject": "bpf: don't emit warnings intended for global subprogs for static subprogs",
          "message": "When btf_prepare_func_args() was generalized to handle both static and\nglobal subprogs, a few warnings/errors that are meant only for global\nsubprog cases started to be emitted for static subprogs, where they are\nsort of expected and irrelavant.\n\nStop polutting verifier logs with irrelevant scary-looking messages.\n\nFixes: e26080d0da87 (\"bpf: prepare btf_prepare_func_args() for handling static subprogs\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240202190529.2374377-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-02-02 18:08:59 -0800",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "6fb3f72702fba97323a89e53f484de58bc59d13c",
      "merge_subject": "Merge branch 'improvements-for-tracking-scalars-in-the-bpf-verifier'",
      "merge_body": "Maxim Mikityanskiy says:\n\n====================\nImprovements for tracking scalars in the BPF verifier\n\nFrom: Maxim Mikityanskiy <maxim@isovalent.com>\n\nThe goal of this series is to extend the verifier's capabilities of\ntracking scalars when they are spilled to stack, especially when the\nspill or fill is narrowing. It also contains a fix by Eduard for\ninfinite loop detection and a state pruning optimization by Eduard that\ncompensates for a verification complexity regression introduced by\ntracking unbounded scalars. These improvements reduce the surface of\nfalse rejections that I saw while working on Cilium codebase.\n\nPatches 1-9 of the original series were previously applied in v2.\n\nPatches 1-2 (Maxim): Support the case when boundary checks are first\nperformed after the register was spilled to the stack.\n\nPatches 3-4 (Maxim): Support narrowing fills.\n\nPatches 5-6 (Eduard): Optimization for state pruning in stacksafe() to\nmitigate the verification complexity regression.\n\nveristat -e file,prog,states -f '!states_diff<50' -f '!states_pct<10' -f '!states_a<10' -f '!states_b<10' -C ...\n\n * Without patch 5:\n\nFile                  Program   States (A)  States (B)  States    (DIFF)\n--------------------  --------  ----------  ----------  ----------------\npyperf100.bpf.o       on_event        4878        6528   +1650 (+33.83%)\npyperf180.bpf.o       on_event        6936       11032   +4096 (+59.05%)\npyperf600.bpf.o       on_event       22271       39455  +17184 (+77.16%)\npyperf600_iter.bpf.o  on_event         400         490     +90 (+22.50%)\nstrobemeta.bpf.o      on_event        4895       14028  +9133 (+186.58%)\n\n * With patch 5:\n\nFile                     Program        States (A)  States (B)  States   (DIFF)\n-----------------------  -------------  ----------  ----------  ---------------\nbpf_xdp.o                tail_lb_ipv4         2770        2224   -546 (-19.71%)\npyperf100.bpf.o          on_event             4878        5848   +970 (+19.89%)\npyperf180.bpf.o          on_event             6936        8868  +1932 (+27.85%)\npyperf600.bpf.o          on_event            22271       29656  +7385 (+33.16%)\npyperf600_iter.bpf.o     on_event              400         450    +50 (+12.50%)\nxdp_synproxy_kern.bpf.o  syncookie_tc          280         226    -54 (-19.29%)\nxdp_synproxy_kern.bpf.o  syncookie_xdp         302         228    -74 (-24.50%)\n\nv2 changes:\n\nFixed comments in patch 1, moved endianness checks to header files in\npatch 12 where possible, added Eduard's ACKs.\n\nv3 changes:\n\nMaxim: Removed __is_scalar_unbounded altogether, addressed Andrii's\ncomments.\n\nEduard: Patch #5 (#14 in v2) changed significantly:\n- Logical changes:\n  - Handling of STACK_{MISC,ZERO} mix turned out to be incorrect:\n    a mix of MISC and ZERO in old state is not equivalent to e.g.\n    just MISC is current state, because verifier could have deduced\n    zero scalars from ZERO slots in old state for some loads.\n  - There is no reason to limit the change only to cases when\n    old or current stack is a spill of unbounded scalar,\n    it is valid to compare any 64-bit scalar spill with fake\n    register impersonating MISC.\n  - STACK_ZERO vs spilled zero case was dropped,\n    after recent changes for zero handling by Andrii and Yonghong\n    it is hard (impossible?) to conjure all ZERO slots for an spi.\n    => the case does not make any difference in veristat results.\n- Use global static variable for unbound_reg (Andrii)\n- Code shuffling to remove duplication in stacksafe() (Andrii)\n====================\n\nLink: https://lore.kernel.org/r/20240127175237.526726-1-maxtram95@gmail.com\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2024-02-02 13:22:15 -0800",
      "commits": [
        {
          "hash": "e67ddd9b1cff7872d43ead73a1403c4e532003d9",
          "subject": "bpf: Track spilled unbounded scalars",
          "message": "Support the pattern where an unbounded scalar is spilled to the stack,\nthen boundary checks are performed on the src register, after which the\nstack frame slot is refilled into a register.\n\nBefore this commit, the verifier didn't treat the src register and the\nstack slot as related if the src register was an unbounded scalar. The\nregister state wasn't copied, the id wasn't preserved, and the stack\nslot was marked as STACK_MISC. Subsequent boundary checks on the src\nregister wouldn't result in updating the boundaries of the spilled\nvariable on the stack.\n\nAfter this commit, the verifier will preserve the bond between src and\ndst even if src is unbounded, which permits to do boundary checks on src\nand refill dst later, still remembering its boundaries. Such a pattern\nis sometimes generated by clang when compiling complex long functions.\n\nOne test is adjusted to reflect that now unbounded scalars are tracked.\n\nSigned-off-by: Maxim Mikityanskiy <maxim@isovalent.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20240127175237.526726-2-maxtram95@gmail.com",
          "author": "Maxim Mikityanskiy <maxim@isovalent.com>",
          "date": "2024-02-02 13:22:14 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
          ]
        },
        {
          "hash": "6be503cec6c9bccd64f72c03697011d2e2b96fc3",
          "subject": "selftests/bpf: Test tracking spilled unbounded scalars",
          "message": "The previous commit added tracking for unbounded scalars on spill. Add\nthe test case to check the new functionality.\n\nSigned-off-by: Maxim Mikityanskiy <maxim@isovalent.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20240127175237.526726-3-maxtram95@gmail.com",
          "author": "Maxim Mikityanskiy <maxim@isovalent.com>",
          "date": "2024-02-02 13:22:14 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
          ]
        },
        {
          "hash": "c1e6148cb4f83cec841db1f066e8db4a86c1f118",
          "subject": "bpf: Preserve boundaries and track scalars on narrowing fill",
          "message": "When the width of a fill is smaller than the width of the preceding\nspill, the information about scalar boundaries can still be preserved,\nas long as it's coerced to the right width (done by coerce_reg_to_size).\nEven further, if the actual value fits into the fill width, the ID can\nbe preserved as well for further tracking of equal scalars.\n\nImplement the above improvements, which makes narrowing fills behave the\nsame as narrowing spills and MOVs between registers.\n\nTwo tests are adjusted to accommodate for endianness differences and to\ntake into account that it's now allowed to do a narrowing fill from the\nleast significant bits.\n\nreg_bounds_sync is added to coerce_reg_to_size to correctly adjust\numin/umax boundaries after the var_off truncation, for example, a 64-bit\nvalue 0xXXXXXXXX00000000, when read as a 32-bit, gets umin = 0, umax =\n0xFFFFFFFF, var_off = (0x0; 0xffffffff00000000), which needs to be\nsynced down to umax = 0, otherwise reg_bounds_sanity_check doesn't pass.\n\nSigned-off-by: Maxim Mikityanskiy <maxim@isovalent.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240127175237.526726-4-maxtram95@gmail.com",
          "author": "Maxim Mikityanskiy <maxim@isovalent.com>",
          "date": "2024-02-02 13:22:14 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
          ]
        },
        {
          "hash": "067313a85c6f213932518f12f628810f0092492b",
          "subject": "selftests/bpf: Add test cases for narrowing fill",
          "message": "The previous commit allowed to preserve boundaries and track IDs of\nscalars on narrowing fills. Add test cases for that pattern.\n\nSigned-off-by: Maxim Mikityanskiy <maxim@isovalent.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20240127175237.526726-5-maxtram95@gmail.com",
          "author": "Maxim Mikityanskiy <maxim@isovalent.com>",
          "date": "2024-02-02 13:22:14 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
          ]
        },
        {
          "hash": "6efbde200bf3cf2dbf6e7181893fed13a79c789b",
          "subject": "bpf: Handle scalar spill vs all MISC in stacksafe()",
          "message": "When check_stack_read_fixed_off() reads value from an spi\nall stack slots of which are set to STACK_{MISC,INVALID},\nthe destination register is set to unbound SCALAR_VALUE.\n\nExploit this fact by allowing stacksafe() to use a fake\nunbound scalar register to compare 'mmmm mmmm' stack value\nin old state vs spilled 64-bit scalar in current state\nand vice versa.\n\nVeristat results after this patch show some gains:\n\n./veristat -C -e file,prog,states -f 'states_pct>10'  not-opt after\nFile                     Program                States   (DIFF)\n-----------------------  ---------------------  ---------------\nbpf_overlay.o            tail_rev_nodeport_lb4    -45 (-15.85%)\nbpf_xdp.o                tail_lb_ipv4            -541 (-19.57%)\npyperf100.bpf.o          on_event                -680 (-10.42%)\npyperf180.bpf.o          on_event               -2164 (-19.62%)\npyperf600.bpf.o          on_event               -9799 (-24.84%)\nstrobemeta.bpf.o         on_event               -9157 (-65.28%)\nxdp_synproxy_kern.bpf.o  syncookie_tc             -54 (-19.29%)\nxdp_synproxy_kern.bpf.o  syncookie_xdp            -74 (-24.50%)\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240127175237.526726-6-maxtram95@gmail.com",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-02-02 13:22:14 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "73a28d9d000e8d20b4b3c516b74ee92afe3ae4be",
          "subject": "selftests/bpf: States pruning checks for scalar vs STACK_MISC",
          "message": "Check that stacksafe() compares spilled scalars with STACK_MISC.\nThe following combinations are explored:\n- old spill of imprecise scalar is equivalent to cur STACK_{MISC,INVALID}\n  (plus error in unpriv mode);\n- old spill of precise scalar is not equivalent to cur STACK_MISC;\n- old STACK_MISC is equivalent to cur scalar;\n- old STACK_MISC is not equivalent to cur non-scalar.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240127175237.526726-7-maxtram95@gmail.com",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2024-02-02 13:22:14 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "4d8ebe1304e99cf6e08e432c23041638d6d1de56",
      "merge_subject": "Merge branch 'trusted-ptr_to_btf_id-arg-support-in-global-subprogs'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nTrusted PTR_TO_BTF_ID arg support in global subprogs\n\nThis patch set follows recent changes that added btf_decl_tag-based argument\nannotation support for global subprogs. This time we add ability to pass\nPTR_TO_BTF_ID (BTF-aware kernel pointers) arguments into global subprograms.\nWe support explicitly trusted arguments only, for now.\n\nPatch #1 adds logic for arg:trusted tag support on the verifier side. Default\nsemantic of such arguments is non-NULL, enforced on caller side. But patch #2\nadds arg:nullable tag that can be combined with arg:trusted to make callee\nexplicitly do the NULL check, which helps implement \"optional\" PTR_TO_BTF_ID\narguments.\n\nPatch #3 adds libbpf-side __arg_trusted and __arg_nullable macros.\n\nPatch #4 adds a bunch of tests validating __arg_trusted in combination with\n__arg_nullable.\n\nv2->v3:\n  - went back to arg:nullable and __arg_nullable naming;\n  - rebased on latest bpf-next after prepartory patches landed;\nv1->v2:\n  - added fix up to type enforcement changes, landed earlier;\n  - dropped bpf_core_cast() changes, will post them separately, as they now\n    are not used in added tests;\n  - dropped arg:untrusted support (Alexei);\n  - renamed arg:nullable to arg:maybe_null (Alexei);\n  - and also added task_struct___local flavor tests (Alexei).\n====================\n\nLink: https://lore.kernel.org/r/20240130000648.2144827-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-01-30 09:41:51 -0800",
      "commits": [
        {
          "hash": "e2b3c4ff5d183da6d1863c2321413406a2752e7a",
          "subject": "bpf: add __arg_trusted global func arg tag",
          "message": "Add support for passing PTR_TO_BTF_ID registers to global subprogs.\nCurrently only PTR_TRUSTED flavor of PTR_TO_BTF_ID is supported.\nNon-NULL semantics is assumed, so caller will be forced to prove\nPTR_TO_BTF_ID can't be NULL.\n\nNote, we disallow global subprogs to destroy passed in PTR_TO_BTF_ID\narguments, even the trusted one. We achieve that by not setting\nref_obj_id when validating subprog code. This basically enforces (in\nRust terms) borrowing semantics vs move semantics. Borrowing semantics\nseems to be a better fit for isolated global subprog validation\napproach.\n\nImplementation-wise, we utilize existing logic for matching\nuser-provided BTF type to kernel-side BTF type, used by BPF CO-RE logic\nand following same matching rules. We enforce a unique match for types.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240130000648.2144827-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-30 09:41:50 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "8f2b44cd9d69ec36c9ce9623993978babb575ee8",
          "subject": "bpf: add arg:nullable tag to be combined with trusted pointers",
          "message": "Add ability to mark arg:trusted arguments with optional arg:nullable\ntag to mark it as PTR_TO_BTF_ID_OR_NULL variant, which will allow\ncallers to pass NULL, and subsequently will force global subprog's code\nto do NULL check. This allows to have \"optional\" PTR_TO_BTF_ID values\npassed into global subprogs.\n\nFor now arg:nullable cannot be combined with anything else.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240130000648.2144827-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-30 09:41:50 -0800",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "d28bb1a86e68a3d523e0acee8281bb904dd7f451",
          "subject": "libbpf: add __arg_trusted and __arg_nullable tag macros",
          "message": "Add __arg_trusted to annotate global func args that accept trusted\nPTR_TO_BTF_ID arguments.\n\nAlso add __arg_nullable to combine with __arg_trusted (and maybe other\ntags in the future) to force global subprog itself (i.e., callee) to do\nNULL checks, as opposed to default non-NULL semantics (and thus caller's\nresponsibility to ensure non-NULL values).\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240130000648.2144827-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-30 09:41:50 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf_helpers.h"
          ]
        },
        {
          "hash": "c381203eadb76d5601fc04b814317e7608af5f5c",
          "subject": "selftests/bpf: add trusted global subprog arg tests",
          "message": "Add a bunch of test cases validating behavior of __arg_trusted and its\ncombination with __arg_nullable tag. We also validate CO-RE flavor\nsupport by kernel for __arg_trusted args.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240130000648.2144827-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-30 09:41:50 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_global_ptr_args.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "c8632acf193beac64bbdaebef013368c480bf74f",
      "merge_subject": "Merge branch 'bpf-token'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nBPF token\n\nThis patch set is a combination of three BPF token-related patch sets ([0],\n[1], [2]) with fixes ([3]) to kernel-side token_fd passing APIs incorporated\ninto relevant patches, bpf_token_capable() changes requested by\nChristian Brauner, and necessary libbpf and BPF selftests side adjustments.\n\nThis patch set introduces an ability to delegate a subset of BPF subsystem\nfunctionality from privileged system-wide daemon (e.g., systemd or any other\ncontainer manager) through special mount options for userns-bound BPF FS to\na *trusted* unprivileged application. Trust is the key here. This\nfunctionality is not about allowing unconditional unprivileged BPF usage.\nEstablishing trust, though, is completely up to the discretion of respective\nprivileged application that would create and mount a BPF FS instance with\ndelegation enabled, as different production setups can and do achieve it\nthrough a combination of different means (signing, LSM, code reviews, etc),\nand it's undesirable and infeasible for kernel to enforce any particular way\nof validating trustworthiness of particular process.\n\nThe main motivation for this work is a desire to enable containerized BPF\napplications to be used together with user namespaces. This is currently\nimpossible, as CAP_BPF, required for BPF subsystem usage, cannot be namespaced\nor sandboxed, as a general rule. E.g., tracing BPF programs, thanks to BPF\nhelpers like bpf_probe_read_kernel() and bpf_probe_read_user() can safely read\narbitrary memory, and it's impossible to ensure that they only read memory of\nprocesses belonging to any given namespace. This means that it's impossible to\nhave a mechanically verifiable namespace-aware CAP_BPF capability, and as such\nanother mechanism to allow safe usage of BPF functionality is necessary.\n\nBPF FS delegation mount options and BPF token derived from such BPF FS instance\nis such a mechanism. Kernel makes no assumption about what \"trusted\"\nconstitutes in any particular case, and it's up to specific privileged\napplications and their surrounding infrastructure to decide that. What kernel\nprovides is a set of APIs to setup and mount special BPF FS instance and\nderive BPF tokens from it. BPF FS and BPF token are both bound to its owning\nuserns and in such a way are constrained inside intended container. Users can\nthen pass BPF token FD to privileged bpf() syscall commands, like BPF map\ncreation and BPF program loading, to perform such operations without having\ninit userns privileges.\n\nThis version incorporates feedback and suggestions ([4]) received on earlier\niterations of BPF token approach, and instead of allowing to create BPF tokens\ndirectly assuming capable(CAP_SYS_ADMIN), we instead enhance BPF FS to accept\na few new delegation mount options. If these options are used and BPF FS itself\nis properly created, set up, and mounted inside the user namespaced container,\nuser application is able to derive a BPF token object from BPF FS instance, and\npass that token to bpf() syscall. As explained in patch #3, BPF token itself\ndoesn't grant access to BPF functionality, but instead allows kernel to do\nnamespaced capabilities checks (ns_capable() vs capable()) for CAP_BPF,\nCAP_PERFMON, CAP_NET_ADMIN, and CAP_SYS_ADMIN, as applicable. So it forms one\nhalf of a puzzle and allows container managers and sys admins to have safe and\nflexible configuration options: determining which containers get delegation of\nBPF functionality through BPF FS, and then which applications within such\ncontainers are allowed to perform bpf() commands, based on namespaces\ncapabilities.\n\nPrevious attempt at addressing this very same problem ([5]) attempted to\nutilize authoritative LSM approach, but was conclusively rejected by upstream\nLSM maintainers. BPF token concept is not changing anything about LSM\napproach, but can be combined with LSM hooks for very fine-grained security\npolicy. Some ideas about making BPF token more convenient to use with LSM (in\nparticular custom BPF LSM programs) was briefly described in recent LSF/MM/BPF\n2023 presentation ([6]). E.g., an ability to specify user-provided data\n(context), which in combination with BPF LSM would allow implementing a very\ndynamic and fine-granular custom security policies on top of BPF token. In the\ninterest of minimizing API surface area and discussions this was relegated to\nfollow up patches, as it's not essential to the fundamental concept of\ndelegatable BPF token.\n\nIt should be noted that BPF token is conceptually quite similar to the idea of\n/dev/bpf device file, proposed by Song a while ago ([7]). The biggest\ndifference is the idea of using virtual anon_inode file to hold BPF token and\nallowing multiple independent instances of them, each (potentially) with its\nown set of restrictions. And also, crucially, BPF token approach is not using\nany special stateful task-scoped flags. Instead, bpf() syscall accepts\ntoken_fd parameters explicitly for each relevant BPF command. This addresses\nmain concerns brought up during the /dev/bpf discussion, and fits better with\noverall BPF subsystem design.\n\nSecond part of this patch set adds full support for BPF token in libbpf's BPF\nobject high-level API. Good chunk of the changes rework libbpf feature\ndetection internals, which are the most affected by BPF token presence.\n\nBesides internal refactorings, libbpf allows to pass location of BPF FS from\nwhich BPF token should be created by libbpf. This can be done explicitly though\na new bpf_object_open_opts.bpf_token_path field. But we also add implicit BPF\ntoken creation logic to BPF object load step, even without any explicit\ninvolvement of the user. If the environment is setup properly, BPF token will\nbe created transparently and used implicitly. This allows for all existing\napplication to gain BPF token support by just linking with latest version of\nlibbpf library. No source code modifications are required.  All that under\nassumption that privileged container management agent properly set up default\nBPF FS instance at /sys/bpf/fs to allow BPF token creation.\n\nlibbpf adds support to override default BPF FS location for BPF token creation\nthrough LIBBPF_BPF_TOKEN_PATH envvar knowledge. This allows admins or container\nmanagers to mount BPF token-enabled BPF FS at non-standard location without the\nneed to coordinate with applications.  LIBBPF_BPF_TOKEN_PATH can also be used\nto disable BPF token implicit creation by setting it to an empty value.\n\n  [0] https://patchwork.kernel.org/project/netdevbpf/list/?series=805707&state=*\n  [1] https://patchwork.kernel.org/project/netdevbpf/list/?series=810260&state=*\n  [2] https://patchwork.kernel.org/project/netdevbpf/list/?series=809800&state=*\n  [3] https://patchwork.kernel.org/project/netdevbpf/patch/20231219053150.336991-1-andrii@kernel.org/\n  [4] https://lore.kernel.org/bpf/20230704-hochverdient-lehne-eeb9eeef785e@brauner/\n  [5] https://lore.kernel.org/bpf/20230412043300.360803-1-andrii@kernel.org/\n  [6] http://vger.kernel.org/bpfconf2023_material/Trusted_unprivileged_BPF_LSFMM2023.pdf\n  [7] https://lore.kernel.org/bpf/20190627201923.2589391-2-songliubraving@fb.com/\n\nv1->v2:\n  - disable BPF token creation in init userns, and simplify\n    bpf_token_capable() logic (Christian);\n  - use kzalloc/kfree instead of kvzalloc/kvfree (Linus);\n  - few more selftest cases to validate LSM and BPF token interations.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\n====================\n\nLink: https://lore.kernel.org/r/20240124022127.2379740-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2024-01-24 16:21:03 -0800",
      "commits": [
        {
          "hash": "ed1ad5a7415de8be121055e7ab1303d2be5407e0",
          "subject": "bpf: Align CAP_NET_ADMIN checks with bpf_capable() approach",
          "message": "Within BPF syscall handling code CAP_NET_ADMIN checks stand out a bit\ncompared to CAP_BPF and CAP_PERFMON checks. For the latter, CAP_BPF or\nCAP_PERFMON are checked first, but if they are not set, CAP_SYS_ADMIN\ntakes over and grants whatever part of BPF syscall is required.\n\nSimilar kind of checks that involve CAP_NET_ADMIN are not so consistent.\nOne out of four uses does follow CAP_BPF/CAP_PERFMON model: during\nBPF_PROG_LOAD, if the type of BPF program is \"network-related\" either\nCAP_NET_ADMIN or CAP_SYS_ADMIN is required to proceed.\n\nBut in three other cases CAP_NET_ADMIN is required even if CAP_SYS_ADMIN\nis set:\n  - when creating DEVMAP/XDKMAP/CPU_MAP maps;\n  - when attaching CGROUP_SKB programs;\n  - when handling BPF_PROG_QUERY command.\n\nThis patch is changing the latter three cases to follow BPF_PROG_LOAD\nmodel, that is allowing to proceed under either CAP_NET_ADMIN or\nCAP_SYS_ADMIN.\n\nThis also makes it cleaner in subsequent BPF token patches to switch\nwholesomely to a generic bpf_token_capable(int cap) check, that always\nfalls back to CAP_SYS_ADMIN if requested capability is missing.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: Yafang Shao <laoar.shao@gmail.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-2-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:00 -0800",
          "modified_files": [
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "6fe01d3cbb924a72493eb3f4722dfcfd1c194234",
          "subject": "bpf: Add BPF token delegation mount options to BPF FS",
          "message": "Add few new mount options to BPF FS that allow to specify that a given\nBPF FS instance allows creation of BPF token (added in the next patch),\nand what sort of operations are allowed under BPF token. As such, we get\n4 new mount options, each is a bit mask\n  - `delegate_cmds` allow to specify which bpf() syscall commands are\n    allowed with BPF token derived from this BPF FS instance;\n  - if BPF_MAP_CREATE command is allowed, `delegate_maps` specifies\n    a set of allowable BPF map types that could be created with BPF token;\n  - if BPF_PROG_LOAD command is allowed, `delegate_progs` specifies\n    a set of allowable BPF program types that could be loaded with BPF token;\n  - if BPF_PROG_LOAD command is allowed, `delegate_attachs` specifies\n    a set of allowable BPF program attach types that could be loaded with\n    BPF token; delegate_progs and delegate_attachs are meant to be used\n    together, as full BPF program type is, in general, determined\n    through both program type and program attach type.\n\nCurrently, these mount options accept the following forms of values:\n  - a special value \"any\", that enables all possible values of a given\n  bit set;\n  - numeric value (decimal or hexadecimal, determined by kernel\n  automatically) that specifies a bit mask value directly;\n  - all the values for a given mount option are combined, if specified\n  multiple times. E.g., `mount -t bpf nodev /path/to/mount -o\n  delegate_maps=0x1 -o delegate_maps=0x2` will result in a combined 0x3\n  mask.\n\nIdeally, more convenient (for humans) symbolic form derived from\ncorresponding UAPI enums would be accepted (e.g., `-o\ndelegate_progs=kprobe|tracepoint`) and I intend to implement this, but\nit requires a bunch of UAPI header churn, so I postponed it until this\nfeature lands upstream or at least there is a definite consensus that\nthis feature is acceptable and is going to make it, just to minimize\namount of wasted effort and not increase amount of non-essential code to\nbe reviewed.\n\nAttentive reader will notice that BPF FS is now marked as\nFS_USERNS_MOUNT, which theoretically makes it mountable inside non-init\nuser namespace as long as the process has sufficient *namespaced*\ncapabilities within that user namespace. But in reality we still\nrestrict BPF FS to be mountable only by processes with CAP_SYS_ADMIN *in\ninit userns* (extra check in bpf_fill_super()). FS_USERNS_MOUNT is added\nto allow creating BPF FS context object (i.e., fsopen(\"bpf\")) from\ninside unprivileged process inside non-init userns, to capture that\nuserns as the owning userns. It will still be required to pass this\ncontext object back to privileged process to instantiate and mount it.\n\nThis manipulation is important, because capturing non-init userns as the\nowning userns of BPF FS instance (super block) allows to use that userns\nto constraint BPF token to that userns later on (see next patch). So\ncreating BPF FS with delegation inside unprivileged userns will restrict\nderived BPF token objects to only \"work\" inside that intended userns,\nmaking it scoped to a intended \"container\". Also, setting these\ndelegation options requires capable(CAP_SYS_ADMIN), so unprivileged\nprocess cannot set this up without involvement of a privileged process.\n\nThere is a set of selftests at the end of the patch set that simulates\nthis sequence of steps and validates that everything works as intended.\nBut careful review is requested to make sure there are no missed gaps in\nthe implementation and testing.\n\nThis somewhat subtle set of aspects is the result of previous\ndiscussions ([0]) about various user namespace implications and\ninteractions with BPF token functionality and is necessary to contain\nBPF token inside intended user namespace.\n\n  [0] https://lore.kernel.org/bpf/20230704-hochverdient-lehne-eeb9eeef785e@brauner/\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: Christian Brauner <brauner@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-3-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:00 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/inode.c"
          ]
        },
        {
          "hash": "35f96de04127d332a5c5e8a155d31f452f88c76d",
          "subject": "bpf: Introduce BPF token object",
          "message": "Add new kind of BPF kernel object, BPF token. BPF token is meant to\nallow delegating privileged BPF functionality, like loading a BPF\nprogram or creating a BPF map, from privileged process to a *trusted*\nunprivileged process, all while having a good amount of control over which\nprivileged operations could be performed using provided BPF token.\n\nThis is achieved through mounting BPF FS instance with extra delegation\nmount options, which determine what operations are delegatable, and also\nconstraining it to the owning user namespace (as mentioned in the\nprevious patch).\n\nBPF token itself is just a derivative from BPF FS and can be created\nthrough a new bpf() syscall command, BPF_TOKEN_CREATE, which accepts BPF\nFS FD, which can be attained through open() API by opening BPF FS mount\npoint. Currently, BPF token \"inherits\" delegated command, map types,\nprog type, and attach type bit sets from BPF FS as is. In the future,\nhaving an BPF token as a separate object with its own FD, we can allow\nto further restrict BPF token's allowable set of things either at the\ncreation time or after the fact, allowing the process to guard itself\nfurther from unintentionally trying to load undesired kind of BPF\nprograms. But for now we keep things simple and just copy bit sets as is.\n\nWhen BPF token is created from BPF FS mount, we take reference to the\nBPF super block's owning user namespace, and then use that namespace for\nchecking all the {CAP_BPF, CAP_PERFMON, CAP_NET_ADMIN, CAP_SYS_ADMIN}\ncapabilities that are normally only checked against init userns (using\ncapable()), but now we check them using ns_capable() instead (if BPF\ntoken is provided). See bpf_token_capable() for details.\n\nSuch setup means that BPF token in itself is not sufficient to grant BPF\nfunctionality. User namespaced process has to *also* have necessary\ncombination of capabilities inside that user namespace. So while\npreviously CAP_BPF was useless when granted within user namespace, now\nit gains a meaning and allows container managers and sys admins to have\na flexible control over which processes can and need to use BPF\nfunctionality within the user namespace (i.e., container in practice).\nAnd BPF FS delegation mount options and derived BPF tokens serve as\na per-container \"flag\" to grant overall ability to use bpf() (plus further\nrestrict on which parts of bpf() syscalls are treated as namespaced).\n\nNote also, BPF_TOKEN_CREATE command itself requires ns_capable(CAP_BPF)\nwithin the BPF FS owning user namespace, rounding up the ns_capable()\nstory of BPF token. Also creating BPF token in init user namespace is\ncurrently not supported, given BPF token doesn't have any effect in init\nuser namespace anyways.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: Christian Brauner <brauner@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-4-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:01 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/Makefile",
            "kernel/bpf/inode.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/token.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "a177fc2bf6fd83704854feaf7aae926b1df4f0b9",
          "subject": "bpf: Add BPF token support to BPF_MAP_CREATE command",
          "message": "Allow providing token_fd for BPF_MAP_CREATE command to allow controlled\nBPF map creation from unprivileged process through delegated BPF token.\nNew BPF_F_TOKEN_FD flag is added to specify together with BPF token FD\nfor BPF_MAP_CREATE command.\n\nWire through a set of allowed BPF map types to BPF token, derived from\nBPF FS at BPF token creation time. This, in combination with allowed_cmds\nallows to create a narrowly-focused BPF token (controlled by privileged\nagent) with a restrictive set of BPF maps that application can attempt\nto create.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-5-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:01 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/inode.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/token.c",
            "tools/include/uapi/linux/bpf.h",
            "tools/testing/selftests/bpf/prog_tests/libbpf_probes.c",
            "tools/testing/selftests/bpf/prog_tests/libbpf_str.c"
          ]
        },
        {
          "hash": "9ea7c4bf17e39d463eb4782f948f401d9764b1b3",
          "subject": "bpf: Add BPF token support to BPF_BTF_LOAD command",
          "message": "Accept BPF token FD in BPF_BTF_LOAD command to allow BTF data loading\nthrough delegated BPF token. BPF_F_TOKEN_FD flag has to be specified\nwhen passing BPF token FD. Given BPF_BTF_LOAD command didn't have flags\nfield before, we also add btf_flags field.\n\nBTF loading is a pretty straightforward operation, so as long as BPF\ntoken is created with allow_cmds granting BPF_BTF_LOAD command, kernel\nproceeds to parsing BTF data and creating BTF object.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-6-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:01 -0800",
          "modified_files": [
            "include/uapi/linux/bpf.h",
            "kernel/bpf/syscall.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "caf8f28e036c4ba1e823355da6c0c01c39e70ab9",
          "subject": "bpf: Add BPF token support to BPF_PROG_LOAD command",
          "message": "Add basic support of BPF token to BPF_PROG_LOAD. BPF_F_TOKEN_FD flag\nshould be set in prog_flags field when providing prog_token_fd.\n\nWire through a set of allowed BPF program types and attach types,\nderived from BPF FS at BPF token creation time. Then make sure we\nperform bpf_token_capable() checks everywhere where it's relevant.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-7-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:01 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/core.c",
            "kernel/bpf/inode.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/token.c",
            "tools/include/uapi/linux/bpf.h",
            "tools/testing/selftests/bpf/prog_tests/libbpf_probes.c",
            "tools/testing/selftests/bpf/prog_tests/libbpf_str.c"
          ]
        },
        {
          "hash": "bbc1d24724e110b86a1a7c3c1724ce0d62cc1e2e",
          "subject": "bpf: Take into account BPF token when fetching helper protos",
          "message": "Instead of performing unconditional system-wide bpf_capable() and\nperfmon_capable() calls inside bpf_base_func_proto() function (and other\nsimilar ones) to determine eligibility of a given BPF helper for a given\nprogram, use previously recorded BPF token during BPF_PROG_LOAD command\nhandling to inform the decision.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-8-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:01 -0800",
          "modified_files": [
            "drivers/media/rc/bpf-lirc.c",
            "include/linux/bpf.h",
            "kernel/bpf/cgroup.c",
            "kernel/bpf/helpers.c",
            "kernel/bpf/syscall.c",
            "kernel/trace/bpf_trace.c",
            "net/core/filter.c",
            "net/ipv4/bpf_tcp_ca.c",
            "net/netfilter/nf_bpf_link.c"
          ]
        },
        {
          "hash": "d79a3549754725bb90e58104417449edddf3da3d",
          "subject": "bpf: Consistently use BPF token throughout BPF verifier logic",
          "message": "Remove remaining direct queries to perfmon_capable() and bpf_capable()\nin BPF verifier logic and instead use BPF token (if available) to make\ndecisions about privileges.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-9-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:01 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/filter.h",
            "kernel/bpf/arraymap.c",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c",
            "net/core/filter.c"
          ]
        },
        {
          "hash": "1b67772e4e3f16cd647b229cae95fc06d120be08",
          "subject": "bpf,lsm: Refactor bpf_prog_alloc/bpf_prog_free LSM hooks",
          "message": "Based on upstream discussion ([0]), rework existing\nbpf_prog_alloc_security LSM hook. Rename it to bpf_prog_load and instead\nof passing bpf_prog_aux, pass proper bpf_prog pointer for a full BPF\nprogram struct. Also, we pass bpf_attr union with all the user-provided\narguments for BPF_PROG_LOAD command.  This will give LSMs as much\ninformation as we can basically provide.\n\nThe hook is also BPF token-aware now, and optional bpf_token struct is\npassed as a third argument. bpf_prog_load LSM hook is called after\na bunch of sanity checks were performed, bpf_prog and bpf_prog_aux were\nallocated and filled out, but right before performing full-fledged BPF\nverification step.\n\nbpf_prog_free LSM hook is now accepting struct bpf_prog argument, for\nconsistency. SELinux code is adjusted to all new names, types, and\nsignatures.\n\nNote, given that bpf_prog_load (previously bpf_prog_alloc) hook can be\nused by some LSMs to allocate extra security blob, but also by other\nLSMs to reject BPF program loading, we need to make sure that\nbpf_prog_free LSM hook is called after bpf_prog_load/bpf_prog_alloc one\n*even* if the hook itself returned error. If we don't do that, we run\nthe risk of leaking memory. This seems to be possible today when\ncombining SELinux and BPF LSM, as one example, depending on their\nrelative ordering.\n\nAlso, for BPF LSM setup, add bpf_prog_load and bpf_prog_free to\nsleepable LSM hooks list, as they are both executed in sleepable\ncontext. Also drop bpf_prog_load hook from untrusted, as there is no\nissue with refcount or anything else anymore, that originally forced us\nto add it to untrusted list in c0c852dd1876 (\"bpf: Do not mark certain LSM\nhook arguments as trusted\"). We now trigger this hook much later and it\nshould not be an issue anymore.\n\n  [0] https://lore.kernel.org/bpf/9fe88aef7deabbe87d3fc38c4aea3c69.paul@paul-moore.com/\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: Paul Moore <paul@paul-moore.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-10-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:01 -0800",
          "modified_files": [
            "include/linux/lsm_hook_defs.h",
            "include/linux/security.h",
            "kernel/bpf/bpf_lsm.c",
            "kernel/bpf/syscall.c",
            "security/security.c",
            "security/selinux/hooks.c"
          ]
        },
        {
          "hash": "a2431c7eabcf9bd5a1e7a1f7ecded40fdda4a8c5",
          "subject": "bpf,lsm: Refactor bpf_map_alloc/bpf_map_free LSM hooks",
          "message": "Similarly to bpf_prog_alloc LSM hook, rename and extend bpf_map_alloc\nhook into bpf_map_create, taking not just struct bpf_map, but also\nbpf_attr and bpf_token, to give a fuller context to LSMs.\n\nUnlike bpf_prog_alloc, there is no need to move the hook around, as it\ncurrently is firing right before allocating BPF map ID and FD, which\nseems to be a sweet spot.\n\nBut like bpf_prog_alloc/bpf_prog_free combo, make sure that bpf_map_free\nLSM hook is called even if bpf_map_create hook returned error, as if few\nLSMs are combined together it could be that one LSM successfully\nallocated security blob for its needs, while subsequent LSM rejected BPF\nmap creation. The former LSM would still need to free up LSM blob, so we\nneed to ensure security_bpf_map_free() is called regardless of the\noutcome.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: Paul Moore <paul@paul-moore.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-11-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:01 -0800",
          "modified_files": [
            "include/linux/lsm_hook_defs.h",
            "include/linux/security.h",
            "kernel/bpf/bpf_lsm.c",
            "kernel/bpf/syscall.c",
            "security/security.c",
            "security/selinux/hooks.c"
          ]
        },
        {
          "hash": "f568a3d49af9aed813a184353592efe29b0e3d16",
          "subject": "bpf,lsm: Add BPF token LSM hooks",
          "message": "Wire up bpf_token_create and bpf_token_free LSM hooks, which allow to\nallocate LSM security blob (we add `void *security` field to struct\nbpf_token for that), but also control who can instantiate BPF token.\nThis follows existing pattern for BPF map and BPF prog.\n\nAlso add security_bpf_token_allow_cmd() and security_bpf_token_capable()\nLSM hooks that allow LSM implementation to control and negate (if\nnecessary) BPF token's delegation of a specific bpf_cmd and capability,\nrespectively.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: Paul Moore <paul@paul-moore.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-12-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:01 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/lsm_hook_defs.h",
            "include/linux/security.h",
            "kernel/bpf/bpf_lsm.c",
            "kernel/bpf/token.c",
            "security/security.c"
          ]
        },
        {
          "hash": "639ecd7d6247c48a0175f5b458b648f5d4b6dc34",
          "subject": "libbpf: Add bpf_token_create() API",
          "message": "Add low-level wrapper API for BPF_TOKEN_CREATE command in bpf() syscall.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-13-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:01 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/bpf.h",
            "tools/lib/bpf/libbpf.map"
          ]
        },
        {
          "hash": "364f848375af311150210a1ad3c5bcb800b65b48",
          "subject": "libbpf: Add BPF token support to bpf_map_create() API",
          "message": "Add ability to provide token_fd for BPF_MAP_CREATE command through\nbpf_map_create() API.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-14-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:01 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/bpf.h"
          ]
        },
        {
          "hash": "a3d63e85253b6c9b6aa34b99208e835358a91320",
          "subject": "libbpf: Add BPF token support to bpf_btf_load() API",
          "message": "Allow user to specify token_fd for bpf_btf_load() API that wraps\nkernel's BPF_BTF_LOAD command. This allows loading BTF from unprivileged\nprocess as long as it has BPF token allowing BPF_BTF_LOAD command, which\ncan be created and delegated by privileged process.\n\nWire through new btf_flags as well, so that user can provide\nBPF_F_TOKEN_FD flag, if necessary.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-15-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/bpf.h"
          ]
        },
        {
          "hash": "404cbc149c3866e6ec2bfe1bce52c8864e1f81fc",
          "subject": "libbpf: Add BPF token support to bpf_prog_load() API",
          "message": "Wire through token_fd into bpf_prog_load().\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-16-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/bpf.h"
          ]
        },
        {
          "hash": "fcb9597ff7d1f7c772c1237dd2d04dd44e622501",
          "subject": "selftests/bpf: Add BPF token-enabled tests",
          "message": "Add a selftest that attempts to conceptually replicate intended BPF\ntoken use cases inside user namespaced container.\n\nChild process is forked. It is then put into its own userns and mountns.\nChild creates BPF FS context object. This ensures child userns is\ncaptured as the owning userns for this instance of BPF FS. Given setting\ndelegation mount options is privileged operation, we ensure that child\ncannot set them.\n\nThis context is passed back to privileged parent process through Unix\nsocket, where parent sets up delegation options, creates, and mounts it\nas a detached mount. This mount FD is passed back to the child to be\nused for BPF token creation, which allows otherwise privileged BPF\noperations to succeed inside userns.\n\nWe validate that all of token-enabled privileged commands (BPF_BTF_LOAD,\nBPF_MAP_CREATE, and BPF_PROG_LOAD) work as intended. They should only\nsucceed inside the userns if a) BPF token is provided with proper\nallowed sets of commands and types; and b) namespaces CAP_BPF and other\nprivileges are set. Lacking a) or b) should lead to -EPERM failures.\n\nBased on suggested workflow by Christian Brauner ([0]).\n\n  [0] https://lore.kernel.org/bpf/20230704-hochverdient-lehne-eeb9eeef785e@brauner/\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-17-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/token.c"
          ]
        },
        {
          "hash": "0054493e5141b16e316b8c52d6aa534397e48b6c",
          "subject": "bpf,selinux: Allocate bpf_security_struct per BPF token",
          "message": "Utilize newly added bpf_token_create/bpf_token_free LSM hooks to\nallocate struct bpf_security_struct for each BPF token object in\nSELinux. This just follows similar pattern for BPF prog and map.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-18-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "security/selinux/hooks.c"
          ]
        },
        {
          "hash": "aeaa97b006ddc7a8bf13e4adfdd02b3526f648a7",
          "subject": "bpf: Fail BPF_TOKEN_CREATE if no delegation option was set on BPF FS",
          "message": "It's quite confusing in practice when it's possible to successfully\ncreate a BPF token from BPF FS that didn't have any of delegate_xxx\nmount options set up. While it's not wrong, it's actually more\nmeaningful to reject BPF_TOKEN_CREATE with specific error code (-ENOENT)\nto let user-space know that no token delegation is setup up.\n\nSo, instead of creating empty BPF token that will be always ignored\nbecause it doesn't have any of the allow_xxx bits set, reject it with\n-ENOENT. If we ever need empty BPF token to be possible, we can support\nthat with extra flag passed into BPF_TOKEN_CREATE.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: Christian Brauner <brauner@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-19-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "kernel/bpf/token.c"
          ]
        },
        {
          "hash": "6c1752e0b6ca8c7021d6da3926738d8d88f601a9",
          "subject": "bpf: Support symbolic BPF FS delegation mount options",
          "message": "Besides already supported special \"any\" value and hex bit mask, support\nstring-based parsing of delegation masks based on exact enumerator\nnames. Utilize BTF information of `enum bpf_cmd`, `enum bpf_map_type`,\n`enum bpf_prog_type`, and `enum bpf_attach_type` types to find supported\nsymbolic names (ignoring __MAX_xxx guard values and stripping repetitive\nprefixes like BPF_ for cmd and attach types, BPF_MAP_TYPE_ for maps, and\nBPF_PROG_TYPE_ for prog types). The case doesn't matter, but it is\nnormalized to lower case in mount option output. So \"PROG_LOAD\",\n\"prog_load\", and \"MAP_create\" are all valid values to specify for\ndelegate_cmds options, \"array\" is among supported for map types, etc.\n\nBesides supporting string values, we also support multiple values\nspecified at the same time, using colon (':') separator.\n\nThere are corresponding changes on bpf_show_options side to use known\nvalues to print them in human-readable format, falling back to hex mask\nprinting, if there are any unrecognized bits. This shouldn't be\nnecessary when enum BTF information is present, but in general we should\nalways be able to fall back to this even if kernel was built without BTF.\nAs mentioned, emitted symbolic names are normalized to be all lower case.\n\nExample below shows various ways to specify delegate_cmds options\nthrough mount command and how mount options are printed back:\n\n12/14 14:39:07.604\nvmuser@archvm:~/local/linux/tools/testing/selftests/bpf\n$ mount | rg token\n\n  $ sudo mkdir -p /sys/fs/bpf/token\n  $ sudo mount -t bpf bpffs /sys/fs/bpf/token \\\n               -o delegate_cmds=prog_load:MAP_CREATE \\\n               -o delegate_progs=kprobe \\\n               -o delegate_attachs=xdp\n  $ mount | grep token\n  bpffs on /sys/fs/bpf/token type bpf (rw,relatime,delegate_cmds=map_create:prog_load,delegate_progs=kprobe,delegate_attachs=xdp)\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-20-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "kernel/bpf/inode.c"
          ]
        },
        {
          "hash": "0350f9d99ee538f2ccf179f0216e704a5f39b317",
          "subject": "selftests/bpf: Utilize string values for delegate_xxx mount options",
          "message": "Use both hex-based and string-based way to specify delegate mount\noptions for BPF FS.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-21-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/token.c"
          ]
        },
        {
          "hash": "ea4d587354eb5e32dfa93cebb055b072f518b193",
          "subject": "libbpf: Split feature detectors definitions from cached results",
          "message": "Split a list of supported feature detectors with their corresponding\ncallbacks from actual cached supported/missing values. This will allow\nto have more flexible per-token or per-object feature detectors in\nsubsequent refactorings.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-22-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "tools/lib/bpf/libbpf.c"
          ]
        },
        {
          "hash": "d6dd1d49367ab03832b3c4b6f8211765d488c82b",
          "subject": "libbpf: Further decouple feature checking logic from bpf_object",
          "message": "Add feat_supported() helper that accepts feature cache instead of\nbpf_object. This allows low-level code in bpf.c to not know or care\nabout higher-level concept of bpf_object, yet it will be able to utilize\ncustom feature checking in cases where BPF token might influence the\noutcome.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-23-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/libbpf.c",
            "tools/lib/bpf/libbpf_internal.h"
          ]
        },
        {
          "hash": "05f9cdd55d61cf9c6283fd3dc0edc7cad09bd7fe",
          "subject": "libbpf: Move feature detection code into its own file",
          "message": "It's quite a lot of well isolated code, so it seems like a good\ncandidate to move it out of libbpf.c to reduce its size.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-24-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "tools/lib/bpf/Build",
            "tools/lib/bpf/elf.c",
            "tools/lib/bpf/features.c",
            "tools/lib/bpf/libbpf.c",
            "tools/lib/bpf/libbpf_internal.h",
            "tools/lib/bpf/str_error.h"
          ]
        },
        {
          "hash": "f3dcee938f485cf403ba2acf1f1548afe637c904",
          "subject": "libbpf: Wire up token_fd into feature probing logic",
          "message": "Adjust feature probing callbacks to take into account optional token_fd.\nIn unprivileged contexts, some feature detectors would fail to detect\nkernel support just because BPF program, BPF map, or BTF object can't be\nloaded due to privileged nature of those operations. So when BPF object\nis loaded with BPF token, this token should be used for feature probing.\n\nThis patch is setting support for this scenario, but we don't yet pass\nnon-zero token FD. This will be added in the next patch.\n\nWe also switched BPF cookie detector from using kprobe program to\ntracepoint one, as tracepoint is somewhat less dangerous BPF program\ntype and has higher likelihood of being allowed through BPF token in the\nfuture. This change has no effect on detection behavior.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-25-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/features.c",
            "tools/lib/bpf/libbpf.c",
            "tools/lib/bpf/libbpf_internal.h",
            "tools/lib/bpf/libbpf_probes.c"
          ]
        },
        {
          "hash": "6b434b61b4d9e0e59f2947ce0f58f6fb4de048d8",
          "subject": "libbpf: Wire up BPF token support at BPF object level",
          "message": "Add BPF token support to BPF object-level functionality.\n\nBPF token is supported by BPF object logic either as an explicitly\nprovided BPF token from outside (through BPF FS path), or implicitly\n(unless prevented through bpf_object_open_opts).\n\nImplicit mode is assumed to be the most common one for user namespaced\nunprivileged workloads. The assumption is that privileged container\nmanager sets up default BPF FS mount point at /sys/fs/bpf with BPF token\ndelegation options (delegate_{cmds,maps,progs,attachs} mount options).\nBPF object during loading will attempt to create BPF token from\n/sys/fs/bpf location, and pass it for all relevant operations\n(currently, map creation, BTF load, and program load).\n\nIn this implicit mode, if BPF token creation fails due to whatever\nreason (BPF FS is not mounted, or kernel doesn't support BPF token,\netc), this is not considered an error. BPF object loading sequence will\nproceed with no BPF token.\n\nIn explicit BPF token mode, user provides explicitly custom BPF FS mount\npoint path. In such case, BPF object will attempt to create BPF token\nfrom provided BPF FS location. If BPF token creation fails, that is\nconsidered a critical error and BPF object load fails with an error.\n\nLibbpf provides a way to disable implicit BPF token creation, if it\ncauses any troubles (BPF token is designed to be completely optional and\nshouldn't cause any problems even if provided, but in the world of BPF\nLSM, custom security logic can be installed that might change outcome\ndepending on the presence of BPF token). To disable libbpf's default BPF\ntoken creation behavior user should provide either invalid BPF token FD\n(negative), or empty bpf_token_path option.\n\nBPF token presence can influence libbpf's feature probing, so if BPF\nobject has associated BPF token, feature probing is instructed to use\nBPF object-specific feature detection cache and token FD.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-26-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:02 -0800",
          "modified_files": [
            "tools/lib/bpf/btf.c",
            "tools/lib/bpf/libbpf.c",
            "tools/lib/bpf/libbpf.h",
            "tools/lib/bpf/libbpf_internal.h"
          ]
        },
        {
          "hash": "d5baf0cac627fb3a00d9235955a388e5930b6d0e",
          "subject": "selftests/bpf: Add BPF object loading tests with explicit token passing",
          "message": "Add a few tests that attempt to load BPF object containing privileged\nmap, program, and the one requiring mandatory BTF uploading into the\nkernel (to validate token FD propagation to BPF_BTF_LOAD command).\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-27-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:03 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/token.c",
            "tools/testing/selftests/bpf/progs/priv_map.c",
            "tools/testing/selftests/bpf/progs/priv_prog.c"
          ]
        },
        {
          "hash": "b73d08d1318a2dde5bacbab77d0e2fd2aa47c933",
          "subject": "selftests/bpf: Add tests for BPF object load with implicit token",
          "message": "Add a test to validate libbpf's implicit BPF token creation from default\nBPF FS location (/sys/fs/bpf). Also validate that disabling this\nimplicit BPF token creation works.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-28-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:03 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/token.c"
          ]
        },
        {
          "hash": "cac270ad79afe212ed7986e8d271c72521cd8212",
          "subject": "libbpf: Support BPF token path setting through LIBBPF_BPF_TOKEN_PATH envvar",
          "message": "To allow external admin authority to override default BPF FS location\n(/sys/fs/bpf) for implicit BPF token creation, teach libbpf to recognize\nLIBBPF_BPF_TOKEN_PATH envvar. If it is specified and user application\ndidn't explicitly specify bpf_token_path option, it will be treated\nexactly like bpf_token_path option, overriding default /sys/fs/bpf\nlocation and making BPF token mandatory.\n\nSuggested-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-29-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:03 -0800",
          "modified_files": [
            "tools/lib/bpf/libbpf.c",
            "tools/lib/bpf/libbpf.h"
          ]
        },
        {
          "hash": "fadf54935e859c4d512aed6ad54f639b87a3b4d3",
          "subject": "selftests/bpf: Add tests for LIBBPF_BPF_TOKEN_PATH envvar",
          "message": "Add new subtest validating LIBBPF_BPF_TOKEN_PATH envvar semantics.\nExtend existing test to validate that LIBBPF_BPF_TOKEN_PATH allows to\ndisable implicit BPF token creation by setting envvar to empty string.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-30-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:03 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/token.c"
          ]
        },
        {
          "hash": "906ee42cb1be1152ef24465704cc89edc3f571c1",
          "subject": "selftests/bpf: Incorporate LSM policy to token-based tests",
          "message": "Add tests for LSM interactions (both bpf_token_capable and bpf_token_cmd\nLSM hooks) with BPF token in bpf() subsystem. Now child process passes\nback token FD for parent to be able to do tests with token originating\nin \"wrong\" userns. But we also create token in initns and check that\ntoken LSMs don't accidentally reject BPF operations when capable()\nchecks pass without BPF token.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-31-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2024-01-24 16:21:03 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/token.c",
            "tools/testing/selftests/bpf/progs/token_lsm.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "8b593021319d4893a8fbeb7bd1f668657e68403c",
      "merge_subject": "Merge branch 'Registrating struct_ops types from modules'",
      "merge_body": "Kui-Feng Lee says:\n\n====================\nGiven the current constraints of the current implementation,\nstruct_ops cannot be registered dynamically. This presents a\nsignificant limitation for modules like coming fuse-bpf, which seeks\nto implement a new struct_ops type. To address this issue, a new API\nis introduced that allows the registration of new struct_ops types\nfrom modules.\n\nPreviously, struct_ops types were defined in bpf_struct_ops_types.h\nand collected as a static array. The new API lets callers add new\nstruct_ops types dynamically. The static array has been removed and\nreplaced by the per-btf struct_ops_tab.\n\nThe struct_ops subsystem relies on BTF to determine the layout of\nvalues in a struct_ops map and identify the subsystem that the\nstruct_ops map registers to. However, the kernel BTF does not include\nthe type information of struct_ops types defined by a module. The\nstruct_ops subsystem requires knowledge of the corresponding module\nfor a given struct_ops map and the utilization of BTF information from\nthat module. We empower libbpf to determine the correct module for\naccessing the BTF information and pass an identity (FD) of the module\nbtf to the kernel. The kernel looks up type information and registered\nstruct_ops types directly from the given btf.\n\nIf a module exits while one or more struct_ops maps still refer to a\nstruct_ops type defined by the module, it can lead to unforeseen\ncomplications. Therefore, it is crucial to ensure that a module\nremains intact as long as any struct_ops map is still linked to a\nstruct_ops type defined by the module. To achieve this, every\nstruct_ops map holds a reference to the module while being registered.\n\nChanges from v16:\n\n - Fix unnecessary bpf_struct_ops_link_create() removing/adding.\n\n - Rename REGISTER_BPF_STRUCT_OPS() to register_bpf_struct_ops().\n\n - Implement bpf_map_struct_ops_info_fill() for !CONFIG_BPF_JIT.\n\nChanges from v15:\n\n - Fix the misleading commit message of part 4.\n\n - Introduce BPF_F_VTYPE_BTF_OBJ_FD flag to struct bpf_attr to tell\n   if value_type_btf_obj_fd is set or not.\n\n - Introduce links_cnt to struct bpf_struct_ops_map to avoid accessing\n   struct bpf_struct_ops_desc in bpf_struct_ops_map_put_progs() after\n   calling module_put() against the owner module of the struct_ops\n   type. (Part 9)\n\nChanges from v14:\n\n - Rebase. Add cif_stub required by\n   the commit 2cd3e3772e413 (\"x86/cfi,bpf: Fix bpf_struct_ops CFI\")\n\n - Remove creating struct_ops map without bpf_testmod.ko from the\n   test.\n\n - Check the name of btf returned by bpf_map_info by getting the name\n   with bpf_btf_get_info_by_fd().\n\n - Change value_type_btf_obj_fd to a signed type to allow the 0 fd.\n\nChanges from v13:\n\n - Change the test case to use bpf_map_create() to create a struct_ops\n   map while testmod.ko is unloaded.\n\n - Move bpf_struct_ops_find*() to btf.c.\n\n - Use btf_is_module() to replace btf != btf_vmlinux.\n\nChanges from v12:\n\n - Rebase to for-next to fix conflictions.\n\nChanges from v11:\n\n - bpf_struct_ops_maps hold only the refcnt to the module, but not\n   btf. (patch 1)\n\n - Fix warning messages. (patch 1, 9 and 10)\n\n - Remove unnecessary conditional compiling of CONFIG_BPF_JIT.\n   (patch 4, 9 and 10)\n\n - Fix the commit log of the patch 7 to explain how a btf is pass from\n   the user space and how the kernel handle it.\n\n - bpf_struct_ops_maps hold the module defining it's type, but not\n   btf. A map will hold the module through its life-span from\n   allocating to being free. (patch 8)\n\n - Change selftests and tracing __bpf_struct_ops_map_free() to wait\n   for the release of the bpf_testmod module.\n\n - Include btf_obj_id in bpf_map_info. (patch 14)\n\nChanges from v10:\n\n - Guard btf.c from CONFIG_BPF_JIT=n. This patchset has introduced\n   symbols from bpf_struct_ops.c which is only built when\n   CONFIG_BPF_JIT=y.\n\n - Fix the warning of unused errout_free label by moving code that is\n   leaked to patch 8 to patch 7.\n\nChanges from v9:\n\n - Remove the call_rcu_tasks_trace() changes from kern_sync_rcu().\n\n - Trace btf_put() in the test case to ensure the release of kmod's\n   btf, or the consequent tests may fail for using kmod's unloaded old\n   btf instead the new one created after loading again. The kmod's btf\n   may live for awhile after unloading the kmod, for a map being freed\n   asynchronized is still holding the btf.\n\n - Split \"add struct_ops_tab to btf\" into tow patches by adding\n   \"make struct_ops_map support btfs other than btf_vmlinux\".\n\n - Flip the order of \"pass attached BTF to the bpf_struct_ops\n   subsystem\" and \"hold module for bpf_struct_ops_map\" to make it more\n   reasonable.\n\n - Fix the compile errors of a missing header file.\n\nChanges from v8:\n\n - Rename bpf_struct_ops_init_one() to bpf_struct_ops_desc_init().\n\n - Move code that using BTF_ID_LIST to the newly added patch 2.\n\n - Move code that lookup struct_ops types from a given module to the\n   newly added patch 5.\n\n - Store the pointers of btf at st_maps.\n\n - Add test cases for the cases of modules being unload.\n\n - Call bpf_struct_ops_init() in btf_add_struct_ops() to fix an\n   inconsistent issue.\n\nChanges from v7:\n\n - Fix check_struct_ops_btf_id() to use attach btf if there is instead\n   of btf_vmlinux.\n\nChanges from v6:\n\n - Change returned error code to -EINVAL for the case of\n   bpf_try_get_module().\n\n - Return an error code from bpf_struct_ops_init().\n\n - Fix the dependency issue of testing_helpers.c and\n   rcu_tasks_trace_gp.skel.h.\n\nChanges from v5:\n\n - As the 2nd patch, we introduce \"bpf_struct_ops_desc\". This change\n   involves moving certain members of \"bpf_struct_ops\" to\n   \"bpf_struct_ops_desc\", which becomes a part of\n   \"btf_struct_ops_tab\". This ensures that these members remain\n   accessible even when the owner module of a \"bpf_struct_ops\" is\n   unloaded.\n\n - Correct the order of arguments when calling\n    in the 3rd patch.\n\n - Remove the owner argument from bpf_struct_ops_init_one(). Instead,\n   callers should fill in st_ops->owner.\n\n - Make sure to hold the owner module when calling\n   bpf_struct_ops_find() and bpf_struct_ops_find_value() in the 6th\n   patch.\n\n - Merge the functions register_bpf_struct_ops_btf() and\n   register_bpf_struct_ops() into a single function and relocate it to\n   btf.c for better organization and clarity.\n\n - Undo the name modifications made to find_kernel_btf_id() and\n   find_ksym_btf_id() in the 8th patch.\n\nChanges from v4:\n\n - Fix the dependency between testing_helpers.o and\n   rcu_tasks_trace_gp.skel.h.\n\nChanges from v3:\n\n - Fix according to the feedback for v3.\n\n   - Change of the order of arguments to make btf as the first\n     argument.\n\n   - Use btf_try_get_module() instead of try_get_module() since the\n     module pointed by st_ops->owner can gone while some one is still\n     holding its btf.\n\n   - Move variables defined by BPF_STRUCT_OPS_COMMON_VALUE to struct\n     bpf_struct_ops_common_value to validation easier.\n\n   - Register the struct_ops type defined by bpf_testmod in its init\n     function.\n\n   - Rename field name to 'value_type_btf_obj_fd' to make it explicit.\n\n   - Fix leaking of btf objects on error.\n\n   - st_maps hold their modules to keep modules alive and prevent they\n     from unloading.\n\n   - bpf_map of libbpf keeps mod_btf_fd instead of a pointer to module_btf.\n\n   - Do call_rcu_tasks_trace() in kern_sync_rcu() to ensure the\n     bpf_testmod is unloaded properly. It uses rcu_tasks_trace_gp to\n     trigger call_rcu_tasks_trace() in the kernel.\n\n - Merge and reorder patches in a reasonable order.\n\nChanges from v2:\n\n - Remove struct_ops array, and add a per-btf (module) struct_ops_tab\n   to collect registered struct_ops types.\n\n - Validate value_type by checking member names and types.\n---\nv16: https://lore.kernel.org/all/20240118014930.1992551-1-thinker.li@gmail.com/\nv15: https://lore.kernel.org/all/20231220222654.1435895-1-thinker.li@gmail.com/\nv14: https://lore.kernel.org/all/20231217081132.1025020-1-thinker.li@gmail.com/\nv13: https://lore.kernel.org/all/20231209002709.535966-1-thinker.li@gmail.com/\nv12: https://lore.kernel.org/all/20231207013950.1689269-1-thinker.li@gmail.com/\nv11: https://lore.kernel.org/all/20231106201252.1568931-1-thinker.li@gmail.com/\nv10: https://lore.kernel.org/all/20231103232202.3664407-1-thinker.li@gmail.com/\nv9: https://lore.kernel.org/all/20231101204519.677870-1-thinker.li@gmail.com/\nv8: https://lore.kernel.org/all/20231030192810.382942-1-thinker.li@gmail.com/\nv7: https://lore.kernel.org/all/20231027211702.1374597-1-thinker.li@gmail.com/\nv6: https://lore.kernel.org/all/20231022050335.2579051-11-thinker.li@gmail.com/\nv5: https://lore.kernel.org/all/20231017162306.176586-1-thinker.li@gmail.com/\nv4: https://lore.kernel.org/all/20231013224304.187218-1-thinker.li@gmail.com/\nv3: https://lore.kernel.org/all/20230920155923.151136-1-thinker.li@gmail.com/\nv2: https://lore.kernel.org/all/20230913061449.1918219-1-thinker.li@gmail.com/\n====================\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_date": "2024-01-23 17:12:52 -0800",
      "commits": [
        {
          "hash": "3b1f89e747cd4b24244f2798a35d28815b744303",
          "subject": "bpf: refactory struct_ops type initialization to a function.",
          "message": "Move the majority of the code to bpf_struct_ops_init_one(), which can then\nbe utilized for the initialization of newly registered dynamically\nallocated struct_ops types in the following patches.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-2-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 16:37:43 -0800",
          "modified_files": [
            "include/linux/btf.h",
            "kernel/bpf/bpf_struct_ops.c",
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "95678395386d45fa0a075d2e7a6866326a469d76",
          "subject": "bpf: get type information with BTF_ID_LIST",
          "message": "Get ready to remove bpf_struct_ops_init() in the future. By using\nBTF_ID_LIST, it is possible to gather type information while building\ninstead of runtime.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-3-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 16:37:43 -0800",
          "modified_files": [
            "kernel/bpf/bpf_struct_ops.c"
          ]
        },
        {
          "hash": "4c5763ed996a61b51d721d0968d0df957826ea49",
          "subject": "bpf, net: introduce bpf_struct_ops_desc.",
          "message": "Move some of members of bpf_struct_ops to bpf_struct_ops_desc.  type_id is\nunavailabe in bpf_struct_ops anymore. Modules should get it from the btf\nreceived by kmod's init function.\n\nCc: netdev@vger.kernel.org\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-4-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 16:37:44 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/bpf_struct_ops.c",
            "kernel/bpf/verifier.c",
            "net/bpf/bpf_dummy_struct_ops.c",
            "net/ipv4/bpf_tcp_ca.c"
          ]
        },
        {
          "hash": "e61995111a76633376419d1bccede8696e94e6e5",
          "subject": "bpf: add struct_ops_tab to btf.",
          "message": "Maintain a registry of registered struct_ops types in the per-btf (module)\nstruct_ops_tab. This registry allows for easy lookup of struct_ops types\nthat are registered by a specific module.\n\nIt is a preparation work for supporting kernel module struct_ops in a\nlatter patch. Each struct_ops will be registered under its own kernel\nmodule btf and will be stored in the newly added btf->struct_ops_tab. The\nbpf verifier and bpf syscall (e.g. prog and map cmd) can find the\nstruct_ops and its btf type/size/id... information from\nbtf->struct_ops_tab.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-5-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 16:37:44 -0800",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "47f4f657acd5d04c78c5c5ac7022cba9ce3b4a7d",
          "subject": "bpf: make struct_ops_map support btfs other than btf_vmlinux.",
          "message": "Once new struct_ops can be registered from modules, btf_vmlinux is no\nlonger the only btf that struct_ops_map would face.  st_map should remember\nwhat btf it should use to get type information.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-6-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 16:37:44 -0800",
          "modified_files": [
            "kernel/bpf/bpf_struct_ops.c"
          ]
        },
        {
          "hash": "1338b93346587a2a6ac79bbcf55ef5b357745573",
          "subject": "bpf: pass btf object id in bpf_map_info.",
          "message": "Include btf object id (btf_obj_id) in bpf_map_info so that tools (ex:\nbpftools struct_ops dump) know the correct btf from the kernel to look up\ntype information of struct_ops types.\n\nSince struct_ops types can be defined and registered in a module. The\ntype information of a struct_ops type are defined in the btf of the\nmodule defining it.  The userspace tools need to know which btf is for\nthe module defining a struct_ops type.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-7-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 16:37:44 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/bpf_struct_ops.c",
            "kernel/bpf/syscall.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "689423db3bda2244c24db8a64de4cdb37be1de41",
          "subject": "bpf: lookup struct_ops types from a given module BTF.",
          "message": "This is a preparation for searching for struct_ops types from a specified\nmodule. BTF is always btf_vmlinux now. This patch passes a pointer of BTF\nto bpf_struct_ops_find_value() and bpf_struct_ops_find(). Once the new\nregistration API of struct_ops types is used, other BTFs besides\nbtf_vmlinux can also be passed to them.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-8-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 16:37:44 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/bpf_struct_ops.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "fcc2c1fb0651477c8ed78a3a293c175ccd70697a",
          "subject": "bpf: pass attached BTF to the bpf_struct_ops subsystem",
          "message": "Pass the fd of a btf from the userspace to the bpf() syscall, and then\nconvert the fd into a btf. The btf is generated from the module that\ndefines the target BPF struct_ops type.\n\nIn order to inform the kernel about the module that defines the target\nstruct_ops type, the userspace program needs to provide a btf fd for the\nrespective module's btf. This btf contains essential information on the\ntypes defined within the module, including the target struct_ops type.\n\nA btf fd must be provided to the kernel for struct_ops maps and for the bpf\nprograms attached to those maps.\n\nIn the case of the bpf programs, the attach_btf_obj_fd parameter is passed\nas part of the bpf_attr and is converted into a btf. This btf is then\nstored in the prog->aux->attach_btf field. Here, it just let the verifier\naccess attach_btf directly.\n\nIn the case of struct_ops maps, a btf fd is passed as value_type_btf_obj_fd\nof bpf_attr. The bpf_struct_ops_map_alloc() function converts the fd to a\nbtf and stores it as st_map->btf. A flag BPF_F_VTYPE_BTF_OBJ_FD is added\nfor map_flags to indicate that the value of value_type_btf_obj_fd is set.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-9-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 16:37:44 -0800",
          "modified_files": [
            "include/uapi/linux/bpf.h",
            "kernel/bpf/bpf_struct_ops.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "e3f87fdfed7b770dd7066b02262b12747881e76d",
          "subject": "bpf: hold module refcnt in bpf_struct_ops map creation and prog verification.",
          "message": "To ensure that a module remains accessible whenever a struct_ops object of\na struct_ops type provided by the module is still in use.\n\nstruct bpf_struct_ops_map doesn't hold a refcnt to btf anymore since a\nmodule will hold a refcnt to it's btf already. But, struct_ops programs are\ndifferent. They hold their associated btf, not the module since they need\nonly btf to assure their types (signatures).\n\nHowever, verifier holds the refcnt of the associated module of a struct_ops\ntype temporarily when verify a struct_ops prog. Verifier needs the help\nfrom the verifier operators (struct bpf_verifier_ops) provided by the owner\nmodule to verify data access of a prog, provide information, and generate\ncode.\n\nThis patch also add a count of links (links_cnt) to bpf_struct_ops_map. It\navoids bpf_struct_ops_map_put_progs() from accessing btf after calling\nmodule_put() in bpf_struct_ops_map_free().\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-10-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 16:37:44 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "kernel/bpf/bpf_struct_ops.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "612d087d4ba54cef47946e22e5dabad762dd7ed5",
          "subject": "bpf: validate value_type",
          "message": "A value_type should consist of three components: refcnt, state, and data.\nrefcnt and state has been move to struct bpf_struct_ops_common_value to\nmake it easier to check the value type.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-11-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 16:37:45 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/bpf_struct_ops.c"
          ]
        },
        {
          "hash": "f6be98d19985411ca1f3d53413d94d5b7f41c200",
          "subject": "bpf, net: switch to dynamic registration",
          "message": "Replace the static list of struct_ops types with per-btf struct_ops_tab to\nenable dynamic registration.\n\nBoth bpf_dummy_ops and bpf_tcp_ca now utilize the registration function\ninstead of being listed in bpf_struct_ops_types.h.\n\nCc: netdev@vger.kernel.org\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-12-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 17:12:46 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/btf.h",
            "kernel/bpf/bpf_struct_ops.c",
            "kernel/bpf/bpf_struct_ops_types.h",
            "kernel/bpf/btf.c",
            "net/bpf/bpf_dummy_struct_ops.c",
            "net/ipv4/bpf_tcp_ca.c"
          ]
        },
        {
          "hash": "9e926acda0c2e21bca431a1818665ddcd6939755",
          "subject": "libbpf: Find correct module BTFs for struct_ops maps and progs.",
          "message": "Locate the module BTFs for struct_ops maps and progs and pass them to the\nkernel. This ensures that the kernel correctly resolves type IDs from the\nappropriate module BTFs.\n\nFor the map of a struct_ops object, the FD of the module BTF is set to\nbpf_map to keep a reference to the module BTF. The FD is passed to the\nkernel as value_type_btf_obj_fd when the struct_ops object is loaded.\n\nFor a bpf_struct_ops prog, attach_btf_obj_fd of bpf_prog is the FD of a\nmodule BTF in the kernel.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240119225005.668602-13-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 17:12:52 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/bpf.h",
            "tools/lib/bpf/libbpf.c",
            "tools/lib/bpf/libbpf_probes.c"
          ]
        },
        {
          "hash": "7c81c2490c73e614c6d48e4f339f4f224140b565",
          "subject": "bpf: export btf_ctx_access to modules.",
          "message": "The module requires the use of btf_ctx_access() to invoke\nbpf_tracing_btf_ctx_access() from a module. This function is valuable for\nimplementing validation functions that ensure proper access to ctx.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-14-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 17:12:52 -0800",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "0253e0590e2dc46996534371d56b5297099aed4e",
          "subject": "selftests/bpf: test case for register_bpf_struct_ops().",
          "message": "Create a new struct_ops type called bpf_testmod_ops within the bpf_testmod\nmodule. When a struct_ops object is registered, the bpf_testmod module will\ninvoke test_2 from the module.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-15-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Kui-Feng Lee <thinker.li@gmail.com>",
          "date": "2024-01-23 17:12:52 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.h",
            "tools/testing/selftests/bpf/prog_tests/test_struct_ops_module.c",
            "tools/testing/selftests/bpf/progs/struct_ops_module.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "55c14321dbf06c9e32050e99b2555c2f8f6429da",
      "merge_subject": "Merge branch 'bpf-inline-bpf_kptr_xchg'",
      "merge_body": "Hou Tao says:\n\n====================\nThe motivation of inlining bpf_kptr_xchg() comes from the performance\nprofiling of bpf memory allocator benchmark [1]. The benchmark uses\nbpf_kptr_xchg() to stash the allocated objects and to pop the stashed\nobjects for free. After inling bpf_kptr_xchg(), the performance for\nobject free on 8-CPUs VM increases about 2%~10%. However the performance\ngain comes with costs: both the kasan and kcsan checks on the pointer\nwill be unavailable. Initially the inline is implemented in do_jit() for\nx86-64 directly, but I think it will more portable to implement the\ninline in verifier.\n\nPatch #1 supports inlining bpf_kptr_xchg() helper and enables it on\nx86-4. Patch #2 factors out a helper for newly-added test in patch #3.\nPatch #3 tests whether the inlining of bpf_kptr_xchg() is expected.\nPlease see individual patches for more details. And comments are always\nwelcome.\n\nChange Log:\nv3:\n  * rebased on bpf-next tree\n  * patch 1 & 2: Add Rvb-by and Ack-by tags from Eduard\n  * patch 3: use inline assembly and naked function instead of c code\n             (suggested by Eduard)\n\nv2: https://lore.kernel.org/bpf/20231223104042.1432300-1-houtao@huaweicloud.com/\n  * rebased on bpf-next tree\n  * drop patch #1 in v1 due to discussion in [2]\n  * patch #1: add the motivation in the commit message, merge patch #1\n              and #3 into the new patch in v2. (Daniel)\n  * patch #2/#3: newly-added patch to test the inlining of\n                 bpf_kptr_xchg() (Eduard)\n\nv1: https://lore.kernel.org/bpf/95b8c2cd-44d5-5fe1-60b5-7e8218779566@huaweicloud.com/\n\n[1]: https://lore.kernel.org/bpf/20231221141501.3588586-1-houtao@huaweicloud.com/\n[2]: https://lore.kernel.org/bpf/fd94efb9-4a56-c982-dc2e-c66be5202cb7@huaweicloud.com/\n====================\n\nLink: https://lore.kernel.org/r/20240105104819.3916743-1-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-01-23 14:40:21 -0800",
      "commits": [
        {
          "hash": "7c05e7f3e74e7e550534d524e04d7e6f78d6fa24",
          "subject": "bpf: Support inlining bpf_kptr_xchg() helper",
          "message": "The motivation of inlining bpf_kptr_xchg() comes from the performance\nprofiling of bpf memory allocator benchmark. The benchmark uses\nbpf_kptr_xchg() to stash the allocated objects and to pop the stashed\nobjects for free. After inling bpf_kptr_xchg(), the performance for\nobject free on 8-CPUs VM increases about 2%~10%. The inline also has\ndownside: both the kasan and kcsan checks on the pointer will be\nunavailable.\n\nbpf_kptr_xchg() can be inlined by converting the calling of\nbpf_kptr_xchg() into an atomic_xchg() instruction. But the conversion\ndepends on two conditions:\n1) JIT backend supports atomic_xchg() on pointer-sized word\n2) For the specific arch, the implementation of xchg is the same as\n   atomic_xchg() on pointer-sized words.\n\nIt seems most 64-bit JIT backends satisfies these two conditions. But\nas a precaution, defining a weak function bpf_jit_supports_ptr_xchg()\nto state whether such conversion is safe and only supporting inline for\n64-bit host.\n\nFor x86-64, it supports BPF_XCHG atomic operation and both xchg() and\natomic_xchg() use arch_xchg() to implement the exchange, so enabling the\ninline of bpf_kptr_xchg() on x86-64 first.\n\nReviewed-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20240105104819.3916743-2-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hou Tao <houtao1@huawei.com>",
          "date": "2024-01-23 14:40:21 -0800",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/filter.h",
            "kernel/bpf/core.c",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b4b7a4099b8ccea224577003fcf9d321bf0817b7",
          "subject": "selftests/bpf: Factor out get_xlated_program() helper",
          "message": "Both test_verifier and test_progs use get_xlated_program(), so moving\nthe helper into testing_helpers.h to reuse it.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20240105104819.3916743-3-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hou Tao <houtao1@huawei.com>",
          "date": "2024-01-23 14:40:21 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/ctx_rewrite.c",
            "tools/testing/selftests/bpf/test_verifier.c",
            "tools/testing/selftests/bpf/testing_helpers.c",
            "tools/testing/selftests/bpf/testing_helpers.h"
          ]
        },
        {
          "hash": "17bda53e43bc41d881ca6a02b3c6f5376c55b3d3",
          "subject": "selftests/bpf: Test the inlining of bpf_kptr_xchg()",
          "message": "The test uses bpf_prog_get_info_by_fd() to obtain the xlated\ninstructions of the program first. Since these instructions have\nalready been rewritten by the verifier, the tests then checks whether\nthe rewritten instructions are as expected. And to ensure LLVM generates\ncode exactly as expected, use inline assembly and a naked function.\n\nSuggested-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240105104819.3916743-4-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hou Tao <houtao1@huawei.com>",
          "date": "2024-01-23 14:40:21 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/kptr_xchg_inline.c",
            "tools/testing/selftests/bpf/progs/kptr_xchg_inline.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "5fe4ee6ae187523f710f1b93024437a073d88b17",
      "merge_subject": "Merge branch 'relax-tracing-prog-recursive-attach-rules'",
      "merge_body": "Dmitrii Dolgov says:\n\n====================\nRelax tracing prog recursive attach rules\n\nCurrently, it's not allowed to attach an fentry/fexit prog to another\nfentry/fexit. At the same time it's not uncommon to see a tracing\nprogram with lots of logic in use, and the attachment limitation\nprevents usage of fentry/fexit for performance analysis (e.g. with\n\"bpftool prog profile\" command) in this case. An example could be\nfalcosecurity libs project that uses tp_btf tracing programs for\noffloading certain part of logic into tail-called programs, but the\nuse-case is still generic enough -- a tracing program could be\ncomplicated and heavy enough to warrant its profiling, yet frustratingly\nit's not possible to do so use best tooling for that.\n\nFollowing the corresponding discussion [1], the reason for that is to\navoid tracing progs call cycles without introducing more complex\nsolutions. But currently it seems impossible to load and attach tracing\nprograms in a way that will form such a cycle. Replace \"no same type\"\nrequirement with verification that no more than one level of attachment\nnesting is allowed. In this way only one fentry/fexit program could be\nattached to another fentry/fexit to cover profiling use case, and still\nno cycle could be formed.\n\nThe series contains a test for recursive attachment, as well as a fix +\ntest for an issue in re-attachment branch of bpf_tracing_prog_attach.\nWhen preparing the test for the main change set, I've stumbled upon the\npossibility to construct a sequence of events when attach_btf would be\nNULL while computing a trampoline key. It doesn't look like this issue\nis triggered by the main change, because the reproduces doesn't actually\nneed to have an fentry attachment chain.\n\n[1]: https://lore.kernel.org/bpf/20191108064039.2041889-16-ast@kernel.org/\n====================\n\nLink: https://lore.kernel.org/r/20240103190559.14750-1-9erthalion6@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-01-04 20:40:54 -0800",
      "commits": [
        {
          "hash": "19bfcdf9498aa968ea293417fbbc39e523527ca8",
          "subject": "bpf: Relax tracing prog recursive attach rules",
          "message": "Currently, it's not allowed to attach an fentry/fexit prog to another\none fentry/fexit. At the same time it's not uncommon to see a tracing\nprogram with lots of logic in use, and the attachment limitation\nprevents usage of fentry/fexit for performance analysis (e.g. with\n\"bpftool prog profile\" command) in this case. An example could be\nfalcosecurity libs project that uses tp_btf tracing programs.\n\nFollowing the corresponding discussion [1], the reason for that is to\navoid tracing progs call cycles without introducing more complex\nsolutions. But currently it seems impossible to load and attach tracing\nprograms in a way that will form such a cycle. The limitation is coming\nfrom the fact that attach_prog_fd is specified at the prog load (thus\nmaking it impossible to attach to a program loaded after it in this\nway), as well as tracing progs not implementing link_detach.\n\nReplace \"no same type\" requirement with verification that no more than\none level of attachment nesting is allowed. In this way only one\nfentry/fexit program could be attached to another fentry/fexit to cover\nprofiling use case, and still no cycle could be formed. To implement,\nadd a new field into bpf_prog_aux to track nested attachment for tracing\nprograms.\n\n[1]: https://lore.kernel.org/bpf/20191108064039.2041889-16-ast@kernel.org/\n\nAcked-by: Jiri Olsa <olsajiri@gmail.com>\nAcked-by: Song Liu <song@kernel.org>\nSigned-off-by: Dmitrii Dolgov <9erthalion6@gmail.com>\nLink: https://lore.kernel.org/r/20240103190559.14750-2-9erthalion6@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dmitrii Dolgov <9erthalion6@gmail.com>",
          "date": "2024-01-04 20:31:34 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "5c5371e069e1ffc204dda8b20c609b170b823165",
          "subject": "selftests/bpf: Add test for recursive attachment of tracing progs",
          "message": "Verify the fact that only one fentry prog could be attached to another\nfentry, building up an attachment chain of limited size. Use existing\nbpf_testmod as a start of the chain.\n\nAcked-by: Jiri Olsa <olsajiri@gmail.com>\nAcked-by: Song Liu <song@kernel.org>\nSigned-off-by: Dmitrii Dolgov <9erthalion6@gmail.com>\nLink: https://lore.kernel.org/r/20240103190559.14750-3-9erthalion6@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dmitrii Dolgov <9erthalion6@gmail.com>",
          "date": "2024-01-04 20:40:14 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/recursive_attach.c",
            "tools/testing/selftests/bpf/progs/fentry_recursive.c",
            "tools/testing/selftests/bpf/progs/fentry_recursive_target.c"
          ]
        },
        {
          "hash": "715d82ba636cb3629a6e18a33bb9dbe53f9936ee",
          "subject": "bpf: Fix re-attachment branch in bpf_tracing_prog_attach",
          "message": "The following case can cause a crash due to missing attach_btf:\n\n1) load rawtp program\n2) load fentry program with rawtp as target_fd\n3) create tracing link for fentry program with target_fd = 0\n4) repeat 3\n\nIn the end we have:\n\n- prog->aux->dst_trampoline == NULL\n- tgt_prog == NULL (because we did not provide target_fd to link_create)\n- prog->aux->attach_btf == NULL (the program was loaded with attach_prog_fd=X)\n- the program was loaded for tgt_prog but we have no way to find out which one\n\n    BUG: kernel NULL pointer dereference, address: 0000000000000058\n    Call Trace:\n     <TASK>\n     ? __die+0x20/0x70\n     ? page_fault_oops+0x15b/0x430\n     ? fixup_exception+0x22/0x330\n     ? exc_page_fault+0x6f/0x170\n     ? asm_exc_page_fault+0x22/0x30\n     ? bpf_tracing_prog_attach+0x279/0x560\n     ? btf_obj_id+0x5/0x10\n     bpf_tracing_prog_attach+0x439/0x560\n     __sys_bpf+0x1cf4/0x2de0\n     __x64_sys_bpf+0x1c/0x30\n     do_syscall_64+0x41/0xf0\n     entry_SYSCALL_64_after_hwframe+0x6e/0x76\n\nReturn -EINVAL in this situation.\n\nFixes: f3a95075549e0 (\"bpf: Allow trampoline re-attach for tracing and lsm programs\")\nCc: stable@vger.kernel.org\nSigned-off-by: Jiri Olsa <olsajiri@gmail.com>\nAcked-by: Jiri Olsa <olsajiri@gmail.com>\nAcked-by: Song Liu <song@kernel.org>\nSigned-off-by: Dmitrii Dolgov <9erthalion6@gmail.com>\nLink: https://lore.kernel.org/r/20240103190559.14750-4-9erthalion6@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Jiri Olsa <olsajiri@gmail.com>",
          "date": "2024-01-04 20:40:19 -0800",
          "modified_files": [
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "e02feb3f1f47509ec1e07b604bfbeff8c3b4e639",
          "subject": "selftests/bpf: Test re-attachment fix for bpf_tracing_prog_attach",
          "message": "Add a test case to verify the fix for \"prog->aux->dst_trampoline and\ntgt_prog is NULL\" branch in bpf_tracing_prog_attach. The sequence of\nevents:\n\n1. load rawtp program\n2. load fentry program with rawtp as target_fd\n3. create tracing link for fentry program with target_fd = 0\n4. repeat 3\n\nAcked-by: Jiri Olsa <olsajiri@gmail.com>\nAcked-by: Song Liu <song@kernel.org>\nSigned-off-by: Dmitrii Dolgov <9erthalion6@gmail.com>\nLink: https://lore.kernel.org/r/20240103190559.14750-5-9erthalion6@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dmitrii Dolgov <9erthalion6@gmail.com>",
          "date": "2024-01-04 20:40:49 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/recursive_attach.c",
            "tools/testing/selftests/bpf/progs/fentry_recursive_target.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "f8506c5734902ebda5c7b4778859b46d0a2ae5f3",
      "merge_subject": "Merge branch 'bpf-reduce-memory-usage-for-bpf_global_percpu_ma'",
      "merge_body": "Yonghong Song says:\n\n====================\nbpf: Reduce memory usage for bpf_global_percpu_ma\n\nCurrently when a bpf program intends to allocate memory for percpu kptr,\nthe verifier will call bpf_mem_alloc_init() to prefill all supported\nunit sizes and this caused memory consumption very big for large number\nof cpus. For example, for 128-cpu system, the total memory consumption\nwith initial prefill is ~175MB. Things will become worse for systems\nwith even more cpus.\n\nPatch 1 avoids unnecessary extra percpu memory allocation.\nPatch 2 adds objcg to bpf_mem_alloc at init stage so objcg can be\nassociated with root cgroup and objcg can be passed to later\nbpf_mem_alloc_percpu_unit_init().\nPatch 3 addresses memory consumption issue by avoiding to prefill\nwith all unit sizes, i.e. only prefilling with user specified size.\nPatch 4 further reduces memory consumption by limiting the\nnumber of prefill entries for percpu memory allocation.\nPatch 5 has much smaller low/high watermarks for percpu allocation\nto reduce memory consumption.\nPatch 6 rejects percpu memory allocation with bpf_global_percpu_ma\nwhen allocation size is greater than 512 bytes.\nPatch 7 fixed test_bpf_ma test due to Patch 5.\nPatch 8 added one test to show the verification failure log message.\n\nChangelogs:\n  v5 -> v6:\n    . Change bpf_mem_alloc_percpu_init() to add objcg as one of parameters.\n      For bpf_global_percpu_ma, the objcg is NULL, corresponding root memcg.\n  v4 -> v5:\n    . Do not do bpf_global_percpu_ma initialization at init stage, instead\n      doing initialization when the verifier knows it is going to be used\n      by bpf prog.\n    . Using much smaller low/high watermarks for percpu allocation.\n  v3 -> v4:\n    . Add objcg to bpf_mem_alloc during init stage.\n    . Initialize objcg at init stage but use it in bpf_mem_alloc_percpu_unit_init().\n    . Remove check_obj_size() in bpf_mem_alloc_percpu_unit_init().\n  v2 -> v3:\n    . Clear the bpf_mem_cache if prefill fails.\n    . Change test_bpf_ma percpu allocation tests to use bucket_size\n      as allocation size instead of bucket_size - 8.\n    . Remove __GFP_ZERO flag from __alloc_percpu_gfp() call.\n  v1 -> v2:\n    . Avoid unnecessary extra percpu memory allocation.\n    . Add a separate function to do bpf_global_percpu_ma initialization\n    . promote.\n    . Promote function static 'sizes' array to file static.\n    . Add comments to explain to refill only one item for percpu alloc.\n====================\n\nLink: https://lore.kernel.org/r/20231222031729.1287957-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2024-01-03 21:08:27 -0800",
      "commits": [
        {
          "hash": "9beda16c257d55213f70adee2f16d7f13a8502e1",
          "subject": "bpf: Avoid unnecessary extra percpu memory allocation",
          "message": "Currently, for percpu memory allocation, say if the user\nrequests allocation size to be 32 bytes, the actually\ncalculated size will be 40 bytes and it further rounds\nto 64 bytes, and eventually 64 bytes are allocated,\nwasting 32-byte memory.\n\nChange bpf_mem_alloc() to calculate the cache index\nbased on the user-provided allocation size so unnecessary\nextra memory can be avoided.\n\nSuggested-by: Hou Tao <houtao1@huawei.com>\nAcked-by: Hou Tao <houtao1@huawei.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20231222031734.1288400-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-01-03 21:08:25 -0800",
          "modified_files": [
            "kernel/bpf/memalloc.c"
          ]
        },
        {
          "hash": "9fc8e802048ad150e8032c4f3dbf40112160cfe9",
          "subject": "bpf: Add objcg to bpf_mem_alloc",
          "message": "The objcg is a bpf_mem_alloc level property since all bpf_mem_cache's\nare with the same objcg. This patch made such a property explicit.\nThe next patch will use this property to save and restore objcg\nfor percpu unit allocator.\n\nAcked-by: Hou Tao <houtao1@huawei.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20231222031739.1288590-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-01-03 21:08:25 -0800",
          "modified_files": [
            "include/linux/bpf_mem_alloc.h",
            "kernel/bpf/memalloc.c"
          ]
        },
        {
          "hash": "c39aa3b289e9c10d0d246cd919b06809f13b72b8",
          "subject": "bpf: Allow per unit prefill for non-fix-size percpu memory allocator",
          "message": "Commit 41a5db8d8161 (\"Add support for non-fix-size percpu mem allocation\")\nadded support for non-fix-size percpu memory allocation.\nSuch allocation will allocate percpu memory for all buckets on all\ncpus and the memory consumption is in the order to quadratic.\nFor example, let us say, 4 cpus, unit size 16 bytes, so each\ncpu has 16 * 4 = 64 bytes, with 4 cpus, total will be 64 * 4 = 256 bytes.\nThen let us say, 8 cpus with the same unit size, each cpu\nhas 16 * 8 = 128 bytes, with 8 cpus, total will be 128 * 8 = 1024 bytes.\nSo if the number of cpus doubles, the number of memory consumption\nwill be 4 times. So for a system with large number of cpus, the\nmemory consumption goes up quickly with quadratic order.\nFor example, for 4KB percpu allocation, 128 cpus. The total memory\nconsumption will 4KB * 128 * 128 = 64MB. Things will become\nworse if the number of cpus is bigger (e.g., 512, 1024, etc.)\n\nIn Commit 41a5db8d8161, the non-fix-size percpu memory allocation is\ndone in boot time, so for system with large number of cpus, the initial\npercpu memory consumption is very visible. For example, for 128 cpu\nsystem, the total percpu memory allocation will be at least\n(16 + 32 + 64 + 96 + 128 + 196 + 256 + 512 + 1024 + 2048 + 4096)\n  * 128 * 128 = ~138MB.\nwhich is pretty big. It will be even bigger for larger number of cpus.\n\nNote that the current prefill also allocates 4 entries if the unit size\nis less than 256. So on top of 138MB memory consumption, this will\nadd more consumption with\n3 * (16 + 32 + 64 + 96 + 128 + 196 + 256) * 128 * 128 = ~38MB.\nNext patch will try to reduce this memory consumption.\n\nLater on, Commit 1fda5bb66ad8 (\"bpf: Do not allocate percpu memory\nat init stage\") moved the non-fix-size percpu memory allocation\nto bpf verificaiton stage. Once a particular bpf_percpu_obj_new()\nis called by bpf program, the memory allocator will try to fill in\nthe cache with all sizes, causing the same amount of percpu memory\nconsumption as in the boot stage.\n\nTo reduce the initial percpu memory consumption for non-fix-size\npercpu memory allocation, instead of filling the cache with all\nsupported allocation sizes, this patch intends to fill the cache\nonly for the requested size. As typically users will not use large\npercpu data structure, this can save memory significantly.\nFor example, the allocation size is 64 bytes with 128 cpus.\nThen total percpu memory amount will be 64 * 128 * 128 = 1MB,\nmuch less than previous 138MB.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231222031745.1289082-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-01-03 21:08:25 -0800",
          "modified_files": [
            "include/linux/bpf_mem_alloc.h",
            "kernel/bpf/memalloc.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "5b95e638f134e552b5ba2976326c02babe248615",
          "subject": "bpf: Refill only one percpu element in memalloc",
          "message": "Typically for percpu map element or data structure, once allocated,\nmost operations are lookup or in-place update. Deletion are really\nrare. Currently, for percpu data strcture, 4 elements will be\nrefilled if the size is <= 256. Let us just do with one element\nfor percpu data. For example, for size 256 and 128 cpus, the\npotential saving will be 3 * 256 * 128 * 128 = 12MB.\n\nAcked-by: Hou Tao <houtao1@huawei.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20231222031750.1289290-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-01-03 21:08:25 -0800",
          "modified_files": [
            "kernel/bpf/memalloc.c"
          ]
        },
        {
          "hash": "0e2ba9f96f9b82893ba19170ae48d46003f8ef44",
          "subject": "bpf: Use smaller low/high marks for percpu allocation",
          "message": "Currently, refill low/high marks are set with the assumption\nof normal non-percpu memory allocation. For example, for\nan allocation size 256, for non-percpu memory allocation,\nlow mark is 32 and high mark is 96, resulting in the\nbatch allocation of 48 elements and the allocated memory\nwill be 48 * 256 = 12KB for this particular cpu.\nAssuming an 128-cpu system, the total memory consumption\nacross all cpus will be 12K * 128 = 1.5MB memory.\n\nThis might be okay for non-percpu allocation, but may not be\ngood for percpu allocation, which will consume 1.5MB * 128 = 192MB\nmemory in the worst case if every cpu has a chance of memory\nallocation.\n\nIn practice, percpu allocation is very rare compared to\nnon-percpu allocation. So let us have smaller low/high marks\nwhich can avoid unnecessary memory consumption.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231222031755.1289671-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-01-03 21:08:25 -0800",
          "modified_files": [
            "kernel/bpf/memalloc.c"
          ]
        },
        {
          "hash": "5c1a37653260ed5d9c8b26fb7fe7b99629612982",
          "subject": "bpf: Limit up to 512 bytes for bpf_global_percpu_ma allocation",
          "message": "For percpu data structure allocation with bpf_global_percpu_ma,\nthe maximum data size is 4K. But for a system with large\nnumber of cpus, bigger data size (e.g., 2K, 4K) might consume\na lot of memory. For example, the percpu memory consumption\nwith unit size 2K and 1024 cpus will be 2K * 1K * 1k = 2GB\nmemory.\n\nWe should discourage such usage. Let us limit the maximum data\nsize to be 512 for bpf_global_percpu_ma allocation.\n\nAcked-by: Hou Tao <houtao1@huawei.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20231222031801.1290841-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-01-03 21:08:26 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "21f5a801c171dff4e728e38f62cf626c4197d07c",
          "subject": "selftests/bpf: Cope with 512 bytes limit with bpf_global_percpu_ma",
          "message": "In the previous patch, the maximum data size for bpf_global_percpu_ma\nis 512 bytes. This breaks selftest test_bpf_ma. The test is adjusted\nin two aspects:\n  - Since the maximum allowed data size for bpf_global_percpu_ma is\n    512, remove all tests beyond that, names sizes 1024, 2048 and 4096.\n  - Previously the percpu data size is bucket_size - 8 in order to\n    avoid percpu allocation into the next bucket. This patch removed\n    such data size adjustment thanks to Patch 1.\n\nAlso, a better way to generate BTF type is used than adding\na member to the value struct.\n\nAcked-by: Hou Tao <houtao1@huawei.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20231222031807.1292853-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-01-03 21:08:26 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/test_bpf_ma.c",
            "tools/testing/selftests/bpf/progs/test_bpf_ma.c"
          ]
        },
        {
          "hash": "adc8c4549d9e74d2359c217d2478b18ecdd15c91",
          "subject": "selftests/bpf: Add a selftest with > 512-byte percpu allocation size",
          "message": "Add a selftest to capture the verification failure when the allocation\nsize is greater than 512.\n\nAcked-by: Hou Tao <houtao1@huawei.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20231222031812.1293190-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2024-01-03 21:08:26 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/percpu_alloc_fail.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "a640de4cf9fec0caf43ccb7404ec9f0fde9a6a65",
      "merge_subject": "Merge branch 'bpf-simplify-checking-size-of-helper-accesses'",
      "merge_body": "Andrei Matei says:\n\n====================\nbpf: Simplify checking size of helper accesses\n\nv3->v4:\n- kept only the minimal change, undoing debatable changes (Andrii)\n- dropped the second patch from before, with changes to the error\n  message (Andrii)\n- extracted the new test into a separate patch (Andrii)\n- added Acked by Andrii\n\nv2->v3:\n- split the error-logging function to a separate patch (Andrii)\n- make the error buffers smaller (Andrii)\n- include size of memory region for PTR_TO_MEM (Andrii)\n- nits from Andrii and Eduard\n\nv1->v2:\n- make the error message include more info about the context of the\n  zero-sized access (Andrii)\n====================\n\nLink: https://lore.kernel.org/r/20231221232225.568730-1-andreimatei1@gmail.com\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2024-01-03 10:37:57 -0800",
      "commits": [
        {
          "hash": "8a021e7fa10576eeb3938328f39bbf98fe7d4715",
          "subject": "bpf: Simplify checking size of helper accesses",
          "message": "This patch simplifies the verification of size arguments associated to\npointer arguments to helpers and kfuncs. Many helpers take a pointer\nargument followed by the size of the memory access performed to be\nperformed through that pointer. Before this patch, the handling of the\nsize argument in check_mem_size_reg() was confusing and wasteful: if the\nsize register's lower bound was 0, then the verification was done twice:\nonce considering the size of the access to be the lower-bound of the\nrespective argument, and once considering the upper bound (even if the\ntwo are the same). The upper bound checking is a super-set of the\nlower-bound checking(*), except: the only point of the lower-bound check\nis to handle the case where zero-sized-accesses are explicitly not\nallowed and the lower-bound is zero. This static condition is now\nchecked explicitly, replacing a much more complex, expensive and\nconfusing verification call to check_helper_mem_access().\n\nError messages change in this patch. Before, messages about illegal\nzero-size accesses depended on the type of the pointer and on other\nconditions, and sometimes the message was plain wrong: in some tests\nthat changed you'll see that the old message was something like \"R1 min\nvalue is outside of the allowed memory range\", where R1 is the pointer\nregister; the error was wrongly claiming that the pointer was bad\ninstead of the size being bad. Other times the information that the size\ncame for a register with a possible range of values was wrong, and the\nerror presented the size as a fixed zero. Now the errors refer to the\nright register. However, the old error messages did contain useful\ninformation about the pointer register which is now lost; recovering\nthis information was deemed not important enough.\n\n(*) Besides standing to reason that the checks for a bigger size access\nare a super-set of the checks for a smaller size access, I have also\nmechanically verified this by reading the code for all types of\npointers. I could convince myself that it's true for all but\nPTR_TO_BTF_ID (check_ptr_to_btf_access). There, simply looking\nline-by-line does not immediately prove what we want. If anyone has any\nqualms, let me know.\n\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20231221232225.568730-2-andreimatei1@gmail.com",
          "author": "Andrei Matei <andreimatei1@gmail.com>",
          "date": "2024-01-03 10:37:56 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_helper_value_access.c",
            "tools/testing/selftests/bpf/progs/verifier_raw_stack.c"
          ]
        },
        {
          "hash": "72187506de4f19fcc8ae63a2b2f36d75e5259d9d",
          "subject": "bpf: Add a possibly-zero-sized read test",
          "message": "This patch adds a test for the condition that the previous patch mucked\nwith - illegal zero-sized helper memory access. As opposed to existing\ntests, this new one uses a size whose lower bound is zero, as opposed to\na known-zero one.\n\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20231221232225.568730-3-andreimatei1@gmail.com",
          "author": "Andrei Matei <andreimatei1@gmail.com>",
          "date": "2024-01-03 10:37:56 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_helper_value_access.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "85dd93ac6e00adf09fc27e4d2e7f5c9aaf275d38",
      "merge_subject": "Merge branch 'enhance-bpf-global-subprogs-with-argument-tags'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nEnhance BPF global subprogs with argument tags\n\nThis patch set adds verifier support for annotating user's global BPF subprog\narguments with few commonly requested annotations, to improve global subprog\nverification experience.\n\nThese tags are:\n  - ability to annotate a special PTR_TO_CTX argument;\n  - ability to annotate a generic PTR_TO_MEM as non-null.\n\nWe utilize btf_decl_tag attribute for this and provide two helper macros as\npart of bpf_helpers.h in libbpf (patch #8).\n\nBesides this we also add abilit to pass a pointer to dynptr into global\nsubprog. This is done based on type name match (struct bpf_dynptr *). This\nallows to pass dynptrs into global subprogs, for use cases that deal with\nvariable-sized generic memory pointers.\n\nBig chunk of the patch set (patches #1 through #5) are various refactorings to\nmake verifier internals around global subprog validation logic easier to\nextend and support long term, eliminating BTF parsing logic duplication,\nfactoring out argument expectation definitions from BTF parsing, etc.\n\nNew functionality is added in patch #6 (ctx and non-null) and patch #7\n(dynptr), extending global subprog checks with awareness for arg tags.\n\nPatch #9 adds simple tests validating each of the added tags and dynptr\nargument passing.\n\nPatch #10 adds a simple negative case for freplace programs to make sure that\ntarget BPF programs with \"unreliable\" BTF func proto cannot be freplaced.\n\nv2->v3:\n  - patch #10 improved by checking expected verifier error (Eduard);\nv1->v2:\n  - dropped packet args for now (Eduard);\n  - added back unreliable=true detection for entry BPF programs (Eduard);\n  - improved subprog arg validation (Eduard);\n  - switched dynptr arg from tag to just type name based check (Eduard).\n====================\n\nLink: https://lore.kernel.org/r/20231215011334.2307144-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-12-19 18:06:47 -0800",
      "commits": [
        {
          "hash": "4ba1d0f23414135e4f426dae4cb5cdc2ce246f89",
          "subject": "bpf: abstract away global subprog arg preparation logic from reg state setup",
          "message": "btf_prepare_func_args() is used to understand expectations and\nrestrictions on global subprog arguments. But current implementation is\nhard to extend, as it intermixes BTF-based func prototype parsing and\ninterpretation logic with setting up register state at subprog entry.\n\nWorse still, those registers are not completely set up inside\nbtf_prepare_func_args(), requiring some more logic later in\ndo_check_common(). Like calling mark_reg_unknown() and similar\ninitialization operations.\n\nThis intermixing of BTF interpretation and register state setup is\nproblematic. First, it causes duplication of BTF parsing logic for global\nsubprog verification (to set up initial state of global subprog) and\nglobal subprog call sites analysis (when we need to check that whatever\nis being passed into global subprog matches expectations), performed in\nbtf_check_subprog_call().\n\nGiven we want to extend global func argument with tags later, this\nduplication is problematic. So refactor btf_prepare_func_args() to do\nonly BTF-based func proto and args parsing, returning high-level\nargument \"expectations\" only, with no regard to specifics of register\nstate. I.e., if it's a context argument, instead of setting register\nstate to PTR_TO_CTX, we return ARG_PTR_TO_CTX enum for that argument as\n\"an argument specification\" for further processing inside\ndo_check_common(). Similarly for SCALAR arguments, PTR_TO_MEM, etc.\n\nThis allows to reuse btf_prepare_func_args() in following patches at\nglobal subprog call site analysis time. It also keeps register setup\ncode consistently in one place, do_check_common().\n\nBesides all this, we cache this argument specs information inside\nenv->subprog_info, eliminating the need to redo these potentially\nexpensive BTF traversals, especially if BPF program's BTF is big and/or\nthere are lots of global subprog calls.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-19 18:06:46 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "5eccd2db42d77e3570619c32d39e39bf486607cf",
          "subject": "bpf: reuse btf_prepare_func_args() check for main program BTF validation",
          "message": "Instead of btf_check_subprog_arg_match(), use btf_prepare_func_args()\nlogic to validate \"trustworthiness\" of main BPF program's BTF information,\nif it is present.\n\nWe ignored results of original BTF check anyway, often times producing\nconfusing and ominously-sounding \"reg type unsupported for arg#0\nfunction\" message, which has no apparent effect on program correctness\nand verification process.\n\nAll the -EFAULT returning sanity checks are already performed in\ncheck_btf_info_early(), so there is zero reason to have this duplication\nof logic between btf_check_subprog_call() and btf_check_subprog_arg_match().\nDropping btf_check_subprog_arg_match() simplifies\nbtf_check_func_arg_match() further removing `bool processing_call` flag.\n\nOne subtle bit that was done by btf_check_subprog_arg_match() was\npotentially marking main program's BTF as unreliable. We do this\nexplicitly now with a dedicated simple check, preserving the original\nbehavior, but now based on well factored btf_prepare_func_args() logic.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-19 18:06:46 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/prog_tests/log_fixup.c",
            "tools/testing/selftests/bpf/progs/cgrp_kfunc_failure.c",
            "tools/testing/selftests/bpf/progs/task_kfunc_failure.c"
          ]
        },
        {
          "hash": "e26080d0da87f20222ca6712b65f95a856fadee0",
          "subject": "bpf: prepare btf_prepare_func_args() for handling static subprogs",
          "message": "Generalize btf_prepare_func_args() to support both global and static\nsubprogs. We are going to utilize this property in the next patch,\nreusing btf_prepare_func_args() for subprog call logic instead of\nreparsing BTF information in a completely separate implementation.\n\nbtf_prepare_func_args() now detects whether subprog is global or static\nmakes slight logic adjustments for static func cases, like not failing\nfatally (-EFAULT) for conditions that are allowable for static subprogs.\n\nSomewhat subtle (but major!) difference is the handling of pointer arguments.\nBoth global and static functions need to handle special context\narguments (which are pointers to predefined type names), but static\nsubprogs give up on any other pointers, falling back to marking subprog\nas \"unreliable\", disabling the use of BTF type information altogether.\n\nFor global functions, though, we are assuming that such pointers to\nunrecognized types are just pointers to fixed-sized memory region (or\nerror out if size cannot be established, like for `void *` pointers).\n\nThis patch accommodates these small differences and sets up a stage for\nrefactoring in the next patch, eliminating a separate BTF-based parsing\nlogic in btf_check_func_arg_match().\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-19 18:06:46 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "c5a7244759b1eeacc59d0426fb73859afa942d0d",
          "subject": "bpf: move subprog call logic back to verifier.c",
          "message": "Subprog call logic in btf_check_subprog_call() currently has both a lot\nof BTF parsing logic (which is, presumably, what justified putting it\ninto btf.c), but also a bunch of register state checks, some of each\nutilize deep verifier logic helpers, necessarily exported from\nverifier.c: check_ptr_off_reg(), check_func_arg_reg_off(),\nand check_mem_reg().\n\nGoing forward, btf_check_subprog_call() will have a minimum of\nBTF-related logic, but will get more internal verifier logic related to\nregister state manipulation. So move it into verifier.c to minimize\namount of verifier-specific logic exposed to btf.c.\n\nWe do this move before refactoring btf_check_func_arg_match() to\npreserve as much history post-refactoring as possible.\n\nNo functional changes.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-19 18:06:46 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "f18c3d88deedf0defc3e4800341cc7bcaaabcdf9",
          "subject": "bpf: reuse subprog argument parsing logic for subprog call checks",
          "message": "Remove duplicated BTF parsing logic when it comes to subprog call check.\nInstead, use (potentially cached) results of btf_prepare_func_args() to\nabstract away expectations of each subprog argument in generic terms\n(e.g., \"this is pointer to context\", or \"this is a pointer to memory of\nsize X\"), and then use those simple high-level argument type\nexpectations to validate actual register states to check if they match\nexpectations.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-19 18:06:46 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/test_global_func5.c"
          ]
        },
        {
          "hash": "94e1c70a34523b5e1529e4ec508316acc6a26a2b",
          "subject": "bpf: support 'arg:xxx' btf_decl_tag-based hints for global subprog args",
          "message": "Add support for annotating global BPF subprog arguments to provide more\ninformation about expected semantics of the argument. Currently,\nverifier relies purely on argument's BTF type information, and supports\nthree general use cases: scalar, pointer-to-context, and\npointer-to-fixed-size-memory.\n\nScalar and pointer-to-fixed-mem work well in practice and are quite\nnatural to use. But pointer-to-context is a bit problematic, as typical\nBPF users don't realize that they need to use a special type name to\nsignal to verifier that argument is not just some pointer, but actually\na PTR_TO_CTX. Further, even if users do know which type to use, it is\nlimiting in situations where the same BPF program logic is used across\nfew different program types. Common case is kprobes, tracepoints, and\nperf_event programs having a helper to send some data over BPF perf\nbuffer. bpf_perf_event_output() requires `ctx` argument, and so it's\nquite cumbersome to share such global subprog across few BPF programs of\ndifferent types, necessitating extra static subprog that is context\ntype-agnostic.\n\nLong story short, there is a need to go beyond types and allow users to\nadd hints to global subprog arguments to define expectations.\n\nThis patch adds such support for two initial special tags:\n  - pointer to context;\n  - non-null qualifier for generic pointer arguments.\n\nAll of the above came up in practice already and seem generally useful\nadditions. Non-null qualifier is an often requested feature, which\ncurrently has to be worked around by having unnecessary NULL checks\ninside subprogs even if we know that arguments are never NULL. Pointer\nto context was discussed earlier.\n\nAs for implementation, we utilize btf_decl_tag attribute and set up an\n\"arg:xxx\" convention to specify argument hint. As such:\n  - btf_decl_tag(\"arg:ctx\") is a PTR_TO_CTX hint;\n  - btf_decl_tag(\"arg:nonnull\") marks pointer argument as not allowed to\n    be NULL, making NULL check inside global subprog unnecessary.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-19 18:06:46 -0800",
          "modified_files": [
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "a64bfe618665ea9c722f922cba8c6e3234eac5ac",
          "subject": "bpf: add support for passing dynptr pointer to global subprog",
          "message": "Add ability to pass a pointer to dynptr into global functions.\nThis allows to have global subprogs that accept and work with generic\ndynptrs that are created by caller. Dynptr argument is detected based on\nthe name of a struct type, if it's \"bpf_dynptr\", it's assumed to be\na proper dynptr pointer. Both actual struct and forward struct\ndeclaration types are supported.\n\nThis is conceptually exactly the same semantics as\nbpf_user_ringbuf_drain()'s use of dynptr to pass a variable-sized\npointer to ringbuf record. So we heavily rely on CONST_PTR_TO_DYNPTR\nbits of already existing logic in the verifier.\n\nDuring global subprog validation, we mark such CONST_PTR_TO_DYNPTR as\nhaving LOCAL type, as that's the most unassuming type of dynptr and it\ndoesn't have any special helpers that can try to free or acquire extra\nreferences (unlike skb, xdp, or ringbuf dynptr). So that seems like a safe\n\"choice\" to make from correctness standpoint. It's still possible to\npass any type of dynptr to such subprog, though, because generic dynptr\nhelpers, like getting data/slice pointers, read/write memory copying\nroutines, dynptr adjustment and getter routines all work correctly with\nany type of dynptr.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-19 18:06:46 -0800",
          "modified_files": [
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "aae9c25dda159045b223ecb471cd0729ccec8285",
          "subject": "libbpf: add __arg_xxx macros for annotating global func args",
          "message": "Add a set of __arg_xxx macros which can be used to augment BPF global\nsubprogs/functions with extra information for use by BPF verifier.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-19 18:06:47 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf_helpers.h"
          ]
        },
        {
          "hash": "0a0ffcac92d5b41133c97d260ad1f320572783a5",
          "subject": "selftests/bpf: add global subprog annotation tests",
          "message": "Add test cases to validate semantics of global subprog argument\nannotations:\n  - non-null pointers;\n  - context argument;\n  - const dynptr passing;\n  - packet pointers (data, metadata, end).\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-10-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-19 18:06:47 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_global_subprogs.c"
          ]
        },
        {
          "hash": "f0a5056222f2cfa6d40b4c888cb6b01e8569e282",
          "subject": "selftests/bpf: add freplace of BTF-unreliable main prog test",
          "message": "Add a test validating that freplace'ing another main (entry) BPF program\nfails if the target BPF program doesn't have valid/expected func proto BTF.\n\nWe extend fexit_bpf2bpf test to allow to specify expected log message\nfor negative test cases (where freplace program is expected to fail to\nload).\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-11-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-19 18:06:47 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/fexit_bpf2bpf.c",
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/freplace_unreliable_prog.c",
            "tools/testing/selftests/bpf/progs/verifier_btf_unreliable_prog.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "c337f237291b41b308c80124236876cf66c77906",
      "merge_subject": "Merge branch 'bpf-support-to-track-bpf_jne'",
      "merge_body": "Menglong Dong says:\n\n====================\nbpf: support to track BPF_JNE\n\nFor now, the reg bounds is not handled for BPF_JNE case, which can cause\nthe failure of following case:\n\n  /* The type of \"a\" is u32 */\n  if (a > 0 && a < 100) {\n    /* the range of the register for a is [0, 99], not [1, 99],\n     * and will cause the following error:\n     *\n     *   invalid zero-sized read\n     *\n     * as a can be 0.\n     */\n    bpf_skb_store_bytes(skb, xx, xx, a, 0);\n  }\n\nIn the code above, \"a > 0\" will be compiled to \"if a == 0 goto xxx\". In\nthe TRUE branch, the dst_reg will be marked as known to 0. However, in the\nfallthrough(FALSE) branch, the dst_reg will not be handled, which makes\nthe [min, max] for a is [0, 99], not [1, 99].\n\nIn the 1st patch, we reduce the range of the dst reg if the src reg is a\nconst and is exactly the edge of the dst reg For BPF_JNE.\n\nIn the 2nd patch, we remove reduplicated s32 casting in \"crafted_cases\".\n\nIn the 3rd patch, we just activate the test case for this logic in\nrange_cond(), which is committed by Andrii in the\ncommit 8863238993e2 (\"selftests/bpf: BPF register range bounds tester\").\n\nIn the 4th patch, we convert the case above to a testcase and add it to\nverifier_bounds.c.\n\nChanges since v4:\n- add the 2nd patch\n- add \"{U32, U32, {0, U32_MAX}, {U32_MAX, U32_MAX}}\" that we missed in the\n  3rd patch\n- add some comments to the function that we add in the 4th patch\n- add reg_not_equal_const() in the 4th patch\n\nChanges since v3:\n- do some adjustment to the crafted cases that we added in the 2nd patch\n- add the 3rd patch\n\nChanges since v2:\n- fix a typo in the subject of the 1st patch\n- add some comments to the 1st patch, as Eduard advised\n- add some cases to the \"crafted_cases\"\n\nChanges since v1:\n- simplify the code in the 1st patch\n- introduce the 2nd patch for the testing\n====================\n\nLink: https://lore.kernel.org/r/20231219134800.1550388-1-menglong8.dong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-12-19 17:18:56 -0800",
      "commits": [
        {
          "hash": "d028f87517d6775dccff4ddbca2740826f9e53f1",
          "subject": "bpf: make the verifier tracks the \"not equal\" for regs",
          "message": "We can derive some new information for BPF_JNE in regs_refine_cond_op().\nTake following code for example:\n\n  /* The type of \"a\" is u32 */\n  if (a > 0 && a < 100) {\n    /* the range of the register for a is [0, 99], not [1, 99],\n     * and will cause the following error:\n     *\n     *   invalid zero-sized read\n     *\n     * as a can be 0.\n     */\n    bpf_skb_store_bytes(skb, xx, xx, a, 0);\n  }\n\nIn the code above, \"a > 0\" will be compiled to \"jmp xxx if a == 0\". In the\nTRUE branch, the dst_reg will be marked as known to 0. However, in the\nfallthrough(FALSE) branch, the dst_reg will not be handled, which makes\nthe [min, max] for a is [0, 99], not [1, 99].\n\nFor BPF_JNE, we can reduce the range of the dst reg if the src reg is a\nconst and is exactly the edge of the dst reg.\n\nSigned-off-by: Menglong Dong <menglong8.dong@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231219134800.1550388-2-menglong8.dong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Menglong Dong <menglong8.dong@gmail.com>",
          "date": "2023-12-19 17:18:55 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "1de584832375d0dc4234ee406185384a58fb96ac",
          "subject": "selftests/bpf: remove reduplicated s32 casting in \"crafted_cases\"",
          "message": "The \"S32_MIN\" is already defined with s32 casting, so there is no need\nto do it again.\n\nSigned-off-by: Menglong Dong <menglong8.dong@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231219134800.1550388-3-menglong8.dong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Menglong Dong <menglong8.dong@gmail.com>",
          "date": "2023-12-19 17:18:55 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/reg_bounds.c"
          ]
        },
        {
          "hash": "31d9cc96b1e3b28daf74938cb1233231474bbcf6",
          "subject": "selftests/bpf: activate the OP_NE logic in range_cond()",
          "message": "The edge range checking for the registers is supported by the verifier\nnow, so we can activate the extended logic in\ntools/testing/selftests/bpf/prog_tests/reg_bounds.c/range_cond() to test\nsuch logic.\n\nBesides, I added some cases to the \"crafted_cases\" array for this logic.\nThese cases are mainly used to test the edge of the src reg and dst reg.\n\nAll reg bounds testings has passed in the SLOW_TESTS mode:\n\n$ export SLOW_TESTS=1 && ./test_progs -t reg_bounds -j\nSummary: 65/18959832 PASSED, 0 SKIPPED, 0 FAILED\n\nSigned-off-by: Menglong Dong <menglong8.dong@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231219134800.1550388-4-menglong8.dong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Menglong Dong <menglong8.dong@gmail.com>",
          "date": "2023-12-19 17:18:55 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/reg_bounds.c"
          ]
        },
        {
          "hash": "463ea64eb008b7abb63245ed69446b404bf042b1",
          "subject": "selftests/bpf: add testcase to verifier_bounds.c for BPF_JNE",
          "message": "Add testcase for the logic that the verifier tracks the BPF_JNE for regs.\nThe assembly function \"reg_not_equal_const()\" and \"reg_equal_const\" that\nwe add is exactly converted from the following case:\n\n  u32 a = bpf_get_prandom_u32();\n  u64 b = 0;\n\n  a %= 8;\n  /* the \"a > 0\" here will be optimized to \"a != 0\" */\n  if (a > 0) {\n    /* now the range of a should be [1, 7] */\n    bpf_skb_store_bytes(skb, 0, &b, a, 0);\n  }\n\nSigned-off-by: Menglong Dong <menglong8.dong@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231219134800.1550388-5-menglong8.dong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Menglong Dong <menglong8.dong@gmail.com>",
          "date": "2023-12-19 17:18:56 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_bounds.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "4af20ab9edee62aa2bb5b6f31b7f029de14e0756",
      "merge_subject": "Merge branch 'bpf-fix-accesses-to-uninit-stack-slots'",
      "merge_body": "Andrei Matei says:\n\n====================\nbpf: fix accesses to uninit stack slots\n\nFix two related issues issues around verifying stack accesses:\n1. accesses to uninitialized stack memory was allowed inconsistently\n2. the maximum stack depth needed for a program was not always\nmaintained correctly\n\nThe two issues are fixed together in one commit because the code for one\naffects the other.\n\nV4 to V5:\n- target bpf-next (Alexei)\n\nV3 to V4:\n- minor fixup to comment in patch 1 (Eduard)\n- C89-style in patch 3 (Andrii)\n\nV2 to V3:\n- address review comments from Andrii and Eduard\n- drop new verifier tests in favor of editing existing tests to check\n  for stack depth\n- append a patch with a bit of cleanup coming out of the previous review\n====================\n\nLink: https://lore.kernel.org/r/20231208032519.260451-1-andreimatei1@gmail.com\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2023-12-08 14:19:01 -0800",
      "commits": [
        {
          "hash": "92e1567ee3e3f6f160e320890ac77eec50bf8e7d",
          "subject": "bpf: Add some comments to stack representation",
          "message": "Add comments to the datastructure tracking the stack state, as the\nmapping between each stack slot and where its state is stored is not\nentirely obvious.\n\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231208032519.260451-2-andreimatei1@gmail.com",
          "author": "Andrei Matei <andreimatei1@gmail.com>",
          "date": "2023-12-08 14:19:00 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h"
          ]
        },
        {
          "hash": "6b4a64bafd107e521c01eec3453ce94a3fb38529",
          "subject": "bpf: Fix accesses to uninit stack slots",
          "message": "Privileged programs are supposed to be able to read uninitialized stack\nmemory (ever since 6715df8d5) but, before this patch, these accesses\nwere permitted inconsistently. In particular, accesses were permitted\nabove state->allocated_stack, but not below it. In other words, if the\nstack was already \"large enough\", the access was permitted, but\notherwise the access was rejected instead of being allowed to \"grow the\nstack\". This undesired rejection was happening in two places:\n- in check_stack_slot_within_bounds()\n- in check_stack_range_initialized()\nThis patch arranges for these accesses to be permitted. A bunch of tests\nthat were relying on the old rejection had to change; all of them were\nchanged to add also run unprivileged, in which case the old behavior\npersists. One tests couldn't be updated - global_func16 - because it\ncan't run unprivileged for other reasons.\n\nThis patch also fixes the tracking of the stack size for variable-offset\nreads. This second fix is bundled in the same commit as the first one\nbecause they're inter-related. Before this patch, writes to the stack\nusing registers containing a variable offset (as opposed to registers\nwith fixed, known values) were not properly contributing to the\nfunction's needed stack size. As a result, it was possible for a program\nto verify, but then to attempt to read out-of-bounds data at runtime\nbecause a too small stack had been allocated for it.\n\nEach function tracks the size of the stack it needs in\nbpf_subprog_info.stack_depth, which is maintained by\nupdate_stack_depth(). For regular memory accesses, check_mem_access()\nwas calling update_state_depth() but it was passing in only the fixed\npart of the offset register, ignoring the variable offset. This was\nincorrect; the minimum possible value of that register should be used\ninstead.\n\nThis tracking is now fixed by centralizing the tracking of stack size in\ngrow_stack_state(), and by lifting the calls to grow_stack_state() to\ncheck_stack_access_within_bounds() as suggested by Andrii. The code is\nnow simpler and more convincingly tracks the correct maximum stack size.\ncheck_stack_range_initialized() can now rely on enough stack having been\nallocated for the access; this helps with the fix for the first issue.\n\nA few tests were changed to also check the stack depth computation. The\none that fails without this patch is verifier_var_off:stack_write_priv_vs_unpriv.\n\nFixes: 01f810ace9ed3 (\"bpf: Allow variable-offset stack access\")\nReported-by: Hao Sun <sunhao.th@gmail.com>\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20231208032519.260451-3-andreimatei1@gmail.com\n\nCloses: https://lore.kernel.org/bpf/CABWLsev9g8UP_c3a=1qbuZUi20tGoUXoU07FPf-5FLvhOKOY+Q@mail.gmail.com/",
          "author": "Andrei Matei <andreimatei1@gmail.com>",
          "date": "2023-12-08 14:19:00 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/iters.c",
            "tools/testing/selftests/bpf/progs/test_global_func16.c",
            "tools/testing/selftests/bpf/progs/verifier_basic_stack.c",
            "tools/testing/selftests/bpf/progs/verifier_int_ptr.c",
            "tools/testing/selftests/bpf/progs/verifier_raw_stack.c",
            "tools/testing/selftests/bpf/progs/verifier_var_off.c",
            "tools/testing/selftests/bpf/verifier/atomic_cmpxchg.c",
            "tools/testing/selftests/bpf/verifier/calls.c"
          ]
        },
        {
          "hash": "2929bfac006d8f8e22b307d04e0d71bcb84db698",
          "subject": "bpf: Minor cleanup around stack bounds",
          "message": "Push the rounding up of stack offsets into the function responsible for\ngrowing the stack, rather than relying on all the callers to do it.\nUncertainty about whether the callers did it or not tripped up people in\na previous review.\n\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231208032519.260451-4-andreimatei1@gmail.com",
          "author": "Andrei Matei <andreimatei1@gmail.com>",
          "date": "2023-12-08 14:19:00 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "483af466e4ee3326d150877ea0626e95c67a395e",
      "merge_subject": "Merge branch 'bpf-fix-verification-of-indirect-var-off-stack-access'",
      "merge_body": "Andrei Matei says:\n\n====================\nbpf: fix verification of indirect var-off stack access\n\nV4 to V5:\n  - split the test into a separate patch\n\nV3 to V4:\n  - include a test per Eduard's request\n  - target bpf-next per Alexei's request (patches didn't change)\n\nV2 to V3:\n  - simplify checks for max_off (don't call\n    check_stack_slot_within_bounds for it)\n  - append a commit to protect against overflow in the addition of the\n    register and the offset\n\nV1 to V2:\n  - fix max_off calculation for access size = 0\n====================\n\nLink: https://lore.kernel.org/r/20231207041150.229139-1-andreimatei1@gmail.com\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2023-12-07 13:58:14 -0800",
      "commits": [
        {
          "hash": "a833a17aeac73b33f79433d7cee68d5cafd71e4f",
          "subject": "bpf: Fix verification of indirect var-off stack access",
          "message": "This patch fixes a bug around the verification of possibly-zero-sized\nstack accesses. When the access was done through a var-offset stack\npointer, check_stack_access_within_bounds was incorrectly computing the\nmaximum-offset of a zero-sized read to be the same as the register's min\noffset. Instead, we have to take in account the register's maximum\npossible value. The patch also simplifies how the max offset is checked;\nthe check is now simpler than for min offset.\n\nThe bug was allowing accesses to erroneously pass the\ncheck_stack_access_within_bounds() checks, only to later crash in\ncheck_stack_range_initialized() when all the possibly-affected stack\nslots are iterated (this time with a correct max offset).\ncheck_stack_range_initialized() is relying on\ncheck_stack_access_within_bounds() for its accesses to the\nstack-tracking vector to be within bounds; in the case of zero-sized\naccesses, we were essentially only verifying that the lowest possible\nslot was within bounds. We would crash when the max-offset of the stack\npointer was >= 0 (which shouldn't pass verification, and hopefully is\nnot something anyone's code attempts to do in practice).\n\nThanks Hao for reporting!\n\nFixes: 01f810ace9ed3 (\"bpf: Allow variable-offset stack access\")\nReported-by: Hao Sun <sunhao.th@gmail.com>\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20231207041150.229139-2-andreimatei1@gmail.com\n\nCloses: https://lore.kernel.org/bpf/CACkBjsZGEUaRCHsmaX=h-efVogsRfK1FPxmkgb0Os_frnHiNdw@mail.gmail.com/",
          "author": "Andrei Matei <andreimatei1@gmail.com>",
          "date": "2023-12-07 13:57:53 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "e28bd359bcc8eb849aaa475f3c3f9705fba26d6e",
          "subject": "bpf: Add verifier regression test for previous patch",
          "message": "Add a regression test for var-off zero-sized reads.\n\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231207041150.229139-3-andreimatei1@gmail.com",
          "author": "Andrei Matei <andreimatei1@gmail.com>",
          "date": "2023-12-07 13:58:02 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_var_off.c"
          ]
        },
        {
          "hash": "1d38a9ee81570c4bd61f557832dead4d6f816760",
          "subject": "bpf: Guard stack limits against 32bit overflow",
          "message": "This patch promotes the arithmetic around checking stack bounds to be\ndone in the 64-bit domain, instead of the current 32bit. The arithmetic\nimplies adding together a 64-bit register with a int offset. The\nregister was checked to be below 1<<29 when it was variable, but not\nwhen it was fixed. The offset either comes from an instruction (in which\ncase it is 16 bit), from another register (in which case the caller\nchecked it to be below 1<<29 [1]), or from the size of an argument to a\nkfunc (in which case it can be a u32 [2]). Between the register being\ninconsistently checked to be below 1<<29, and the offset being up to an\nu32, it appears that we were open to overflowing the `int`s which were\ncurrently used for arithmetic.\n\n[1] https://github.com/torvalds/linux/blob/815fb87b753055df2d9e50f6cd80eb10235fe3e9/kernel/bpf/verifier.c#L7494-L7498\n[2] https://github.com/torvalds/linux/blob/815fb87b753055df2d9e50f6cd80eb10235fe3e9/kernel/bpf/verifier.c#L11904\n\nReported-by: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20231207041150.229139-4-andreimatei1@gmail.com",
          "author": "Andrei Matei <andreimatei1@gmail.com>",
          "date": "2023-12-07 13:58:10 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "c35919dcce2855d68cf45ffa427b8ea78e4f7c68",
      "merge_subject": "Merge branch 'bpf-token-and-bpf-fs-based-delegation'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nBPF token and BPF FS-based delegation\n\nThis patch set introduces an ability to delegate a subset of BPF subsystem\nfunctionality from privileged system-wide daemon (e.g., systemd or any other\ncontainer manager) through special mount options for userns-bound BPF FS to\na *trusted* unprivileged application. Trust is the key here. This\nfunctionality is not about allowing unconditional unprivileged BPF usage.\nEstablishing trust, though, is completely up to the discretion of respective\nprivileged application that would create and mount a BPF FS instance with\ndelegation enabled, as different production setups can and do achieve it\nthrough a combination of different means (signing, LSM, code reviews, etc),\nand it's undesirable and infeasible for kernel to enforce any particular way\nof validating trustworthiness of particular process.\n\nThe main motivation for this work is a desire to enable containerized BPF\napplications to be used together with user namespaces. This is currently\nimpossible, as CAP_BPF, required for BPF subsystem usage, cannot be namespaced\nor sandboxed, as a general rule. E.g., tracing BPF programs, thanks to BPF\nhelpers like bpf_probe_read_kernel() and bpf_probe_read_user() can safely read\narbitrary memory, and it's impossible to ensure that they only read memory of\nprocesses belonging to any given namespace. This means that it's impossible to\nhave a mechanically verifiable namespace-aware CAP_BPF capability, and as such\nanother mechanism to allow safe usage of BPF functionality is necessary.BPF FS\ndelegation mount options and BPF token derived from such BPF FS instance is\nsuch a mechanism. Kernel makes no assumption about what \"trusted\" constitutes\nin any particular case, and it's up to specific privileged applications and\ntheir surrounding infrastructure to decide that. What kernel provides is a set\nof APIs to setup and mount special BPF FS instanecs and derive BPF tokens from\nit. BPF FS and BPF token are both bound to its owning userns and in such a way\nare constrained inside intended container. Users can then pass BPF token FD to\nprivileged bpf() syscall commands, like BPF map creation and BPF program\nloading, to perform such operations without having init userns privileged.\n\nThis version incorporates feedback and suggestions ([3]) received on v3 of\nthis patch set, and instead of allowing to create BPF tokens directly assuming\ncapable(CAP_SYS_ADMIN), we instead enhance BPF FS to accept a few new\ndelegation mount options. If these options are used and BPF FS itself is\nproperly created, set up, and mounted inside the user namespaced container,\nuser application is able to derive a BPF token object from BPF FS instance,\nand pass that token to bpf() syscall. As explained in patch #3, BPF token\nitself doesn't grant access to BPF functionality, but instead allows kernel to\ndo namespaced capabilities checks (ns_capable() vs capable()) for CAP_BPF,\nCAP_PERFMON, CAP_NET_ADMIN, and CAP_SYS_ADMIN, as applicable. So it forms one\nhalf of a puzzle and allows container managers and sys admins to have safe and\nflexible configuration options: determining which containers get delegation of\nBPF functionality through BPF FS, and then which applications within such\ncontainers are allowed to perform bpf() commands, based on namespaces\ncapabilities.\n\nPrevious attempt at addressing this very same problem ([0]) attempted to\nutilize authoritative LSM approach, but was conclusively rejected by upstream\nLSM maintainers. BPF token concept is not changing anything about LSM\napproach, but can be combined with LSM hooks for very fine-grained security\npolicy. Some ideas about making BPF token more convenient to use with LSM (in\nparticular custom BPF LSM programs) was briefly described in recent LSF/MM/BPF\n2023 presentation ([1]). E.g., an ability to specify user-provided data\n(context), which in combination with BPF LSM would allow implementing a very\ndynamic and fine-granular custom security policies on top of BPF token. In the\ninterest of minimizing API surface area and discussions this was relegated to\nfollow up patches, as it's not essential to the fundamental concept of\ndelegatable BPF token.\n\nIt should be noted that BPF token is conceptually quite similar to the idea of\n/dev/bpf device file, proposed by Song a while ago ([2]). The biggest\ndifference is the idea of using virtual anon_inode file to hold BPF token and\nallowing multiple independent instances of them, each (potentially) with its\nown set of restrictions. And also, crucially, BPF token approach is not using\nany special stateful task-scoped flags. Instead, bpf() syscall accepts\ntoken_fd parameters explicitly for each relevant BPF command. This addresses\nmain concerns brought up during the /dev/bpf discussion, and fits better with\noverall BPF subsystem design.\n\nThis patch set adds a basic minimum of functionality to make BPF token idea\nuseful and to discuss API and functionality. Currently only low-level libbpf\nAPIs support creating and passing BPF token around, allowing to test kernel\nfunctionality, but for the most part is not sufficient for real-world\napplications, which typically use high-level libbpf APIs based on `struct\nbpf_object` type. This was done with the intent to limit the size of patch set\nand concentrate on mostly kernel-side changes. All the necessary plumbing for\nlibbpf will be sent as a separate follow up patch set kernel support makes it\nupstream.\n\nAnother part that should happen once kernel-side BPF token is established, is\na set of conventions between applications (e.g., systemd), tools (e.g.,\nbpftool), and libraries (e.g., libbpf) on exposing delegatable BPF FS\ninstance(s) at well-defined locations to allow applications take advantage of\nthis in automatic fashion without explicit code changes on BPF application's\nside. But I'd like to postpone this discussion to after BPF token concept\nlands.\n\n  [0] https://lore.kernel.org/bpf/20230412043300.360803-1-andrii@kernel.org/\n  [1] http://vger.kernel.org/bpfconf2023_material/Trusted_unprivileged_BPF_LSFMM2023.pdf\n  [2] https://lore.kernel.org/bpf/20190627201923.2589391-2-songliubraving@fb.com/\n  [3] https://lore.kernel.org/bpf/20230704-hochverdient-lehne-eeb9eeef785e@brauner/\n\nv11->v12:\n  - enforce exact userns match in bpf_token_capable() and\n    bpf_token_allow_cmd() checks, for added strictness (Christian);\nv10->v11:\n  - fix BPF FS root check to disallow using bind-mounted subdirectory of BPF\n    FS instance (Christian);\n  - further restrict BPF_TOKEN_CREATE command to be executed from inside\n    exactly the same user namespace as the one used to create BPF FS instance\n    (Christian);\nv9->v10:\n  - slight adjustments in LSM parts (Paul);\n  - setting delegate_xxx  options require capable(CAP_SYS_ADMIN) (Christian);\n  - simplify BPF_TOKEN_CREATE UAPI by accepting BPF FS FD directly (Christian);\nv8->v9:\n  - fix issue in selftests due to sys/mount.h header (Jiri);\n  - fix warning in doc comments in LSM hooks (kernel test robot);\nv7->v8:\n  - add bpf_token_allow_cmd and bpf_token_capable hooks (Paul);\n  - inline bpf_token_alloc() into bpf_token_create() to prevent accidental\n    divergence with security_bpf_token_create() hook (Paul);\nv6->v7:\n  - separate patches to refactor bpf_prog_alloc/bpf_map_alloc LSM hooks, as\n    discussed with Paul, and now they also accept struct bpf_token;\n  - added bpf_token_create/bpf_token_free to allow LSMs (SELinux,\n    specifically) to set up security LSM blob (Paul);\n  - last patch also wires bpf_security_struct setup by SELinux, similar to how\n    it's done for BPF map/prog, though I'm not sure if that's enough, so worst\n    case it's easy to drop this patch if more full fledged SELinux\n    implementation will be done separately;\n  - small fixes for issues caught by code reviews (Jiri, Hou);\n  - fix for test_maps test that doesn't use LIBBPF_OPTS() macro (CI);\nv5->v6:\n  - fix possible use of uninitialized variable in selftests (CI);\n  - don't use anon_inode, instead create one from BPF FS instance (Christian);\n  - don't store bpf_token inside struct bpf_map, instead pass it explicitly to\n    map_check_btf(). We do store bpf_token inside prog->aux, because it's used\n    during verification and even can be checked during attach time for some\n    program types;\n  - LSM hooks are left intact pending the conclusion of discussion with Paul\n    Moore; I'd prefer to do LSM-related changes as a follow up patch set\n    anyways;\nv4->v5:\n  - add pre-patch unifying CAP_NET_ADMIN handling inside kernel/bpf/syscall.c\n    (Paul Moore);\n  - fix build warnings and errors in selftests and kernel, detected by CI and\n    kernel test robot;\nv3->v4:\n  - add delegation mount options to BPF FS;\n  - BPF token is derived from the instance of BPF FS and associates itself\n    with BPF FS' owning userns;\n  - BPF token doesn't grant BPF functionality directly, it just turns\n    capable() checks into ns_capable() checks within BPF FS' owning user;\n  - BPF token cannot be pinned;\nv2->v3:\n  - make BPF_TOKEN_CREATE pin created BPF token in BPF FS, and disallow\n    BPF_OBJ_PIN for BPF token;\nv1->v2:\n  - fix build failures on Kconfig with CONFIG_BPF_SYSCALL unset;\n  - drop BPF_F_TOKEN_UNKNOWN_* flags and simplify UAPI (Stanislav).\n====================\n\nLink: https://lore.kernel.org/r/20231130185229.2688956-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-12-06 10:03:01 -0800",
      "commits": [
        {
          "hash": "909fa05dd3c181e5b403912889057f7cdbf3906c",
          "subject": "bpf: align CAP_NET_ADMIN checks with bpf_capable() approach",
          "message": "Within BPF syscall handling code CAP_NET_ADMIN checks stand out a bit\ncompared to CAP_BPF and CAP_PERFMON checks. For the latter, CAP_BPF or\nCAP_PERFMON are checked first, but if they are not set, CAP_SYS_ADMIN\ntakes over and grants whatever part of BPF syscall is required.\n\nSimilar kind of checks that involve CAP_NET_ADMIN are not so consistent.\nOne out of four uses does follow CAP_BPF/CAP_PERFMON model: during\nBPF_PROG_LOAD, if the type of BPF program is \"network-related\" either\nCAP_NET_ADMIN or CAP_SYS_ADMIN is required to proceed.\n\nBut in three other cases CAP_NET_ADMIN is required even if CAP_SYS_ADMIN\nis set:\n  - when creating DEVMAP/XDKMAP/CPU_MAP maps;\n  - when attaching CGROUP_SKB programs;\n  - when handling BPF_PROG_QUERY command.\n\nThis patch is changing the latter three cases to follow BPF_PROG_LOAD\nmodel, that is allowing to proceed under either CAP_NET_ADMIN or\nCAP_SYS_ADMIN.\n\nThis also makes it cleaner in subsequent BPF token patches to switch\nwholesomely to a generic bpf_token_capable(int cap) check, that always\nfalls back to CAP_SYS_ADMIN if requested capability is missing.\n\nCc: Jakub Kicinski <kuba@kernel.org>\nAcked-by: Yafang Shao <laoar.shao@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:02:58 -0800",
          "modified_files": [
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "40bba140c60fbb3ee8df6203c82fbd3de9f19d95",
          "subject": "bpf: add BPF token delegation mount options to BPF FS",
          "message": "Add few new mount options to BPF FS that allow to specify that a given\nBPF FS instance allows creation of BPF token (added in the next patch),\nand what sort of operations are allowed under BPF token. As such, we get\n4 new mount options, each is a bit mask\n  - `delegate_cmds` allow to specify which bpf() syscall commands are\n    allowed with BPF token derived from this BPF FS instance;\n  - if BPF_MAP_CREATE command is allowed, `delegate_maps` specifies\n    a set of allowable BPF map types that could be created with BPF token;\n  - if BPF_PROG_LOAD command is allowed, `delegate_progs` specifies\n    a set of allowable BPF program types that could be loaded with BPF token;\n  - if BPF_PROG_LOAD command is allowed, `delegate_attachs` specifies\n    a set of allowable BPF program attach types that could be loaded with\n    BPF token; delegate_progs and delegate_attachs are meant to be used\n    together, as full BPF program type is, in general, determined\n    through both program type and program attach type.\n\nCurrently, these mount options accept the following forms of values:\n  - a special value \"any\", that enables all possible values of a given\n  bit set;\n  - numeric value (decimal or hexadecimal, determined by kernel\n  automatically) that specifies a bit mask value directly;\n  - all the values for a given mount option are combined, if specified\n  multiple times. E.g., `mount -t bpf nodev /path/to/mount -o\n  delegate_maps=0x1 -o delegate_maps=0x2` will result in a combined 0x3\n  mask.\n\nIdeally, more convenient (for humans) symbolic form derived from\ncorresponding UAPI enums would be accepted (e.g., `-o\ndelegate_progs=kprobe|tracepoint`) and I intend to implement this, but\nit requires a bunch of UAPI header churn, so I postponed it until this\nfeature lands upstream or at least there is a definite consensus that\nthis feature is acceptable and is going to make it, just to minimize\namount of wasted effort and not increase amount of non-essential code to\nbe reviewed.\n\nAttentive reader will notice that BPF FS is now marked as\nFS_USERNS_MOUNT, which theoretically makes it mountable inside non-init\nuser namespace as long as the process has sufficient *namespaced*\ncapabilities within that user namespace. But in reality we still\nrestrict BPF FS to be mountable only by processes with CAP_SYS_ADMIN *in\ninit userns* (extra check in bpf_fill_super()). FS_USERNS_MOUNT is added\nto allow creating BPF FS context object (i.e., fsopen(\"bpf\")) from\ninside unprivileged process inside non-init userns, to capture that\nuserns as the owning userns. It will still be required to pass this\ncontext object back to privileged process to instantiate and mount it.\n\nThis manipulation is important, because capturing non-init userns as the\nowning userns of BPF FS instance (super block) allows to use that userns\nto constraint BPF token to that userns later on (see next patch). So\ncreating BPF FS with delegation inside unprivileged userns will restrict\nderived BPF token objects to only \"work\" inside that intended userns,\nmaking it scoped to a intended \"container\". Also, setting these\ndelegation options requires capable(CAP_SYS_ADMIN), so unprivileged\nprocess cannot set this up without involvement of a privileged process.\n\nThere is a set of selftests at the end of the patch set that simulates\nthis sequence of steps and validates that everything works as intended.\nBut careful review is requested to make sure there are no missed gaps in\nthe implementation and testing.\n\nThis somewhat subtle set of aspects is the result of previous\ndiscussions ([0]) about various user namespace implications and\ninteractions with BPF token functionality and is necessary to contain\nBPF token inside intended user namespace.\n\n  [0] https://lore.kernel.org/bpf/20230704-hochverdient-lehne-eeb9eeef785e@brauner/\n\nAcked-by: Christian Brauner <brauner@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:02:58 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/inode.c"
          ]
        },
        {
          "hash": "4527358b76861dfd64ee34aba45d81648fbc8a61",
          "subject": "bpf: introduce BPF token object",
          "message": "Add new kind of BPF kernel object, BPF token. BPF token is meant to\nallow delegating privileged BPF functionality, like loading a BPF\nprogram or creating a BPF map, from privileged process to a *trusted*\nunprivileged process, all while having a good amount of control over which\nprivileged operations could be performed using provided BPF token.\n\nThis is achieved through mounting BPF FS instance with extra delegation\nmount options, which determine what operations are delegatable, and also\nconstraining it to the owning user namespace (as mentioned in the\nprevious patch).\n\nBPF token itself is just a derivative from BPF FS and can be created\nthrough a new bpf() syscall command, BPF_TOKEN_CREATE, which accepts BPF\nFS FD, which can be attained through open() API by opening BPF FS mount\npoint. Currently, BPF token \"inherits\" delegated command, map types,\nprog type, and attach type bit sets from BPF FS as is. In the future,\nhaving an BPF token as a separate object with its own FD, we can allow\nto further restrict BPF token's allowable set of things either at the\ncreation time or after the fact, allowing the process to guard itself\nfurther from unintentionally trying to load undesired kind of BPF\nprograms. But for now we keep things simple and just copy bit sets as is.\n\nWhen BPF token is created from BPF FS mount, we take reference to the\nBPF super block's owning user namespace, and then use that namespace for\nchecking all the {CAP_BPF, CAP_PERFMON, CAP_NET_ADMIN, CAP_SYS_ADMIN}\ncapabilities that are normally only checked against init userns (using\ncapable()), but now we check them using ns_capable() instead (if BPF\ntoken is provided). See bpf_token_capable() for details.\n\nSuch setup means that BPF token in itself is not sufficient to grant BPF\nfunctionality. User namespaced process has to *also* have necessary\ncombination of capabilities inside that user namespace. So while\npreviously CAP_BPF was useless when granted within user namespace, now\nit gains a meaning and allows container managers and sys admins to have\na flexible control over which processes can and need to use BPF\nfunctionality within the user namespace (i.e., container in practice).\nAnd BPF FS delegation mount options and derived BPF tokens serve as\na per-container \"flag\" to grant overall ability to use bpf() (plus further\nrestrict on which parts of bpf() syscalls are treated as namespaced).\n\nNote also, BPF_TOKEN_CREATE command itself requires ns_capable(CAP_BPF)\nwithin the BPF FS owning user namespace, rounding up the ns_capable()\nstory of BPF token.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:02:59 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/Makefile",
            "kernel/bpf/inode.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/token.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "688b7270b3cb75e8ac78123d719967db40336e5b",
          "subject": "bpf: add BPF token support to BPF_MAP_CREATE command",
          "message": "Allow providing token_fd for BPF_MAP_CREATE command to allow controlled\nBPF map creation from unprivileged process through delegated BPF token.\n\nWire through a set of allowed BPF map types to BPF token, derived from\nBPF FS at BPF token creation time. This, in combination with allowed_cmds\nallows to create a narrowly-focused BPF token (controlled by privileged\nagent) with a restrictive set of BPF maps that application can attempt\nto create.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:02:59 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/inode.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/token.c",
            "tools/include/uapi/linux/bpf.h",
            "tools/testing/selftests/bpf/prog_tests/libbpf_probes.c",
            "tools/testing/selftests/bpf/prog_tests/libbpf_str.c"
          ]
        },
        {
          "hash": "ee54b1a910e4d49c9a104f31ae3f5b979131adf8",
          "subject": "bpf: add BPF token support to BPF_BTF_LOAD command",
          "message": "Accept BPF token FD in BPF_BTF_LOAD command to allow BTF data loading\nthrough delegated BPF token. BTF loading is a pretty straightforward\noperation, so as long as BPF token is created with allow_cmds granting\nBPF_BTF_LOAD command, kernel proceeds to parsing BTF data and creating\nBTF object.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:02:59 -0800",
          "modified_files": [
            "include/uapi/linux/bpf.h",
            "kernel/bpf/syscall.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "e1cef620f598853a90f17701fcb1057a6768f7b8",
          "subject": "bpf: add BPF token support to BPF_PROG_LOAD command",
          "message": "Add basic support of BPF token to BPF_PROG_LOAD. Wire through a set of\nallowed BPF program types and attach types, derived from BPF FS at BPF\ntoken creation time. Then make sure we perform bpf_token_capable()\nchecks everywhere where it's relevant.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:02:59 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/core.c",
            "kernel/bpf/inode.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/token.c",
            "tools/include/uapi/linux/bpf.h",
            "tools/testing/selftests/bpf/prog_tests/libbpf_probes.c",
            "tools/testing/selftests/bpf/prog_tests/libbpf_str.c"
          ]
        },
        {
          "hash": "4cbb270e115bc197ff2046aeb54cc951666b16ec",
          "subject": "bpf: take into account BPF token when fetching helper protos",
          "message": "Instead of performing unconditional system-wide bpf_capable() and\nperfmon_capable() calls inside bpf_base_func_proto() function (and other\nsimilar ones) to determine eligibility of a given BPF helper for a given\nprogram, use previously recorded BPF token during BPF_PROG_LOAD command\nhandling to inform the decision.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:02:59 -0800",
          "modified_files": [
            "drivers/media/rc/bpf-lirc.c",
            "include/linux/bpf.h",
            "kernel/bpf/cgroup.c",
            "kernel/bpf/helpers.c",
            "kernel/bpf/syscall.c",
            "kernel/trace/bpf_trace.c",
            "net/core/filter.c",
            "net/ipv4/bpf_tcp_ca.c",
            "net/netfilter/nf_bpf_link.c"
          ]
        },
        {
          "hash": "8062fb12de99b2da33754c6a3be1bfc30d9a35f4",
          "subject": "bpf: consistently use BPF token throughout BPF verifier logic",
          "message": "Remove remaining direct queries to perfmon_capable() and bpf_capable()\nin BPF verifier logic and instead use BPF token (if available) to make\ndecisions about privileges.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:02:59 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/filter.h",
            "kernel/bpf/arraymap.c",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c",
            "net/core/filter.c"
          ]
        },
        {
          "hash": "c3dd6e94df7193f33f45d33303f5e85afb2a72dc",
          "subject": "bpf,lsm: refactor bpf_prog_alloc/bpf_prog_free LSM hooks",
          "message": "Based on upstream discussion ([0]), rework existing\nbpf_prog_alloc_security LSM hook. Rename it to bpf_prog_load and instead\nof passing bpf_prog_aux, pass proper bpf_prog pointer for a full BPF\nprogram struct. Also, we pass bpf_attr union with all the user-provided\narguments for BPF_PROG_LOAD command.  This will give LSMs as much\ninformation as we can basically provide.\n\nThe hook is also BPF token-aware now, and optional bpf_token struct is\npassed as a third argument. bpf_prog_load LSM hook is called after\na bunch of sanity checks were performed, bpf_prog and bpf_prog_aux were\nallocated and filled out, but right before performing full-fledged BPF\nverification step.\n\nbpf_prog_free LSM hook is now accepting struct bpf_prog argument, for\nconsistency. SELinux code is adjusted to all new names, types, and\nsignatures.\n\nNote, given that bpf_prog_load (previously bpf_prog_alloc) hook can be\nused by some LSMs to allocate extra security blob, but also by other\nLSMs to reject BPF program loading, we need to make sure that\nbpf_prog_free LSM hook is called after bpf_prog_load/bpf_prog_alloc one\n*even* if the hook itself returned error. If we don't do that, we run\nthe risk of leaking memory. This seems to be possible today when\ncombining SELinux and BPF LSM, as one example, depending on their\nrelative ordering.\n\nAlso, for BPF LSM setup, add bpf_prog_load and bpf_prog_free to\nsleepable LSM hooks list, as they are both executed in sleepable\ncontext. Also drop bpf_prog_load hook from untrusted, as there is no\nissue with refcount or anything else anymore, that originally forced us\nto add it to untrusted list in c0c852dd1876 (\"bpf: Do not mark certain LSM\nhook arguments as trusted\"). We now trigger this hook much later and it\nshould not be an issue anymore.\n\n  [0] https://lore.kernel.org/bpf/9fe88aef7deabbe87d3fc38c4aea3c69.paul@paul-moore.com/\n\nAcked-by: Paul Moore <paul@paul-moore.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-10-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:02:59 -0800",
          "modified_files": [
            "include/linux/lsm_hook_defs.h",
            "include/linux/security.h",
            "kernel/bpf/bpf_lsm.c",
            "kernel/bpf/syscall.c",
            "security/security.c",
            "security/selinux/hooks.c"
          ]
        },
        {
          "hash": "66d636d70a79c1d37e3eea67ab50969e6aaef983",
          "subject": "bpf,lsm: refactor bpf_map_alloc/bpf_map_free LSM hooks",
          "message": "Similarly to bpf_prog_alloc LSM hook, rename and extend bpf_map_alloc\nhook into bpf_map_create, taking not just struct bpf_map, but also\nbpf_attr and bpf_token, to give a fuller context to LSMs.\n\nUnlike bpf_prog_alloc, there is no need to move the hook around, as it\ncurrently is firing right before allocating BPF map ID and FD, which\nseems to be a sweet spot.\n\nBut like bpf_prog_alloc/bpf_prog_free combo, make sure that bpf_map_free\nLSM hook is called even if bpf_map_create hook returned error, as if few\nLSMs are combined together it could be that one LSM successfully\nallocated security blob for its needs, while subsequent LSM rejected BPF\nmap creation. The former LSM would still need to free up LSM blob, so we\nneed to ensure security_bpf_map_free() is called regardless of the\noutcome.\n\nAcked-by: Paul Moore <paul@paul-moore.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-11-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:02:59 -0800",
          "modified_files": [
            "include/linux/lsm_hook_defs.h",
            "include/linux/security.h",
            "kernel/bpf/bpf_lsm.c",
            "kernel/bpf/syscall.c",
            "security/security.c",
            "security/selinux/hooks.c"
          ]
        },
        {
          "hash": "d734ca7b33dbf60eb15dcf7c44f3da7073356777",
          "subject": "bpf,lsm: add BPF token LSM hooks",
          "message": "Wire up bpf_token_create and bpf_token_free LSM hooks, which allow to\nallocate LSM security blob (we add `void *security` field to struct\nbpf_token for that), but also control who can instantiate BPF token.\nThis follows existing pattern for BPF map and BPF prog.\n\nAlso add security_bpf_token_allow_cmd() and security_bpf_token_capable()\nLSM hooks that allow LSM implementation to control and negate (if\nnecessary) BPF token's delegation of a specific bpf_cmd and capability,\nrespectively.\n\nAcked-by: Paul Moore <paul@paul-moore.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-12-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:03:00 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/lsm_hook_defs.h",
            "include/linux/security.h",
            "kernel/bpf/bpf_lsm.c",
            "kernel/bpf/token.c",
            "security/security.c"
          ]
        },
        {
          "hash": "ecd435143eb03611e25694141bf59d1c04ad5b9e",
          "subject": "libbpf: add bpf_token_create() API",
          "message": "Add low-level wrapper API for BPF_TOKEN_CREATE command in bpf() syscall.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-13-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:03:00 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/bpf.h",
            "tools/lib/bpf/libbpf.map"
          ]
        },
        {
          "hash": "37891cea6699200fb83eae464ebe1c0f73040474",
          "subject": "libbpf: add BPF token support to bpf_map_create() API",
          "message": "Add ability to provide token_fd for BPF_MAP_CREATE command through\nbpf_map_create() API.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-14-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:03:00 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/bpf.h"
          ]
        },
        {
          "hash": "1a8df7fa00aac35aff9ef1941c5334b3a01d09e4",
          "subject": "libbpf: add BPF token support to bpf_btf_load() API",
          "message": "Allow user to specify token_fd for bpf_btf_load() API that wraps\nkernel's BPF_BTF_LOAD command. This allows loading BTF from unprivileged\nprocess as long as it has BPF token allowing BPF_BTF_LOAD command, which\ncan be created and delegated by privileged process.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-15-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:03:00 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/bpf.h"
          ]
        },
        {
          "hash": "1571740a9ba036f26cc5211a86021199987219e8",
          "subject": "libbpf: add BPF token support to bpf_prog_load() API",
          "message": "Wire through token_fd into bpf_prog_load().\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-16-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:03:00 -0800",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/bpf.h"
          ]
        },
        {
          "hash": "dc5196fac40c2cb96330bcb98eef868a7fd225b3",
          "subject": "selftests/bpf: add BPF token-enabled tests",
          "message": "Add a selftest that attempts to conceptually replicate intended BPF\ntoken use cases inside user namespaced container.\n\nChild process is forked. It is then put into its own userns and mountns.\nChild creates BPF FS context object. This ensures child userns is\ncaptured as the owning userns for this instance of BPF FS. Given setting\ndelegation mount options is privileged operation, we ensure that child\ncannot set them.\n\nThis context is passed back to privileged parent process through Unix\nsocket, where parent sets up delegation options, creates, and mounts it\nas a detached mount. This mount FD is passed back to the child to be\nused for BPF token creation, which allows otherwise privileged BPF\noperations to succeed inside userns.\n\nWe validate that all of token-enabled privileged commands (BPF_BTF_LOAD,\nBPF_MAP_CREATE, and BPF_PROG_LOAD) work as intended. They should only\nsucceed inside the userns if a) BPF token is provided with proper\nallowed sets of commands and types; and b) namespaces CAP_BPF and other\nprivileges are set. Lacking a) or b) should lead to -EPERM failures.\n\nBased on suggested workflow by Christian Brauner ([0]).\n\n  [0] https://lore.kernel.org/bpf/20230704-hochverdient-lehne-eeb9eeef785e@brauner/\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-17-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:03:00 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/token.c"
          ]
        },
        {
          "hash": "36fb94944b35062db15ab3059f4123048cac658c",
          "subject": "bpf,selinux: allocate bpf_security_struct per BPF token",
          "message": "Utilize newly added bpf_token_create/bpf_token_free LSM hooks to\nallocate struct bpf_security_struct for each BPF token object in\nSELinux. This just follows similar pattern for BPF prog and map.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-18-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-06 10:03:00 -0800",
          "modified_files": [
            "security/selinux/hooks.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "3aee2bf9c49be2144460d7267560232e3d45d367",
      "merge_subject": "Merge branch 'complete-bpf-verifier-precision-tracking-support-for-register-spills'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nComplete BPF verifier precision tracking support for register spills\n\nAdd support to BPF verifier to track and support register spill/fill to/from\nstack regardless if it was done through read-only R10 register (which is the\nonly form supported today), or through a general register after copying R10\ninto it, while also potentially modifying offset.\n\nOnce we add register this generic spill/fill support to precision\nbacktracking, we can take advantage of it to stop doing eager STACK_ZERO\nconversion on register spill. Instead we can rely on (im)precision of spilled\nconst zero register to improve verifier state pruning efficiency. This\nsituation of using const zero register to initialize stack slots is very\ncommon with __builtin_memset() usage or just zero-initializing variables on\nthe stack, and it causes unnecessary state duplication, as that STACK_ZERO\nknowledge is often not necessary for correctness, as those zero values are\nnever used in precise context. Thus, relying on register imprecision helps\ntremendously, especially in real-world BPF programs.\n\nTo make spilled const zero register behave completely equivalently to\nSTACK_ZERO, we need to improve few other small pieces, which is done in the\nsecond part of the patch set. See individual patches for details. There are\nalso two small bug fixes spotted during STACK_ZERO debugging.\n\nThe patch set consists of logically three changes:\n  - patch #1 (and corresponding tests in patch #2) is fixing/impoving precision\n    propagation for stack spills/fills. This can be landed as a stand-alone\n    improvement;\n  - patches #3 through #9 is improving verification scalability by utilizing\n    register (im)precision instead of eager STACK_ZERO. These changes depend\n    on patch #1.\n  - patch #10 is a memory efficiency improvement to how instruction/jump\n    history is tracked and maintained. It depends on patch #1, but is not\n    strictly speaking required, even though I believe it's a good long-term\n    solution to have a path-dependent per-instruction information. Kind\n    of like a path-dependent counterpart to path-agnostic insn_aux array.\n\nv3->v3:\n  - fixed up Fixes tag (Alexei);\n  - fixed few more selftests to not use BPF_ST instruction in inline asm\n    directly, checked with CI, it was happy (CI);\nv2->v3:\n  - BPF_ST instruction workaround (Eduard);\n  - force dereference in added tests to catch problems (Eduard);\n  - some commit message massaging (Alexei);\nv1->v2:\n  - clean ups, WARN_ONCE(), insn_flags helpers added (Eduard);\n  - added more selftests for STACK_ZERO/STACK_MISC cases (Eduard);\n  - a bit more detailed explanation of effect of avoiding STACK_ZERO in favor\n    of register spill in patch #8 commit (Alexei);\n  - global shared instruction history refactoring moved to be the last patch\n    in the series to make it easier to revert it, if applied (Alexei).\n====================\n\nLink: https://lore.kernel.org/r/20231205184248.1502704-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-12-05 13:41:34 -0800",
      "commits": [
        {
          "hash": "41f6f64e6999a837048b1bd13a2f8742964eca6b",
          "subject": "bpf: support non-r10 register spill/fill to/from stack in precision tracking",
          "message": "Use instruction (jump) history to record instructions that performed\nregister spill/fill to/from stack, regardless if this was done through\nread-only r10 register, or any other register after copying r10 into it\n*and* potentially adjusting offset.\n\nTo make this work reliably, we push extra per-instruction flags into\ninstruction history, encoding stack slot index (spi) and stack frame\nnumber in extra 10 bit flags we take away from prev_idx in instruction\nhistory. We don't touch idx field for maximum performance, as it's\nchecked most frequently during backtracking.\n\nThis change removes basically the last remaining practical limitation of\nprecision backtracking logic in BPF verifier. It fixes known\ndeficiencies, but also opens up new opportunities to reduce number of\nverified states, explored in the subsequent patches.\n\nThere are only three differences in selftests' BPF object files\naccording to veristat, all in the positive direction (less states).\n\nFile                                    Program        Insns (A)  Insns (B)  Insns  (DIFF)  States (A)  States (B)  States (DIFF)\n--------------------------------------  -------------  ---------  ---------  -------------  ----------  ----------  -------------\ntest_cls_redirect_dynptr.bpf.linked3.o  cls_redirect        2987       2864  -123 (-4.12%)         240         231    -9 (-3.75%)\nxdp_synproxy_kern.bpf.linked3.o         syncookie_tc       82848      82661  -187 (-0.23%)        5107        5073   -34 (-0.67%)\nxdp_synproxy_kern.bpf.linked3.o         syncookie_xdp      85116      84964  -152 (-0.18%)        5162        5130   -32 (-0.62%)\n\nNote, I avoided renaming jmp_history to more generic insn_hist to\nminimize number of lines changed and potential merge conflicts between\nbpf and bpf-next trees.\n\nNotice also cur_hist_entry pointer reset to NULL at the beginning of\ninstruction verification loop. This pointer avoids the problem of\nrelying on last jump history entry's insn_idx to determine whether we\nalready have entry for current instruction or not. It can happen that we\nadded jump history entry because current instruction is_jmp_point(), but\nalso we need to add instruction flags for stack access. In this case, we\ndon't want to entries, so we need to reuse last added entry, if it is\npresent.\n\nRelying on insn_idx comparison has the same ambiguity problem as the one\nthat was fixed recently in [0], so we avoid that.\n\n  [0] https://patchwork.kernel.org/project/netdevbpf/patch/20231110002638.4168352-3-andrii@kernel.org/\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nReported-by: Tao Lyu <tao.lyu@epfl.ch>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-05 13:40:20 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c",
            "tools/testing/selftests/bpf/verifier/precise.c"
          ]
        },
        {
          "hash": "876301881c436bf38e83a2c0d276a24b642e4aab",
          "subject": "selftests/bpf: add stack access precision test",
          "message": "Add a new selftests that validates precision tracking for stack access\ninstruction, using both r10-based and non-r10-based accesses. For\nnon-r10 ones we also make sure to have non-zero var_off to validate that\nfinal stack offset is tracked properly in instruction history\ninformation inside verifier.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-05 13:40:20 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c"
          ]
        },
        {
          "hash": "ab125ed3ec1c10ccc36bc98c7a4256ad114a3dae",
          "subject": "bpf: fix check for attempt to corrupt spilled pointer",
          "message": "When register is spilled onto a stack as a 1/2/4-byte register, we set\nslot_type[BPF_REG_SIZE - 1] (plus potentially few more below it,\ndepending on actual spill size). So to check if some stack slot has\nspilled register we need to consult slot_type[7], not slot_type[0].\n\nTo avoid the need to remember and double-check this in the future, just\nuse is_spilled_reg() helper.\n\nFixes: 27113c59b6d0 (\"bpf: Check the other end of slot_type for STACK_SPILL\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-05 13:40:20 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "eaf18febd6ebc381aeb61543705148b3e28c7c47",
          "subject": "bpf: preserve STACK_ZERO slots on partial reg spills",
          "message": "Instead of always forcing STACK_ZERO slots to STACK_MISC, preserve it in\nsituations where this is possible. E.g., when spilling register as\n1/2/4-byte subslots on the stack, all the remaining bytes in the stack\nslot do not automatically become unknown. If we knew they contained\nzeroes, we can preserve those STACK_ZERO markers.\n\nAdd a helper mark_stack_slot_misc(), similar to scrub_spilled_slot(),\nbut that doesn't overwrite either STACK_INVALID nor STACK_ZERO. Note\nthat we need to take into account possibility of being in unprivileged\nmode, in which case STACK_INVALID is forced to STACK_MISC for correctness,\nas treating STACK_INVALID as equivalent STACK_MISC is only enabled in\nprivileged mode.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-05 13:40:20 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b33ceb6a3d2ee07fdd836373383a6d4783581324",
          "subject": "selftests/bpf: validate STACK_ZERO is preserved on subreg spill",
          "message": "Add tests validating that STACK_ZERO slots are preserved when slot is\npartially overwritten with subregister spill.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-05 13:40:20 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
          ]
        },
        {
          "hash": "e322f0bcb8d371f4606eaf141c7f967e1a79bcb7",
          "subject": "bpf: preserve constant zero when doing partial register restore",
          "message": "Similar to special handling of STACK_ZERO, when reading 1/2/4 bytes from\nstack from slot that has register spilled into it and that register has\na constant value zero, preserve that zero and mark spilled register as\nprecise for that. This makes spilled const zero register and STACK_ZERO\ncases equivalent in their behavior.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-05 13:40:21 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "add1cd7f22e61756987865ada9fe95cd86569025",
          "subject": "selftests/bpf: validate zero preservation for sub-slot loads",
          "message": "Validate that 1-, 2-, and 4-byte loads from stack slots not aligned on\n8-byte boundary still preserve zero, when loading from all-STACK_ZERO\nsub-slots, or when stack sub-slots are covered by spilled register with\nknown constant zero value.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-05 13:40:21 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
          ]
        },
        {
          "hash": "18a433b62061e3d787bfc3e670fa711fecbd7cb4",
          "subject": "bpf: track aligned STACK_ZERO cases as imprecise spilled registers",
          "message": "Now that precision backtracing is supporting register spill/fill to/from\nstack, there is another oportunity to be exploited here: minimizing\nprecise STACK_ZERO cases. With a simple code change we can rely on\ninitially imprecise register spill tracking for cases when register\nspilled to stack was a known zero.\n\nThis is a very common case for initializing on the stack variables,\nincluding rather large structures. Often times zero has no special\nmeaning for the subsequent BPF program logic and is often overwritten\nwith non-zero values soon afterwards. But due to STACK_ZERO vs\nSTACK_MISC tracking, such initial zero initialization actually causes\nduplication of verifier states as STACK_ZERO is clearly different than\nSTACK_MISC or spilled SCALAR_VALUE register.\n\nThe effect of this (now) trivial change is huge, as can be seen below.\nThese are differences between BPF selftests, Cilium, and Meta-internal\nBPF object files relative to previous patch in this series. You can see\nimprovements ranging from single-digit percentage improvement for\ninstructions and states, all the way to 50-60% reduction for some of\nMeta-internal host agent programs, and even some Cilium programs.\n\nFor Meta-internal ones I left only the differences for largest BPF\nobject files by states/instructions, as there were too many differences\nin the overall output. All the differences were improvements, reducting\nnumber of states and thus instructions validated.\n\nNote, Meta-internal BPF object file names are not printed below.\nMany copies of balancer_ingress are actually many different\nconfigurations of Katran, so they are different BPF programs, which\nexplains state reduction going from -16% all the way to 31%, depending\non BPF program logic complexity.\n\nI also tooked a closer look at a few small-ish BPF programs to validate\nthe behavior. Let's take bpf_iter_netrlink.bpf.o (first row below).\nWhile it's just 8 vs 5 states, verifier log is still pretty long to\ninclude it here. But the reduction in states is due to the following\npiece of C code:\n\n        unsigned long ino;\n\n\t...\n\n        sk = s->sk_socket;\n        if (!sk) {\n                ino = 0;\n        } else {\n                inode = SOCK_INODE(sk);\n                bpf_probe_read_kernel(&ino, sizeof(ino), &inode->i_ino);\n        }\n        BPF_SEQ_PRINTF(seq, \"%-8u %-8lu\\n\", s->sk_drops.counter, ino);\n\treturn 0;\n\nYou can see that in some situations `ino` is zero-initialized, while in\nothers it's unknown value filled out by bpf_probe_read_kernel(). Before\nthis change code after if/else branches have to be validated twice. Once\nwith (precise) ino == 0, due to eager STACK_ZERO logic, and then again\nfor when ino is just STACK_MISC. But BPF_SEQ_PRINTF() doesn't care about\nprecise value of ino, so with the change in this patch verifier is able\nto prune states from after one of the branches, reducing number of total\nstates (and instructions) required for successful validation.\n\nSimilar principle applies to bigger real-world applications, just at\na much larger scale.\n\nSELFTESTS\n=========\nFile                                     Program                  Insns (A)  Insns (B)  Insns    (DIFF)  States (A)  States (B)  States (DIFF)\n---------------------------------------  -----------------------  ---------  ---------  ---------------  ----------  ----------  -------------\nbpf_iter_netlink.bpf.linked3.o           dump_netlink                   148        104    -44 (-29.73%)           8           5   -3 (-37.50%)\nbpf_iter_unix.bpf.linked3.o              dump_unix                     8474       8404     -70 (-0.83%)         151         147    -4 (-2.65%)\nbpf_loop.bpf.linked3.o                   stack_check                    560        324   -236 (-42.14%)          42          24  -18 (-42.86%)\nlocal_storage_bench.bpf.linked3.o        get_local                      120         77    -43 (-35.83%)           9           6   -3 (-33.33%)\nloop6.bpf.linked3.o                      trace_virtqueue_add_sgs      10167       9868    -299 (-2.94%)         226         206   -20 (-8.85%)\npyperf600_bpf_loop.bpf.linked3.o         on_event                      4872       3423  -1449 (-29.74%)         322         229  -93 (-28.88%)\nstrobemeta.bpf.linked3.o                 on_event                    180697     176036   -4661 (-2.58%)        4780        4734   -46 (-0.96%)\ntest_cls_redirect.bpf.linked3.o          cls_redirect                 65594      65401    -193 (-0.29%)        4230        4212   -18 (-0.43%)\ntest_global_func_args.bpf.linked3.o      test_cls                       145        136      -9 (-6.21%)          10           9   -1 (-10.00%)\ntest_l4lb.bpf.linked3.o                  balancer_ingress              4760       2612  -2148 (-45.13%)         113         102   -11 (-9.73%)\ntest_l4lb_noinline.bpf.linked3.o         balancer_ingress              4845       4877     +32 (+0.66%)         219         221    +2 (+0.91%)\ntest_l4lb_noinline_dynptr.bpf.linked3.o  balancer_ingress              2072       2087     +15 (+0.72%)          97          98    +1 (+1.03%)\ntest_seg6_loop.bpf.linked3.o             __add_egr_x                  12440       9975  -2465 (-19.82%)         364         353   -11 (-3.02%)\ntest_tcp_hdr_options.bpf.linked3.o       estab                         2558       2572     +14 (+0.55%)         179         180    +1 (+0.56%)\ntest_xdp_dynptr.bpf.linked3.o            _xdp_tx_iptunnel               645        596     -49 (-7.60%)          26          24    -2 (-7.69%)\ntest_xdp_noinline.bpf.linked3.o          balancer_ingress_v6           3520       3516      -4 (-0.11%)         216         216    +0 (+0.00%)\nxdp_synproxy_kern.bpf.linked3.o          syncookie_tc                 82661      81241   -1420 (-1.72%)        5073        5155   +82 (+1.62%)\nxdp_synproxy_kern.bpf.linked3.o          syncookie_xdp                84964      82297   -2667 (-3.14%)        5130        5157   +27 (+0.53%)\n\nMETA-INTERNAL\n=============\nProgram                                 Insns (A)  Insns (B)  Insns      (DIFF)  States (A)  States (B)  States   (DIFF)\n--------------------------------------  ---------  ---------  -----------------  ----------  ----------  ---------------\nbalancer_ingress                            27925      23608    -4317 (-15.46%)        1488        1482      -6 (-0.40%)\nbalancer_ingress                            31824      27546    -4278 (-13.44%)        1658        1652      -6 (-0.36%)\nbalancer_ingress                            32213      27935    -4278 (-13.28%)        1689        1683      -6 (-0.36%)\nbalancer_ingress                            32213      27935    -4278 (-13.28%)        1689        1683      -6 (-0.36%)\nbalancer_ingress                            31824      27546    -4278 (-13.44%)        1658        1652      -6 (-0.36%)\nbalancer_ingress                            38647      29562    -9085 (-23.51%)        2069        1835   -234 (-11.31%)\nbalancer_ingress                            38647      29562    -9085 (-23.51%)        2069        1835   -234 (-11.31%)\nbalancer_ingress                            40339      30792    -9547 (-23.67%)        2193        1934   -259 (-11.81%)\nbalancer_ingress                            37321      29055    -8266 (-22.15%)        1972        1795    -177 (-8.98%)\nbalancer_ingress                            38176      29753    -8423 (-22.06%)        2008        1831    -177 (-8.81%)\nbalancer_ingress                            29193      20910    -8283 (-28.37%)        1599        1422   -177 (-11.07%)\nbalancer_ingress                            30013      21452    -8561 (-28.52%)        1645        1447   -198 (-12.04%)\nbalancer_ingress                            28691      24290    -4401 (-15.34%)        1545        1531     -14 (-0.91%)\nbalancer_ingress                            34223      28965    -5258 (-15.36%)        1984        1875    -109 (-5.49%)\nbalancer_ingress                            35481      26158    -9323 (-26.28%)        2095        1806   -289 (-13.79%)\nbalancer_ingress                            35481      26158    -9323 (-26.28%)        2095        1806   -289 (-13.79%)\nbalancer_ingress                            35868      26455    -9413 (-26.24%)        2140        1827   -313 (-14.63%)\nbalancer_ingress                            35868      26455    -9413 (-26.24%)        2140        1827   -313 (-14.63%)\nbalancer_ingress                            35481      26158    -9323 (-26.28%)        2095        1806   -289 (-13.79%)\nbalancer_ingress                            35481      26158    -9323 (-26.28%)        2095        1806   -289 (-13.79%)\nbalancer_ingress                            34844      29485    -5359 (-15.38%)        2036        1918    -118 (-5.80%)\nfbflow_egress                                3256       2652     -604 (-18.55%)         218         192    -26 (-11.93%)\nfbflow_ingress                               1026        944       -82 (-7.99%)          70          63     -7 (-10.00%)\nsslwall_tc_egress                            8424       7360    -1064 (-12.63%)         498         458     -40 (-8.03%)\nsyar_accept_protect                         15040       9539    -5501 (-36.58%)         364         220   -144 (-39.56%)\nsyar_connect_tcp_v6                         15036       9535    -5501 (-36.59%)         360         216   -144 (-40.00%)\nsyar_connect_udp_v4                         15039       9538    -5501 (-36.58%)         361         217   -144 (-39.89%)\nsyar_connect_connect4_protect4              24805      15833    -8972 (-36.17%)         756         480   -276 (-36.51%)\nsyar_lsm_file_open                         167772     151813    -15959 (-9.51%)        1836        1667    -169 (-9.20%)\nsyar_namespace_create_new                   14805       9304    -5501 (-37.16%)         353         209   -144 (-40.79%)\nsyar_python3_detect                         17531      12030    -5501 (-31.38%)         391         247   -144 (-36.83%)\nsyar_ssh_post_fork                          16412      10911    -5501 (-33.52%)         405         261   -144 (-35.56%)\nsyar_enter_execve                           14728       9227    -5501 (-37.35%)         345         201   -144 (-41.74%)\nsyar_enter_execveat                         14728       9227    -5501 (-37.35%)         345         201   -144 (-41.74%)\nsyar_exit_execve                            16622      11121    -5501 (-33.09%)         376         232   -144 (-38.30%)\nsyar_exit_execveat                          16622      11121    -5501 (-33.09%)         376         232   -144 (-38.30%)\nsyar_syscalls_kill                          15288       9787    -5501 (-35.98%)         398         254   -144 (-36.18%)\nsyar_task_enter_pivot_root                  14898       9397    -5501 (-36.92%)         357         213   -144 (-40.34%)\nsyar_syscalls_setreuid                      16678      11177    -5501 (-32.98%)         429         285   -144 (-33.57%)\nsyar_syscalls_setuid                        16678      11177    -5501 (-32.98%)         429         285   -144 (-33.57%)\nsyar_syscalls_process_vm_readv              14959       9458    -5501 (-36.77%)         364         220   -144 (-39.56%)\nsyar_syscalls_process_vm_writev             15757      10256    -5501 (-34.91%)         390         246   -144 (-36.92%)\ndo_uprobe                                   15519      10018    -5501 (-35.45%)         373         229   -144 (-38.61%)\nedgewall                                   179715      55783  -123932 (-68.96%)       12607        3999  -8608 (-68.28%)\nbictcp_state                                 7570       4131    -3439 (-45.43%)         496         269   -227 (-45.77%)\ncubictcp_state                               7570       4131    -3439 (-45.43%)         496         269   -227 (-45.77%)\ntcp_rate_skb_delivered                        447        272     -175 (-39.15%)          29          18    -11 (-37.93%)\nkprobe__bbr_set_state                        4566       2615    -1951 (-42.73%)         209         124    -85 (-40.67%)\nkprobe__bictcp_state                         4566       2615    -1951 (-42.73%)         209         124    -85 (-40.67%)\ninet_sock_set_state                          1501       1337     -164 (-10.93%)          93          85      -8 (-8.60%)\ntcp_retransmit_skb                           1145        981     -164 (-14.32%)          67          59     -8 (-11.94%)\ntcp_retransmit_synack                        1183        951     -232 (-19.61%)          67          55    -12 (-17.91%)\nbpf_tcptuner                                 1459       1187     -272 (-18.64%)          99          80    -19 (-19.19%)\ntw_egress                                     801        776       -25 (-3.12%)          69          66      -3 (-4.35%)\ntw_ingress                                    795        770       -25 (-3.14%)          69          66      -3 (-4.35%)\nttls_tc_ingress                             19025      19383      +358 (+1.88%)         470         465      -5 (-1.06%)\nttls_nat_egress                               490        299     -191 (-38.98%)          33          20    -13 (-39.39%)\nttls_nat_ingress                              448        285     -163 (-36.38%)          32          21    -11 (-34.38%)\ntw_twfw_egress                             511127     212071  -299056 (-58.51%)       16733        8504  -8229 (-49.18%)\ntw_twfw_ingress                            500095     212069  -288026 (-57.59%)       16223        8504  -7719 (-47.58%)\ntw_twfw_tc_eg                              511113     212064  -299049 (-58.51%)       16732        8504  -8228 (-49.18%)\ntw_twfw_tc_in                              500095     212069  -288026 (-57.59%)       16223        8504  -7719 (-47.58%)\ntw_twfw_egress                              12632      12435      -197 (-1.56%)         276         260     -16 (-5.80%)\ntw_twfw_ingress                             12631      12454      -177 (-1.40%)         278         261     -17 (-6.12%)\ntw_twfw_tc_eg                               12595      12435      -160 (-1.27%)         274         259     -15 (-5.47%)\ntw_twfw_tc_in                               12631      12454      -177 (-1.40%)         278         261     -17 (-6.12%)\ntw_xdp_dump                                   266        209      -57 (-21.43%)           9           8     -1 (-11.11%)\n\nCILIUM\n=========\nFile           Program                           Insns (A)  Insns (B)  Insns     (DIFF)  States (A)  States (B)  States  (DIFF)\n-------------  --------------------------------  ---------  ---------  ----------------  ----------  ----------  --------------\nbpf_host.o     cil_to_netdev                          6047       4578   -1469 (-24.29%)         362         249  -113 (-31.22%)\nbpf_host.o     handle_lxc_traffic                     2227       1585    -642 (-28.83%)         156         103   -53 (-33.97%)\nbpf_host.o     tail_handle_ipv4_from_netdev           2244       1458    -786 (-35.03%)         163         106   -57 (-34.97%)\nbpf_host.o     tail_handle_nat_fwd_ipv4              21022      10479  -10543 (-50.15%)        1289         670  -619 (-48.02%)\nbpf_host.o     tail_handle_nat_fwd_ipv6              15433      11375   -4058 (-26.29%)         905         643  -262 (-28.95%)\nbpf_host.o     tail_ipv4_host_policy_ingress          2219       1367    -852 (-38.40%)         161          96   -65 (-40.37%)\nbpf_host.o     tail_nodeport_nat_egress_ipv4         22460      19862   -2598 (-11.57%)        1469        1293  -176 (-11.98%)\nbpf_host.o     tail_nodeport_nat_ingress_ipv4         5526       3534   -1992 (-36.05%)         366         243  -123 (-33.61%)\nbpf_host.o     tail_nodeport_nat_ingress_ipv6         5132       4256    -876 (-17.07%)         241         219    -22 (-9.13%)\nbpf_host.o     tail_nodeport_nat_ipv6_egress          3702       3542     -160 (-4.32%)         215         205    -10 (-4.65%)\nbpf_lxc.o      tail_handle_nat_fwd_ipv4              21022      10479  -10543 (-50.15%)        1289         670  -619 (-48.02%)\nbpf_lxc.o      tail_handle_nat_fwd_ipv6              15433      11375   -4058 (-26.29%)         905         643  -262 (-28.95%)\nbpf_lxc.o      tail_ipv4_ct_egress                    5073       3374   -1699 (-33.49%)         262         172   -90 (-34.35%)\nbpf_lxc.o      tail_ipv4_ct_ingress                   5093       3385   -1708 (-33.54%)         262         172   -90 (-34.35%)\nbpf_lxc.o      tail_ipv4_ct_ingress_policy_only       5093       3385   -1708 (-33.54%)         262         172   -90 (-34.35%)\nbpf_lxc.o      tail_ipv6_ct_egress                    4593       3878    -715 (-15.57%)         194         151   -43 (-22.16%)\nbpf_lxc.o      tail_ipv6_ct_ingress                   4606       3891    -715 (-15.52%)         194         151   -43 (-22.16%)\nbpf_lxc.o      tail_ipv6_ct_ingress_policy_only       4606       3891    -715 (-15.52%)         194         151   -43 (-22.16%)\nbpf_lxc.o      tail_nodeport_nat_ingress_ipv4         5526       3534   -1992 (-36.05%)         366         243  -123 (-33.61%)\nbpf_lxc.o      tail_nodeport_nat_ingress_ipv6         5132       4256    -876 (-17.07%)         241         219    -22 (-9.13%)\nbpf_overlay.o  tail_handle_nat_fwd_ipv4              20524      10114  -10410 (-50.72%)        1271         638  -633 (-49.80%)\nbpf_overlay.o  tail_nodeport_nat_egress_ipv4         22718      19490   -3228 (-14.21%)        1475        1275  -200 (-13.56%)\nbpf_overlay.o  tail_nodeport_nat_ingress_ipv4         5526       3534   -1992 (-36.05%)         366         243  -123 (-33.61%)\nbpf_overlay.o  tail_nodeport_nat_ingress_ipv6         5132       4256    -876 (-17.07%)         241         219    -22 (-9.13%)\nbpf_overlay.o  tail_nodeport_nat_ipv6_egress          3638       3548      -90 (-2.47%)         209         203     -6 (-2.87%)\nbpf_overlay.o  tail_rev_nodeport_lb4                  4368       3820    -548 (-12.55%)         248         215   -33 (-13.31%)\nbpf_overlay.o  tail_rev_nodeport_lb6                  2867       2428    -439 (-15.31%)         167         140   -27 (-16.17%)\nbpf_sock.o     cil_sock6_connect                      1718       1703      -15 (-0.87%)         100          99     -1 (-1.00%)\nbpf_xdp.o      tail_handle_nat_fwd_ipv4              12917      12443     -474 (-3.67%)         875         849    -26 (-2.97%)\nbpf_xdp.o      tail_handle_nat_fwd_ipv6              13515      13264     -251 (-1.86%)         715         702    -13 (-1.82%)\nbpf_xdp.o      tail_lb_ipv4                          39492      36367    -3125 (-7.91%)        2430        2251   -179 (-7.37%)\nbpf_xdp.o      tail_lb_ipv6                          80441      78058    -2383 (-2.96%)        3647        3523   -124 (-3.40%)\nbpf_xdp.o      tail_nodeport_ipv6_dsr                 1038        901    -137 (-13.20%)          61          55     -6 (-9.84%)\nbpf_xdp.o      tail_nodeport_nat_egress_ipv4         13027      12096     -931 (-7.15%)         868         809    -59 (-6.80%)\nbpf_xdp.o      tail_nodeport_nat_ingress_ipv4         7617       5900   -1717 (-22.54%)         522         413  -109 (-20.88%)\nbpf_xdp.o      tail_nodeport_nat_ingress_ipv6         7575       7395     -180 (-2.38%)         383         374     -9 (-2.35%)\nbpf_xdp.o      tail_rev_nodeport_lb4                  6808       6739      -69 (-1.01%)         403         396     -7 (-1.74%)\nbpf_xdp.o      tail_rev_nodeport_lb6                 16173      15847     -326 (-2.02%)        1010         990    -20 (-1.98%)\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-05 13:40:21 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "064e0bea19b356c5d5f48a4549d80a3c03ce898b",
          "subject": "selftests/bpf: validate precision logic in partial_stack_load_preserves_zeros",
          "message": "Enhance partial_stack_load_preserves_zeros subtest with detailed\nprecision propagation log checks. We know expect fp-16 to be spilled,\ninitially imprecise, zero const register, which is later marked as\nprecise even when partial stack slot load is performed, even if it's not\na register fill (!).\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-10-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-05 13:40:21 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "ce3c49da11d77aa7d53cd549d308eb5f7fed8576",
      "merge_subject": "Merge branch 'bpf-fix-the-release-of-inner-map'",
      "merge_body": "Hou Tao says:\n\n====================\nbpf: Fix the release of inner map\n\nFrom: Hou Tao <houtao1@huawei.com>\n\nHi,\n\nThe patchset aims to fix the release of inner map in map array or map\nhtab. The release of inner map is different with normal map. For normal\nmap, the map is released after the bpf program which uses the map is\ndestroyed, because the bpf program tracks the used maps. However bpf\nprogram can not track the used inner map because these inner map may be\nupdated or deleted dynamically, and for now the ref-counter of inner map\nis decreased after the inner map is remove from outer map, so the inner\nmap may be freed before the bpf program, which is accessing the inner\nmap, exits and there will be use-after-free problem as demonstrated by\npatch #6.\n\nThe patchset fixes the problem by deferring the release of inner map.\nThe freeing of inner map is deferred according to the sleepable\nattributes of the bpf programs which own the outer map. Patch #1 fixes\nthe warning when running the newly-added selftest under interpreter\nmode. Patch #2 adds more parameters to .map_fd_put_ptr() to prepare for\nthe fix. Patch #3 fixes the incorrect value of need_defer when freeing\nthe fd array. Patch #4 fixes the potential use-after-free problem by\nusing call_rcu_tasks_trace() and call_rcu() to wait for one tasks trace\nRCU GP and one RCU GP unconditionally. Patch #5 optimizes the free of\ninner map by removing the unnecessary RCU GP waiting. Patch #6 adds a\nselftest to demonstrate the potential use-after-free problem. Patch #7\nupdates a selftest to update outer map in syscall bpf program.\n\nPlease see individual patches for more details. And comments are always\nwelcome.\n\nChange Log:\nv5:\n * patch #3: rename fd_array_map_delete_elem_with_deferred_free() to\n             __fd_array_map_delete_elem() (Alexei)\n * patch #5: use atomic64_t instead of atomic_t to prevent potential\n             overflow (Alexei)\n * patch #7: use ptr_to_u64() helper instead of force casting to initialize\n             pointers in bpf_attr (Alexei)\n\nv4: https://lore.kernel.org/bpf/20231130140120.1736235-1-houtao@huaweicloud.com\n  * patch #2: don't use \"deferred\", use \"need_defer\" uniformly\n  * patch #3: newly-added, fix the incorrect value of need_defer during\n              fd array free.\n  * patch #4: doesn't consider the case in which bpf map is not used by\n              any bpf program and only use sleepable_refcnt to remove\n\t      unnecessary tasks trace RCU GP (Alexei)\n  * patch #4: remove memory barriers added due to cautiousness (Alexei)\n\nv3: https://lore.kernel.org/bpf/20231124113033.503338-1-houtao@huaweicloud.com\n  * multiple variable renamings (Martin)\n  * define BPF_MAP_RCU_GP/BPF_MAP_RCU_TT_GP as bit (Martin)\n  * use call_rcu() and its variants instead of synchronize_rcu() (Martin)\n  * remove unnecessary mask in bpf_map_free_deferred() (Martin)\n  * place atomic_or() and the related smp_mb() together (Martin)\n  * add patch #6 to demonstrate that updating outer map in syscall\n    program is dead-lock free (Alexei)\n  * update comments about the memory barrier in bpf_map_fd_put_ptr()\n  * update commit message for patch #3 and #4 to describe more details\n\nv2: https://lore.kernel.org/bpf/20231113123324.3914612-1-houtao@huaweicloud.com\n  * defer the invocation of ops->map_free() instead of bpf_map_put() (Martin)\n  * update selftest to make it being reproducible under JIT mode (Martin)\n  * remove unnecessary preparatory patches\n\nv1: https://lore.kernel.org/bpf/20231107140702.1891778-1-houtao@huaweicloud.com\n====================\n\nLink: https://lore.kernel.org/r/20231204140425.1480317-1-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-12-04 17:50:27 -0800",
      "commits": [
        {
          "hash": "169410eba271afc9f0fb476d996795aa26770c6d",
          "subject": "bpf: Check rcu_read_lock_trace_held() before calling bpf map helpers",
          "message": "These three bpf_map_{lookup,update,delete}_elem() helpers are also\navailable for sleepable bpf program, so add the corresponding lock\nassertion for sleepable bpf program, otherwise the following warning\nwill be reported when a sleepable bpf program manipulates bpf map under\ninterpreter mode (aka bpf_jit_enable=0):\n\n  WARNING: CPU: 3 PID: 4985 at kernel/bpf/helpers.c:40 ......\n  CPU: 3 PID: 4985 Comm: test_progs Not tainted 6.6.0+ #2\n  Hardware name: QEMU Standard PC (i440FX + PIIX, 1996) ......\n  RIP: 0010:bpf_map_lookup_elem+0x54/0x60\n  ......\n  Call Trace:\n   <TASK>\n   ? __warn+0xa5/0x240\n   ? bpf_map_lookup_elem+0x54/0x60\n   ? report_bug+0x1ba/0x1f0\n   ? handle_bug+0x40/0x80\n   ? exc_invalid_op+0x18/0x50\n   ? asm_exc_invalid_op+0x1b/0x20\n   ? __pfx_bpf_map_lookup_elem+0x10/0x10\n   ? rcu_lockdep_current_cpu_online+0x65/0xb0\n   ? rcu_is_watching+0x23/0x50\n   ? bpf_map_lookup_elem+0x54/0x60\n   ? __pfx_bpf_map_lookup_elem+0x10/0x10\n   ___bpf_prog_run+0x513/0x3b70\n   __bpf_prog_run32+0x9d/0xd0\n   ? __bpf_prog_enter_sleepable_recur+0xad/0x120\n   ? __bpf_prog_enter_sleepable_recur+0x3e/0x120\n   bpf_trampoline_6442580665+0x4d/0x1000\n   __x64_sys_getpgid+0x5/0x30\n   ? do_syscall_64+0x36/0xb0\n   entry_SYSCALL_64_after_hwframe+0x6e/0x76\n   </TASK>\n\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231204140425.1480317-2-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hou Tao <houtao1@huawei.com>",
          "date": "2023-12-04 17:50:26 -0800",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "20c20bd11a0702ce4dc9300c3da58acf551d9725",
          "subject": "bpf: Add map and need_defer parameters to .map_fd_put_ptr()",
          "message": "map is the pointer of outer map, and need_defer needs some explanation.\nneed_defer tells the implementation to defer the reference release of\nthe passed element and ensure that the element is still alive before\nthe bpf program, which may manipulate it, exits.\n\nThe following three cases will invoke map_fd_put_ptr() and different\nneed_defer values will be passed to these callers:\n\n1) release the reference of the old element in the map during map update\n   or map deletion. The release must be deferred, otherwise the bpf\n   program may incur use-after-free problem, so need_defer needs to be\n   true.\n2) release the reference of the to-be-added element in the error path of\n   map update. The to-be-added element is not visible to any bpf\n   program, so it is OK to pass false for need_defer parameter.\n3) release the references of all elements in the map during map release.\n   Any bpf program which has access to the map must have been exited and\n   released, so need_defer=false will be OK.\n\nThese two parameters will be used by the following patches to fix the\npotential use-after-free problem for map-in-map.\n\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231204140425.1480317-3-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hou Tao <houtao1@huawei.com>",
          "date": "2023-12-04 17:50:26 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/arraymap.c",
            "kernel/bpf/hashtab.c",
            "kernel/bpf/map_in_map.c",
            "kernel/bpf/map_in_map.h"
          ]
        },
        {
          "hash": "79d93b3c6ffd79abcd8e43345980aa1e904879c4",
          "subject": "bpf: Set need_defer as false when clearing fd array during map free",
          "message": "Both map deletion operation, map release and map free operation use\nfd_array_map_delete_elem() to remove the element from fd array and\nneed_defer is always true in fd_array_map_delete_elem(). For the map\ndeletion operation and map release operation, need_defer=true is\nnecessary, because the bpf program, which accesses the element in fd\narray, may still alive. However for map free operation, it is certain\nthat the bpf program which owns the fd array has already been exited, so\nsetting need_defer as false is appropriate for map free operation.\n\nSo fix it by adding need_defer parameter to bpf_fd_array_map_clear() and\nadding a new helper __fd_array_map_delete_elem() to handle the map\ndeletion, map release and map free operations correspondingly.\n\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231204140425.1480317-4-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hou Tao <houtao1@huawei.com>",
          "date": "2023-12-04 17:50:26 -0800",
          "modified_files": [
            "kernel/bpf/arraymap.c"
          ]
        },
        {
          "hash": "876673364161da50eed6b472d746ef88242b2368",
          "subject": "bpf: Defer the free of inner map when necessary",
          "message": "When updating or deleting an inner map in map array or map htab, the map\nmay still be accessed by non-sleepable program or sleepable program.\nHowever bpf_map_fd_put_ptr() decreases the ref-counter of the inner map\ndirectly through bpf_map_put(), if the ref-counter is the last one\n(which is true for most cases), the inner map will be freed by\nops->map_free() in a kworker. But for now, most .map_free() callbacks\ndon't use synchronize_rcu() or its variants to wait for the elapse of a\nRCU grace period, so after the invocation of ops->map_free completes,\nthe bpf program which is accessing the inner map may incur\nuse-after-free problem.\n\nFix the free of inner map by invoking bpf_map_free_deferred() after both\none RCU grace period and one tasks trace RCU grace period if the inner\nmap has been removed from the outer map before. The deferment is\naccomplished by using call_rcu() or call_rcu_tasks_trace() when\nreleasing the last ref-counter of bpf map. The newly-added rcu_head\nfield in bpf_map shares the same storage space with work field to\nreduce the size of bpf_map.\n\nFixes: bba1dc0b55ac (\"bpf: Remove redundant synchronize_rcu.\")\nFixes: 638e4b825d52 (\"bpf: Allows per-cpu maps and map-in-map in sleepable programs\")\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231204140425.1480317-5-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hou Tao <houtao1@huawei.com>",
          "date": "2023-12-04 17:50:26 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/map_in_map.c",
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "af66bfd3c8538ed21cf72af18426fc4a408665cf",
          "subject": "bpf: Optimize the free of inner map",
          "message": "When removing the inner map from the outer map, the inner map will be\nfreed after one RCU grace period and one RCU tasks trace grace\nperiod, so it is certain that the bpf program, which may access the\ninner map, has exited before the inner map is freed.\n\nHowever there is no need to wait for one RCU tasks trace grace period if\nthe outer map is only accessed by non-sleepable program. So adding\nsleepable_refcnt in bpf_map and increasing sleepable_refcnt when adding\nthe outer map into env->used_maps for sleepable program. Although the\nmax number of bpf program is INT_MAX - 1, the number of bpf programs\nwhich are being loaded may be greater than INT_MAX, so using atomic64_t\ninstead of atomic_t for sleepable_refcnt. When removing the inner map\nfrom the outer map, using sleepable_refcnt to decide whether or not a\nRCU tasks trace grace period is needed before freeing the inner map.\n\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231204140425.1480317-6-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hou Tao <houtao1@huawei.com>",
          "date": "2023-12-04 17:50:26 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/core.c",
            "kernel/bpf/map_in_map.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "1624918be84a8bcc4f592e55635bc4fe4a96460a",
          "subject": "selftests/bpf: Add test cases for inner map",
          "message": "Add test cases to test the race between the destroy of inner map due to\nmap-in-map update and the access of inner map in bpf program. The\nfollowing 4 combinations are added:\n(1) array map in map array + bpf program\n(2) array map in map array + sleepable bpf program\n(3) array map in map htab + bpf program\n(4) array map in map htab + sleepable bpf program\n\nBefore applying the fixes, when running `./test_prog -a map_in_map`, the\nfollowing error was reported:\n\n  ==================================================================\n  BUG: KASAN: slab-use-after-free in array_map_update_elem+0x48/0x3e0\n  Read of size 4 at addr ffff888114f33824 by task test_progs/1858\n\n  CPU: 1 PID: 1858 Comm: test_progs Tainted: G           O     6.6.0+ #7\n  Hardware name: QEMU Standard PC (i440FX + PIIX, 1996) ......\n  Call Trace:\n   <TASK>\n   dump_stack_lvl+0x4a/0x90\n   print_report+0xd2/0x620\n   kasan_report+0xd1/0x110\n   __asan_load4+0x81/0xa0\n   array_map_update_elem+0x48/0x3e0\n   bpf_prog_be94a9f26772f5b7_access_map_in_array+0xe6/0xf6\n   trace_call_bpf+0x1aa/0x580\n   kprobe_perf_func+0xdd/0x430\n   kprobe_dispatcher+0xa0/0xb0\n   kprobe_ftrace_handler+0x18b/0x2e0\n   0xffffffffc02280f7\n  RIP: 0010:__x64_sys_getpgid+0x1/0x30\n  ......\n   </TASK>\n\n  Allocated by task 1857:\n   kasan_save_stack+0x26/0x50\n   kasan_set_track+0x25/0x40\n   kasan_save_alloc_info+0x1e/0x30\n   __kasan_kmalloc+0x98/0xa0\n   __kmalloc_node+0x6a/0x150\n   __bpf_map_area_alloc+0x141/0x170\n   bpf_map_area_alloc+0x10/0x20\n   array_map_alloc+0x11f/0x310\n   map_create+0x28a/0xb40\n   __sys_bpf+0x753/0x37c0\n   __x64_sys_bpf+0x44/0x60\n   do_syscall_64+0x36/0xb0\n   entry_SYSCALL_64_after_hwframe+0x6e/0x76\n\n  Freed by task 11:\n   kasan_save_stack+0x26/0x50\n   kasan_set_track+0x25/0x40\n   kasan_save_free_info+0x2b/0x50\n   __kasan_slab_free+0x113/0x190\n   slab_free_freelist_hook+0xd7/0x1e0\n   __kmem_cache_free+0x170/0x260\n   kfree+0x9b/0x160\n   kvfree+0x2d/0x40\n   bpf_map_area_free+0xe/0x20\n   array_map_free+0x120/0x2c0\n   bpf_map_free_deferred+0xd7/0x1e0\n   process_one_work+0x462/0x990\n   worker_thread+0x370/0x670\n   kthread+0x1b0/0x200\n   ret_from_fork+0x3a/0x70\n   ret_from_fork_asm+0x1b/0x30\n\n  Last potentially related work creation:\n   kasan_save_stack+0x26/0x50\n   __kasan_record_aux_stack+0x94/0xb0\n   kasan_record_aux_stack_noalloc+0xb/0x20\n   __queue_work+0x331/0x950\n   queue_work_on+0x75/0x80\n   bpf_map_put+0xfa/0x160\n   bpf_map_fd_put_ptr+0xe/0x20\n   bpf_fd_array_map_update_elem+0x174/0x1b0\n   bpf_map_update_value+0x2b7/0x4a0\n   __sys_bpf+0x2551/0x37c0\n   __x64_sys_bpf+0x44/0x60\n   do_syscall_64+0x36/0xb0\n   entry_SYSCALL_64_after_hwframe+0x6e/0x76\n\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231204140425.1480317-7-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hou Tao <houtao1@huawei.com>",
          "date": "2023-12-04 17:50:27 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/map_in_map.c",
            "tools/testing/selftests/bpf/progs/access_map_in_map.c"
          ]
        },
        {
          "hash": "e3dd40828534a67931e0dd00fcd35846271fd4e8",
          "subject": "selftests/bpf: Test outer map update operations in syscall program",
          "message": "Syscall program is running with rcu_read_lock_trace being held, so if\nbpf_map_update_elem() or bpf_map_delete_elem() invokes\nsynchronize_rcu_tasks_trace() when operating on an outer map, there will\nbe dead-lock, so add a test to guarantee that it is dead-lock free.\n\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231204140425.1480317-8-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hou Tao <houtao1@huawei.com>",
          "date": "2023-12-04 17:50:27 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/syscall.c",
            "tools/testing/selftests/bpf/progs/syscall.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "90679706d486d3cb202d1b377a230f1f22edaf00",
      "merge_subject": "Merge branch 'bpf-verifier-retval-logic-fixes'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nBPF verifier retval logic fixes\n\nThis patch set fixes BPF verifier logic around validating and enforcing return\nvalues for BPF programs that have specific range of expected return values.\nBoth sync and async callbacks have similar logic and are fixes as well.\nA few tests are added that would fail without the fixes in this patch set.\n\nAlso, while at it, we update retval checking logic to use smin/smax range\ninstead of tnum, avoiding future potential issues if expected range cannot be\nrepresented precisely by tnum (e.g., [0, 2] is not representable by tnum and\nis treated as [0, 3]).\n\nThere is a little bit of refactoring to unify async callback and program exit\nlogic to avoid duplication of checks as much as possible.\n\nv4->v5:\n  - fix timer_bad_ret test on no-alu32 flavor (CI);\nv3->v4:\n  - add back bpf_func_state rearrangement patch;\n  - simplified patch #4 as suggested (Shung-Hsi);\nv2->v3:\n  - more carefullly switch from umin/umax to smin/smax;\nv1->v2:\n  - drop tnum from retval checks (Eduard);\n  - use smin/smax instead of umin/umax (Alexei).\n====================\n\nLink: https://lore.kernel.org/r/20231202175705.885270-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-12-02 11:36:51 -0800",
      "commits": [
        {
          "hash": "45b5623f2d721c25d1a2fdc8c4600fb4b7b61c75",
          "subject": "bpf: rearrange bpf_func_state fields to save a bit of memory",
          "message": "It's a trivial rearrangement saving 8 bytes. We have 4 bytes of padding\nat the end which can be filled with another field without increasing\nstruct bpf_func_state.\n\ncopy_func_state() logic remains correct without any further changes.\n\nBEFORE\n======\nstruct bpf_func_state {\n        struct bpf_reg_state       regs[11];             /*     0  1320 */\n        /* --- cacheline 20 boundary (1280 bytes) was 40 bytes ago --- */\n        int                        callsite;             /*  1320     4 */\n        u32                        frameno;              /*  1324     4 */\n        u32                        subprogno;            /*  1328     4 */\n        u32                        async_entry_cnt;      /*  1332     4 */\n        bool                       in_callback_fn;       /*  1336     1 */\n\n        /* XXX 7 bytes hole, try to pack */\n\n        /* --- cacheline 21 boundary (1344 bytes) --- */\n        struct tnum                callback_ret_range;   /*  1344    16 */\n        bool                       in_async_callback_fn; /*  1360     1 */\n        bool                       in_exception_callback_fn; /*  1361     1 */\n\n        /* XXX 2 bytes hole, try to pack */\n\n        int                        acquired_refs;        /*  1364     4 */\n        struct bpf_reference_state * refs;               /*  1368     8 */\n        int                        allocated_stack;      /*  1376     4 */\n\n        /* XXX 4 bytes hole, try to pack */\n\n        struct bpf_stack_state *   stack;                /*  1384     8 */\n\n        /* size: 1392, cachelines: 22, members: 13 */\n        /* sum members: 1379, holes: 3, sum holes: 13 */\n        /* last cacheline: 48 bytes */\n};\n\nAFTER\n=====\nstruct bpf_func_state {\n        struct bpf_reg_state       regs[11];             /*     0  1320 */\n        /* --- cacheline 20 boundary (1280 bytes) was 40 bytes ago --- */\n        int                        callsite;             /*  1320     4 */\n        u32                        frameno;              /*  1324     4 */\n        u32                        subprogno;            /*  1328     4 */\n        u32                        async_entry_cnt;      /*  1332     4 */\n        struct tnum                callback_ret_range;   /*  1336    16 */\n        /* --- cacheline 21 boundary (1344 bytes) was 8 bytes ago --- */\n        bool                       in_callback_fn;       /*  1352     1 */\n        bool                       in_async_callback_fn; /*  1353     1 */\n        bool                       in_exception_callback_fn; /*  1354     1 */\n\n        /* XXX 1 byte hole, try to pack */\n\n        int                        acquired_refs;        /*  1356     4 */\n        struct bpf_reference_state * refs;               /*  1360     8 */\n        struct bpf_stack_state *   stack;                /*  1368     8 */\n        int                        allocated_stack;      /*  1376     4 */\n\n        /* size: 1384, cachelines: 22, members: 13 */\n        /* sum members: 1379, holes: 1, sum holes: 1 */\n        /* padding: 4 */\n        /* last cacheline: 40 bytes */\n};\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-02 11:36:50 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h"
          ]
        },
        {
          "hash": "5fad52bee30414270104525e3a0266327a6e9d11",
          "subject": "bpf: provide correct register name for exception callback retval check",
          "message": "bpf_throw() is checking R1, so let's report R1 in the log.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-02 11:36:50 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/exceptions_assert.c",
            "tools/testing/selftests/bpf/progs/exceptions_fail.c"
          ]
        },
        {
          "hash": "0acd03a5bd188b0c501d285d938439618bd855c4",
          "subject": "bpf: enforce precision of R0 on callback return",
          "message": "Given verifier checks actual value, r0 has to be precise, so we need to\npropagate precision properly. r0 also has to be marked as read,\notherwise subsequent state comparisons will ignore such register as\nunimportant and precision won't really help here.\n\nFixes: 69c087ba6225 (\"bpf: Add bpf_for_each_map_elem() helper\")\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-02 11:36:50 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "8fa4ecd49b81ccd9d1d87f1c8b2260e218644878",
          "subject": "bpf: enforce exact retval range on subprog/callback exit",
          "message": "Instead of relying on potentially imprecise tnum representation of\nexpected return value range for callbacks and subprogs, validate that\nsmin/smax range satisfy exact expected range of return values.\n\nE.g., if callback would need to return [0, 2] range, tnum can't\nrepresent this precisely and instead will allow [0, 3] range. By\nchecking smin/smax range, we can make sure that subprog/callback indeed\nreturns only valid [0, 2] range.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-02 11:36:50 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "60a6b2c78c62d0a99ccb7ad5edc950f79e56306a",
          "subject": "selftests/bpf: add selftest validating callback result is enforced",
          "message": "BPF verifier expects callback subprogs to return values from specified\nrange (typically [0, 1]). This requires that r0 at exit is both precise\n(because we rely on specific value range) and is marked as read\n(otherwise state comparison will ignore such register as unimportant).\n\nAdd a simple test that validates that all these conditions are enforced.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-02 11:36:50 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c"
          ]
        },
        {
          "hash": "c871d0e00f0e8c207ce8ff89025e35cc49a8a3c3",
          "subject": "bpf: enforce precise retval range on program exit",
          "message": "Similarly to subprog/callback logic, enforce return value of BPF program\nusing more precise smin/smax range.\n\nWe need to adjust a bunch of tests due to a changed format of an error\nmessage.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-02 11:36:50 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/exceptions_assert.c",
            "tools/testing/selftests/bpf/progs/exceptions_fail.c",
            "tools/testing/selftests/bpf/progs/test_global_func15.c",
            "tools/testing/selftests/bpf/progs/timer_failure.c",
            "tools/testing/selftests/bpf/progs/user_ringbuf_fail.c",
            "tools/testing/selftests/bpf/progs/verifier_cgroup_inv_retcode.c",
            "tools/testing/selftests/bpf/progs/verifier_netfilter_retcode.c",
            "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c"
          ]
        },
        {
          "hash": "0ef24c8dfae24a4b8aa2e92eac20faecdc5502e5",
          "subject": "bpf: unify async callback and program retval checks",
          "message": "Use common logic to verify program return values and async callback\nreturn values. This allows to avoid duplication of any extra steps\nnecessary, like precision marking, which will be added in the next\npatch.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-02 11:36:50 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "eabe518de533a4291996020977054a7a7b78c7d3",
          "subject": "bpf: enforce precision of R0 on program/async callback return",
          "message": "Given we enforce a valid range for program and async callback return\nvalue, we must mark R0 as precise to avoid incorrect state pruning.\n\nFixes: b5dc0163d8fd (\"bpf: precise scalar_value tracking\")\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-02 11:36:51 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "e02dea158ddaebe6e725be715e0009923b96ec8e",
          "subject": "selftests/bpf: validate async callback return value check correctness",
          "message": "Adjust timer/timer_ret_1 test to validate more carefully verifier logic\nof enforcing async callback return value. This test will pass only if\nreturn result is marked precise and read.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-10-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-02 11:36:51 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/timer_failure.c"
          ]
        },
        {
          "hash": "5c19e1d05e9e71b42d8e779f41959254239709da",
          "subject": "selftests/bpf: adjust global_func15 test to validate prog exit precision",
          "message": "Add one more subtest to  global_func15 selftest to validate that\nverifier properly marks r0 as precise and avoids erroneous state pruning\nof the branch that has return value outside of expected [0, 1] value.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-11-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-02 11:36:51 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/test_global_func15.c"
          ]
        },
        {
          "hash": "81eff2e36481c5cf4a2ac906ae56c3fbd3e6f305",
          "subject": "bpf: simplify tnum output if a fully known constant",
          "message": "Emit tnum representation as just a constant if all bits are known.\nUse decimal-vs-hex logic to determine exact format of emitted\nconstant value, just like it's done for register range values.\nFor that move tnum_strn() to kernel/bpf/log.c to reuse decimal-vs-hex\ndetermination logic and constants.\n\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-12-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-12-02 11:36:51 -0800",
          "modified_files": [
            "kernel/bpf/log.c",
            "kernel/bpf/tnum.c",
            "tools/testing/selftests/bpf/progs/verifier_direct_packet_access.c",
            "tools/testing/selftests/bpf/progs/verifier_int_ptr.c",
            "tools/testing/selftests/bpf/progs/verifier_stack_ptr.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "acb12c859ac7c36d6d7632280fd1e263188cb07f",
      "merge_subject": "Merge branch 'verify-callbacks-as-if-they-are-called-unknown-number-of-times'",
      "merge_body": "Eduard Zingerman says:\n\n====================\nverify callbacks as if they are called unknown number of times\n\nThis series updates verifier logic for callback functions handling.\nCurrent master simulates callback body execution exactly once,\nwhich leads to verifier not detecting unsafe programs like below:\n\n    static int unsafe_on_zero_iter_cb(__u32 idx, struct num_context *ctx)\n    {\n        ctx->i = 0;\n        return 0;\n    }\n\n    SEC(\"?raw_tp\")\n    int unsafe_on_zero_iter(void *unused)\n    {\n        struct num_context loop_ctx = { .i = 32 };\n        __u8 choice_arr[2] = { 0, 1 };\n\n        bpf_loop(100, unsafe_on_zero_iter_cb, &loop_ctx, 0);\n        return choice_arr[loop_ctx.i];\n    }\n\nThis was reported previously in [0].\nThe basic idea of the fix is to schedule callback entry state for\nverification in env->head until some identical, previously visited\nstate in current DFS state traversal is found. Same logic as with open\ncoded iterators, and builds on top recent fixes [1] for those.\n\nThe series is structured as follows:\n- patches #1,2,3 update strobemeta, xdp_synproxy selftests and\n  bpf_loop_bench benchmark to allow convergence of the bpf_loop\n  callback states;\n- patches #4,5 just shuffle the code a bit;\n- patch #6 is the main part of the series;\n- patch #7 adds test cases for #6;\n- patch #8 extend patch #6 with same speculative scalar widening\n  logic, as used for open coded iterators;\n- patch #9 adds test cases for #8;\n- patch #10 extends patch #6 to track maximal number of callback\n  executions specifically for bpf_loop();\n- patch #11 adds test cases for #10.\n\nVeristat results comparing this series to master+patches #1,2,3 using selftests\nshow the following difference:\n\nFile                       Program        States (A)  States (B)  States (DIFF)\n-------------------------  -------------  ----------  ----------  -------------\nbpf_loop_bench.bpf.o       benchmark               1           2  +1 (+100.00%)\npyperf600_bpf_loop.bpf.o   on_event              322         407  +85 (+26.40%)\nstrobemeta_bpf_loop.bpf.o  on_event              113         151  +38 (+33.63%)\nxdp_synproxy_kern.bpf.o    syncookie_tc          341         291  -50 (-14.66%)\nxdp_synproxy_kern.bpf.o    syncookie_xdp         344         301  -43 (-12.50%)\n\nVeristat results comparing this series to master using Tetragon BPF\nfiles [2] also show some differences.\nStates diff varies from +2% to +15% on 23 programs out of 186,\nno new failures.\n\nChangelog:\n- V3 [5] -> V4, changes suggested by Andrii:\n  - validate mark_chain_precision() result in patch #10;\n  - renaming s/cumulative_callback_depth/callback_unroll_depth/.\n- V2 [4] -> V3:\n  - fixes in expected log messages for test cases:\n    - callback_result_precise;\n    - parent_callee_saved_reg_precise_with_callback;\n    - parent_stack_slot_precise_with_callback;\n  - renamings (suggested by Alexei):\n    - s/callback_iter_depth/cumulative_callback_depth/\n    - s/is_callback_iter_next/calls_callback/\n    - s/mark_callback_iter_next/mark_calls_callback/\n  - prepare_func_exit() updated to exit with -EFAULT when\n    callee->in_callback_fn is true but calls_callback() is not true\n    for callsite;\n  - test case 'bpf_loop_iter_limit_nested' rewritten to use return\n    value check instead of verifier log message checks\n    (suggested by Alexei).\n- V1 [3] -> V2, changes suggested by Andrii:\n  - small changes for error handling code in __check_func_call();\n  - callback body processing log is now matched in relevant\n    verifier_subprog_precision.c tests;\n  - R1 passed to bpf_loop() is now always marked as precise;\n  - log level 2 message for bpf_loop() iteration termination instead of\n    iteration depth messages;\n  - __no_msg macro removed;\n  - bpf_loop_iter_limit_nested updated to avoid using __no_msg;\n  - commit message for patch #3 updated according to Alexei's request.\n\n[0] https://lore.kernel.org/bpf/CA+vRuzPChFNXmouzGG+wsy=6eMcfr1mFG0F3g7rbg-sedGKW3w@mail.gmail.com/\n[1] https://lore.kernel.org/bpf/20231024000917.12153-1-eddyz87@gmail.com/\n[2] git@github.com:cilium/tetragon.git\n[3] https://lore.kernel.org/bpf/20231116021803.9982-1-eddyz87@gmail.com/T/#t\n[4] https://lore.kernel.org/bpf/20231118013355.7943-1-eddyz87@gmail.com/T/#t\n[5] https://lore.kernel.org/bpf/20231120225945.11741-1-eddyz87@gmail.com/T/#t\n====================\n\nLink: https://lore.kernel.org/r/20231121020701.26440-1-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-11-20 18:36:41 -0800",
      "commits": [
        {
          "hash": "977bc146d4eb7070118d8a974919b33bb52732b4",
          "subject": "selftests/bpf: track tcp payload offset as scalar in xdp_synproxy",
          "message": "This change prepares syncookie_{tc,xdp} for update in callbakcs\nverification logic. To allow bpf_loop() verification converge when\nmultiple callback itreations are considered:\n- track offset inside TCP payload explicitly, not as a part of the\n  pointer;\n- make sure that offset does not exceed MAX_PACKET_OFF enforced by\n  verifier;\n- make sure that offset is tracked as unbound scalar between\n  iterations, otherwise verifier won't be able infer that bpf_loop\n  callback reaches identical states.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-11-20 18:33:35 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/xdp_synproxy_kern.c"
          ]
        },
        {
          "hash": "87eb0152bcc102ecbda866978f4e54db5a3be1ef",
          "subject": "selftests/bpf: track string payload offset as scalar in strobemeta",
          "message": "This change prepares strobemeta for update in callbacks verification\nlogic. To allow bpf_loop() verification converge when multiple\ncallback iterations are considered:\n- track offset inside strobemeta_payload->payload directly as scalar\n  value;\n- at each iteration make sure that remaining\n  strobemeta_payload->payload capacity is sufficient for execution of\n  read_{map,str}_var functions;\n- make sure that offset is tracked as unbound scalar between\n  iterations, otherwise verifier won't be able infer that bpf_loop\n  callback reaches identical states.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-3-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-11-20 18:33:35 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/strobemeta.h"
          ]
        },
        {
          "hash": "f40bfd1679446b22d321e64a1fa98b7d07d2be08",
          "subject": "selftests/bpf: fix bpf_loop_bench for new callback verification scheme",
          "message": "This is a preparatory change. A follow-up patch \"bpf: verify callbacks\nas if they are called unknown number of times\" changes logic for\ncallbacks handling. While previously callbacks were verified as a\nsingle function call, new scheme takes into account that callbacks\ncould be executed unknown number of times.\n\nThis has dire implications for bpf_loop_bench:\n\n    SEC(\"fentry/\" SYS_PREFIX \"sys_getpgid\")\n    int benchmark(void *ctx)\n    {\n            for (int i = 0; i < 1000; i++) {\n                    bpf_loop(nr_loops, empty_callback, NULL, 0);\n                    __sync_add_and_fetch(&hits, nr_loops);\n            }\n            return 0;\n    }\n\nW/o callbacks change verifier sees it as a 1000 calls to\nempty_callback(). However, with callbacks change things become\nexponential:\n- i=0: state exploring empty_callback is scheduled with i=0 (a);\n- i=1: state exploring empty_callback is scheduled with i=1;\n  ...\n- i=999: state exploring empty_callback is scheduled with i=999;\n- state (a) is popped from stack;\n- i=1: state exploring empty_callback is scheduled with i=1;\n  ...\n\nAvoid this issue by rewriting outer loop as bpf_loop().\nUnfortunately, this adds a function call to a loop at runtime, which\nnegatively affects performance:\n\n            throughput               latency\n   before:  149.919 \u00b1 0.168 M ops/s, 6.670 ns/op\n   after :  137.040 \u00b1 0.187 M ops/s, 7.297 ns/op\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-11-20 18:33:35 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/bpf_loop_bench.c"
          ]
        },
        {
          "hash": "683b96f9606ab7308ffb23c46ab43cecdef8a241",
          "subject": "bpf: extract __check_reg_arg() utility function",
          "message": "Split check_reg_arg() into two utility functions:\n- check_reg_arg() operating on registers from current verifier state;\n- __check_reg_arg() operating on a specific set of registers passed as\n  a parameter;\n\nThe __check_reg_arg() function would be used by a follow-up change for\ncallbacks handling.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-5-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-11-20 18:33:35 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "58124a98cb8eda69d248d7f1de954c8b2767c945",
          "subject": "bpf: extract setup_func_entry() utility function",
          "message": "Move code for simulated stack frame creation to a separate utility\nfunction. This function would be used in the follow-up change for\ncallbacks handling.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-6-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-11-20 18:33:35 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "ab5cfac139ab8576fb54630d4cca23c3e690ee90",
          "subject": "bpf: verify callbacks as if they are called unknown number of times",
          "message": "Prior to this patch callbacks were handled as regular function calls,\nexecution of callback body was modeled exactly once.\nThis patch updates callbacks handling logic as follows:\n- introduces a function push_callback_call() that schedules callback\n  body verification in env->head stack;\n- updates prepare_func_exit() to reschedule callback body verification\n  upon BPF_EXIT;\n- as calls to bpf_*_iter_next(), calls to callback invoking functions\n  are marked as checkpoints;\n- is_state_visited() is updated to stop callback based iteration when\n  some identical parent state is found.\n\nPaths with callback function invoked zero times are now verified first,\nwhich leads to necessity to modify some selftests:\n- the following negative tests required adding release/unlock/drop\n  calls to avoid previously masked unrelated error reports:\n  - cb_refs.c:underflow_prog\n  - exceptions_fail.c:reject_rbtree_add_throw\n  - exceptions_fail.c:reject_with_cp_reference\n- the following precision tracking selftests needed change in expected\n  log trace:\n  - verifier_subprog_precision.c:callback_result_precise\n    (note: r0 precision is no longer propagated inside callback and\n           I think this is a correct behavior)\n  - verifier_subprog_precision.c:parent_callee_saved_reg_precise_with_callback\n  - verifier_subprog_precision.c:parent_stack_slot_precise_with_callback\n\nReported-by: Andrew Werner <awerner32@gmail.com>\nCloses: https://lore.kernel.org/bpf/CA+vRuzPChFNXmouzGG+wsy=6eMcfr1mFG0F3g7rbg-sedGKW3w@mail.gmail.com/\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-7-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-11-20 18:35:44 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/cb_refs.c",
            "tools/testing/selftests/bpf/progs/exceptions_fail.c",
            "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c"
          ]
        },
        {
          "hash": "958465e217dbf5fc6677d42d8827fb3073d86afd",
          "subject": "selftests/bpf: tests for iterating callbacks",
          "message": "A set of test cases to check behavior of callback handling logic,\ncheck if verifier catches the following situations:\n- program not safe on second callback iteration;\n- program not safe on zero callback iterations;\n- infinite loop inside a callback.\n\nVerify that callback logic works for bpf_loop, bpf_for_each_map_elem,\nbpf_user_ringbuf_drain, bpf_find_vma.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-8-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-11-20 18:36:40 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_iterating_callbacks.c"
          ]
        },
        {
          "hash": "cafe2c21508a38cdb3ed22708842e957b2572c3e",
          "subject": "bpf: widening for callback iterators",
          "message": "Callbacks are similar to open coded iterators, so add imprecise\nwidening logic for callback body processing. This makes callback based\nloops behave identically to open coded iterators, e.g. allowing to\nverify programs like below:\n\n  struct ctx { u32 i; };\n  int cb(u32 idx, struct ctx* ctx)\n  {\n          ++ctx->i;\n          return 0;\n  }\n  ...\n  struct ctx ctx = { .i = 0 };\n  bpf_loop(100, cb, &ctx, 0);\n  ...\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-9-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-11-20 18:36:40 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "9f3330aa644d6d979eb064c46e85c62d4b4eac75",
          "subject": "selftests/bpf: test widening for iterating callbacks",
          "message": "A test case to verify that imprecise scalars widening is applied to\ncallback entering state, when callback call is simulated repeatedly.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-10-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-11-20 18:36:40 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_iterating_callbacks.c"
          ]
        },
        {
          "hash": "bb124da69c47dd98d69361ec13244ece50bec63e",
          "subject": "bpf: keep track of max number of bpf_loop callback iterations",
          "message": "In some cases verifier can't infer convergence of the bpf_loop()\niteration. E.g. for the following program:\n\n    static int cb(__u32 idx, struct num_context* ctx)\n    {\n        ctx->i++;\n        return 0;\n    }\n\n    SEC(\"?raw_tp\")\n    int prog(void *_)\n    {\n        struct num_context ctx = { .i = 0 };\n        __u8 choice_arr[2] = { 0, 1 };\n\n        bpf_loop(2, cb, &ctx, 0);\n        return choice_arr[ctx.i];\n    }\n\nEach 'cb' simulation would eventually return to 'prog' and reach\n'return choice_arr[ctx.i]' statement. At which point ctx.i would be\nmarked precise, thus forcing verifier to track multitude of separate\nstates with {.i=0}, {.i=1}, ... at bpf_loop() callback entry.\n\nThis commit allows \"brute force\" handling for such cases by limiting\nnumber of callback body simulations using 'umax' value of the first\nbpf_loop() parameter.\n\nFor this, extend bpf_func_state with 'callback_depth' field.\nIncrement this field when callback visiting state is pushed to states\ntraversal stack. For frame #N it's 'callback_depth' field counts how\nmany times callback with frame depth N+1 had been executed.\nUse bpf_func_state specifically to allow independent tracking of\ncallback depths when multiple nested bpf_loop() calls are present.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-11-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-11-20 18:36:40 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c"
          ]
        },
        {
          "hash": "57e2a52deeb12ab84c15c6d0fb93638b5b94001b",
          "subject": "selftests/bpf: check if max number of bpf_loop iterations is tracked",
          "message": "Check that even if bpf_loop() callback simulation does not converge to\na specific state, verification could proceed via \"brute force\"\nsimulation of maximal number of callback calls.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-12-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-11-20 18:36:40 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_iterating_callbacks.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "16b3129e14bf2e7505512568b11c437c840a0c19",
      "merge_subject": "Merge branch 'bpf-verifier-log-improvements'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nBPF verifier log improvements\n\nThis patch set moves a big chunk of verifier log related code from gigantic\nverifier.c file into more focused kernel/bpf/log.c. This is not essential to\nthe rest of functionality in this patch set, so I can undo it, but it felt\nlike it's good to start chipping away from 20K+ verifier.c whenever we can.\n\nThe main purpose of the patch set, though, is in improving verifier log\nfurther.\n\nPatches #3-#4 start printing out register state even if that register is\nspilled into stack slot. Previously we'd get only spilled register type, but\nno additional information, like SCALAR_VALUE's ranges. Super limiting during\ndebugging. For cases of register spills smaller than 8 bytes, we also print\nout STACK_MISC/STACK_ZERO/STACK_INVALID markers. This, among other things,\nwill make it easier to write tests for these mixed spill/misc cases.\n\nPatch #5 prints map name for PTR_TO_MAP_VALUE/PTR_TO_MAP_KEY/CONST_PTR_TO_MAP\nregisters. In big production BPF programs, it's important to map assembly to\nactual map, and it's often non-trivial. Having map name helps.\n\nPatch #6 just removes visual noise in form of ubiquitous imm=0 and off=0. They\nare default values, omit them.\n\nPatch #7 is probably the most controversial, but it reworks how verifier log\nprints numbers. For small valued integers we use decimals, but for large ones\nwe switch to hexadecimal. From personal experience this is a much more useful\nconvention. We can tune what consitutes \"small value\", for now it's 16-bit\nrange.\n\nPatch #8 prints frame number for PTR_TO_CTX registers, if that frame is\ndifferent from the \"current\" one. This removes ambiguity and confusion,\nespecially in complicated cases with multiple subprogs passing around\npointers.\n\nv2->v3:\n  - adjust reg_bounds tester to parse hex form of reg state as well;\n  - print reg->range as unsigned (Alexei);\nv1->v2:\n  - use verbose_snum() for range and offset in register state (Eduard);\n  - fixed typos and added acks from Eduard and Stanislav.\n====================\n\nLink: https://lore.kernel.org/r/20231118034623.3320920-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-11-18 11:40:00 -0800",
      "commits": [
        {
          "hash": "db840d389bad60ce6f3aadc1079da13e7e993a16",
          "subject": "bpf: move verbose_linfo() into kernel/bpf/log.c",
          "message": "verifier.c is huge. Let's try to move out parts that are logging-related\ninto log.c, as we previously did with bpf_log() and other related stuff.\nThis patch moves line info verbose output routines: it's pretty\nself-contained and isolated code, so there is no problem with this.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231118034623.3320920-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-18 11:39:58 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/log.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "42feb6620accded89cad5f455665e21281813d79",
          "subject": "bpf: move verifier state printing code to kernel/bpf/log.c",
          "message": "Move a good chunk of code from verifier.c to log.c: verifier state\nverbose printing logic. This is an important and very much\nlogging/debugging oriented code. It fits the overlall log.c's focus on\nverifier logging, and moving it allows to keep growing it without\nunnecessarily adding to verifier.c code that otherwise contains a core\nverification logic.\n\nThere are not many shared dependencies between this code and the rest of\nverifier.c code, except a few single-line helpers for various register\ntype checks and a bit of state \"scratching\" helpers. We move all such\ntrivial helpers into include/bpf/bpf_verifier.h as static inlines.\n\nNo functional changes in this patch.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231118034623.3320920-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-18 11:39:59 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/log.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "009f5465be3636e9ce795cfbd5d3109d8978774d",
          "subject": "bpf: extract register state printing",
          "message": "Extract printing register state representation logic into a separate\nhelper, as we are going to reuse it for spilled register state printing\nin the next patch. This also nicely reduces code nestedness.\n\nNo functional changes.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231118034623.3320920-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-18 11:39:59 -0800",
          "modified_files": [
            "kernel/bpf/log.c"
          ]
        },
        {
          "hash": "67d43dfbb42d6575304daea67733c88fbf536a1c",
          "subject": "bpf: print spilled register state in stack slot",
          "message": "Print the same register state representation when printing stack state,\nas we do for normal registers. Note that if stack slot contains\nsubregister spill (1, 2, or 4 byte long), we'll still emit \"m0?\" mask\nfor those bytes that are not part of spilled register.\n\nWhile means we can get something like fp-8=0000scalar() for a 4-byte\nspill with other 4 bytes still being STACK_ZERO.\n\nSome example before and after, taken from the log of\npyperf_subprogs.bpf.o:\n\n49: (7b) *(u64 *)(r10 -256) = r1      ; frame1: R1_w=ctx(off=0,imm=0) R10=fp0 fp-256_w=ctx\n49: (7b) *(u64 *)(r10 -256) = r1      ; frame1: R1_w=ctx(off=0,imm=0) R10=fp0 fp-256_w=ctx(off=0,imm=0)\n\n150: (7b) *(u64 *)(r10 -264) = r0     ; frame1: R0_w=map_value_or_null(id=6,off=0,ks=192,vs=4,imm=0) R10=fp0 fp-264_w=map_value_or_null\n150: (7b) *(u64 *)(r10 -264) = r0     ; frame1: R0_w=map_value_or_null(id=6,off=0,ks=192,vs=4,imm=0) R10=fp0 fp-264_w=map_value_or_null(id=6,off=0,ks=192,vs=4,imm=0)\n\n5192: (61) r1 = *(u32 *)(r10 -272)    ; frame1: R1_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=15,var_off=(0x0; 0xf)) R10=fp0 fp-272=\n5192: (61) r1 = *(u32 *)(r10 -272)    ; frame1: R1_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=15,var_off=(0x0; 0xf)) R10=fp0 fp-272=????scalar(smin=smin32=0,smax=umax=smax32=umax32=15,var_off=(0x0; 0xf))\n\nWhile at it, do a few other simple clean ups:\n  - skip slot if it's not scratched before detecting whether it's valid;\n  - move taking spilled_reg pointer outside of switch (only DYNPTR has\n    to adjust that to get to the \"main\" slot);\n  - don't recalculate types_buf second time for MISC/ZERO/default case.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231118034623.3320920-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-18 11:39:59 -0800",
          "modified_files": [
            "kernel/bpf/log.c"
          ]
        },
        {
          "hash": "0c95c9fdb696f35c7864785ba84cb9a50152daff",
          "subject": "bpf: emit map name in register state if applicable and available",
          "message": "In complicated real-world applications, whenever debugging some\nverification error through verifier log, it often would be very useful\nto see map name for PTR_TO_MAP_VALUE register. Usually this needs to be\ninferred from key/value sizes and maybe trying to guess C code location,\nbut it's not always clear.\n\nGiven verifier has the name, and it's never too long, let's just emit it\nfor ptr_to_map_key, ptr_to_map_value, and const_ptr_to_map registers. We\nreshuffle the order a bit, so that map name, key size, and value size\nappear before offset and immediate values, which seems like a more\nlogical order.\n\nCurrent output:\n\n  R1_w=map_ptr(map=array_map,ks=4,vs=8,off=0,imm=0)\n\nBut we'll get rid of useless off=0 and imm=0 parts in the next patch.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231118034623.3320920-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-18 11:39:59 -0800",
          "modified_files": [
            "kernel/bpf/log.c",
            "tools/testing/selftests/bpf/prog_tests/spin_lock.c"
          ]
        },
        {
          "hash": "1db747d75b1dbe17bf4283ed87bd3b7a92010f34",
          "subject": "bpf: omit default off=0 and imm=0 in register state log",
          "message": "Simplify BPF verifier log further by omitting default (and frequently\nirrelevant) off=0 and imm=0 parts for non-SCALAR_VALUE registers. As can\nbe seen from fixed tests, this is often a visual noise for PTR_TO_CTX\nregister and even for PTR_TO_PACKET registers.\n\nOmitting default values follows the rest of register state logic: we\nomit default values to keep verifier log succinct and to highlight\ninteresting state that deviates from default one. E.g., we do the same\nfor var_off, when it's unknown, which gives no additional information.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231118034623.3320920-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-18 11:39:59 -0800",
          "modified_files": [
            "kernel/bpf/log.c",
            "tools/testing/selftests/bpf/prog_tests/align.c",
            "tools/testing/selftests/bpf/prog_tests/log_buf.c",
            "tools/testing/selftests/bpf/prog_tests/spin_lock.c",
            "tools/testing/selftests/bpf/progs/exceptions_assert.c"
          ]
        },
        {
          "hash": "0f8dbdbc641b45a5fa31d497f9fc83ffe1174fa3",
          "subject": "bpf: smarter verifier log number printing logic",
          "message": "Instead of always printing numbers as either decimals (and in some\ncases, like for \"imm=%llx\", in hexadecimals), decide the form based on\nactual values. For numbers in a reasonably small range (currently,\n[0, U16_MAX] for unsigned values, and [S16_MIN, S16_MAX] for signed ones),\nemit them as decimals. In all other cases, even for signed values,\nemit them in hexadecimals.\n\nFor large values hex form is often times way more useful: it's easier to\nsee an exact difference between 0xffffffff80000000 and 0xffffffff7fffffff,\nthan between 18446744071562067966 and 18446744071562067967, as one\nparticular example.\n\nSmall values representing small pointer offsets or application\nconstants, on the other hand, are way more useful to be represented in\ndecimal notation.\n\nAdjust reg_bounds register state parsing logic to take into account this\nchange.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231118034623.3320920-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-18 11:39:59 -0800",
          "modified_files": [
            "kernel/bpf/log.c",
            "tools/testing/selftests/bpf/prog_tests/reg_bounds.c",
            "tools/testing/selftests/bpf/progs/exceptions_assert.c"
          ]
        },
        {
          "hash": "46862ee854b4f5a315d63b677ca3af14a89aefeb",
          "subject": "bpf: emit frameno for PTR_TO_STACK regs if it differs from current one",
          "message": "It's possible to pass a pointer to parent's stack to child subprogs. In\nsuch case verifier state output is ambiguous not showing whether\nregister container a pointer to \"current\" stack, belonging to current\nsubprog (frame), or it's actually a pointer to one of parent frames.\n\nSo emit this information if frame number differs between the state which\nregister is part of. E.g., if current state is in frame 2 and it has\na register pointing to stack in grand parent state (frame #0), we'll see\nsomething like 'R1=fp[0]-16', while \"local stack pointer\" will be just\n'R2=fp-16'.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231118034623.3320920-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-18 11:39:59 -0800",
          "modified_files": [
            "kernel/bpf/log.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "9cea90c01f4bddfb4cea12a9c23eef6414714503",
      "merge_subject": "Merge branch 'bpf-register-bounds-range-vs-range-support'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nBPF register bounds range vs range support\n\nThis patch set is a continuation of work started in [0]. It adds a big set of\nmanual, auto-generated, and now also random test cases validating BPF\nverifier's register bounds tracking and deduction logic.\n\nFirst few patches generalize verifier's logic to handle conditional jumps and\ncorresponding range adjustments in case when two non-const registers are\ncompared to each other. Patch #1 generalizes reg_set_min_max() portion, while\npatch #2 does the same for is_branch_taken() part of the overall solution.\n\nPatch #3 improves equality and inequality for cases when BPF program code\nmixes 64-bit and 32-bit uses of the same register. Depending on specific\nsequence, it's possible to get to the point where u64/s64 bounds will be very\ngeneric (e.g., after signed 32-bit comparison), while we still keep pretty\ntight u32/s32 bounds. If in such state we proceed with 32-bit equality or\ninequality comparison, reg_set_min_max() might have to deal with adjusting s32\nbounds for two registers that don't overlap, which breaks reg_set_min_max().\nThis doesn't manifest in <range> vs <const> cases, because if that happens\nreg_set_min_max() in effect will force s32 bounds to be a new \"impossible\"\nconstant (from original smin32/smax32 bounds point of view). Things get tricky\nwhen we have <range> vs <range> adjustments, so instead of trying to somehow\nmake sense out of such situations, it's best to detect such impossible\nsituations and prune the branch that can't be taken in is_branch_taken()\nlogic.  This equality/inequality was the only such category of situations with\nauto-generated tests added later in the patch set.\n\nBut when we start mixing arithmetic operations in different numeric domains\nand conditionals, things get even hairier. So, patch #4 adds sanity checking\nlogic after all ALU/ALU64, JMP/JMP32, and LDX operations. By default, instead\nof failing verification, we conservatively reset range bounds to unknown\nvalues, reporting violation in verifier log (if verbose logs are requested).\nBut to aid development, detection, and debugging, we also introduce a new test\nflag, BPF_F_TEST_SANITY_STRICT, which triggers verification failure on range\nsanity violation.\n\nPatch #11 sets BPF_F_TEST_SANITY_STRICT by default for test_progs and\ntest_verifier. Patch #12 adds support for controlling this in veristat for\ntesting with production BPF object files.\n\nGetting back to BPF verifier, patches #5 and #6 complete verifier's range\ntracking logic clean up. See respective patches for details.\n\nWith kernel-side taken care of, we move to testing. We start with building\na tester that validates existing <range> vs <scalar> verifier logic for range\nbounds. Patch #7 implements an initial version of such a tester. We guard\nmillions of generated tests behind SLOW_TESTS=1 envvar requirement, but also\nhave a relatively small number of tricky cases that came up during development\nand debugging of this work. Those will be executed as part of a normal\ntest_progs run.\n\nPatch #8 simulates more nuanced JEQ/JNE logic we added to verifier in patch #3.\nPatch #9 adds <range> vs <range> \"slow tests\".\n\nPatch #10 is a completely new one, it adds a bunch of randomly generated cases\nto be run normally, without SLOW_TESTS=1 guard. This should help to get\na bunch of cover, and hopefully find some remaining latent problems if\nverifier proactively as part of normal BPF CI runs.\n\nFinally, a tiny test which was, amazingly, an initial motivation for this\nwhole work, is added in lucky patch #13, demonstrating how verifier is now\nsmart enough to track actual number of elements in the array and won't require\nadditional checks on loop iteration variable inside the bpf_for() open-coded\niterator loop.\n\n  [0] https://patchwork.kernel.org/project/netdevbpf/list/?series=798308&state=*\n\nv1->v2:\n  - use x < y => y > x property to minimize reg_set_min_max (Eduard);\n  - fix for JEQ/JNE logic in reg_bounds.c (Eduard);\n  - split BPF_JSET and !BPF_JSET cases handling (Shung-Hsi);\n  - adjustments to reg_bounds.c to make it easier to follow (Alexei);\n  - added acks (Eduard, Shung-Hsi).\n====================\n\nLink: https://lore.kernel.org/r/20231112010609.848406-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-11-15 12:03:43 -0800",
      "commits": [
        {
          "hash": "67420501e8681ae18f9f0ea0a69cd2f432100e70",
          "subject": "bpf: generalize reg_set_min_max() to handle non-const register comparisons",
          "message": "Generalize bounds adjustment logic of reg_set_min_max() to handle not\njust register vs constant case, but in general any register vs any\nregister cases. For most of the operations it's trivial extension based\non range vs range comparison logic, we just need to properly pick\nmin/max of a range to compare against min/max of the other range.\n\nFor BPF_JSET we keep the original capabilities, just make sure JSET is\nintegrated in the common framework. This is manifested in the\ninternal-only BPF_JSET + BPF_X \"opcode\" to allow for simpler and more\nuniform rev_opcode() handling. See the code for details. This allows to\nreuse the same code exactly both for TRUE and FALSE branches without\nexplicitly handling both conditions with custom code.\n\nNote also that now we don't need a special handling of BPF_JEQ/BPF_JNE\ncase none of the registers are constants. This is now just a normal\ngeneric case handled by reg_set_min_max().\n\nTo make tnum handling cleaner, tnum_with_subreg() helper is added, as\nthat's a common operator when dealing with 32-bit subregister bounds.\nThis keeps the overall logic much less noisy when it comes to tnums.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231112010609.848406-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:41 -0800",
          "modified_files": [
            "include/linux/tnum.h",
            "kernel/bpf/tnum.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "96381879a370425a30b810906946f64c0726450e",
          "subject": "bpf: generalize is_scalar_branch_taken() logic",
          "message": "Generalize is_branch_taken logic for SCALAR_VALUE register to handle\ncases when both registers are not constants. Previously supported\n<range> vs <scalar> cases are a natural subset of more generic <range>\nvs <range> set of cases.\n\nGeneralized logic relies on straightforward segment intersection checks.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231112010609.848406-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:41 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "be41a203bb9e0159099e189e510388fe61962eb8",
          "subject": "bpf: enhance BPF_JEQ/BPF_JNE is_branch_taken logic",
          "message": "Use 32-bit subranges to prune some 64-bit BPF_JEQ/BPF_JNE conditions\nthat otherwise would be \"inconclusive\" (i.e., is_branch_taken() would\nreturn -1). This can happen, for example, when registers are initialized\nas 64-bit u64/s64, then compared for inequality as 32-bit subregisters,\nand then followed by 64-bit equality/inequality check. That 32-bit\ninequality can establish some pattern for lower 32 bits of a register\n(e.g., s< 0 condition determines whether the bit #31 is zero or not),\nwhile overall 64-bit value could be anything (according to a value range\nrepresentation).\n\nThis is not a fancy quirky special case, but actually a handling that's\nnecessary to prevent correctness issue with BPF verifier's range\ntracking: set_range_min_max() assumes that register ranges are\nnon-overlapping, and if that condition is not guaranteed by\nis_branch_taken() we can end up with invalid ranges, where min > max.\n\n  [0] https://lore.kernel.org/bpf/CACkBjsY2q1_fUohD7hRmKGqv1MV=eP2f6XK8kjkYNw7BaiF8iQ@mail.gmail.com/\n\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231112010609.848406-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:42 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "5f99f312bd3bedb3b266b0d26376a8c500cdc97f",
          "subject": "bpf: add register bounds sanity checks and sanitization",
          "message": "Add simple sanity checks that validate well-formed ranges (min <= max)\nacross u64, s64, u32, and s32 ranges. Also for cases when the value is\nconstant (either 64-bit or 32-bit), we validate that ranges and tnums\nare in agreement.\n\nThese bounds checks are performed at the end of BPF_ALU/BPF_ALU64\noperations, on conditional jumps, and for LDX instructions (where subreg\nzero/sign extension is probably the most important to check). This\ncovers most of the interesting cases.\n\nAlso, we validate the sanity of the return register when manually\nadjusting it for some special helpers.\n\nBy default, sanity violation will trigger a warning in verifier log and\nresetting register bounds to \"unbounded\" ones. But to aid development\nand debugging, BPF_F_TEST_SANITY_STRICT flag is added, which will\ntrigger hard failure of verification with -EFAULT on register bounds\nviolations. This allows selftests to catch such issues. veristat will\nalso gain a CLI option to enable this behavior.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231112010609.848406-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:42 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "3cf98cf594ea923b8b1e0385b580d3d8aae68c06",
          "subject": "bpf: remove redundant s{32,64} -> u{32,64} deduction logic",
          "message": "Equivalent checks were recently added in more succinct and, arguably,\nsafer form in:\n  - f188765f23a5 (\"bpf: derive smin32/smax32 from umin32/umax32 bounds\");\n  - 2e74aef782d3 (\"bpf: derive smin/smax from umin/max bounds\").\n\nThe checks we are removing in this patch set do similar checks to detect\nif entire u32/u64 range has signed bit set or not set, but does it with\ntwo separate checks.\n\nFurther, we forcefully overwrite either smin or smax (and 32-bit equvalents)\nwithout applying normal min/max intersection logic. It's not clear why\nthat would be correct in all cases and seems to work by accident. This\nlogic is also \"gated\" by previous signed -> unsigned derivation, which\nreturns early.\n\nAll this is quite confusing and seems error-prone, while we already have\nat least equivalent checks happening earlier. So remove this duplicate\nand error-prone logic to simplify things a bit.\n\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231112010609.848406-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:42 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "cf5fe3c71c5a34ac0108afc550407c672d0a032d",
          "subject": "bpf: make __reg{32,64}_deduce_bounds logic more robust",
          "message": "This change doesn't seem to have any effect on selftests and production\nBPF object files, but we preemptively try to make it more robust.\n\nFirst, \"learn sign from signed bounds\" comment is misleading, as we are\nlearning not just sign, but also values.\n\nSecond, we simplify the check for determining whether entire range is\npositive or negative similarly to other checks added earlier, using\nappropriate u32/u64 cast and single comparisons. As explain in comments\nin __reg64_deduce_bounds(), the checks are equivalent.\n\nLast but not least, smin/smax and s32_min/s32_max reassignment based on\nmin/max of both umin/umax and smin/smax (and 32-bit equivalents) is hard\nto explain and justify. We are updating unsigned bounds from signed\nbounds, why would we update signed bounds at the same time? This might\nbe correct, but it's far from obvious why and the code or comments don't\ntry to justify this. Given we've added a separate deduction of signed\nbounds from unsigned bounds earlier, this seems at least redundant, if\nnot just wrong.\n\nIn short, we remove doubtful pieces, and streamline the rest to follow\nthe logic and approach of the rest of reg_bounds_sync() checks.\n\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231112010609.848406-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:42 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "8863238993e23ccc6d5a9d4ff9f1c043f88f692e",
          "subject": "selftests/bpf: BPF register range bounds tester",
          "message": "Add test to validate BPF verifier's register range bounds tracking logic.\n\nThe main bulk is a lot of auto-generated tests based on a small set of\nseed values for lower and upper 32 bits of full 64-bit values.\nCurrently we validate only range vs const comparisons, but the idea is\nto start validating range over range comparisons in subsequent patch set.\n\nWhen setting up initial register ranges we treat registers as one of\nu64/s64/u32/s32 numeric types, and then independently perform conditional\ncomparisons based on a potentially different u64/s64/u32/s32 types. This\ntests lots of tricky cases of deriving bounds information across\ndifferent numeric domains.\n\nGiven there are lots of auto-generated cases, we guard them behind\nSLOW_TESTS=1 envvar requirement, and skip them altogether otherwise.\nWith current full set of upper/lower seed value, all supported\ncomparison operators and all the combinations of u64/s64/u32/s32 number\ndomains, we get about 7.7 million tests, which run in about 35 minutes\non my local qemu instance without parallelization. But we also split\nthose tests by init/cond numeric types, which allows to rely on\ntest_progs's parallelization of tests with `-j` option, getting run time\ndown to about 5 minutes on 8 cores. It's still something that shouldn't\nbe run during normal test_progs run.  But we can run it a reasonable\ntime, and so perhaps a nightly CI test run (once we have it) would be\na good option for this.\n\nWe also add a small set of tricky conditions that came up during\ndevelopment and triggered various bugs or corner cases in either\nselftest's reimplementation of range bounds logic or in verifier's logic\nitself. These are fast enough to be run as part of normal test_progs\ntest run and are great for a quick sanity checking.\n\nLet's take a look at test output to understand what's going on:\n\n  $ sudo ./test_progs -t reg_bounds_crafted\n  #191/1   reg_bounds_crafted/(u64)[0; 0xffffffff] (u64)< 0:OK\n  ...\n  #191/115 reg_bounds_crafted/(u64)[0; 0x17fffffff] (s32)< 0:OK\n  ...\n  #191/137 reg_bounds_crafted/(u64)[0xffffffff; 0x100000000] (u64)== 0:OK\n\nEach test case is uniquely and fully described by this generated string.\nE.g.: \"(u64)[0; 0x17fffffff] (s32)< 0\". This means that we\ninitialize a register (R6) in such a way that verifier knows that it can\nhave a value in [(u64)0; (u64)0x17fffffff] range. Another\nregister (R7) is also set up as u64, but this time a constant (zero in\nthis case). They then are compared using 32-bit signed < operation.\nResulting TRUE/FALSE branches are evaluated (including cases where it's\nknown that one of the branches will never be taken, in which case we\nvalidate that verifier also determines this as a dead code). Test\nvalidates that verifier's final register state matches expected state\nbased on selftest's own reg_state logic, implemented from scratch for\ncross-checking purposes.\n\nThese test names can be conveniently used for further debugging, and if -vv\nverboseness is requested we can get a corresponding verifier log (with\nmark_precise logs filtered out as irrelevant and distracting). Example below is\nslightly redacted for brevity, omitting irrelevant register output in\nsome places, marked with [...].\n\n  $ sudo ./test_progs -a 'reg_bounds_crafted/(u32)[0; U32_MAX] (s32)< -1' -vv\n  ...\n  VERIFIER LOG:\n  ========================\n  func#0 @0\n  0: R1=ctx(off=0,imm=0) R10=fp0\n  0: (05) goto pc+2\n  3: (85) call bpf_get_current_pid_tgid#14      ; R0_w=scalar()\n  4: (bc) w6 = w0                       ; R0_w=scalar() R6_w=scalar(smin=0,smax=umax=4294967295,var_off=(0x0; 0xffffffff))\n  5: (85) call bpf_get_current_pid_tgid#14      ; R0_w=scalar()\n  6: (bc) w7 = w0                       ; R0_w=scalar() R7_w=scalar(smin=0,smax=umax=4294967295,var_off=(0x0; 0xffffffff))\n  7: (b4) w1 = 0                        ; R1_w=0\n  8: (b4) w2 = -1                       ; R2=4294967295\n  9: (ae) if w6 < w1 goto pc-9\n  9: R1=0 R6=scalar(smin=0,smax=umax=4294967295,var_off=(0x0; 0xffffffff))\n  10: (2e) if w6 > w2 goto pc-10\n  10: R2=4294967295 R6=scalar(smin=0,smax=umax=4294967295,var_off=(0x0; 0xffffffff))\n  11: (b4) w1 = -1                      ; R1_w=4294967295\n  12: (b4) w2 = -1                      ; R2_w=4294967295\n  13: (ae) if w7 < w1 goto pc-13        ; R1_w=4294967295 R7=4294967295\n  14: (2e) if w7 > w2 goto pc-14\n  14: R2_w=4294967295 R7=4294967295\n  15: (bc) w0 = w6                      ; [...] R6=scalar(id=1,smin=0,smax=umax=4294967295,var_off=(0x0; 0xffffffff))\n  16: (bc) w0 = w7                      ; [...] R7=4294967295\n  17: (ce) if w6 s< w7 goto pc+3        ; R6=scalar(id=1,smin=0,smax=umax=4294967295,smin32=-1,var_off=(0x0; 0xffffffff)) R7=4294967295\n  18: (bc) w0 = w6                      ; [...] R6=scalar(id=1,smin=0,smax=umax=4294967295,smin32=-1,var_off=(0x0; 0xffffffff))\n  19: (bc) w0 = w7                      ; [...] R7=4294967295\n  20: (95) exit\n\n  from 17 to 21: [...]\n  21: (bc) w0 = w6                      ; [...] R6=scalar(id=1,smin=umin=umin32=2147483648,smax=umax=umax32=4294967294,smax32=-2,var_off=(0x80000000; 0x7fffffff))\n  22: (bc) w0 = w7                      ; [...] R7=4294967295\n  23: (95) exit\n\n  from 13 to 1: [...]\n  1: [...]\n  1: (b7) r0 = 0                        ; R0_w=0\n  2: (95) exit\n  processed 24 insns (limit 1000000) max_states_per_insn 0 total_states 2 peak_states 2 mark_read 1\n  =====================\n\nVerifier log above is for `(u32)[0; U32_MAX] (s32)< -1` use cases, where u32\nrange is used for initialization, followed by signed < operator. Note\nhow we use w6/w7 in this case for register initialization (it would be\nR6/R7 for 64-bit types) and then `if w6 s< w7` for comparison at\ninstruction #17. It will be `if R6 < R7` for 64-bit unsigned comparison.\nAbove example gives a good impression of the overall structure of a BPF\nprograms generated for reg_bounds tests.\n\nIn the future, this \"framework\" can be extended to test not just\nconditional jumps, but also arithmetic operations. Adding randomized\ntesting is another possibility.\n\nSome implementation notes. We basically have our own generics-like\noperations on numbers, where all the numbers are stored in u64, but how\nthey are interpreted is passed as runtime argument enum num_t. Further,\n`struct range` represents a bounds range, and those are collected\ntogether into a minimal `struct reg_state`, which collects range bounds\nacross all four numberical domains: u64, s64, u32, s64.\n\nBased on these primitives and `enum op` representing possible\nconditional operation (<, <=, >, >=, ==, !=), there is a set of generic\nhelpers to perform \"range arithmetics\", which is used to maintain struct\nreg_state. We simulate what verifier will do for reg bounds of R6 and R7\nregisters using these range and reg_state primitives. Simulated\ninformation is used to determine branch taken conclusion and expected\nexact register state across all four number domains.\n\nImplementation of \"range arithmetics\" is more generic than what verifier\nis currently performing: it allows range over range comparisons and\nadjustments. This is the intended end goal of this patch set overall and verifier\nlogic is enhanced in subsequent patches in this series to handle range\nvs range operations, at which point selftests are extended to validate\nthese conditions as well. For now it's range vs const cases only.\n\nNote that tests are split into multiple groups by their numeric types\nfor initialization of ranges and for comparison operation. This allows\nto use test_progs's -j parallelization to speed up tests, as we now have\n16 groups of parallel running tests. Overall reduction of running time\nthat allows is pretty good, we go down from more than 30 minutes to\nslightly less than 5 minutes running time.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231112010609.848406-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:42 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/reg_bounds.c"
          ]
        },
        {
          "hash": "774f94c5e74d86d554c4fd1e97c517a1a7ee7fe0",
          "subject": "selftests/bpf: adjust OP_EQ/OP_NE handling to use subranges for branch taken",
          "message": "Similar to kernel-side BPF verifier logic enhancements, use 32-bit\nsubrange knowledge for is_branch_taken() logic in reg_bounds selftests.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231112010609.848406-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:42 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/reg_bounds.c"
          ]
        },
        {
          "hash": "2b0d204e368b306d4db894749947ed591b667ec5",
          "subject": "selftests/bpf: add range x range test to reg_bounds",
          "message": "Now that verifier supports range vs range bounds adjustments, validate\nthat by checking each generated range against every other generated\nrange, across all supported operators (everything by JSET).\n\nWe also add few cases that were problematic during development either\nfor verifier or for selftest's range tracking implementation.\n\nNote that we utilize the same trick with splitting everything into\nmultiple independent parallelizable tests, but init_t and cond_t. This\nbrings down verification time in parallel mode from more than 8 hours\ndown to less that 1.5 hours. 106 million cases were successfully\nvalidate for range vs range logic, in addition to about 7 million range\nvs const cases, added in earlier patch.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231112010609.848406-10-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:42 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/reg_bounds.c"
          ]
        },
        {
          "hash": "dab16659c50e8c9c7c5d9584beacec28c769dcca",
          "subject": "selftests/bpf: add randomized reg_bounds tests",
          "message": "Add random cases generation to reg_bounds.c and run them without\nSLOW_TESTS=1 to increase a chance of BPF CI catching latent issues.\n\nSuggested-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231112010609.848406-11-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:42 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/reg_bounds.c"
          ]
        },
        {
          "hash": "8c5677f8b31e92b57be7d5d0fbb1ac66eedf4f91",
          "subject": "selftests/bpf: set BPF_F_TEST_SANITY_SCRIPT by default",
          "message": "Make sure to set BPF_F_TEST_SANITY_STRICT program flag by default across\nmost verifier tests (and a bunch of others that set custom prog flags).\n\nThere are currently two tests that do fail validation, if enforced\nstrictly: verifier_bounds/crossing_64_bit_signed_boundary_2 and\nverifier_bounds/crossing_32_bit_signed_boundary_2. To accommodate them,\nwe teach test_loader a flag negation:\n\n__flag(!<flagname>) will *clear* specified flag, allowing easy opt-out.\n\nWe apply __flag(!BPF_F_TEST_SANITY_STRICT) to these to tests.\n\nAlso sprinkle BPF_F_TEST_SANITY_STRICT everywhere where we already set\ntest-only BPF_F_TEST_RND_HI32 flag, for completeness.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231112010609.848406-12-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:42 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/bpf_verif_scale.c",
            "tools/testing/selftests/bpf/progs/verifier_bounds.c",
            "tools/testing/selftests/bpf/test_loader.c",
            "tools/testing/selftests/bpf/test_sock_addr.c",
            "tools/testing/selftests/bpf/test_verifier.c",
            "tools/testing/selftests/bpf/testing_helpers.c"
          ]
        },
        {
          "hash": "a5c57f81eb2b5d6de4f46e47fd85be50d179bfd8",
          "subject": "veristat: add ability to set BPF_F_TEST_SANITY_STRICT flag with -r flag",
          "message": "Add a new flag -r (--test-sanity), similar to -t (--test-states), to add\nextra BPF program flags when loading BPF programs.\n\nThis allows to use veristat to easily catch sanity violations in\nproduction BPF programs.\n\nreg_bounds tests are also enforcing BPF_F_TEST_SANITY_STRICT flag now.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231112010609.848406-13-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:43 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/reg_bounds.c",
            "tools/testing/selftests/bpf/veristat.c"
          ]
        },
        {
          "hash": "882e3d873c2d8a2aebbc6c192aa1a2990b9d5b27",
          "subject": "selftests/bpf: add iter test requiring range x range logic",
          "message": "Add a simple verifier test that requires deriving reg bounds for one\nregister from another register that's not a constant. This is\na realistic example of iterating elements of an array with fixed maximum\nnumber of elements, but smaller actual number of elements.\n\nThis small example was an original motivation for doing this whole patch\nset in the first place, yes.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231112010609.848406-14-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-15 12:03:43 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/iters.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "8c74b27f4b30cd896ccf387102410a65b4a35c25",
      "merge_subject": "Merge branch 'bpf-control-flow-graph-and-precision-backtrack-fixes'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nBPF control flow graph and precision backtrack fixes\n\nA small fix to BPF verifier's CFG logic around handling and reporting ldimm64\ninstructions. Patch #1 was previously submitted separately ([0]), and so this\npatch set supersedes that patch.\n\nSecond patch is fixing obscure corner case in mark_chain_precise() logic. See\npatch for details. Patch #3 adds a dedicated test, however fragile it might.\n\n  [0] https://patchwork.kernel.org/project/netdevbpf/patch/20231101205626.119243-1-andrii@kernel.org/\n====================\n\nLink: https://lore.kernel.org/r/20231110002638.4168352-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-11-09 20:11:20 -0800",
      "commits": [
        {
          "hash": "3feb263bb516ee7e1da0acd22b15afbb9a7daa19",
          "subject": "bpf: handle ldimm64 properly in check_cfg()",
          "message": "ldimm64 instructions are 16-byte long, and so have to be handled\nappropriately in check_cfg(), just like the rest of BPF verifier does.\n\nThis has implications in three places:\n  - when determining next instruction for non-jump instructions;\n  - when determining next instruction for callback address ldimm64\n    instructions (in visit_func_call_insn());\n  - when checking for unreachable instructions, where second half of\n    ldimm64 is expected to be unreachable;\n\nWe take this also as an opportunity to report jump into the middle of\nldimm64. And adjust few test_verifier tests accordingly.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nReported-by: Hao Sun <sunhao.th@gmail.com>\nFixes: 475fb78fbf48 (\"bpf: verifier (add branch/goto checks)\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231110002638.4168352-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 20:11:20 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/verifier/ld_imm64.c"
          ]
        },
        {
          "hash": "4bb7ea946a370707315ab774432963ce47291946",
          "subject": "bpf: fix precision backtracking instruction iteration",
          "message": "Fix an edge case in __mark_chain_precision() which prematurely stops\nbacktracking instructions in a state if it happens that state's first\nand last instruction indexes are the same. This situations doesn't\nnecessarily mean that there were no instructions simulated in a state,\nbut rather that we starting from the instruction, jumped around a bit,\nand then ended up at the same instruction before checkpointing or\nmarking precision.\n\nTo distinguish between these two possible situations, we need to consult\njump history. If it's empty or contain a single record \"bridging\" parent\nstate and first instruction of processed state, then we indeed\nbacktracked all instructions in this state. But if history is not empty,\nwe are definitely not done yet.\n\nMove this logic inside get_prev_insn_idx() to contain it more nicely.\nUse -ENOENT return code to denote \"we are out of instructions\"\nsituation.\n\nThis bug was exposed by verifier_loop1.c's bounded_recursion subtest, once\nthe next fix in this patch set is applied.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nFixes: b5dc0163d8fd (\"bpf: precise scalar_value tracking\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231110002638.4168352-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 20:11:20 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "62ccdb11d3c63dc697dea1fd92b3496fe43dcc1e",
          "subject": "selftests/bpf: add edge case backtracking logic test",
          "message": "Add a dedicated selftests to try to set up conditions to have a state\nwith same first and last instruction index, but it actually is a loop\n3->4->1->2->3. This confuses mark_chain_precision() if verifier doesn't\ntake into account jump history.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231110002638.4168352-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 20:11:20 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_precision.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "3f6d04d742d9fbd492a79e28e7cfe4e2a97c66e5",
      "merge_subject": "Merge branch 'allow-bpf_refcount_acquire-of-mapval-obtained-via-direct-ld'",
      "merge_body": "Dave Marchevsky says:\n\n====================\nAllow bpf_refcount_acquire of mapval obtained via direct LD\n\nConsider this BPF program:\n\n  struct cgv_node {\n    int d;\n    struct bpf_refcount r;\n    struct bpf_rb_node rb;\n  };\n\n  struct val_stash {\n    struct cgv_node __kptr *v;\n  };\n\n  struct {\n    __uint(type, BPF_MAP_TYPE_ARRAY);\n    __type(key, int);\n    __type(value, struct val_stash);\n    __uint(max_entries, 10);\n  } array_map SEC(\".maps\");\n\n  long bpf_program(void *ctx)\n  {\n    struct val_stash *mapval;\n    struct cgv_node *p;\n    int idx = 0;\n\n    mapval = bpf_map_lookup_elem(&array_map, &idx);\n    if (!mapval || !mapval->v) { /* omitted */ }\n\n    p = bpf_refcount_acquire(mapval->v); /* Verification FAILs here */\n\n    /* Add p to some tree */\n    return 0;\n  }\n\nVerification fails on the refcount_acquire:\n\n  160: (79) r1 = *(u64 *)(r8 +8)        ; R1_w=untrusted_ptr_or_null_cgv_node(id=11,off=0,imm=0) R8_w=map_value(id=10,off=0,ks=8,vs=16,imm=0) refs=6\n  161: (b7) r2 = 0                      ; R2_w=0 refs=6\n  162: (85) call bpf_refcount_acquire_impl#117824\n  arg#0 is neither owning or non-owning ref\n\nThe above verifier dump is actually from sched_ext's scx_flatcg [0],\nwhich is the motivating usecase for this series' changes. Specifically,\nscx_flatcg stashes a rb_node type w/ cgroup-specific info (struct\ncgv_node) in a map when the cgroup is created, then later puts that\ncgroup's node in a rbtree in .enqueue . Making struct cgv_node\nrefcounted would simplify the code a bit by virtue of allowing us to\nremove the kptr_xchg's, but \"later puts that cgroups node in a rbtree\"\nis not possible without a refcount_acquire, which suffers from the above\nverification failure.\n\nIf we get rid of PTR_UNTRUSTED flag, and add MEM_ALLOC | NON_OWN_REF,\nmapval->v would be a non-owning ref and verification would succeed. Due\nto the most recent set of refcount changes [1], which modified\nbpf_obj_drop behavior to not reuse refcounted graph node's underlying\nmemory until after RCU grace period, this is safe to do. Once mapval->v\nhas the correct flags it _is_ a non-owning reference and verification of\nthe motivating example will succeed.\n\n  [0]: https://github.com/sched-ext/sched_ext/blob/52911e1040a0f94b9c426dddcc00be5364a7ae9f/tools/sched_ext/scx_flatcg.bpf.c#L275\n  [1]: https://lore.kernel.org/bpf/20230821193311.3290257-1-davemarchevsky@fb.com/\n\nSummary of patches:\n  * Patch 1 fixes an issue with bpf_refcount_acquire verification\n    letting MAYBE_NULL ptrs through\n    * Patch 2 tests Patch 1's fix\n  * Patch 3 broadens the use of \"free only after RCU GP\" to all\n    user-allocated types\n  * Patch 4 is a small nonfunctional refactoring\n  * Patch 5 changes verifier to mark direct LD of stashed graph node\n    kptr as non-owning ref\n    * Patch 6 tests Patch 5's verifier changes\n\nChangelog:\n\nv1 -> v2: https://lore.kernel.org/bpf/20231025214007.2920506-1-davemarchevsky@fb.com/\n\nSeries title changed to \"Allow bpf_refcount_acquire of mapval obtained via\ndirect LD\". V1's title was mistakenly truncated.\n\n  * Patch 5 (\"bpf: Mark direct ld of stashed bpf_{rb,list}_node as non-owning ref\")\n    * Direct LD of percpu kptr should not have MEM_ALLOC flag (Yonghong)\n  * Patch 6 (\"selftests/bpf: Test bpf_refcount_acquire of node obtained via direct ld\")\n    * Test read from stashed value as well\n====================\n\nLink: https://lore.kernel.org/r/20231107085639.3016113-1-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-11-09 19:07:52 -0800",
      "commits": [
        {
          "hash": "1500a5d9f49cb66906d3ea1c9158df25cc41dd40",
          "subject": "bpf: Add KF_RCU flag to bpf_refcount_acquire_impl",
          "message": "Refcounted local kptrs are kptrs to user-defined types with a\nbpf_refcount field. Recent commits ([0], [1]) modified the lifetime of\nrefcounted local kptrs such that the underlying memory is not reused\nuntil RCU grace period has elapsed.\n\nSeparately, verification of bpf_refcount_acquire calls currently\nsucceeds for MAYBE_NULL non-owning reference input, which is a problem\nas bpf_refcount_acquire_impl has no handling for this case.\n\nThis patch takes advantage of aforementioned lifetime changes to tag\nbpf_refcount_acquire_impl kfunc KF_RCU, thereby preventing MAYBE_NULL\ninput to the kfunc. The KF_RCU flag applies to all kfunc params; it's\nfine for it to apply to the void *meta__ign param as that's populated by\nthe verifier and is tagged __ign regardless.\n\n  [0]: commit 7e26cd12ad1c (\"bpf: Use bpf_mem_free_rcu when\n       bpf_obj_dropping refcounted nodes\") is the actual change to\n       allocation behaivor\n  [1]: commit 0816b8c6bf7f (\"bpf: Consider non-owning refs to refcounted\n       nodes RCU protected\") modified verifier understanding of\n       refcounted local kptrs to match [0]'s changes\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nFixes: 7c50b1cb76ac (\"bpf: Add bpf_refcount_acquire kfunc\")\nLink: https://lore.kernel.org/r/20231107085639.3016113-2-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-11-09 19:07:51 -0800",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "f460e7bdb027d1da93f0c5090b239889cd46a33d",
          "subject": "selftests/bpf: Add test passing MAYBE_NULL reg to bpf_refcount_acquire",
          "message": "The test added in this patch exercises the logic fixed in the previous\npatch in this series. Before the previous patch's changes,\nbpf_refcount_acquire accepts MAYBE_NULL local kptrs; after the change\nthe verifier correctly rejects the such a call.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20231107085639.3016113-3-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-11-09 19:07:51 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/refcounted_kptr_fail.c"
          ]
        },
        {
          "hash": "649924b76ab151a96bdd22a97a993fb0421f134c",
          "subject": "bpf: Use bpf_mem_free_rcu when bpf_obj_dropping non-refcounted nodes",
          "message": "The use of bpf_mem_free_rcu to free refcounted local kptrs was added\nin commit 7e26cd12ad1c (\"bpf: Use bpf_mem_free_rcu when\nbpf_obj_dropping refcounted nodes\"). In the cover letter for the\nseries containing that patch [0] I commented:\n\n    Perhaps it makes sense to move to mem_free_rcu for _all_\n    non-owning refs in the future, not just refcounted. This might\n    allow custom non-owning ref lifetime + invalidation logic to be\n    entirely subsumed by MEM_RCU handling. IMO this needs a bit more\n    thought and should be tackled outside of a fix series, so it's not\n    attempted here.\n\nIt's time to start moving in the \"non-owning refs have MEM_RCU\nlifetime\" direction. As mentioned in that comment, using\nbpf_mem_free_rcu for all local kptrs - not just refcounted - is\nnecessarily the first step towards that goal. This patch does so.\n\nAfter this patch the memory pointed to by all local kptrs will not be\nreused until RCU grace period elapses. The verifier's understanding of\nnon-owning ref validity and the clobbering logic it uses to enforce\nthat understanding are not changed here, that'll happen gradually in\nfuture work, including further patches in the series.\n\n  [0]: https://lore.kernel.org/all/20230821193311.3290257-1-davemarchevsky@fb.com/\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20231107085639.3016113-4-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-11-09 19:07:51 -0800",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "790ce3cfefb1b768dccd4eee324ddef0f0ce3db4",
          "subject": "bpf: Move GRAPH_{ROOT,NODE}_MASK macros into btf_field_type enum",
          "message": "This refactoring patch removes the unused BPF_GRAPH_NODE_OR_ROOT\nbtf_field_type and moves BPF_GRAPH_{NODE,ROOT} macros into the\nbtf_field_type enum. Further patches in the series will use\nBPF_GRAPH_NODE, so let's move this useful definition out of btf.c.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20231107085639.3016113-5-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-11-09 19:07:51 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "1b12171533a9bb23cf6fba7262b479028b65e1e8",
          "subject": "bpf: Mark direct ld of stashed bpf_{rb,list}_node as non-owning ref",
          "message": "This patch enables the following pattern:\n\n  /* mapval contains a __kptr pointing to refcounted local kptr */\n  mapval = bpf_map_lookup_elem(&map, &idx);\n  if (!mapval || !mapval->some_kptr) { /* omitted */ }\n\n  p = bpf_refcount_acquire(&mapval->some_kptr);\n\nCurrently this doesn't work because bpf_refcount_acquire expects an\nowning or non-owning ref. The verifier defines non-owning ref as a type:\n\n  PTR_TO_BTF_ID | MEM_ALLOC | NON_OWN_REF\n\nwhile mapval->some_kptr is PTR_TO_BTF_ID | PTR_UNTRUSTED. It's possible\nto do the refcount_acquire by first bpf_kptr_xchg'ing mapval->some_kptr\ninto a temp kptr, refcount_acquiring that, and xchg'ing back into\nmapval, but this is unwieldy and shouldn't be necessary.\n\nThis patch modifies btf_ld_kptr_type such that user-allocated types are\nmarked MEM_ALLOC and if those types have a bpf_{rb,list}_node they're\nmarked NON_OWN_REF as well. Additionally, due to changes to\nbpf_obj_drop_impl earlier in this series, rcu_protected_object now\nreturns true for all user-allocated types, resulting in\nmapval->some_kptr being marked MEM_RCU.\n\nAfter this patch's changes, mapval->some_kptr is now:\n\n  PTR_TO_BTF_ID | MEM_ALLOC | NON_OWN_REF | MEM_RCU\n\nwhich results in it passing the non-owning ref test, and the motivating\nexample passing verification.\n\nFuture work will likely get rid of special non-owning ref lifetime logic\nin the verifier, at which point we'll be able to delete the NON_OWN_REF\nflag entirely.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20231107085639.3016113-6-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-11-09 19:07:51 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "e9ed8df7187cfdce1075d0ee591544ac15d072f1",
          "subject": "selftests/bpf: Test bpf_refcount_acquire of node obtained via direct ld",
          "message": "This patch demonstrates that verifier changes earlier in this series\nresult in bpf_refcount_acquire(mapval->stashed_kptr) passing\nverification. The added test additionally validates that stashing a kptr\nin mapval and - in a separate BPF program - refcount_acquiring the kptr\nwithout unstashing works as expected at runtime.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20231107085639.3016113-7-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-11-09 19:07:51 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/local_kptr_stash.c",
            "tools/testing/selftests/bpf/progs/local_kptr_stash.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "b0d1c7294671af02369d7a4feaa5a9bb472372c6",
      "merge_subject": "Merge branch 'bpf: __bpf_dynptr_data* and __str annotation'",
      "merge_body": "Song Liu says:\n\n====================\nThis set contains the first 3 patches of set [1]. Currently, [1] is waiting\nfor [3] to be merged to bpf-next tree. So send these 3 patches first to\nunblock other works, such as [2]. This set is verified with new version of\n[1] in CI run [4].\n\nChanges since v12 of [1]:\n1. Reuse bpf_dynptr_slice() in __bpf_dynptr_data(). (Andrii)\n2. Add Acked-by from Vadim Fedorenko.\n\n[1] https://lore.kernel.org/bpf/20231104001313.3538201-1-song@kernel.org/\n[2] https://lore.kernel.org/bpf/20231031134900.1432945-1-vadfed@meta.com/\n[3] https://lore.kernel.org/bpf/20231031215625.2343848-1-davemarchevsky@fb.com/\n[4] https://github.com/kernel-patches/bpf/actions/runs/6779945063/job/18427926114\n====================\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2023-11-09 19:07:44 -0800",
      "commits": [
        {
          "hash": "74523c06ae20b83c5508a98af62393ac34913362",
          "subject": "bpf: Add __bpf_dynptr_data* for in kernel use",
          "message": "Different types of bpf dynptr have different internal data storage.\nSpecifically, SKB and XDP type of dynptr may have non-continuous data.\nTherefore, it is not always safe to directly access dynptr->data.\n\nAdd __bpf_dynptr_data and __bpf_dynptr_data_rw to replace direct access to\ndynptr->data.\n\nUpdate bpf_verify_pkcs7_signature to use __bpf_dynptr_data instead of\ndynptr->data.\n\nSigned-off-by: Song Liu <song@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Vadim Fedorenko <vadim.fedorenko@linux.dev>\nLink: https://lore.kernel.org/bpf/20231107045725.2278852-2-song@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Song Liu <song@kernel.org>",
          "date": "2023-11-09 19:07:38 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/helpers.c",
            "kernel/trace/bpf_trace.c"
          ]
        },
        {
          "hash": "0b51940729150e807fc4b7767164e6bb6cf4f7dd",
          "subject": "bpf: Factor out helper check_reg_const_str()",
          "message": "ARG_PTR_TO_CONST_STR is used to specify constant string args for BPF\nhelpers. The logic that verifies a reg is ARG_PTR_TO_CONST_STR is\nimplemented in check_func_arg().\n\nAs we introduce kfuncs with constant string args, it is necessary to\ndo the same check for kfuncs (in check_kfunc_args). Factor out the logic\nfor ARG_PTR_TO_CONST_STR to a new check_reg_const_str() so that it can be\nreused.\n\ncheck_func_arg() ensures check_reg_const_str() is only called with reg of\ntype PTR_TO_MAP_VALUE. Add a redundent type check in check_reg_const_str()\nto avoid misuse in the future. Other than this redundent check, there is\nno change in behavior.\n\nSigned-off-by: Song Liu <song@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Vadim Fedorenko <vadim.fedorenko@linux.dev>\nLink: https://lore.kernel.org/bpf/20231107045725.2278852-3-song@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Song Liu <song@kernel.org>",
          "date": "2023-11-09 19:07:38 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "045edee19d591e59ed53772bf6dfc9b1ed9577eb",
          "subject": "bpf: Introduce KF_ARG_PTR_TO_CONST_STR",
          "message": "Similar to ARG_PTR_TO_CONST_STR for BPF helpers, KF_ARG_PTR_TO_CONST_STR\nspecifies kfunc args that point to const strings. Annotation \"__str\" is\nused to specify kfunc arg of type KF_ARG_PTR_TO_CONST_STR. Also, add\ndocumentation for the \"__str\" annotation.\n\nbpf_get_file_xattr() will be the first kfunc that uses this type.\n\nSigned-off-by: Song Liu <song@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Vadim Fedorenko <vadim.fedorenko@linux.dev>\nLink: https://lore.kernel.org/bpf/20231107045725.2278852-4-song@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Song Liu <song@kernel.org>",
          "date": "2023-11-09 19:07:38 -0800",
          "modified_files": [
            "Documentation/bpf/kfuncs.rst",
            "kernel/bpf/verifier.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "cd9c127069c040d6b022f1ff32fed4b52b9a4017",
      "merge_subject": "Merge branch 'bpf-register-bounds-logic-and-testing-improvements'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nBPF register bounds logic and testing improvements\n\nThis patch set adds a big set of manual and auto-generated test cases\nvalidating BPF verifier's register bounds tracking and deduction logic. See\ndetails in the last patch.\n\nWe start with building a tester that validates existing <range> vs <scalar>\nverifier logic for range bounds. To make all this work, BPF verifier's logic\nneeded a bunch of improvements to handle some cases that previously were not\ncovered. This had no implications as to correctness of verifier logic, but it\nwas incomplete enough to cause significant disagreements with alternative\nimplementation of register bounds logic that tests in this patch set\nimplement. So we need BPF verifier logic improvements to make all the tests\npass. This is what we do in patches #3 through #9.\n\nThe end goal of this work, though, is to extend BPF verifier range state\ntracking such as to allow to derive new range bounds when comparing non-const\nregisters. There is some more investigative work required to investigate and\nfix existing potential issues with range tracking as part of ALU/ALU64\noperations, so <range> x <range> part of v5 patch set ([0]) is dropped until\nthese issues are sorted out.\n\nFor now, we include preparatory refactorings and clean ups, that set up BPF\nverifier code base to extend the logic to <range> vs <range> logic in\nsubsequent patch set. Patches #10-#16 perform preliminary refactorings without\nfunctionally changing anything. But they do clean up check_cond_jmp_op() logic\nand generalize a bunch of other pieces in is_branch_taken() logic.\n\n  [0] https://patchwork.kernel.org/project/netdevbpf/list/?series=797178&state=*\n\nv5->v6:\n  - dropped <range> vs <range> patches (original patches #18 through #23) to\n    add more register range sanity checks and fix preexisting issues;\n  - comments improvements, addressing other feedback on first 17 patches\n    (Eduard, Alexei);\nv4->v5:\n  - added entirety of verifier reg bounds tracking changes, now handling\n    <range> vs <range> cases (Alexei);\n  - added way more comments trying to explain why deductions added are\n    correct, hopefully they are useful and clarify things a bit (Daniel,\n    Shung-Hsi);\n  - added two preliminary selftests fixes necessary for RELEASE=1 build to\n    work again, it keeps breaking.\nv3->v4:\n  - improvements to reg_bounds tester (progress report, split 32-bit and\n    64-bit ranges, fix various verbosity output issues, etc);\nv2->v3:\n  - fix a subtle little-endianness assumption inside parge_reg_state() (CI);\nv1->v2:\n  - fix compilation when building selftests with llvm-16 toolchain (CI).\n====================\n\nLink: https://lore.kernel.org/r/20231102033759.2541186-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-11-09 18:58:40 -0800",
      "commits": [
        {
          "hash": "2b62aa59d02ed281fa4fc218df3ca91b773e1e62",
          "subject": "selftests/bpf: fix RELEASE=1 build for tc_opts",
          "message": "Compiler complains about malloc(). We also don't need to dynamically\nallocate anything, so make the life easier by using statically sized\nbuffer.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:38 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/tc_opts.c"
          ]
        },
        {
          "hash": "f4c7e887324f5776eef6e6e47a90e0ac8058a7a8",
          "subject": "selftests/bpf: satisfy compiler by having explicit return in btf test",
          "message": "Some compilers complain about get_pprint_mapv_size() not returning value\nin some code paths. Fix with explicit return.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:38 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/btf.c"
          ]
        },
        {
          "hash": "93f7378734b595fb61e89b802002fb7e3a1267d2",
          "subject": "bpf: derive smin/smax from umin/max bounds",
          "message": "Add smin/smax derivation from appropriate umin/umax values. Previously the\nlogic was surprisingly asymmetric, trying to derive umin/umax from smin/smax\n(if possible), but not trying to do the same in the other direction. A simple\naddition to __reg64_deduce_bounds() fixes this.\n\nAdded also generic comment about u64/s64 ranges and their relationship.\nHopefully that helps readers to understand all the bounds deductions\na bit better.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:39 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "d540517990a9d105bf0312760665964916ac044f",
          "subject": "bpf: derive smin32/smax32 from umin32/umax32 bounds",
          "message": "All the logic that applies to u64 vs s64, equally applies for u32 vs s32\nrelationships (just taken in a smaller 32-bit numeric space). So do the\nsame deduction of smin32/smax32 from umin32/umax32, if we can.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:39 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "c1efab6468fd5ef541d47d81dbb62cca27f8db3b",
          "subject": "bpf: derive subreg bounds from full bounds when upper 32 bits are constant",
          "message": "Comments in code try to explain the idea behind why this is correct.\nPlease check the code and comments.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:39 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "6593f2e6741f03b49bffc9d55ddd4c1c47853c39",
          "subject": "bpf: add special smin32/smax32 derivation from 64-bit bounds",
          "message": "Add a special case where we can derive valid s32 bounds from umin/umax\nor smin/smax by stitching together negative s32 subrange and\nnon-negative s32 subrange. That requires upper 32 bits to form a [N, N+1]\nrange in u32 domain (taking into account wrap around, so 0xffffffff\nto 0x00000000 is a valid [N, N+1] range in this sense). See code comment\nfor concrete examples.\n\nEduard Zingerman also provided an alternative explanation ([0]) for more\nmathematically inclined readers:\n\nSuppose:\n. there are numbers a, b, c\n. 2**31 <= b < 2**32\n. 0 <= c < 2**31\n. umin = 2**32 * a + b\n. umax = 2**32 * (a + 1) + c\n\nThe number of values in the range represented by [umin; umax] is:\n. N = umax - umin + 1 = 2**32 + c - b + 1\n. min(N) = 2**32 + 0 - (2**32-1) + 1 = 2, with b = 2**32-1, c = 0\n. max(N) = 2**32 + (2**31 - 1) - 2**31 + 1 = 2**32, with b = 2**31, c = 2**31-1\n\nHence [(s32)b; (s32)c] forms a valid range.\n\n  [0] https://lore.kernel.org/bpf/d7af631802f0cfae20df77fe70068702d24bbd31.camel@gmail.com/\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:39 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "c51d5ad6543cc36334ef1fcd762d0df767a0bf7e",
          "subject": "bpf: improve deduction of 64-bit bounds from 32-bit bounds",
          "message": "Add a few interesting cases in which we can tighten 64-bit bounds based\non newly learnt information about 32-bit bounds. E.g., when full u64/s64\nregisters are used in BPF program, and then eventually compared as\nu32/s32. The latter comparison doesn't change the value of full\nregister, but it does impose new restrictions on possible lower 32 bits\nof such full registers. And we can use that to derive additional full\nregister bounds information.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231102033759.2541186-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:39 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "d7f00873817129e62f8c70891cb13c8eafe9feef",
          "subject": "bpf: try harder to deduce register bounds from different numeric domains",
          "message": "There are cases (caught by subsequent reg_bounds tests in selftests/bpf)\nwhere performing one round of __reg_deduce_bounds() doesn't propagate\nall the information from, say, s32 to u32 bounds and than from newly\nlearned u32 bounds back to u64 and s64. So perform __reg_deduce_bounds()\ntwice to make sure such derivations are propagated fully after\nreg_bounds_sync().\n\nOne such example is test `(s64)[0xffffffff00000001; 0] (u64)<\n0xffffffff00000000` from selftest patch from this patch set. It demonstrates an\nintricate dance of u64 -> s64 -> u64 -> u32 bounds adjustments, which requires\ntwo rounds of __reg_deduce_bounds(). Here are corresponding refinement log from\nselftest, showing evolution of knowledge.\n\nREFINING (FALSE R1) (u64)SRC=[0xffffffff00000000; U64_MAX] (u64)DST_OLD=[0; U64_MAX] (u64)DST_NEW=[0xffffffff00000000; U64_MAX]\nREFINING (FALSE R1) (u64)SRC=[0xffffffff00000000; U64_MAX] (s64)DST_OLD=[0xffffffff00000001; 0] (s64)DST_NEW=[0xffffffff00000001; -1]\nREFINING (FALSE R1) (s64)SRC=[0xffffffff00000001; -1] (u64)DST_OLD=[0xffffffff00000000; U64_MAX] (u64)DST_NEW=[0xffffffff00000001; U64_MAX]\nREFINING (FALSE R1) (u64)SRC=[0xffffffff00000001; U64_MAX] (u32)DST_OLD=[0; U32_MAX] (u32)DST_NEW=[1; U32_MAX]\n\nR1 initially has smin/smax set to [0xffffffff00000001; -1], while umin/umax is\nunknown. After (u64)< comparison, in FALSE branch we gain knowledge that\numin/umax is [0xffffffff00000000; U64_MAX]. That causes smin/smax to learn that\nzero can't happen and upper bound is -1. Then smin/smax is adjusted from\numin/umax improving lower bound from 0xffffffff00000000 to 0xffffffff00000001.\nAnd then eventually umin32/umax32 bounds are drived from umin/umax and become\n[1; U32_MAX].\n\nSelftest in the last patch is actually implementing a multi-round fixed-point\nconvergence logic, but so far all the tests are handled by two rounds of\nreg_bounds_sync() on the verifier state, so we keep it simple for now.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:39 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "9e314f5d8682e1fe6ac214fb34580a238b6fd3c4",
          "subject": "bpf: drop knowledge-losing __reg_combine_{32,64}_into_{64,32} logic",
          "message": "When performing 32-bit conditional operation operating on lower 32 bits\nof a full 64-bit register, register full value isn't changed. We just\npotentially gain new knowledge about that register's lower 32 bits.\n\nUnfortunately, __reg_combine_{32,64}_into_{64,32} logic that\nreg_set_min_max() performs as a last step, can lose information in some\ncases due to __mark_reg64_unbounded() and __reg_assign_32_into_64().\nThat's bad and completely unnecessary. Especially __reg_assign_32_into_64()\nlooks completely out of place here, because we are not performing\nzero-extending subregister assignment during conditional jump.\n\nSo this patch replaced __reg_combine_* with just a normal\nreg_bounds_sync() which will do a proper job of deriving u64/s64 bounds\nfrom u32/s32, and vice versa (among all other combinations).\n\n__reg_combine_64_into_32() is also used in one more place,\ncoerce_reg_to_size(), while handling 1- and 2-byte register loads.\nLooking into this, it seems like besides marking subregister as\nunbounded before performing reg_bounds_sync(), we were also performing\ndeduction of smin32/smax32 and umin32/umax32 bounds from respective\nsmin/smax and umin/umax bounds. It's now redundant as reg_bounds_sync()\nperforms all the same logic more generically (e.g., without unnecessary\nassumption that upper 32 bits of full register should be zero).\n\nLong story short, we remove __reg_combine_64_into_32() completely, and\ncoerce_reg_to_size() now only does resetting subreg to unbounded and then\nperforming reg_bounds_sync() to recover as much information as possible\nfrom 64-bit umin/umax and smin/smax bounds, set explicitly in\ncoerce_reg_to_size() earlier.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231102033759.2541186-10-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:39 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "c2a3ab094683ddc154879a1364fc7cb0228f96a6",
          "subject": "bpf: rename is_branch_taken reg arguments to prepare for the second one",
          "message": "Just taking mundane refactoring bits out into a separate patch. No\nfunctional changes.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231102033759.2541186-12-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:39 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "c31534267c180f7ed00288d239a501b554885300",
          "subject": "bpf: generalize is_branch_taken() to work with two registers",
          "message": "While still assuming that second register is a constant, generalize\nis_branch_taken-related code to accept two registers instead of register\nplus explicit constant value. This also, as a side effect, allows to\nsimplify check_cond_jmp_op() by unifying BPF_K case with BPF_X case, for\nwhich we use a fake register to represent BPF_K's imm constant as\na register.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231102033759.2541186-13-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:39 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "c697289efe4ef38bc5c62f119cb74433f784b826",
          "subject": "bpf: move is_branch_taken() down",
          "message": "Move is_branch_taken() slightly down. In subsequent patched we'll need\nboth flip_opcode() and is_pkt_ptr_branch_taken() for is_branch_taken(),\nbut instead of sprinkling forward declarations around, it makes more\nsense to move is_branch_taken() lower below is_pkt_ptr_branch_taken(),\nand also keep it closer to very tightly related reg_set_min_max(), as\nthey are two critical parts of the same SCALAR range tracking logic.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-14-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:39 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b74c2a842bba941945279027083fcee1e9aaa73f",
          "subject": "bpf: generalize is_branch_taken to handle all conditional jumps in one place",
          "message": "Make is_branch_taken() a single entry point for branch pruning decision\nmaking, handling both pointer vs pointer, pointer vs scalar, and scalar\nvs scalar cases in one place. This also nicely cleans up check_cond_jmp_op().\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-15-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:40 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "4d345887d2e5a1915600cb5d37b16c4088c6ee1c",
          "subject": "bpf: unify 32-bit and 64-bit is_branch_taken logic",
          "message": "Combine 32-bit and 64-bit is_branch_taken logic for SCALAR_VALUE\nregisters. It makes it easier to see parallels between two domains\n(32-bit and 64-bit), and makes subsequent refactoring more\nstraightforward.\n\nNo functional changes.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-16-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:40 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "811476e9cc578cb6c776627ac069dc45a8431791",
          "subject": "bpf: prepare reg_set_min_max for second set of registers",
          "message": "Similarly to is_branch_taken()-related refactorings, start preparing\nreg_set_min_max() to handle more generic case of two non-const\nregisters. Start with renaming arguments to accommodate later addition\nof second register as an input argument.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-17-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:40 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "4621202adc5bc0d1006af37fe8b9aca131387d3c",
          "subject": "bpf: generalize reg_set_min_max() to handle two sets of two registers",
          "message": "Change reg_set_min_max() to take FALSE/TRUE sets of two registers each,\ninstead of assuming that we are always comparing to a constant. For now\nwe still assume that right-hand side registers are constants (and make\nsure that's the case by swapping src/dst regs, if necessary), but\nsubsequent patches will remove this limitation.\n\nreg_set_min_max() is now called unconditionally for any register\ncomparison, so that might include pointer vs pointer. This makes it\nconsistent with is_branch_taken() generality. But we currently only\nsupport adjustments based on SCALAR vs SCALAR comparisons, so\nreg_set_min_max() has to guard itself againts pointers.\n\nTaking two by two registers allows to further unify and simplify\ncheck_cond_jmp_op() logic. We utilize fake register for BPF_K\nconditional jump case, just like with is_branch_taken() part.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-18-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-11-09 18:58:40 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "94e88b8a3e50d3e60c3ba6a5c316729587595210",
      "merge_subject": "Merge branch 'bpf-fix-precision-tracking-for-bpf_alu-bpf_to_be-bpf_end'",
      "merge_body": "Shung-Hsi Yu says:\n\n====================\nbpf: Fix precision tracking for BPF_ALU | BPF_TO_BE | BPF_END\n\nChanges since v1:\n- add test for negation and bswap (Alexei, Eduard)\n- add test for BPF_TO_LE as well to cover all types of BPF_END opcode\n- remove vals map and trigger backtracking with jump instead, based of\n  Eduard's code\n- v1 at https://lore.kernel.org/bpf/20231030132145.20867-1-shung-hsi.yu@suse.com\n\nThis patchset fixes and adds selftest for the issue reported by Mohamed\nMahmoud and Toke H\u00f8iland-J\u00f8rgensen where the kernel can run into a\nverifier bug during backtracking of BPF_ALU | BPF_TO_BE | BPF_END\ninstruction[0]. As seen in the verifier log below, r0 was incorrectly\nmarked as precise even tough its value was not being used.\n\nPatch 1 fixes the issue based on Andrii's analysis, and patch 2 adds a\nselftest for such case using inline assembly. Please see individual\npatch for detail.\n\n    ...\n\tmark_precise: frame2: regs=r2 stack= before 1891: (77) r2 >>= 56\n\tmark_precise: frame2: regs=r2 stack= before 1890: (dc) r2 = be64 r2\n\tmark_precise: frame2: regs=r0,r2 stack= before 1889: (73) *(u8 *)(r1 +47) = r3\n\t...\n\tmark_precise: frame2: regs=r0 stack= before 212: (85) call pc+1617\n\tBUG regs 1\n\tprocessed 5112 insns (limit 1000000) max_states_per_insn 4 total_states 92 peak_states 90 mark_read 20\n\n0: https://lore.kernel.org/r/87jzrrwptf.fsf@toke.dk\n\nShung-Hsi Yu (2):\n  bpf: Fix precision tracking for BPF_ALU | BPF_TO_BE | BPF_END\n  selftests/bpf: precision tracking test for BPF_NEG and BPF_END\n\n kernel/bpf/verifier.c                         |  7 +-\n .../selftests/bpf/prog_tests/verifier.c       |  2 +\n .../selftests/bpf/progs/verifier_precision.c  | 93 +++++++++++++++++++\n 3 files changed, 101 insertions(+), 1 deletion(-)\n create mode 100644 tools/testing/selftests/bpf/progs/verifier_precision.c\n\nbase-commit: c17cda15cc86e65e9725641daddcd7a63cc9ad01\n====================\n\nLink: https://lore.kernel.org/r/20231102053913.12004-1-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-11-01 22:54:28 -0700",
      "commits": [
        {
          "hash": "291d044fd51f8484066300ee42afecf8c8db7b3a",
          "subject": "bpf: Fix precision tracking for BPF_ALU | BPF_TO_BE | BPF_END",
          "message": "BPF_END and BPF_NEG has a different specification for the source bit in\nthe opcode compared to other ALU/ALU64 instructions, and is either\nreserved or use to specify the byte swap endianness. In both cases the\nsource bit does not encode source operand location, and src_reg is a\nreserved field.\n\nbacktrack_insn() currently does not differentiate BPF_END and BPF_NEG\nfrom other ALU/ALU64 instructions, which leads to r0 being incorrectly\nmarked as precise when processing BPF_ALU | BPF_TO_BE | BPF_END\ninstructions. This commit teaches backtrack_insn() to correctly mark\nprecision for such case.\n\nWhile precise tracking of BPF_NEG and other BPF_END instructions are\ncorrect and does not need fixing, this commit opt to process all BPF_NEG\nand BPF_END instructions within the same if-clause to better align with\ncurrent convention used in the verifier (e.g. check_alu_op).\n\nFixes: b5dc0163d8fd (\"bpf: precise scalar_value tracking\")\nCc: stable@vger.kernel.org\nReported-by: Mohamed Mahmoud <mmahmoud@redhat.com>\nCloses: https://lore.kernel.org/r/87jzrrwptf.fsf@toke.dk\nTested-by: Toke H\u00f8iland-J\u00f8rgensen <toke@redhat.com>\nTested-by: Tao Lyu <tao.lyu@epfl.ch>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231102053913.12004-2-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Shung-Hsi Yu <shung-hsi.yu@suse.com>",
          "date": "2023-11-01 22:54:27 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "3c41971550f58f2e006c58aa71e8c23ad312110f",
          "subject": "selftests/bpf: precision tracking test for BPF_NEG and BPF_END",
          "message": "As seen from previous commit that fix backtracking for BPF_ALU | BPF_TO_BE\n| BPF_END, both BPF_NEG and BPF_END require special handling. Add tests\nwritten with inline assembly to check that the verifier does not incorrecly\nuse the src_reg field of BPF_NEG and BPF_END (including bswap added in v4).\n\nSuggested-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231102053913.12004-4-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Shung-Hsi Yu <shung-hsi.yu@suse.com>",
          "date": "2023-11-01 22:54:28 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_precision.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "698b8c5e3b5505ac00102caf9e4843b71192b586",
      "merge_subject": "Merge branch 'relax-allowlist-for-open-coded-css_task-iter'",
      "merge_body": "Chuyi Zhou says:\n\n====================\nRelax allowlist for open-coded css_task iter\n\nHi,\nThe patchset aims to relax the allowlist for open-coded css_task iter\nsuggested by Alexei[1].\n\nPlease see individual patches for more details. And comments are always\nwelcome.\n\nPatch summary:\n * Patch #1: Relax the allowlist and let css_task iter can be used in\n   bpf iters and any sleepable progs.\n * Patch #2: Add a test in cgroup_iters.c which demonstrates how\n   css_task iters can be combined with cgroup iter.\n * Patch #3: Add a test to prove css_task iter can be used in normal\n * sleepable progs.\nlink[1]:https://lore.kernel.org/lkml/CAADnVQKafk_junRyE=-FVAik4hjTRDtThymYGEL8hGTuYoOGpA@mail.gmail.com/\n---\n\nChanges in v2:\n * Fix the incorrect logic in check_css_task_iter_allowlist. Use\n   expected_attach_type to check whether we are using bpf_iters.\n * Link to v1:https://lore.kernel.org/bpf/20231022154527.229117-1-zhouchuyi@bytedance.com/T/#m946f9cde86b44a13265d9a44c5738a711eb578fd\nChanges in v3:\n * Add a testcase to prove css_task can be used in fentry.s\n * Link to v2:https://lore.kernel.org/bpf/20231024024240.42790-1-zhouchuyi@bytedance.com/T/#m14a97041ff56c2df21bc0149449abd275b73f6a3\nChanges in v4:\n * Add Yonghong's ack for patch #1 and patch #2.\n * Solve Yonghong's comments for patch #2\n * Move prog 'iter_css_task_for_each_sleep' from iters_task_failure.c to\n   iters_css_task.c. Use RUN_TESTS to prove we can load this prog.\n * Link to v3:https://lore.kernel.org/bpf/20231025075914.30979-1-zhouchuyi@bytedance.com/T/#m3200d8ad29af4ffab97588e297361d0a45d7585d\n\n---\n====================\n\nLink: https://lore.kernel.org/r/20231031050438.93297-1-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-11-01 22:49:20 -0700",
      "commits": [
        {
          "hash": "3091b667498b0a212e760e1033e5f9b8c33a948f",
          "subject": "bpf: Relax allowlist for css_task iter",
          "message": "The newly added open-coded css_task iter would try to hold the global\ncss_set_lock in bpf_iter_css_task_new, so the bpf side has to be careful in\nwhere it allows to use this iter. The mainly concern is dead locking on\ncss_set_lock. check_css_task_iter_allowlist() in verifier enforced css_task\ncan only be used in bpf_lsm hooks and sleepable bpf_iter.\n\nThis patch relax the allowlist for css_task iter. Any lsm and any iter\n(even non-sleepable) and any sleepable are safe since they would not hold\nthe css_set_lock before entering BPF progs context.\n\nThis patch also fixes the misused BPF_TRACE_ITER in\ncheck_css_task_iter_allowlist which compared bpf_prog_type with\nbpf_attach_type.\n\nFixes: 9c66dc94b62ae (\"bpf: Introduce css_task open-coded iterator kfuncs\")\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20231031050438.93297-2-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
          "date": "2023-11-01 22:49:20 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/iters_task_failure.c"
          ]
        },
        {
          "hash": "f49843afde6771ef6ed5d021eacafacfc98a58bf",
          "subject": "selftests/bpf: Add tests for css_task iter combining with cgroup iter",
          "message": "This patch adds a test which demonstrates how css_task iter can be combined\nwith cgroup iter and it won't cause deadlock, though cgroup iter is not\nsleepable.\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20231031050438.93297-3-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
          "date": "2023-11-01 22:49:20 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/cgroup_iter.c",
            "tools/testing/selftests/bpf/progs/iters_css_task.c"
          ]
        },
        {
          "hash": "d8234d47c4aa494d789b85562fa90e837b4575f9",
          "subject": "selftests/bpf: Add test for using css_task iter in sleepable progs",
          "message": "This Patch add a test to prove css_task iter can be used in normal\nsleepable progs.\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20231031050438.93297-4-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
          "date": "2023-11-01 22:49:20 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/iters.c",
            "tools/testing/selftests/bpf/progs/iters_css_task.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "b479d38ba959a8e3ffc4d9f760a9f2e4b9027e66",
      "merge_subject": "Merge branch 'bpf-fix-incorrect-immediate-spill'",
      "merge_body": "Hao Sun says:\n\n====================\nbpf: Fix incorrect immediate spill\n\nImmediate is incorrectly cast to u32 before being spilled, losing sign\ninformation. The range information is incorrect after load again. Fix\nimmediate spill by remove the cast. The second patch add a test case\nfor this.\n\nSigned-off-by: Hao Sun <sunhao.th@gmail.com>\n---\nChanges in v3:\n- Change the expected log to fix the test case\n- Link to v2: https://lore.kernel.org/r/20231101-fix-check-stack-write-v2-0-cb7c17b869b0@gmail.com\n\nChanges in v2:\n- Add fix and cc tags.\n- Link to v1: https://lore.kernel.org/r/20231026-fix-check-stack-write-v1-0-6b325ef3ce7e@gmail.com\n\n---\n====================\n\nLink: https://lore.kernel.org/r/20231101-fix-check-stack-write-v3-0-f05c2b1473d5@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-11-01 22:30:28 -0700",
      "commits": [
        {
          "hash": "811c363645b33e6e22658634329e95f383dfc705",
          "subject": "bpf: Fix check_stack_write_fixed_off() to correctly spill imm",
          "message": "In check_stack_write_fixed_off(), imm value is cast to u32 before being\nspilled to the stack. Therefore, the sign information is lost, and the\nrange information is incorrect when load from the stack again.\n\nFor the following prog:\n0: r2 = r10\n1: *(u64*)(r2 -40) = -44\n2: r0 = *(u64*)(r2 - 40)\n3: if r0 s<= 0xa goto +2\n4: r0 = 1\n5: exit\n6: r0  = 0\n7: exit\n\nThe verifier gives:\nfunc#0 @0\n0: R1=ctx(off=0,imm=0) R10=fp0\n0: (bf) r2 = r10                      ; R2_w=fp0 R10=fp0\n1: (7a) *(u64 *)(r2 -40) = -44        ; R2_w=fp0 fp-40_w=4294967252\n2: (79) r0 = *(u64 *)(r2 -40)         ; R0_w=4294967252 R2_w=fp0\nfp-40_w=4294967252\n3: (c5) if r0 s< 0xa goto pc+2\nmark_precise: frame0: last_idx 3 first_idx 0 subseq_idx -1\nmark_precise: frame0: regs=r0 stack= before 2: (79) r0 = *(u64 *)(r2 -40)\n3: R0_w=4294967252\n4: (b7) r0 = 1                        ; R0_w=1\n5: (95) exit\nverification time 7971 usec\nstack depth 40\nprocessed 6 insns (limit 1000000) max_states_per_insn 0 total_states 0\npeak_states 0 mark_read 0\n\nSo remove the incorrect cast, since imm field is declared as s32, and\n__mark_reg_known() takes u64, so imm would be correctly sign extended\nby compiler.\n\nFixes: ecdf985d7615 (\"bpf: track immediate values written to stack by BPF_ST instruction\")\nCc: stable@vger.kernel.org\nSigned-off-by: Hao Sun <sunhao.th@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231101-fix-check-stack-write-v3-1-f05c2b1473d5@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hao Sun <sunhao.th@gmail.com>",
          "date": "2023-11-01 22:30:27 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "85eb035e6cfd615071256592e1dbe72c1d99c24b",
          "subject": "selftests/bpf: Add test for immediate spilled to stack",
          "message": "Add a test to check if the verifier correctly reason about the sign\nof an immediate spilled to stack by BPF_ST instruction.\n\nSigned-off-by: Hao Sun <sunhao.th@gmail.com>\nLink: https://lore.kernel.org/r/20231101-fix-check-stack-write-v3-2-f05c2b1473d5@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Hao Sun <sunhao.th@gmail.com>",
          "date": "2023-11-01 22:30:27 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/verifier/bpf_st_mem.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "dedd6c894110371d3c218cf24ecca2f0730408ac",
      "merge_subject": "Merge branch 'exact-states-comparison-for-iterator-convergence-checks'",
      "merge_body": "Eduard Zingerman says:\n\n====================\nexact states comparison for iterator convergence checks\n\nIterator convergence logic in is_state_visited() uses state_equals()\nfor states with branches counter > 0 to check if iterator based loop\nconverges. This is not fully correct because state_equals() relies on\npresence of read and precision marks on registers. These marks are not\nguaranteed to be finalized while state has branches.\nCommit message for patch #3 describes a program that exhibits such\nbehavior.\n\nThis patch-set aims to fix iterator convergence logic by adding notion\nof exact states comparison. Exact comparison does not rely on presence\nof read or precision marks and thus is more strict.\nAs explained in commit message for patch #3 exact comparisons require\naddition of speculative register bounds widening. The end result for\nBPF verifier users could be summarized as follows:\n\n(!) After this update verifier would reject programs that conjure an\n    imprecise value on the first loop iteration and use it as precise\n    on the second (for iterator based loops).\n\nI urge people to at least skim over the commit message for patch #3.\n\nPatches are organized as follows:\n- patches #1,2: moving/extracting utility functions;\n- patch #3: introduces exact mode for states comparison and adds\n  widening heuristic;\n- patch #4: adds test-cases that demonstrate why the series is\n  necessary;\n- patch #5: extends patch #3 with a notion of state loop entries,\n  these entries have to be tracked to correctly identify that\n  different verifier states belong to the same states loop;\n- patch #6: adds a test-case that demonstrates a program\n  which requires loop entry tracking for correct verification;\n- patch #7: just adds a few debug prints.\n\nThe following actions are planned as a followup for this patch-set:\n- implementation has to be adapted for callbacks handling logic as a\n  part of a fix for [1];\n- it is necessary to explore ways to improve widening heuristic to\n  handle iters_task_vma test w/o need to insert barrier_var() calls;\n- explored states eviction logic on cache miss has to be extended\n  to either:\n  - allow eviction of checkpoint states -or-\n  - be sped up in case if there are many active checkpoints associated\n    with the same instruction.\n\nThe patch-set is a followup for mailing list discussion [1].\n\nChangelog:\n- V2 [3] -> V3:\n  - correct check for stack spills in widen_imprecise_scalars(),\n    added test case progs/iters.c:widen_spill to check the behavior\n    (suggested by Andrii);\n  - allow eviction of checkpoint states in is_state_visited() to avoid\n    pathological verifier performance when iterator based loop does not\n    converge (discussion with Alexei).\n- V1 [2] -> V2, applied changes suggested by Alexei offlist:\n  - __explored_state() function removed;\n  - same_callsites() function is now used in clean_live_states();\n  - patches #1,2 are added as preparatory code movement;\n  - in process_iter_next_call() a safeguard is added to verify that\n    cur_st->parent exists and has expected insn index / call sites.\n\n[1] https://lore.kernel.org/bpf/97a90da09404c65c8e810cf83c94ac703705dc0e.camel@gmail.com/\n[2] https://lore.kernel.org/bpf/20231021005939.1041-1-eddyz87@gmail.com/\n[3] https://lore.kernel.org/bpf/20231022010812.9201-1-eddyz87@gmail.com/\n====================\n\nLink: https://lore.kernel.org/r/20231024000917.12153-1-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-10-23 21:49:32 -0700",
      "commits": [
        {
          "hash": "3c4e420cb6536026ddd50eaaff5f30e4f144200d",
          "subject": "bpf: move explored_state() closer to the beginning of verifier.c",
          "message": "Subsequent patches would make use of explored_state() function.\nMove it up to avoid adding unnecessary prototype.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-10-23 21:49:31 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "4c97259abc9bc8df7712f76f58ce385581876857",
          "subject": "bpf: extract same_callsites() as utility function",
          "message": "Extract same_callsites() from clean_live_states() as a utility function.\nThis function would be used by the next patch in the set.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-3-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-10-23 21:49:31 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "2793a8b015f7f1caadb9bce9c63dc659f7522676",
          "subject": "bpf: exact states comparison for iterator convergence checks",
          "message": "Convergence for open coded iterators is computed in is_state_visited()\nby examining states with branches count > 1 and using states_equal().\nstates_equal() computes sub-state relation using read and precision marks.\nRead and precision marks are propagated from children states,\nthus are not guaranteed to be complete inside a loop when branches\ncount > 1. This could be demonstrated using the following unsafe program:\n\n     1. r7 = -16\n     2. r6 = bpf_get_prandom_u32()\n     3. while (bpf_iter_num_next(&fp[-8])) {\n     4.   if (r6 != 42) {\n     5.     r7 = -32\n     6.     r6 = bpf_get_prandom_u32()\n     7.     continue\n     8.   }\n     9.   r0 = r10\n    10.   r0 += r7\n    11.   r8 = *(u64 *)(r0 + 0)\n    12.   r6 = bpf_get_prandom_u32()\n    13. }\n\nHere verifier would first visit path 1-3, create a checkpoint at 3\nwith r7=-16, continue to 4-7,3 with r7=-32.\n\nBecause instructions at 9-12 had not been visitied yet existing\ncheckpoint at 3 does not have read or precision mark for r7.\nThus states_equal() would return true and verifier would discard\ncurrent state, thus unsafe memory access at 11 would not be caught.\n\nThis commit fixes this loophole by introducing exact state comparisons\nfor iterator convergence logic:\n- registers are compared using regs_exact() regardless of read or\n  precision marks;\n- stack slots have to have identical type.\n\nUnfortunately, this is too strict even for simple programs like below:\n\n    i = 0;\n    while(iter_next(&it))\n      i++;\n\nAt each iteration step i++ would produce a new distinct state and\neventually instruction processing limit would be reached.\n\nTo avoid such behavior speculatively forget (widen) range for\nimprecise scalar registers, if those registers were not precise at the\nend of the previous iteration and do not match exactly.\n\nThis a conservative heuristic that allows to verify wide range of\nprograms, however it precludes verification of programs that conjure\nan imprecise value on the first loop iteration and use it as precise\non the second.\n\nTest case iter_task_vma_for_each() presents one of such cases:\n\n        unsigned int seen = 0;\n        ...\n        bpf_for_each(task_vma, vma, task, 0) {\n                if (seen >= 1000)\n                        break;\n                ...\n                seen++;\n        }\n\nHere clang generates the following code:\n\n<LBB0_4>:\n      24:       r8 = r6                          ; stash current value of\n                ... body ...                       'seen'\n      29:       r1 = r10\n      30:       r1 += -0x8\n      31:       call bpf_iter_task_vma_next\n      32:       r6 += 0x1                        ; seen++;\n      33:       if r0 == 0x0 goto +0x2 <LBB0_6>  ; exit on next() == NULL\n      34:       r7 += 0x10\n      35:       if r8 < 0x3e7 goto -0xc <LBB0_4> ; loop on seen < 1000\n\n<LBB0_6>:\n      ... exit ...\n\nNote that counter in r6 is copied to r8 and then incremented,\nconditional jump is done using r8. Because of this precision mark for\nr6 lags one state behind of precision mark on r8 and widening logic\nkicks in.\n\nAdding barrier_var(seen) after conditional is sufficient to force\nclang use the same register for both counting and conditional jump.\n\nThis issue was discussed in the thread [1] which was started by\nAndrew Werner <awerner32@gmail.com> demonstrating a similar bug\nin callback functions handling. The callbacks would be addressed\nin a followup patch.\n\n[1] https://lore.kernel.org/bpf/97a90da09404c65c8e810cf83c94ac703705dc0e.camel@gmail.com/\n\nCo-developed-by: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nCo-developed-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-10-23 21:49:31 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/iters_task_vma.c"
          ]
        },
        {
          "hash": "389ede06c2974b2f878a7ebff6b0f4f707f9db74",
          "subject": "selftests/bpf: tests with delayed read/precision makrs in loop body",
          "message": "These test cases try to hide read and precision marks from loop\nconvergence logic: marks would only be assigned on subsequent loop\niterations or after exploring states pushed to env->head stack first.\nWithout verifier fix to use exact states comparison logic for\niterators convergence these tests (except 'triple_continue') would be\nerrorneously marked as safe.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-5-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-10-23 21:49:31 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/iters.c"
          ]
        },
        {
          "hash": "2a0992829ea3864939d917a5c7b48be6629c6217",
          "subject": "bpf: correct loop detection for iterators convergence",
          "message": "It turns out that .branches > 0 in is_state_visited() is not a\nsufficient condition to identify if two verifier states form a loop\nwhen iterators convergence is computed. This commit adds logic to\ndistinguish situations like below:\n\n (I)            initial       (II)            initial\n                  |                             |\n                  V                             V\n     .---------> hdr                           ..\n     |            |                             |\n     |            V                             V\n     |    .------...                    .------..\n     |    |       |                     |       |\n     |    V       V                     V       V\n     |   ...     ...               .-> hdr     ..\n     |    |       |                |    |       |\n     |    V       V                |    V       V\n     |   succ <- cur               |   succ <- cur\n     |    |                        |    |\n     |    V                        |    V\n     |   ...                       |   ...\n     |    |                        |    |\n     '----'                        '----'\n\nFor both (I) and (II) successor 'succ' of the current state 'cur' was\npreviously explored and has branches count at 0. However, loop entry\n'hdr' corresponding to 'succ' might be a part of current DFS path.\nIf that is the case 'succ' and 'cur' are members of the same loop\nand have to be compared exactly.\n\nCo-developed-by: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nCo-developed-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nReviewed-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-6-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-10-23 21:49:32 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "64870feebecb7130291a55caf0ce839a87405a70",
          "subject": "selftests/bpf: test if state loops are detected in a tricky case",
          "message": "A convoluted test case for iterators convergence logic that\ndemonstrates that states with branch count equal to 0 might still be\na part of not completely explored loop.\n\nE.g. consider the following state diagram:\n\n               initial     Here state 'succ' was processed first,\n                 |         it was eventually tracked to produce a\n                 V         state identical to 'hdr'.\n    .---------> hdr        All branches from 'succ' had been explored\n    |            |         and thus 'succ' has its .branches == 0.\n    |            V\n    |    .------...        Suppose states 'cur' and 'succ' correspond\n    |    |       |         to the same instruction + callsites.\n    |    V       V         In such case it is necessary to check\n    |   ...     ...        whether 'succ' and 'cur' are identical.\n    |    |       |         If 'succ' and 'cur' are a part of the same loop\n    |    V       V         they have to be compared exactly.\n    |   succ <- cur\n    |    |\n    |    V\n    |   ...\n    |    |\n    '----'\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-7-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-10-23 21:49:32 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/iters.c"
          ]
        },
        {
          "hash": "b4d8239534fddc036abe4a0fdbf474d9894d4641",
          "subject": "bpf: print full verifier states on infinite loop detection",
          "message": "Additional logging in is_state_visited(): if infinite loop is detected\nprint full verifier state for both current and equivalent states.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-8-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-10-23 21:49:32 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "bab8ac3c5339d7ec5312dcd836cfa8645edb954f",
      "merge_subject": "Merge branch 'add-open-coded-task-css_task-and-css-iters'",
      "merge_body": "Chuyi Zhou says:\n\n====================\nAdd Open-coded task, css_task and css iters\n\nThis is version 6 of task, css_task and css iters support.\n\n--- Changelog ---\n\nv5 -> v6:\n\nPatch #3:\n * In bpf_iter_task_next, return pos rather than goto out. (Andrii)\nPatch #2, #3, #4:\n * Add the missing __diag_ignore_all to avoid kernel build warning\nPatch #5, #6, #7:\n * Add Andrii's ack\n\nPatch #8:\n * In BPF prog iter_css_task_for_each, return -EPERM rather than 0, and\n   ensure stack_mprotect() in iters.c not success. If not, it would cause\n   the subsequent 'test_lsm' fail, since the 'is_stack' check in\n   test_int_hook(lsm.c) would not be guaranteed.\n   (https://github.com/kernel-patches/bpf/actions/runs/6489662214/job/17624665086?pr=5790)\n\nv4 -> v5:https://lore.kernel.org/lkml/20231007124522.34834-1-zhouchuyi@bytedance.com/\n\nPatch 3~4:\n * Relax the BUILD_BUG_ON check in bpf_iter_task_new and bpf_iter_css_new to avoid\n   netdev/build_32bit CI error.\n   (https://netdev.bots.linux.dev/static/nipa/790929/13412333/build_32bit/stderr)\nPatch 8:\n * Initialize skel pointer to fix the LLVM-16 build CI error\n   (https://github.com/kernel-patches/bpf/actions/runs/6462875618/job/17545170863)\n\nv3 -> v4:https://lore.kernel.org/all/20230925105552.817513-1-zhouchuyi@bytedance.com/\n\n* Address all the comments from Andrii in patch-3 ~ patch-6\n* Collect Tejun's ack\n* Add a extra patch to rename bpf_iter_task.c to bpf_iter_tasks.c\n* Seperate three BPF program files for selftests (iters_task.c iters_css_task.c iters_css.c)\n\nv2 -> v3:https://lore.kernel.org/lkml/20230912070149.969939-1-zhouchuyi@bytedance.com/\n\nPatch 1 (cgroup: Prepare for using css_task_iter_*() in BPF)\n  * Add tj's ack and Alexei's suggest-by.\nPatch 2 (bpf: Introduce css_task open-coded iterator kfuncs)\n  * Use bpf_mem_alloc/bpf_mem_free rather than kzalloc()\n  * Add KF_TRUSTED_ARGS for bpf_iter_css_task_new (Alexei)\n  * Move bpf_iter_css_task's definition from uapi/linux/bpf.h to\n    kernel/bpf/task_iter.c and we can use it from vmlinux.h\n  * Move bpf_iter_css_task_XXX's declaration from bpf_helpers.h to\n    bpf_experimental.h\nPatch 3 (Introduce task open coded iterator kfuncs)\n  * Change th API design keep consistent with SEC(\"iter/task\"), support\n    iterating all threads(BPF_TASK_ITERATE_ALL) and threads of a\n    specific task (BPF_TASK_ITERATE_THREAD).\uff08Andrii)\n  * Move bpf_iter_task's definition from uapi/linux/bpf.h to\n    kernel/bpf/task_iter.c and we can use it from vmlinux.h\n  * Move bpf_iter_task_XXX's declaration from bpf_helpers.h to\n    bpf_experimental.h\nPatch 4 (Introduce css open-coded iterator kfuncs)\n  * Change th API design keep consistent with cgroup_iters, reuse\n    BPF_CGROUP_ITER_DESCENDANTS_PRE/BPF_CGROUP_ITER_DESCENDANTS_POST\n    /BPF_CGROUP_ITER_ANCESTORS_UP(Andrii)\n  * Add KF_TRUSTED_ARGS for bpf_iter_css_new\n  * Move bpf_iter_css's definition from uapi/linux/bpf.h to\n    kernel/bpf/task_iter.c and we can use it from vmlinux.h\n  * Move bpf_iter_css_XXX's declaration from bpf_helpers.h to\n    bpf_experimental.h\nPatch 5 (teach the verifier to enforce css_iter and task_iter in RCU CS)\n  * Add KF flag KF_RCU_PROTECTED to maintain kfuncs which need RCU CS.(Andrii)\n  * Consider STACK_ITER when using bpf_for_each_spilled_reg.\nPatch 6 (Let bpf_iter_task_new accept null task ptr)\n  * Add this extra patch to let bpf_iter_task_new accept a 'nullable'\n  * task pointer(Andrii)\nPatch 7 (selftests/bpf: Add tests for open-coded task and css iter)\n  * Add failure testcase(Alexei)\n\nChanges from v1(https://lore.kernel.org/lkml/20230827072057.1591929-1-zhouchuyi@bytedance.com/):\n- Add a pre-patch to make some preparations before supporting css_task\n  iters.(Alexei)\n- Add an allowlist for css_task iters(Alexei)\n- Let bpf progs do explicit bpf_rcu_read_lock() when using process\n  iters and css_descendant iters.(Alexei)\n---------------------\n\nIn some BPF usage scenarios, it will be useful to iterate the process and\ncss directly in the BPF program. One of the expected scenarios is\ncustomizable OOM victim selection via BPF[1].\n\nInspired by Dave's task_vma iter[2], this patchset adds three types of\nopen-coded iterator kfuncs:\n\n1. bpf_task_iters. It can be used to\n1) iterate all process in the system, like for_each_forcess() in kernel.\n2) iterate all threads in the system.\n3) iterate all threads of a specific task\n\n2. bpf_css_iters. It works like css_task_iter_{start, next, end} and would\nbe used to iterating tasks/threads under a css.\n\n3. css_iters. It works like css_next_descendant_{pre, post} to iterating all\ndescendant css.\n\nBPF programs can use these kfuncs directly or through bpf_for_each macro.\n\nlink[1]: https://lore.kernel.org/lkml/20230810081319.65668-1-zhouchuyi@bytedance.com/\nlink[2]: https://lore.kernel.org/all/20230810183513.684836-1-davemarchevsky@fb.com/\n====================\n\nLink: https://lore.kernel.org/r/20231018061746.111364-1-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-10-19 17:02:47 -0700",
      "commits": [
        {
          "hash": "6da88306811b40a207c94c9da9faf07bdb20776e",
          "subject": "cgroup: Prepare for using css_task_iter_*() in BPF",
          "message": "This patch makes some preparations for using css_task_iter_*() in BPF\nProgram.\n\n1. Flags CSS_TASK_ITER_* are #define-s and it's not easy for bpf prog to\nuse them. Convert them to enum so bpf prog can take them from vmlinux.h.\n\n2. In the next patch we will add css_task_iter_*() in common kfuncs which\nis not safe. Since css_task_iter_*() does spin_unlock_irq() which might\nscrew up irq flags depending on the context where bpf prog is running.\nSo we should use irqsave/irqrestore here and the switching is harmless.\n\nSuggested-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Tejun Heo <tj@kernel.org>\nLink: https://lore.kernel.org/r/20231018061746.111364-2-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
          "date": "2023-10-19 17:02:46 -0700",
          "modified_files": [
            "include/linux/cgroup.h",
            "kernel/cgroup/cgroup.c"
          ]
        },
        {
          "hash": "9c66dc94b62aef23300f05f63404afb8990920b4",
          "subject": "bpf: Introduce css_task open-coded iterator kfuncs",
          "message": "This patch adds kfuncs bpf_iter_css_task_{new,next,destroy} which allow\ncreation and manipulation of struct bpf_iter_css_task in open-coded\niterator style. These kfuncs actually wrapps css_task_iter_{start,next,\nend}. BPF programs can use these kfuncs through bpf_for_each macro for\niteration of all tasks under a css.\n\ncss_task_iter_*() would try to get the global spin-lock *css_set_lock*, so\nthe bpf side has to be careful in where it allows to use this iter.\nCurrently we only allow it in bpf_lsm and bpf iter-s.\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Tejun Heo <tj@kernel.org>\nLink: https://lore.kernel.org/r/20231018061746.111364-3-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
          "date": "2023-10-19 17:02:46 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c",
            "kernel/bpf/task_iter.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "c68a78ffe2cb4207f64fd0f4262818c728c67be0",
          "subject": "bpf: Introduce task open coded iterator kfuncs",
          "message": "This patch adds kfuncs bpf_iter_task_{new,next,destroy} which allow\ncreation and manipulation of struct bpf_iter_task in open-coded iterator\nstyle. BPF programs can use these kfuncs or through bpf_for_each macro to\niterate all processes in the system.\n\nThe API design keep consistent with SEC(\"iter/task\"). bpf_iter_task_new()\naccepts a specific task and iterating type which allows:\n\n1. iterating all process in the system (BPF_TASK_ITER_ALL_PROCS)\n\n2. iterating all threads in the system (BPF_TASK_ITER_ALL_THREADS)\n\n3. iterating all threads of a specific task (BPF_TASK_ITER_PROC_THREADS)\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nLink: https://lore.kernel.org/r/20231018061746.111364-4-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
          "date": "2023-10-19 17:02:46 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c",
            "kernel/bpf/task_iter.c",
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "7251d0905e7518bcb990c8e9a3615b1bb23c78f2",
          "subject": "bpf: Introduce css open-coded iterator kfuncs",
          "message": "This Patch adds kfuncs bpf_iter_css_{new,next,destroy} which allow\ncreation and manipulation of struct bpf_iter_css in open-coded iterator\nstyle. These kfuncs actually wrapps css_next_descendant_{pre, post}.\ncss_iter can be used to:\n\n1) iterating a sepcific cgroup tree with pre/post/up order\n\n2) iterating cgroup_subsystem in BPF Prog, like\nfor_each_mem_cgroup_tree/cpuset_for_each_descendant_pre in kernel.\n\nThe API design is consistent with cgroup_iter. bpf_iter_css_new accepts\nparameters defining iteration order and starting css. Here we also reuse\nBPF_CGROUP_ITER_DESCENDANTS_PRE, BPF_CGROUP_ITER_DESCENDANTS_POST,\nBPF_CGROUP_ITER_ANCESTORS_UP enums.\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Tejun Heo <tj@kernel.org>\nLink: https://lore.kernel.org/r/20231018061746.111364-5-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
          "date": "2023-10-19 17:02:46 -0700",
          "modified_files": [
            "kernel/bpf/cgroup_iter.c",
            "kernel/bpf/helpers.c",
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "dfab99df147b0d364f0c199f832ff2aedfb2265a",
          "subject": "bpf: teach the verifier to enforce css_iter and task_iter in RCU CS",
          "message": "css_iter and task_iter should be used in rcu section. Specifically, in\nsleepable progs explicit bpf_rcu_read_lock() is needed before use these\niters. In normal bpf progs that have implicit rcu_read_lock(), it's OK to\nuse them directly.\n\nThis patch adds a new a KF flag KF_RCU_PROTECTED for bpf_iter_task_new and\nbpf_iter_css_new. It means the kfunc should be used in RCU CS. We check\nwhether we are in rcu cs before we want to invoke this kfunc. If the rcu\nprotection is guaranteed, we would let st->type = PTR_TO_STACK | MEM_RCU.\nOnce user do rcu_unlock during the iteration, state MEM_RCU of regs would\nbe cleared. is_iter_reg_valid_init() will reject if reg->type is UNTRUSTED.\n\nIt is worth noting that currently, bpf_rcu_read_unlock does not\nclear the state of the STACK_ITER reg, since bpf_for_each_spilled_reg\nonly considers STACK_SPILL. This patch also let bpf_for_each_spilled_reg\nsearch STACK_ITER.\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231018061746.111364-6-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
          "date": "2023-10-19 17:02:46 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "include/linux/btf.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "cb3ecf7915a1d7ce5304402f4d8616d9fa5193f7",
          "subject": "bpf: Let bpf_iter_task_new accept null task ptr",
          "message": "When using task_iter to iterate all threads of a specific task, we enforce\nthat the user must pass a valid task pointer to ensure safety. However,\nwhen iterating all threads/process in the system, BPF verifier still\nrequire a valid ptr instead of \"nullable\" pointer, even though it's\npointless, which is a kind of surprising from usability standpoint. It\nwould be nice if we could let that kfunc accept a explicit null pointer\nwhen we are using BPF_TASK_ITER_ALL_{PROCS, THREADS} and a valid pointer\nwhen using BPF_TASK_ITER_THREAD.\n\nGiven a trival kfunc:\n\t__bpf_kfunc void FN(struct TYPE_A *obj);\n\nBPF Prog would reject a nullptr for obj. The error info is:\n\"arg#x pointer type xx xx must point to scalar, or struct with scalar\"\nreported by get_kfunc_ptr_arg_type(). The reg->type is SCALAR_VALUE and\nthe btf type of ref_t is not scalar or scalar_struct which leads to the\nrejection of get_kfunc_ptr_arg_type.\n\nThis patch add \"__nullable\" annotation:\n\t__bpf_kfunc void FN(struct TYPE_A *obj__nullable);\nHere __nullable indicates obj can be optional, user can pass a explicit\nnullptr or a normal TYPE_A pointer. In get_kfunc_ptr_arg_type(), we will\ndetect whether the current arg is optional and register is null, If so,\nreturn a new kfunc_ptr_arg_type KF_ARG_PTR_TO_NULL and skip to the next\narg in check_kfunc_args().\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231018061746.111364-7-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
          "date": "2023-10-19 17:02:46 -0700",
          "modified_files": [
            "kernel/bpf/task_iter.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "ddab78cbb52f81f7f7598482602342955b2ff8b8",
          "subject": "selftests/bpf: rename bpf_iter_task.c to bpf_iter_tasks.c",
          "message": "The newly-added struct bpf_iter_task has a name collision with a selftest\nfor the seq_file task iter's bpf skel, so the selftests/bpf/progs file is\nrenamed in order to avoid the collision.\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231018061746.111364-8-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
          "date": "2023-10-19 17:02:47 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/bpf_iter.c",
            "tools/testing/selftests/bpf/progs/bpf_iter_tasks.c"
          ]
        },
        {
          "hash": "130e0f7af9fc7388b90fb016ca13ff3840c48d4a",
          "subject": "selftests/bpf: Add tests for open-coded task and css iter",
          "message": "This patch adds 4 subtests to demonstrate these patterns and validating\ncorrectness.\n\nsubtest1:\n\n1) We use task_iter to iterate all process in the system and search for the\ncurrent process with a given pid.\n\n2) We create some threads in current process context, and use\nBPF_TASK_ITER_PROC_THREADS to iterate all threads of current process. As\nexpected, we would find all the threads of current process.\n\n3) We create some threads and use BPF_TASK_ITER_ALL_THREADS to iterate all\nthreads in the system. As expected, we would find all the threads which was\ncreated.\n\nsubtest2:\n\nWe create a cgroup and add the current task to the cgroup. In the\nBPF program, we would use bpf_for_each(css_task, task, css) to iterate all\ntasks under the cgroup. As expected, we would find the current process.\n\nsubtest3:\n\n1) We create a cgroup tree. In the BPF program, we use\nbpf_for_each(css, pos, root, XXX) to iterate all descendant under the root\nwith pre and post order. As expected, we would find all descendant and the\nlast iterating cgroup in post-order is root cgroup, the first iterating\ncgroup in pre-order is root cgroup.\n\n2) We wse BPF_CGROUP_ITER_ANCESTORS_UP to traverse the cgroup tree starting\nfrom leaf and root separately, and record the height. The diff of the\nhights would be the total tree-high - 1.\n\nsubtest4:\n\nAdd some failure testcase when using css_task, task and css iters, e.g,\nunlock when using task-iters to iterate tasks.\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nLink: https://lore.kernel.org/r/20231018061746.111364-9-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
          "date": "2023-10-19 17:02:47 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/iters.c",
            "tools/testing/selftests/bpf/progs/iters_css.c",
            "tools/testing/selftests/bpf/progs/iters_css_task.c",
            "tools/testing/selftests/bpf/progs/iters_task.c",
            "tools/testing/selftests/bpf/progs/iters_task_failure.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "99c9991f4e5d77328187187d0c921a3b62bfa998",
      "merge_subject": "Merge branch 'bpf-log-improvements'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nThis patch set fixes ambiguity in BPF verifier log output of SCALAR register\nin the parts that emit umin/umax, smin/smax, etc ranges. See patch #4 for\ndetails.\n\nAlso, patch #5 fixes an issue with verifier log missing instruction context\n(state) output for conditionals that trigger precision marking. See details in\nthe patch.\n\nFirst two patches are just improvements to two selftests that are very flaky\nlocally when run in parallel mode.\n\nPatch #3 changes 'align' selftest to be less strict about exact verifier log\noutput (which patch #4 changes, breaking lots of align tests as written). Now\ntest does more of a register substate checks, mostly around expected var_off()\nvalues. This 'align' selftests is one of the more brittle ones and requires\nconstant adjustment when verifier log output changes, without really catching\nany new issues. So hopefully these changes can minimize future support efforts\nfor this specific set of tests.\n====================\n\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>",
      "merge_author": "Daniel Borkmann <daniel@iogearbox.net>",
      "merge_date": "2023-10-16 13:49:41 +0200",
      "commits": [
        {
          "hash": "2d78928c9cf7bee08c3e2344e6e1755412855448",
          "subject": "selftests/bpf: Improve percpu_alloc test robustness",
          "message": "Make these non-serial tests filter BPF programs by intended PID of\na test runner process. This makes it isolated from other parallel tests\nthat might interfere accidentally.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231011223728.3188086-2-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-10-16 13:49:18 +0200",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/percpu_alloc.c",
            "tools/testing/selftests/bpf/progs/percpu_alloc_array.c",
            "tools/testing/selftests/bpf/progs/percpu_alloc_cgrp_local_storage.c"
          ]
        },
        {
          "hash": "08a7078feacf419305d86d36b974c48347f3abb0",
          "subject": "selftests/bpf: Improve missed_kprobe_recursion test robustness",
          "message": "Given missed_kprobe_recursion is non-serial and uses common testing\nkfuncs to count number of recursion misses it's possible that some other\nparallel test can trigger extraneous recursion misses. So we can't\nexpect exactly 1 miss. Relax conditions and expect at least one.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231011223728.3188086-3-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-10-16 13:49:18 +0200",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/missed.c"
          ]
        },
        {
          "hash": "cde785142885e1fc62a9ae92e7aae90285ed3d79",
          "subject": "selftests/bpf: Make align selftests more robust",
          "message": "Align subtest is very specific and finicky about expected verifier log\noutput and format. This is often completely unnecessary as in a bunch of\nsituations test actually cares about var_off part of register state. But\ngiven how exact it is right now, any tiny verifier log changes can lead\nto align tests failures, requiring constant adjustment.\n\nThis patch tries to make this a bit more robust by making logic first\nsearch for specified register and then allowing to match only portion of\nregister state, not everything exactly. This will come handly with\nfollow up changes to SCALAR register output disambiguation.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231011223728.3188086-4-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-10-16 13:49:18 +0200",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/align.c"
          ]
        },
        {
          "hash": "72f8a1de4a7ecb23393a920dface58d5a96f42d8",
          "subject": "bpf: Disambiguate SCALAR register state output in verifier logs",
          "message": "Currently the way that verifier prints SCALAR_VALUE register state (and\nPTR_TO_PACKET, which can have var_off and ranges info as well) is very\nambiguous.\n\nIn the name of brevity we are trying to eliminate \"unnecessary\" output\nof umin/umax, smin/smax, u32_min/u32_max, and s32_min/s32_max values, if\npossible. Current rules are that if any of those have their default\nvalue (which for mins is the minimal value of its respective types: 0,\nS32_MIN, or S64_MIN, while for maxs it's U32_MAX, S32_MAX, S64_MAX, or\nU64_MAX) *OR* if there is another min/max value that as matching value.\nE.g., if smin=100 and umin=100, we'll emit only umin=10, omitting smin\naltogether. This approach has a few problems, being both ambiguous and\nsort-of incorrect in some cases.\n\nAmbiguity is due to missing value could be either default value or value\nof umin/umax or smin/smax. This is especially confusing when we mix\nsigned and unsigned ranges. Quite often, umin=0 and smin=0, and so we'll\nhave only `umin=0` leaving anyone reading verifier log to guess whether\nsmin is actually 0 or it's actually -9223372036854775808 (S64_MIN). And\noften times it's important to know, especially when debugging tricky\nissues.\n\n\"Sort-of incorrectness\" comes from mixing negative and positive values.\nE.g., if umin is some large positive number, it can be equal to smin\nwhich is, interpreted as signed value, is actually some negative value.\nCurrently, that smin will be omitted and only umin will be emitted with\na large positive value, giving an impression that smin is also positive.\n\nAnyway, ambiguity is the biggest issue making it impossible to have an\nexact understanding of register state, preventing any sort of automated\ntesting of verifier state based on verifier log. This patch is\nattempting to rectify the situation by removing ambiguity, while\nminimizing the verboseness of register state output.\n\nThe rules are straightforward:\n  - if some of the values are missing, then it definitely has a default\n  value. I.e., `umin=0` means that umin is zero, but smin is actually\n  S64_MIN;\n  - all the various boundaries that happen to have the same value are\n  emitted in one equality separated sequence. E.g., if umin and smin are\n  both 100, we'll emit `smin=umin=100`, making this explicit;\n  - we do not mix negative and positive values together, and even if\n  they happen to have the same bit-level value, they will be emitted\n  separately with proper sign. I.e., if both umax and smax happen to be\n  0xffffffffffffffff, we'll emit them both separately as\n  `smax=-1,umax=18446744073709551615`;\n  - in the name of a bit more uniformity and consistency,\n  {u32,s32}_{min,max} are renamed to {s,u}{min,max}32, which seems to\n  improve readability.\n\nThe above means that in case of all 4 ranges being, say, [50, 100] range,\nwe'd previously see hugely ambiguous:\n\n    R1=scalar(umin=50,umax=100)\n\nNow, we'll be more explicit:\n\n    R1=scalar(smin=umin=smin32=umin32=50,smax=umax=smax32=umax32=100)\n\nThis is slightly more verbose, but distinct from the case when we don't\nknow anything about signed boundaries and 32-bit boundaries, which under\nnew rules will match the old case:\n\n    R1=scalar(umin=50,umax=100)\n\nAlso, in the name of simplicity of implementation and consistency, order\nfor {s,u}32_{min,max} are emitted *before* var_off. Previously they were\nemitted afterwards, for unclear reasons.\n\nThis patch also includes a few fixes to selftests that expect exact\nregister state to accommodate slight changes to verifier format. You can\nsee that the changes are pretty minimal in common cases.\n\nNote, the special case when SCALAR_VALUE register is a known constant\nisn't changed, we'll emit constant value once, interpreted as signed\nvalue.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231011223728.3188086-5-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-10-16 13:49:18 +0200",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/exceptions_assert.c",
            "tools/testing/selftests/bpf/progs/verifier_ldsx.c"
          ]
        },
        {
          "hash": "1a8a315f008a58f54fecb012b928aa6a494435b3",
          "subject": "bpf: Ensure proper register state printing for cond jumps",
          "message": "Verifier emits relevant register state involved in any given instruction\nnext to it after `;` to the right, if possible. Or, worst case, on the\nseparate line repeating instruction index.\n\nE.g., a nice and simple case would be:\n\n  2: (d5) if r0 s<= 0x0 goto pc+1       ; R0_w=0\n\nBut if there is some intervening extra output (e.g., precision\nbacktracking log) involved, we are supposed to see the state after the\nprecision backtrack log:\n\n  4: (75) if r0 s>= 0x0 goto pc+1\n  mark_precise: frame0: last_idx 4 first_idx 0 subseq_idx -1\n  mark_precise: frame0: regs=r0 stack= before 2: (d5) if r0 s<= 0x0 goto pc+1\n  mark_precise: frame0: regs=r0 stack= before 1: (b7) r0 = 0\n  6: R0_w=0\n\nFirst off, note that in `6: R0_w=0` instruction index corresponds to the\nnext instruction, not to the conditional jump instruction itself, which\nis wrong and we'll get to that.\n\nBut besides that, the above is a happy case that does work today. Yet,\nif it so happens that precision backtracking had to traverse some of the\nparent states, this `6: R0_w=0` state output would be missing.\n\nThis is due to a quirk of print_verifier_state() routine, which performs\nmark_verifier_state_clean(env) at the end. This marks all registers as\n\"non-scratched\", which means that subsequent logic to print *relevant*\nregisters (that is, \"scratched ones\") fails and doesn't see anything\nrelevant to print and skips the output altogether.\n\nprint_verifier_state() is used both to print instruction context, but\nalso to print an **entire** verifier state indiscriminately, e.g.,\nduring precision backtracking (and in a few other situations, like\nduring entering or exiting subprogram).  Which means if we have to print\nentire parent state before getting to printing instruction context\nstate, instruction context is marked as clean and is omitted.\n\nLong story short, this is definitely not intentional. So we fix this\nbehavior in this patch by teaching print_verifier_state() to clear\nscratch state only if it was used to print instruction state, not the\nparent/callback state. This is determined by print_all option, so if\nit's not set, we don't clear scratch state. This fixes missing\ninstruction state for these cases.\n\nAs for the mismatched instruction index, we fix that by making sure we\ncall print_insn_state() early inside check_cond_jmp_op() before we\nadjusted insn_idx based on jump branch taken logic. And with that we get\ndesired correct information:\n\n  9: (16) if w4 == 0x1 goto pc+9\n  mark_precise: frame0: last_idx 9 first_idx 9 subseq_idx -1\n  mark_precise: frame0: parent state regs=r4 stack=: R2_w=1944 R4_rw=P1 R10=fp0\n  mark_precise: frame0: last_idx 8 first_idx 0 subseq_idx 9\n  mark_precise: frame0: regs=r4 stack= before 8: (66) if w4 s> 0x3 goto pc+5\n  mark_precise: frame0: regs=r4 stack= before 7: (b7) r4 = 1\n  9: R4=1\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231011223728.3188086-6-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-10-16 13:49:18 +0200",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "d2dc885b8c9ddb6fc374d93a87f8f2d1b97d2caf",
      "merge_subject": "Merge branch 'Add cgroup sockaddr hooks for unix sockets'",
      "merge_body": "Daan De Meyer says:\n\n====================\nChanges since v10:\n\n* Removed extra check from bpf_sock_addr_set_sun_path() again in favor of\n  calling unix_validate_addr() everywhere in af_unix.c before calling the hooks.\n\nChanges since v9:\n\n* Renamed bpf_sock_addr_set_unix_addr() to bpf_sock_addr_set_sun_path() and\n  rennamed arguments to match the new name.\n* Added an extra check to bpf_sock_addr_set_sun_path() to disallow changing the\n  address of an unnamed unix socket.\n* Removed unnecessary NULL check on uaddrlen in\n  __cgroup_bpf_run_filter_sock_addr().\n\nChanges since v8:\n\n* Added missing test programs to last patch\n\nChanges since v7:\n\n* Fixed formatting nit in comment\n* Renamed from cgroup/connectun to cgroup/connect_unix (and similar for all\n  other hooks)\n\nChanges since v6:\n\n* Actually removed bpf_bind() helper for AF_UNIX hooks.\n* Fixed merge conflict\n* Updated comment to mention uaddrlen is read-only for AF_INET[6]\n* Removed unnecessary forward declaration of struct sock_addr_test\n* Removed unused BPF_CGROUP_RUN_PROG_UNIX_CONNECT()\n* Fixed formatting nit reported by checkpatch\n* Added more information to commit message about recvmsg() on connected socket\n\nChanges since v5:\n\n* Fixed kernel version in bpftool documentation (6.3 => 6.7).\n* Added connection mode socket recvmsg() test.\n* Removed bpf_bind() helper for AF_UNIX hooks.\n* Added missing getpeernameun and getsocknameun BPF test programs.\n* Added note for bind() test being unused currently.\n\nChanges since v4:\n\n* Dropped support for intercepting bind() as when using bind() with unix sockets\n  and a pathname sockaddr, bind() will create an inode in the filesystem that\n  needs to be cleaned up. If the address is rewritten, users might try to clean\n  up the wrong file and leak the actual socket file in the filesystem.\n* Changed bpf_sock_addr_set_unix_addr() to use BTF_KFUNC_HOOK_CGROUP_SKB instead\n  of BTF_KFUNC_HOOK_COMMON.\n* Removed unix socket related changes from BPF_CGROUP_PRE_CONNECT_ENABLED() as\n  unix sockets do not support pre-connect.\n* Added tests for getpeernameun and getsocknameun hooks.\n* We now disallow an empty sockaddr in bpf_sock_addr_set_unix_addr() similar to\n  unix_validate_addr().\n* Removed unnecessary cgroup_bpf_enabled() checks\n* Removed unnecessary error checks\n\nChanges since v3:\n\n* Renamed bpf_sock_addr_set_addr() to bpf_sock_addr_set_unix_addr() and\n  made it only operate on AF_UNIX sockaddrs. This is because for the other\n  families, users usually want to configure more than just the address so\n  a generic interface will not fit the bill here. e.g. for AF_INET and AF_INET6,\n  users would generally also want to be able to configure the port which the\n  current interface doesn't support. So we expose an AF_UNIX specific function\n  instead.\n* Made the tests in the new sock addr tests more generic (similar to test_sock_addr.c),\n  this should make it easier to migrate the other sock addr tests in the future.\n* Removed the new kfunc hook and attached to BTF_KFUNC_HOOK_COMMON instead\n* Set uaddrlen to 0 when the family is AF_UNSPEC\n* Pass in the addrlen to the hook from IPv6 code\n* Fixed mount directory mkdir() to ignore EEXIST\n\nChanges since v2:\n\n* Configuring the sock addr is now done via a new kfunc bpf_sock_addr_set()\n* The addrlen is exposed as u32 in bpf_sock_addr_kern\n* Selftests are updated to use the new kfunc\n* Selftests are now added as a new sock_addr test in prog_tests/\n* Added BTF_KFUNC_HOOK_SOCK_ADDR for BPF_PROG_TYPE_CGROUP_SOCK_ADDR\n* __cgroup_bpf_run_filter_sock_addr() now returns the modified addrlen\n\nChanges since v1:\n\n* Split into multiple patches instead of one single patch\n* Added unix support for all socket address hooks instead of only connect()\n* Switched approach to expose the socket address length to the bpf hook\ninstead of recalculating the socket address length in kernelspace to\nproperly support abstract unix socket addresses\n* Modified socket address hook tests to calculate the socket address length\nonce and pass it around everywhere instead of recalculating the actual unix\nsocket address length on demand.\n* Added some missing section name tests for getpeername()/getsockname()\n\nThis patch series extends the cgroup sockaddr hooks to include support for unix\nsockets. To add support for unix sockets, struct bpf_sock_addr_kern is extended\nto expose the socket address length to the bpf program. Along with that, a new\nkfunc bpf_sock_addr_set_unix_addr() is added to safely allow modifying an\nAF_UNIX sockaddr from bpf programs.\n\nI intend to use these new hooks in systemd to reimplement the LogNamespace=\nfeature, which allows running multiple instances of systemd-journald to\nprocess the logs of different services. systemd-journald also processes\nsyslog messages, so currently, using log namespaces means all services running\nin the same log namespace have to live in the same private mount namespace\nso that systemd can mount the journal namespace's associated syslog socket\nover /dev/log to properly direct syslog messages from all services running\nin that log namespace to the correct systemd-journald instance. We want to\nrelax this requirement so that processes running in disjoint mount namespaces\ncan still run in the same log namespace. To achieve this, we can use these\nnew hooks to rewrite the socket address of any connect(), sendto(), ...\nsyscalls to /dev/log to the socket address of the journal namespace's syslog\nsocket instead, which will transparently do the redirection without requiring\nuse of a mount namespace and mounting over /dev/log.\n\nAside from the above usecase, these hooks can more generally be used to\ntransparently redirect unix sockets to different addresses as required by\nservices.\n====================\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_date": "2023-10-11 17:27:56 -0700",
      "commits": [
        {
          "hash": "feba7b634ef0d003184d6988d96c34ab3c50de59",
          "subject": "selftests/bpf: Add missing section name tests for getpeername/getsockname",
          "message": "These were missed when these hooks were first added so add them now\ninstead to make sure every sockaddr hook has a matching section name\ntest.\n\nSigned-off-by: Daan De Meyer <daan.j.demeyer@gmail.com>\nLink: https://lore.kernel.org/r/20231011185113.140426-2-daan.j.demeyer@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Daan De Meyer <daan.j.demeyer@gmail.com>",
          "date": "2023-10-11 13:24:18 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/section_names.c"
          ]
        },
        {
          "hash": "fefba7d1ae198dcbf8b3b432de46a4e29f8dbd8c",
          "subject": "bpf: Propagate modified uaddrlen from cgroup sockaddr programs",
          "message": "As prep for adding unix socket support to the cgroup sockaddr hooks,\nlet's propagate the sockaddr length back to the caller after running\na bpf cgroup sockaddr hook program. While not important for AF_INET or\nAF_INET6, the sockaddr length is important when working with AF_UNIX\nsockaddrs as the size of the sockaddr cannot be determined just from the\naddress family or the sockaddr's contents.\n\n__cgroup_bpf_run_filter_sock_addr() is modified to take the uaddrlen as\nan input/output argument. After running the program, the modified sockaddr\nlength is stored in the uaddrlen pointer.\n\nSigned-off-by: Daan De Meyer <daan.j.demeyer@gmail.com>\nLink: https://lore.kernel.org/r/20231011185113.140426-3-daan.j.demeyer@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Daan De Meyer <daan.j.demeyer@gmail.com>",
          "date": "2023-10-11 15:03:40 -0700",
          "modified_files": [
            "include/linux/bpf-cgroup.h",
            "include/linux/filter.h",
            "kernel/bpf/cgroup.c",
            "net/ipv4/af_inet.c",
            "net/ipv4/ping.c",
            "net/ipv4/tcp_ipv4.c",
            "net/ipv4/udp.c",
            "net/ipv6/af_inet6.c",
            "net/ipv6/ping.c",
            "net/ipv6/tcp_ipv6.c",
            "net/ipv6/udp.c"
          ]
        },
        {
          "hash": "53e380d21441909b12b6e0782b77187ae4b971c4",
          "subject": "bpf: Add bpf_sock_addr_set_sun_path() to allow writing unix sockaddr from bpf",
          "message": "As prep for adding unix socket support to the cgroup sockaddr hooks,\nlet's add a kfunc bpf_sock_addr_set_sun_path() that allows modifying a unix\nsockaddr from bpf. While this is already possible for AF_INET and AF_INET6,\nwe'll need this kfunc when we add unix socket support since modifying the\naddress for those requires modifying both the address and the sockaddr\nlength.\n\nSigned-off-by: Daan De Meyer <daan.j.demeyer@gmail.com>\nLink: https://lore.kernel.org/r/20231011185113.140426-4-daan.j.demeyer@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Daan De Meyer <daan.j.demeyer@gmail.com>",
          "date": "2023-10-11 16:29:25 -0700",
          "modified_files": [
            "kernel/bpf/btf.c",
            "net/core/filter.c"
          ]
        },
        {
          "hash": "859051dd165ec6cc915f0f2114699021144fd249",
          "subject": "bpf: Implement cgroup sockaddr hooks for unix sockets",
          "message": "These hooks allows intercepting connect(), getsockname(),\ngetpeername(), sendmsg() and recvmsg() for unix sockets. The unix\nsocket hooks get write access to the address length because the\naddress length is not fixed when dealing with unix sockets and\nneeds to be modified when a unix socket address is modified by\nthe hook. Because abstract socket unix addresses start with a\nNUL byte, we cannot recalculate the socket address in kernelspace\nafter running the hook by calculating the length of the unix socket\npath using strlen().\n\nThese hooks can be used when users want to multiplex syscall to a\nsingle unix socket to multiple different processes behind the scenes\nby redirecting the connect() and other syscalls to process specific\nsockets.\n\nWe do not implement support for intercepting bind() because when\nusing bind() with unix sockets with a pathname address, this creates\nan inode in the filesystem which must be cleaned up. If we rewrite\nthe address, the user might try to clean up the wrong file, leaking\nthe socket in the filesystem where it is never cleaned up. Until we\nfigure out a solution for this (and a use case for intercepting bind()),\nwe opt to not allow rewriting the sockaddr in bind() calls.\n\nWe also implement recvmsg() support for connected streams so that\nafter a connect() that is modified by a sockaddr hook, any corresponding\nrecmvsg() on the connected socket can also be modified to make the\nconnected program think it is connected to the \"intended\" remote.\n\nReviewed-by: Kuniyuki Iwashima <kuniyu@amazon.com>\nSigned-off-by: Daan De Meyer <daan.j.demeyer@gmail.com>\nLink: https://lore.kernel.org/r/20231011185113.140426-5-daan.j.demeyer@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Daan De Meyer <daan.j.demeyer@gmail.com>",
          "date": "2023-10-11 17:27:47 -0700",
          "modified_files": [
            "include/linux/bpf-cgroup-defs.h",
            "include/linux/bpf-cgroup.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/cgroup.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c",
            "net/core/filter.c",
            "net/unix/af_unix.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "bf90438c78df885c17a3474276ed39abb4a7c026",
          "subject": "libbpf: Add support for cgroup unix socket address hooks",
          "message": "Add the necessary plumbing to hook up the new cgroup unix sockaddr\nhooks into libbpf.\n\nSigned-off-by: Daan De Meyer <daan.j.demeyer@gmail.com>\nLink: https://lore.kernel.org/r/20231011185113.140426-6-daan.j.demeyer@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Daan De Meyer <daan.j.demeyer@gmail.com>",
          "date": "2023-10-11 17:27:55 -0700",
          "modified_files": [
            "tools/lib/bpf/libbpf.c"
          ]
        },
        {
          "hash": "8b3cba987e6d9464bb533d957de923f891b57bf8",
          "subject": "bpftool: Add support for cgroup unix socket address hooks",
          "message": "Add the necessary plumbing to hook up the new cgroup unix sockaddr\nhooks into bpftool.\n\nSigned-off-by: Daan De Meyer <daan.j.demeyer@gmail.com>\nAcked-by: Quentin Monnet <quentin@isovalent.com>\nLink: https://lore.kernel.org/r/20231011185113.140426-7-daan.j.demeyer@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Daan De Meyer <daan.j.demeyer@gmail.com>",
          "date": "2023-10-11 17:27:55 -0700",
          "modified_files": [
            "tools/bpf/bpftool/Documentation/bpftool-cgroup.rst",
            "tools/bpf/bpftool/Documentation/bpftool-prog.rst",
            "tools/bpf/bpftool/bash-completion/bpftool",
            "tools/bpf/bpftool/cgroup.c",
            "tools/bpf/bpftool/prog.c"
          ]
        },
        {
          "hash": "3243fef6a4c0db2dbb01ee3cf30bd787e65b8d56",
          "subject": "documentation/bpf: Document cgroup unix socket address hooks",
          "message": "Update the documentation to mention the new cgroup unix sockaddr\nhooks.\n\nSigned-off-by: Daan De Meyer <daan.j.demeyer@gmail.com>\nLink: https://lore.kernel.org/r/20231011185113.140426-8-daan.j.demeyer@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Daan De Meyer <daan.j.demeyer@gmail.com>",
          "date": "2023-10-11 17:27:55 -0700",
          "modified_files": [
            "Documentation/bpf/libbpf/program_types.rst"
          ]
        },
        {
          "hash": "af2752ed450e71fc0bd596d0b4b9b805a64ae2c1",
          "subject": "selftests/bpf: Make sure mount directory exists",
          "message": "The mount directory for the selftests cgroup tree might\nnot exist so let's make sure it does exist by creating\nit ourselves if it doesn't exist.\n\nSigned-off-by: Daan De Meyer <daan.j.demeyer@gmail.com>\nLink: https://lore.kernel.org/r/20231011185113.140426-9-daan.j.demeyer@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Daan De Meyer <daan.j.demeyer@gmail.com>",
          "date": "2023-10-11 17:27:55 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/cgroup_helpers.c"
          ]
        },
        {
          "hash": "82ab6b505e8199cc4537f00025a7391973c3847e",
          "subject": "selftests/bpf: Add tests for cgroup unix socket address hooks",
          "message": "These selftests are written in prog_tests style instead of adding\nthem to the existing test_sock_addr tests. Migrating the existing\nsock addr tests to prog_tests style is left for future work. This\ncommit adds support for testing bind() sockaddr hooks, even though\nthere's no unix socket sockaddr hook for bind(). We leave this code\nintact for when the INET and INET6 tests are migrated in the future\nwhich do support intercepting bind().\n\nSigned-off-by: Daan De Meyer <daan.j.demeyer@gmail.com>\nLink: https://lore.kernel.org/r/20231011185113.140426-10-daan.j.demeyer@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Daan De Meyer <daan.j.demeyer@gmail.com>",
          "date": "2023-10-11 17:27:55 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_kfuncs.h",
            "tools/testing/selftests/bpf/network_helpers.c",
            "tools/testing/selftests/bpf/network_helpers.h",
            "tools/testing/selftests/bpf/prog_tests/section_names.c",
            "tools/testing/selftests/bpf/prog_tests/sock_addr.c",
            "tools/testing/selftests/bpf/progs/connect_unix_prog.c",
            "tools/testing/selftests/bpf/progs/getpeername_unix_prog.c",
            "tools/testing/selftests/bpf/progs/getsockname_unix_prog.c",
            "tools/testing/selftests/bpf/progs/recvmsg_unix_prog.c",
            "tools/testing/selftests/bpf/progs/sendmsg_unix_prog.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "cf67d28de348f109251a9045a472a39724e2b4fa",
      "merge_subject": "Merge branch 'implement-cpuv4-support-for-s390x'",
      "merge_body": "Ilya Leoshkevich says:\n\n====================\nImplement cpuv4 support for s390x\n\nv1: https://lore.kernel.org/bpf/20230830011128.1415752-1-iii@linux.ibm.com/\nv1 -> v2:\n- Redo Disable zero-extension for BPF_MEMSX as Puranjay and Alexei\n  suggested.\n- Drop the bpf_ct_insert_entry() patch, it went in via the bpf tree.\n- Rebase, don't apply A-bs because there were fixed conflicts.\n\nHi,\n\nThis series adds the cpuv4 support to the s390x eBPF JIT.\nPatches 1-3 are preliminary bugfixes.\nPatches 4-8 implement the new instructions.\nPatches 9-10 enable the tests.\n\nBest regards,\nIlya\n====================\n\nLink: https://lore.kernel.org/r/20230919101336.2223655-1-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-09-21 14:22:01 -0700",
      "commits": [
        {
          "hash": "577c06af8188d1f6919ef7b62fc1b78fb1b86eb7",
          "subject": "bpf: Disable zero-extension for BPF_MEMSX",
          "message": "On the architectures that use bpf_jit_needs_zext(), e.g., s390x, the\nverifier incorrectly inserts a zero-extension after BPF_MEMSX, leading\nto miscompilations like the one below:\n\n      24:       89 1a ff fe 00 00 00 00 \"r1 = *(s16 *)(r10 - 2);\"       # zext_dst set\n   0x3ff7fdb910e:       lgh     %r2,-2(%r13,%r0)                        # load halfword\n   0x3ff7fdb9114:       llgfr   %r2,%r2                                 # wrong!\n      25:       65 10 00 03 00 00 7f ff if r1 s> 32767 goto +3 <l0_1>   # check_cond_jmp_op()\n\nDisable such zero-extensions. The JITs need to insert sign-extension\nthemselves, if necessary.\n\nSuggested-by: Puranjay Mohan <puranjay12@gmail.com>\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nReviewed-by: Puranjay Mohan <puranjay12@gmail.com>\nLink: https://lore.kernel.org/r/20230919101336.2223655-2-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
          "date": "2023-09-21 14:21:59 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "6cb66eca36f3c6b37447ca79c6d7fc6db339c23e",
          "subject": "selftests/bpf: Unmount the cgroup2 work directory",
          "message": "test_progs -t bind_perm,bpf_obj_pinning/mounted-str-rel fails when\nthe selftests directory is mounted under /mnt, which is a reasonable\nthing to do when sharing the selftests residing on the host with a\nvirtual machine, e.g., using 9p.\n\nThe reason is that cgroup2 is mounted at /mnt and not unmounted,\ncausing subsequent tests that need to access the selftests directory\nto fail.\n\nFix by unmounting it. The kernel maintains a mount stack, so this\nreveals what was mounted there before. Introduce cgroup_workdir_mounted\nin order to maintain idempotency. Make it thread-local in order to\nsupport test_progs -j.\n\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nLink: https://lore.kernel.org/r/20230919101336.2223655-3-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
          "date": "2023-09-21 14:21:59 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/cgroup_helpers.c"
          ]
        },
        {
          "hash": "9873ce2e9c68193371c111a1a9f06064e36b9814",
          "subject": "selftests/bpf: Add big-endian support to the ldsx test",
          "message": "Prepare the ldsx test to run on big-endian systems by adding the\nnecessary endianness checks around narrow memory accesses.\n\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nLink: https://lore.kernel.org/r/20230919101336.2223655-4-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
          "date": "2023-09-21 14:21:59 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/test_ldsx_insn.c",
            "tools/testing/selftests/bpf/progs/verifier_ldsx.c"
          ]
        },
        {
          "hash": "3de55893f6482660e2f04454eb0e65a070523ba0",
          "subject": "s390/bpf: Implement BPF_MOV | BPF_X with sign-extension",
          "message": "Implement the cpuv4 register-to-register move with sign extension. It\nis distinguished from the normal moves by non-zero values in\ninsn->off, which determine the source size. s390x has instructions to\ndeal with all of them: lbr, lhr, lgbr, lghr and lgfr.\n\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nLink: https://lore.kernel.org/r/20230919101336.2223655-5-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
          "date": "2023-09-21 14:21:59 -0700",
          "modified_files": [
            "arch/s390/net/bpf_jit_comp.c"
          ]
        },
        {
          "hash": "738476a079bd238a65c09c63e7437832d96b08e8",
          "subject": "s390/bpf: Implement BPF_MEMSX",
          "message": "Implement the cpuv4 load with sign-extension, which is encoded as\nBPF_MEMSX (and, for internal uses cases only, BPF_PROBE_MEMSX).\n\nThis is the same as BPF_MEM and BPF_PROBE_MEM, but with sign\nextension instead of zero extension, and s390x has the necessary\ninstructions: lgb, lgh and lgf.\n\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nLink: https://lore.kernel.org/r/20230919101336.2223655-6-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
          "date": "2023-09-21 14:21:59 -0700",
          "modified_files": [
            "arch/s390/net/bpf_jit_comp.c"
          ]
        },
        {
          "hash": "90f426d35e01f7d19ede5f331c013f7d81ce670b",
          "subject": "s390/bpf: Implement unconditional byte swap",
          "message": "Implement the cpuv4 unconditional byte swap, which is encoded as\nBPF_ALU64 | BPF_END | BPF_FROM_LE. Since s390x is big-endian, it's\nthe same as the existing BPF_ALU | BPF_END | BPF_FROM_LE.\n\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nLink: https://lore.kernel.org/r/20230919101336.2223655-7-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
          "date": "2023-09-21 14:22:00 -0700",
          "modified_files": [
            "arch/s390/net/bpf_jit_comp.c"
          ]
        },
        {
          "hash": "c690191e23d82137b876d8980c967b87de69a011",
          "subject": "s390/bpf: Implement unconditional jump with 32-bit offset",
          "message": "Implement the cpuv4 unconditional jump with 32-bit offset, which is\nencoded as BPF_JMP32 | BPF_JA and stores the offset in the imm field.\nReuse the existing BPF_JMP | BPF_JA logic.\n\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nLink: https://lore.kernel.org/r/20230919101336.2223655-8-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
          "date": "2023-09-21 14:22:00 -0700",
          "modified_files": [
            "arch/s390/net/bpf_jit_comp.c"
          ]
        },
        {
          "hash": "91d2ad78e90c26189bb27789099a8170edfad2f0",
          "subject": "s390/bpf: Implement signed division",
          "message": "Implement the cpuv4 signed division. It is encoded as unsigned\ndivision, but with off field set to 1. s390x has the necessary\ninstructions: dsgfr, dsgf and dsgr.\n\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nLink: https://lore.kernel.org/r/20230919101336.2223655-9-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
          "date": "2023-09-21 14:22:00 -0700",
          "modified_files": [
            "arch/s390/net/bpf_jit_comp.c"
          ]
        },
        {
          "hash": "48c432382dd44363f5110692a9b469a7b2e962ee",
          "subject": "selftests/bpf: Enable the cpuv4 tests for s390x",
          "message": "Now that all the cpuv4 support is in place, enable the tests.\n\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nLink: https://lore.kernel.org/r/20230919101336.2223655-10-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
          "date": "2023-09-21 14:22:00 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/test_ldsx_insn.c",
            "tools/testing/selftests/bpf/progs/verifier_bswap.c",
            "tools/testing/selftests/bpf/progs/verifier_gotol.c",
            "tools/testing/selftests/bpf/progs/verifier_ldsx.c",
            "tools/testing/selftests/bpf/progs/verifier_movsx.c",
            "tools/testing/selftests/bpf/progs/verifier_sdiv.c"
          ]
        },
        {
          "hash": "c29913bbf4ec172da08157efc7b417f684c42dd6",
          "subject": "selftests/bpf: Trim DENYLIST.s390x",
          "message": "Enable all selftests, except the 2 that have to do with the userspace\nunwinding, and the new exceptions test, in the s390x CI.\n\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nLink: https://lore.kernel.org/r/20230919101336.2223655-11-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
          "date": "2023-09-21 14:22:00 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/DENYLIST.s390x"
          ]
        }
      ]
    },
    {
      "merge_hash": "b3af9c0e89ca721dfed95401c88c8c6e8067b558",
      "merge_subject": "Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next",
      "merge_body": "Alexei Starovoitov says:\n\n====================\nbpf-next 2023-09-19\n\nThe following pull-request contains BPF updates for your *net-next* tree.\n\nWe've added 4 non-merge commits during the last 1 day(s) which contain\na total of 4 files changed, 9 insertions(+), 13 deletions(-).\n\nThe main changes are:\n\n1) A set of fixes for bpf exceptions, from Kumar and Alexei.\n====================\n\nSigned-off-by: David S. Miller <davem@davemloft.net>",
      "merge_author": "David S. Miller <davem@davemloft.net>",
      "merge_date": "2023-09-20 11:56:27 +0100",
      "commits": [
        {
          "hash": "4d84dcc739d5b253096f9e47957c5964709f5772",
          "subject": "selftests/bpf: Print log buffer for exceptions test only on failure",
          "message": "Alexei reported seeing log messages for some test cases even though we\njust wanted to match the error string from the verifier. Move the\nprinting of the log buffer to a guarded condition so that we only print\nit when we fail to match on the expected string in the log buffer,\npreventing unneeded output when running the test.\n\nReported-by: Alexei Starovoitov <ast@kernel.org>\nFixes: d2a93715bfb0 (\"selftests/bpf: Add tests for BPF exceptions\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230918155233.297024-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-19 02:07:36 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/exceptions.c"
          ]
        },
        {
          "hash": "7d3460632da2c2ad5c5708db82a0b72e2b66396c",
          "subject": "bpf: Fix bpf_throw warning on 32-bit arch",
          "message": "On 32-bit architectures, the pointer width is 32-bit, while we try to\ncast from a u64 down to it, the compiler complains on mismatch in\ninteger size. Fix this by first casting to long which should match\nthe pointer width on targets supported by Linux.\n\nFixes: ec5290a178b7 (\"bpf: Prevent KASAN false positive with bpf_throw\")\nReported-by: Matthieu Baerts <matthieu.baerts@tessares.net>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nTested-by: Matthieu Baerts <matthieu.baerts@tessares.net>\nLink: https://lore.kernel.org/r/20230918155233.297024-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-19 02:07:36 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "5bfdb4fbf348f9e1935a6e9c64e7f60cb913fb21",
          "subject": "bpf: Disable exceptions when CONFIG_UNWINDER_FRAME_POINTER=y",
          "message": "The build with CONFIG_UNWINDER_FRAME_POINTER=y is broken for\ncurrent exceptions feature as it assumes ORC unwinder specific fields in\nthe unwind_state. Disable exceptions when frame_pointer unwinder is\nenabled for now.\n\nFixes: fd5d27b70188 (\"arch/x86: Implement arch_bpf_stack_walk\")\nReported-by: Eric Dumazet <edumazet@google.com>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230918155233.297024-4-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-19 02:07:36 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c"
          ]
        },
        {
          "hash": "aec42f36237b09e42eac39f6c74305aec02b4694",
          "subject": "bpf: Remove unused variables.",
          "message": "Remove unused prev_offset, min_size, krec_size variables.\n\nReported-by: kernel test robot <lkp@intel.com>\nCloses: https://lore.kernel.org/oe-kbuild-all/202309190634.fL17FWoT-lkp@intel.com/\nFixes: aaa619ebccb2 (\"bpf: Refactor check_btf_func and split into two phases\")\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-09-19 02:26:47 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "ec6f1b4db95b7eedb3fe85f4f14e08fa0e9281c3",
      "merge_subject": "Merge branch 'exceptions-1-2'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nExceptions - 1/2\n\nThis series implements the _first_ part of the runtime and verifier\nsupport needed to enable BPF exceptions. Exceptions thrown from programs\nare processed as an immediate exit from the program, which unwinds all\nthe active stack frames until the main stack frame, and returns to the\nBPF program's caller. The ability to perform this unwinding safely\nallows the program to test conditions that are always true at runtime\nbut which the verifier has no visibility into.\n\nThus, it also reduces verification effort by safely terminating\nredundant paths that can be taken within a program.\n\nThe patches to perform runtime resource cleanup during the\nframe-by-frame unwinding will be posted as a follow-up to this set.\n\nIt must be noted that exceptions are not an error handling mechanism for\nunlikely runtime conditions, but a way to safely terminate the execution\nof a program in presence of conditions that should never occur at\nruntime. They are meant to serve higher-level primitives such as program\nassertions.\n\nThe following kfuncs and macros are introduced:\n\nAssertion macros are also introduced, please see patch 13 for their\ndocumentation.\n\n/* Description\n *\tThrow a BPF exception from the program, immediately terminating its\n *\texecution and unwinding the stack. The supplied 'cookie' parameter\n *\twill be the return value of the program when an exception is thrown,\n *\tand the default exception callback is used. Otherwise, if an exception\n *\tcallback is set using the '__exception_cb(callback)' declaration tag\n *\ton the main program, the 'cookie' parameter will be the callback's only\n *\tinput argument.\n *\n *\tThus, in case of default exception callback, 'cookie' is subjected to\n *\tconstraints on the program's return value (as with R0 on exit).\n *\tOtherwise, the return value of the marked exception callback will be\n *\tsubjected to the same checks.\n *\n *\tNote that throwing an exception with lingering resources (locks,\n *\treferences, etc.) will lead to a verification error.\n *\n *\tNote that callbacks *cannot* call this helper.\n * Returns\n *\tNever.\n * Throws\n *\tAn exception with the specified 'cookie' value.\n */\nextern void bpf_throw(u64 cookie) __ksym;\n\n/* This macro must be used to mark the exception callback corresponding to the\n * main program. For example:\n *\n * int exception_cb(u64 cookie) {\n *\treturn cookie;\n * }\n *\n * SEC(\"tc\")\n * __exception_cb(exception_cb)\n * int main_prog(struct __sk_buff *ctx) {\n *\t...\n *\treturn TC_ACT_OK;\n * }\n *\n * Here, exception callback for the main program will be 'exception_cb'. Note\n * that this attribute can only be used once, and multiple exception callbacks\n * specified for the main program will lead to verification error.\n */\n\\#define __exception_cb(name) __attribute__((btf_decl_tag(\"exception_callback:\" #name)))\n\nAs such, a program can only install an exception handler once for the\nlifetime of a BPF program, and this handler cannot be changed at\nruntime. The purpose of the handler is to simply interpret the cookie\nvalue supplied by the bpf_throw call, and execute user-defined logic\ncorresponding to it. The primary purpose of allowing a handler is to\ncontrol the return value of the program. The default handler returns the\ncookie value passed to bpf_throw when an exception is thrown.\n\nFixing the handler for the lifetime of the program eliminates tricky and\nexpensive handling in case of runtime changes of the handler callback\nwhen programs begin to nest, where it becomes more complex to save and\nrestore the active handler at runtime.\n\nThis version of offline unwinding based BPF exceptions is truly zero\noverhead, with the exception of generation of a default callback which\ncontains a few instructions to return a default return value (0) when no\nexception callback is supplied by the user.\n\nCallbacks are disallowed from throwing BPF exceptions for now, since\nsuch exceptions need to cross the callback helper boundary (and\ntherefore must care about unwinding kernel state), however it is\npossible to lift this restriction in the future follow-up.\n\nExceptions terminate propogating at program boundaries, hence both\nBPF_PROG_TYPE_EXT and tail call targets return to their caller context\nthe return value of the exception callback, in the event that they throw\nan exception. Thus, exceptions do not cross extension or tail call\nboundary.\n\nHowever, this is mostly an implementation choice, and can be changed to\nsuit more user-friendly semantics.\n\nChangelog:\n----------\nv2 -> v3\nv2: https://lore.kernel.org/bpf/20230809114116.3216687-1-memxor@gmail.com\n\n * Add Dave's Acked-by.\n * Address all comments from Alexei.\n   * Use bpf_is_subprog to check for main prog in bpf_stack_walker.\n   * Drop accidental leftover hunk in libbpf patch.\n   * Split libbpf patch's refactoring to aid review\n   * Disable fentry/fexit in addition to freplace for exception cb.\n   * Add selftests for fentry/fexit/freplace on exception cb and main prog.\n * Use btf_find_by_name_kind in bpf_find_exception_callback_insn_off (Martin)\n * Split KASAN patch into two to aid backporting (Andrey)\n * Move exception callback append step to bpf_object__reloacte (Andrii)\n * Ensure that the exception callback name is unique (Andrii)\n * Keep ASM implementation of assertion macros instead of C, as it does\n   not achieve intended results for bpf_assert_range and other cases.\n\nv1 -> v2\nv1: https://lore.kernel.org/bpf/20230713023232.1411523-1-memxor@gmail.com\n\n * Address all comments from Alexei.\n * Fix a few bugs and corner cases in the implementations found during\n   testing. Also add new selftests for these cases.\n * Reinstate patch to consider ksym.end part of the program (but\n   reworked to cover other corner cases).\n * Implement new style of tagging exception callbacks, add libbpf\n   support for the new declaration tag.\n * Limit support to 64-bit integer types for assertion macros. The\n   compiler ends up performing shifts or bitwise and operations when\n   finally making use of the value, which defeats the purpose of the\n   macro. On noalu32 mode, the shifts may also happen before use,\n   hurting reliability.\n * Comprehensively test assertion macros and their side effects on the\n   verifier state, register bounds, etc.\n * Fix a KASAN false positive warning.\n\nRFC v1 -> v1\nRFC v1: https://lore.kernel.org/bpf/20230405004239.1375399-1-memxor@gmail.com\n\n * Completely rework the unwinding infrastructure to use offline\n   unwinding support.\n * Remove the runtime exception state and program rewriting code.\n * Make bpf_set_exception_callback idempotent to avoid vexing\n   synchronization and state clobbering issues in presence of program\n   nesting.\n * Disable bpf_throw within callback functions, for now.\n * Allow bpf_throw in tail call programs and extension programs,\n   removing limitations of rewrite based unwinding.\n * Expand selftests.\n====================\n\nLink: https://lore.kernel.org/r/20230912233214.1518551-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-09-16 09:36:44 -0700",
      "commits": [
        {
          "hash": "9af27da6313c8f8c6a26c7ea3fe23d6b9664a3a8",
          "subject": "bpf: Use bpf_is_subprog to check for subprogs",
          "message": "We would like to know whether a bpf_prog corresponds to the main prog or\none of the subprogs. The current JIT implementations simply check this\nusing the func_idx in bpf_prog->aux->func_idx. When the index is 0, it\nbelongs to the main program, otherwise it corresponds to some\nsubprogram.\n\nThis will also be necessary to halt exception propagation while walking\nthe stack when an exception is thrown, so we add a simple helper\nfunction to check this, named bpf_is_subprog, and convert existing JIT\nimplementations to also make use of it.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:34:20 -0700",
          "modified_files": [
            "arch/arm64/net/bpf_jit_comp.c",
            "arch/s390/net/bpf_jit_comp.c",
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/bpf.h"
          ]
        },
        {
          "hash": "fd5d27b70188379bb441d404c29a0afb111e1753",
          "subject": "arch/x86: Implement arch_bpf_stack_walk",
          "message": "The plumbing for offline unwinding when we throw an exception in\nprograms would require walking the stack, hence introduce a new\narch_bpf_stack_walk function. This is provided when the JIT supports\nexceptions, i.e. bpf_jit_supports_exceptions is true. The arch-specific\ncode is really minimal, hence it should be straightforward to extend\nthis support to other architectures as well, as it reuses the logic of\narch_stack_walk, but allowing access to unwind_state data.\n\nOnce the stack pointer and frame pointer are known for the main subprog\nduring the unwinding, we know the stack layout and location of any\ncallee-saved registers which must be restored before we return back to\nthe kernel. This handling will be added in the subsequent patches.\n\nNote that while we primarily unwind through BPF frames, which are\neffectively CONFIG_UNWINDER_FRAME_POINTER, we still need one of this or\nCONFIG_UNWINDER_ORC to be able to unwind through the bpf_throw frame\nfrom which we begin walking the stack. We also require both sp and bp\n(stack and frame pointers) from the unwind_state structure, which are\nonly available when one of these two options are enabled.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:34:21 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/filter.h",
            "kernel/bpf/core.c"
          ]
        },
        {
          "hash": "335d1c5b545284d75ef96ee42e461eacefe865bb",
          "subject": "bpf: Implement support for adding hidden subprogs",
          "message": "Introduce support in the verifier for generating a subprogram and\ninclude it as part of a BPF program dynamically after the do_check phase\nis complete. The first user will be the next patch which generates\ndefault exception callbacks if none are set for the program. The phase\nof invocation will be do_misc_fixups. Note that this is an internal\nverifier function, and should be used with instruction blocks which\nuphold the invariants stated in check_subprogs.\n\nSince these subprogs are always appended to the end of the instruction\nsequence of the program, it becomes relatively inexpensive to do the\nrelated adjustments to the subprog_info of the program. Only the fake\nexit subprogram is shifted forward, making room for our new subprog.\n\nThis is useful to insert a new subprogram, get it JITed, and obtain its\nfunction pointer. The next patch will use this functionality to insert a\ndefault exception callback which will be invoked after unwinding the\nstack.\n\nNote that these added subprograms are invisible to userspace, and never\nreported in BPF_OBJ_GET_INFO_BY_ID etc. For now, only a single\nsubprogram is supported, but more can be easily supported in the future.\n\nTo this end, two function counts are introduced now, the existing\nfunc_cnt, and real_func_cnt, the latter including hidden programs. This\nallows us to conver the JIT code to use the real_func_cnt for management\nof resources while syscall path continues working with existing\nfunc_cnt.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-4-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:34:21 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "kernel/bpf/core.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "f18b03fabaa9b7c80e80b72a621f481f0d706ae0",
          "subject": "bpf: Implement BPF exceptions",
          "message": "This patch implements BPF exceptions, and introduces a bpf_throw kfunc\nto allow programs to throw exceptions during their execution at runtime.\nA bpf_throw invocation is treated as an immediate termination of the\nprogram, returning back to its caller within the kernel, unwinding all\nstack frames.\n\nThis allows the program to simplify its implementation, by testing for\nruntime conditions which the verifier has no visibility into, and assert\nthat they are true. In case they are not, the program can simply throw\nan exception from the other branch.\n\nBPF exceptions are explicitly *NOT* an unlikely slowpath error handling\nprimitive, and this objective has guided design choices of the\nimplementation of the them within the kernel (with the bulk of the cost\nfor unwinding the stack offloaded to the bpf_throw kfunc).\n\nThe implementation of this mechanism requires use of add_hidden_subprog\nmechanism introduced in the previous patch, which generates a couple of\ninstructions to move R1 to R0 and exit. The JIT then rewrites the\nprologue of this subprog to take the stack pointer and frame pointer as\ninputs and reset the stack frame, popping all callee-saved registers\nsaved by the main subprog. The bpf_throw function then walks the stack\nat runtime, and invokes this exception subprog with the stack and frame\npointers as parameters.\n\nReviewers must take note that currently the main program is made to save\nall callee-saved registers on x86_64 during entry into the program. This\nis because we must do an equivalent of a lightweight context switch when\nunwinding the stack, therefore we need the callee-saved registers of the\ncaller of the BPF program to be able to return with a sane state.\n\nNote that we have to additionally handle r12, even though it is not used\nby the program, because when throwing the exception the program makes an\nentry into the kernel which could clobber r12 after saving it on the\nstack. To be able to preserve the value we received on program entry, we\npush r12 and restore it from the generated subprogram when unwinding the\nstack.\n\nFor now, bpf_throw invocation fails when lingering resources or locks\nexist in that path of the program. In a future followup, bpf_throw will\nbe extended to perform frame-by-frame unwinding to release lingering\nresources for each stack frame, removing this limitation.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-5-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:34:21 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "include/linux/filter.h",
            "kernel/bpf/core.c",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "aaa619ebccb2b78b3c6d2c0cd72d206ee8fc0025",
          "subject": "bpf: Refactor check_btf_func and split into two phases",
          "message": "This patch splits the check_btf_info's check_btf_func check into two\nseparate phases.  The first phase sets up the BTF and prepares\nfunc_info, but does not perform any validation of required invariants\nfor subprogs just yet. This is left to the second phase, which happens\nwhere check_btf_info executes currently, and performs the line_info and\nCO-RE relocation.\n\nThe reason to perform this split is to obtain the userspace supplied\nfunc_info information before we perform the add_subprog call, where we\nwould now require finding and adding subprogs that may not have a\nbpf_pseudo_call or bpf_pseudo_func instruction in the program.\n\nWe require this as we want to enable userspace to supply exception\ncallbacks that can override the default hidden subprogram generated by\nthe verifier (which performs a hardcoded action). In such a case, the\nexception callback may never be referenced in an instruction, but will\nstill be suitably annotated (by way of BTF declaration tags). For\nfinding this exception callback, we would require the program's BTF\ninformation, and the supplied func_info information which maps BTF type\nIDs to subprograms.\n\nSince the exception callback won't actually be referenced through\ninstructions, later checks in check_cfg and do_check_subprogs will not\nverify the subprog. This means that add_subprog needs to add them in the\nadd_subprog_and_kfunc phase before we move forward, which is why the BTF\nand func_info are required at that point.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-6-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:34:21 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b9ae0c9dd0aca79bffc17be51c2dc148d1f72708",
          "subject": "bpf: Add support for custom exception callbacks",
          "message": "By default, the subprog generated by the verifier to handle a thrown\nexception hardcodes a return value of 0. To allow user-defined logic\nand modification of the return value when an exception is thrown,\nintroduce the 'exception_callback:' declaration tag, which marks a\ncallback as the default exception handler for the program.\n\nThe format of the declaration tag is 'exception_callback:<value>', where\n<value> is the name of the exception callback. Each main program can be\ntagged using this BTF declaratiion tag to associate it with an exception\ncallback. In case the tag is absent, the default callback is used.\n\nAs such, the exception callback cannot be modified at runtime, only set\nduring verification.\n\nAllowing modification of the callback for the current program execution\nat runtime leads to issues when the programs begin to nest, as any\nper-CPU state maintaing this information will have to be saved and\nrestored. We don't want it to stay in bpf_prog_aux as this takes a\nglobal effect for all programs. An alternative solution is spilling\nthe callback pointer at a known location on the program stack on entry,\nand then passing this location to bpf_throw as a parameter.\n\nHowever, since exceptions are geared more towards a use case where they\nare ideally never invoked, optimizing for this use case and adding to\nthe complexity has diminishing returns.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-7-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:34:21 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "b62bf8a5e9110922f58f6ea8fe747e1759f49e61",
          "subject": "bpf: Perform CFG walk for exception callback",
          "message": "Since exception callbacks are not referenced using bpf_pseudo_func and\nbpf_pseudo_call instructions, check_cfg traversal will never explore\ninstructions of the exception callback. Even after adding the subprog,\nthe program will then fail with a 'unreachable insn' error.\n\nWe thus need to begin walking from the start of the exception callback\nagain in check_cfg after a complete CFG traversal finishes, so as to\nexplore the CFG rooted at the exception callback.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-8-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:34:21 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "a923819fb2c5be029a69c0ca53239865c9bc05dd",
          "subject": "bpf: Treat first argument as return value for bpf_throw",
          "message": "In case of the default exception callback, change the behavior of\nbpf_throw, where the passed cookie value is no longer ignored, but\nis instead the return value of the default exception callback. As\nsuch, we need to place restrictions on the value being passed into\nbpf_throw in such a case, only allowing those permitted by the\ncheck_return_code function.\n\nThus, bpf_throw can now control the return value of the program from\neach call site without having the user install a custom exception\ncallback just to override the return value when an exception is thrown.\n\nWe also modify the hidden subprog instructions to now move BPF_REG_1 to\nBPF_REG_0, so as to set the return value before exit in the default\ncallback.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-9-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:34:21 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "7ccb84f04cda1dd6f64f352e9795db308e9cdc0c",
          "subject": "mm: kasan: Declare kasan_unpoison_task_stack_below in kasan.h",
          "message": "We require access to this kasan helper in BPF code in the next patch\nwhere we have to unpoison the task stack when we unwind and reset the\nstack frame from bpf_throw, and it never really unpoisons the poisoned\nstack slots on entry when compiler instrumentation is generated by\nCONFIG_KASAN_STACK and inline instrumentation is supported.\n\nAlso, remove the declaration from mm/kasan/kasan.h as we put it in the\nheader file kasan.h.\n\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Dmitry Vyukov <dvyukov@google.com>\nCc: Vincenzo Frascino <vincenzo.frascino@arm.com>\nSuggested-by: Andrey Konovalov <andreyknvl@gmail.com>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nReviewed-by: Andrey Konovalov <andreyknvl@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-10-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:34:21 -0700",
          "modified_files": [
            "include/linux/kasan.h",
            "mm/kasan/kasan.h"
          ]
        },
        {
          "hash": "ec5290a178b787b2f8b21581fdadc919bd004e12",
          "subject": "bpf: Prevent KASAN false positive with bpf_throw",
          "message": "The KASAN stack instrumentation when CONFIG_KASAN_STACK is true poisons\nthe stack of a function when it is entered and unpoisons it when\nleaving. However, in the case of bpf_throw, we will never return as we\nswitch our stack frame to the BPF exception callback. Later, this\ndiscrepancy will lead to confusing KASAN splats when kernel resumes\nexecution on return from the BPF program.\n\nFix this by unpoisoning everything below the stack pointer of the BPF\nprogram, which should cover the range that would not be unpoisoned. An\nexample splat is below:\n\nBUG: KASAN: stack-out-of-bounds in stack_trace_consume_entry+0x14e/0x170\nWrite of size 8 at addr ffffc900013af958 by task test_progs/227\n\nCPU: 0 PID: 227 Comm: test_progs Not tainted 6.5.0-rc2-g43f1c6c9052a-dirty #26\nHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.16.2-2.fc39 04/01/2014\nCall Trace:\n <TASK>\n dump_stack_lvl+0x4a/0x80\n print_report+0xcf/0x670\n ? arch_stack_walk+0x79/0x100\n kasan_report+0xda/0x110\n ? stack_trace_consume_entry+0x14e/0x170\n ? stack_trace_consume_entry+0x14e/0x170\n ? __pfx_stack_trace_consume_entry+0x10/0x10\n stack_trace_consume_entry+0x14e/0x170\n ? __sys_bpf+0xf2e/0x41b0\n arch_stack_walk+0x8b/0x100\n ? __sys_bpf+0xf2e/0x41b0\n ? bpf_prog_test_run_skb+0x341/0x1c70\n ? bpf_prog_test_run_skb+0x341/0x1c70\n stack_trace_save+0x9b/0xd0\n ? __pfx_stack_trace_save+0x10/0x10\n ? __kasan_slab_free+0x109/0x180\n ? bpf_prog_test_run_skb+0x341/0x1c70\n ? __sys_bpf+0xf2e/0x41b0\n ? __x64_sys_bpf+0x78/0xc0\n ? do_syscall_64+0x3c/0x90\n ? entry_SYSCALL_64_after_hwframe+0x6e/0xd8\n kasan_save_stack+0x33/0x60\n ? kasan_save_stack+0x33/0x60\n ? kasan_set_track+0x25/0x30\n ? kasan_save_free_info+0x2b/0x50\n ? __kasan_slab_free+0x109/0x180\n ? kmem_cache_free+0x191/0x460\n ? bpf_prog_test_run_skb+0x341/0x1c70\n kasan_set_track+0x25/0x30\n kasan_save_free_info+0x2b/0x50\n __kasan_slab_free+0x109/0x180\n kmem_cache_free+0x191/0x460\n bpf_prog_test_run_skb+0x341/0x1c70\n ? __pfx_bpf_prog_test_run_skb+0x10/0x10\n ? __fget_light+0x51/0x220\n __sys_bpf+0xf2e/0x41b0\n ? __might_fault+0xa2/0x170\n ? __pfx___sys_bpf+0x10/0x10\n ? lock_release+0x1de/0x620\n ? __might_fault+0xcd/0x170\n ? __pfx_lock_release+0x10/0x10\n ? __pfx_blkcg_maybe_throttle_current+0x10/0x10\n __x64_sys_bpf+0x78/0xc0\n ? syscall_enter_from_user_mode+0x20/0x50\n do_syscall_64+0x3c/0x90\n entry_SYSCALL_64_after_hwframe+0x6e/0xd8\nRIP: 0033:0x7f0fbb38880d\nCode: ff c3 66 2e 0f 1f 84 00 00 00 00 00 90 f3 0f 1e fa 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d\n89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d f3 45 12 00 f7 d8 64\n89 01 48\nRSP: 002b:00007ffe13907de8 EFLAGS: 00000206 ORIG_RAX: 0000000000000141\nRAX: ffffffffffffffda RBX: 00007ffe13908708 RCX: 00007f0fbb38880d\nRDX: 0000000000000050 RSI: 00007ffe13907e20 RDI: 000000000000000a\nRBP: 00007ffe13907e00 R08: 0000000000000000 R09: 00007ffe13907e20\nR10: 0000000000000064 R11: 0000000000000206 R12: 0000000000000003\nR13: 0000000000000000 R14: 00007f0fbb532000 R15: 0000000000cfbd90\n </TASK>\n\nThe buggy address belongs to stack of task test_progs/227\nKASAN internal error: frame info validation failed; invalid marker: 0\n\nThe buggy address belongs to the virtual mapping at\n [ffffc900013a8000, ffffc900013b1000) created by:\n kernel_clone+0xcd/0x600\n\nThe buggy address belongs to the physical page:\npage:00000000b70f4332 refcount:1 mapcount:0 mapping:0000000000000000 index:0x0 pfn:0x11418f\nflags: 0x2fffe0000000000(node=0|zone=2|lastcpupid=0x7fff)\npage_type: 0xffffffff()\nraw: 02fffe0000000000 0000000000000000 dead000000000122 0000000000000000\nraw: 0000000000000000 0000000000000000 00000001ffffffff 0000000000000000\npage dumped because: kasan: bad access detected\n\nMemory state around the buggy address:\n ffffc900013af800: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n ffffc900013af880: 00 00 00 f1 f1 f1 f1 00 00 00 f3 f3 f3 f3 f3 00\n>ffffc900013af900: 00 00 00 00 00 00 00 00 00 00 00 f1 00 00 00 00\n                                                    ^\n ffffc900013af980: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n ffffc900013afa00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n==================================================================\nDisabling lock debugging due to kernel taint\n\nCc: Andrey Ryabinin <ryabinin.a.a@gmail.com>\nCc: Alexander Potapenko <glider@google.com>\nCc: Andrey Konovalov <andreyknvl@gmail.com>\nCc: Dmitry Vyukov <dvyukov@google.com>\nCc: Vincenzo Frascino <vincenzo.frascino@arm.com>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nAcked-by: Andrey Konovalov <andreyknvl@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-11-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:34:22 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "66d9111f3517f85ef2af0337ece02683ce0faf21",
          "subject": "bpf: Detect IP == ksym.end as part of BPF program",
          "message": "Now that bpf_throw kfunc is the first such call instruction that has\nnoreturn semantics within the verifier, this also kicks in dead code\nelimination in unprecedented ways. For one, any instruction following\na bpf_throw call will never be marked as seen. Moreover, if a callchain\nends up throwing, any instructions after the call instruction to the\neventually throwing subprog in callers will also never be marked as\nseen.\n\nThe tempting way to fix this would be to emit extra 'int3' instructions\nwhich bump the jited_len of a program, and ensure that during runtime\nwhen a program throws, we can discover its boundaries even if the call\ninstruction to bpf_throw (or to subprogs that always throw) is emitted\nas the final instruction in the program.\n\nAn example of such a program would be this:\n\ndo_something():\n\t...\n\tr0 = 0\n\texit\n\nfoo():\n\tr1 = 0\n\tcall bpf_throw\n\tr0 = 0\n\texit\n\nbar(cond):\n\tif r1 != 0 goto pc+2\n\tcall do_something\n\texit\n\tcall foo\n\tr0 = 0  // Never seen by verifier\n\texit\t//\n\nmain(ctx):\n\tr1 = ...\n\tcall bar\n\tr0 = 0\n\texit\n\nHere, if we do end up throwing, the stacktrace would be the following:\n\nbpf_throw\nfoo\nbar\nmain\n\nIn bar, the final instruction emitted will be the call to foo, as such,\nthe return address will be the subsequent instruction (which the JIT\nemits as int3 on x86). This will end up lying outside the jited_len of\nthe program, thus, when unwinding, we will fail to discover the return\naddress as belonging to any program and end up in a panic due to the\nunreliable stack unwinding of BPF programs that we never expect.\n\nTo remedy this case, make bpf_prog_ksym_find treat IP == ksym.end as\npart of the BPF program, so that is_bpf_text_address returns true when\nsuch a case occurs, and we are able to unwind reliably when the final\ninstruction ends up being a call instruction.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-12-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:34:22 -0700",
          "modified_files": [
            "kernel/bpf/core.c"
          ]
        },
        {
          "hash": "fd548e1a46185000191a89cae4be560e076ed6c7",
          "subject": "bpf: Disallow fentry/fexit/freplace for exception callbacks",
          "message": "During testing, it was discovered that extensions to exception callbacks\nhad no checks, upon running a testcase, the kernel ended up running off\nthe end of a program having final call as bpf_throw, and hitting int3\ninstructions.\n\nThe reason is that while the default exception callback would have reset\nthe stack frame to return back to the main program's caller, the\nreplacing extension program will simply return back to bpf_throw, which\nwill instead return back to the program and the program will continue\nexecution, now in an undefined state where anything could happen.\n\nThe way to support extensions to an exception callback would be to mark\nthe BPF_PROG_TYPE_EXT main subprog as an exception_cb, and prevent it\nfrom calling bpf_throw. This would make the JIT produce a prologue that\nrestores saved registers and reset the stack frame. But let's not do\nthat until there is a concrete use case for this, and simply disallow\nthis for now.\n\nSimilar issues will exist for fentry and fexit cases, where trampoline\nsaves data on the stack when invoking exception callback, which however\nwill then end up resetting the stack frame, and on return, the fexit\nprogram will never will invoked as the return address points to the main\nprogram's caller in the kernel. Instead of additional complexity and\nback and forth between the two stacks to enable such a use case, simply\nforbid it.\n\nOne key point here to note is that currently X86_TAIL_CALL_OFFSET didn't\nrequire any modifications, even though we emit instructions before the\ncorresponding endbr64 instruction. This is because we ensure that a main\nsubprog never serves as an exception callback, and therefore the\nexception callback (which will be a global subprog) can never serve as\nthe tail call target, eliminating any discrepancies. However, once we\nsupport a BPF_PROG_TYPE_EXT to also act as an exception callback, it\nwill end up requiring change to the tail call offset to account for the\nextra instructions. For simplicitly, tail calls could be disabled for\nsuch targets.\n\nNoting the above, it appears better to wait for a concrete use case\nbefore choosing to permit extension programs to replace exception\ncallbacks.\n\nAs a precaution, we disable fentry and fexit for exception callbacks as\nwell.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-13-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:36:32 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "06d686f771ddc27a8554cd8f5b22e071040dc90e",
          "subject": "bpf: Fix kfunc callback register type handling",
          "message": "The kfunc code to handle KF_ARG_PTR_TO_CALLBACK does not check the reg\ntype before using reg->subprogno. This can accidently permit invalid\npointers from being passed into callback helpers (e.g. silently from\ndifferent paths). Likewise, reg->subprogno from the per-register type\nunion may not be meaningful either. We need to reject any other type\nexcept PTR_TO_FUNC.\n\nAcked-by: Dave Marchevsky <davemarchevsky@fb.com>\nFixes: 5d92ddc3de1b (\"bpf: Add callback validation to kfunc verifier logic\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-14-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:36:43 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "6c918709bd30852258e66b3f566c9614e3f29e35",
          "subject": "libbpf: Refactor bpf_object__reloc_code",
          "message": "Refactor bpf_object__append_subprog_code out of bpf_object__reloc_code\nto be able to reuse it to append subprog related code for the exception\ncallback to the main program.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-15-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:36:43 -0700",
          "modified_files": [
            "tools/lib/bpf/libbpf.c"
          ]
        },
        {
          "hash": "7e2925f6723702bcfcfdf8f73d5e85f7514d4b9f",
          "subject": "libbpf: Add support for custom exception callbacks",
          "message": "Add support to libbpf to append exception callbacks when loading a\nprogram. The exception callback is found by discovering the declaration\ntag 'exception_callback:<value>' and finding the callback in the value\nof the tag.\n\nThe process is done in two steps. First, for each main program, the\nbpf_object__sanitize_and_load_btf function finds and marks its\ncorresponding exception callback as defined by the declaration tag on\nit. Second, bpf_object__reloc_code is modified to append the indicated\nexception callback at the end of the instruction iteration (since\nexception callback will never be appended in that loop, as it is not\ndirectly referenced).\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-16-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:36:43 -0700",
          "modified_files": [
            "tools/lib/bpf/libbpf.c"
          ]
        },
        {
          "hash": "d6ea06803212d992cbab24466f491ee0178bf9e0",
          "subject": "selftests/bpf: Add BPF assertion macros",
          "message": "Add macros implementing an 'assert' statement primitive using macros,\nbuilt on top of the BPF exceptions support introduced in previous\npatches.\n\nThe bpf_assert_*_with variants allow supplying a value which can the be\ninspected within the exception handler to signify the assert statement\nthat led to the program being terminated abruptly, or be returned by the\ndefault exception handler.\n\nNote that only 64-bit scalar values are supported with these assertion\nmacros, as during testing I found other cases quite unreliable in\npresence of compiler shifts/manipulations extracting the value of the\nright width from registers scrubbing the verifier's bounds information\nand knowledge about the value in the register.\n\nThus, it is easier to reliably support this feature with only the full\nregister width, and support both signed and unsigned variants.\n\nThe bpf_assert_range is interesting in particular, which clamps the\nvalue in the [begin, end] (both inclusive) range within verifier state,\nand emits a check for the same at runtime.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-17-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:36:43 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "d2a93715bfb0655a63bb1687f43f48eb2e61717b",
          "subject": "selftests/bpf: Add tests for BPF exceptions",
          "message": "Add selftests to cover success and failure cases of API usage, runtime\nbehavior and invariants that need to be maintained for implementation\ncorrectness.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-18-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-09-16 09:36:43 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/DENYLIST.aarch64",
            "tools/testing/selftests/bpf/DENYLIST.s390x",
            "tools/testing/selftests/bpf/prog_tests/exceptions.c",
            "tools/testing/selftests/bpf/progs/exceptions.c",
            "tools/testing/selftests/bpf/progs/exceptions_assert.c",
            "tools/testing/selftests/bpf/progs/exceptions_ext.c",
            "tools/testing/selftests/bpf/progs/exceptions_fail.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "5bbb9e1f08352a381a6e8a17b5180170b2a93685",
      "merge_subject": "Merge branch 'bpf-x64-fix-tailcall-infinite-loop'",
      "merge_body": "Leon Hwang says:\n\n====================\nbpf, x64: Fix tailcall infinite loop\n\nThis patch series fixes a tailcall infinite loop on x64.\n\nFrom commit ebf7d1f508a73871 (\"bpf, x64: rework pro/epilogue and tailcall\nhandling in JIT\"), the tailcall on x64 works better than before.\n\nFrom commit e411901c0b775a3a (\"bpf: allow for tailcalls in BPF subprograms\nfor x64 JIT\"), tailcall is able to run in BPF subprograms on x64.\n\nFrom commit 5b92a28aae4dd0f8 (\"bpf: Support attaching tracing BPF program\nto other BPF programs\"), BPF program is able to trace other BPF programs.\n\nHow about combining them all together?\n\n1. FENTRY/FEXIT on a BPF subprogram.\n2. A tailcall runs in the BPF subprogram.\n3. The tailcall calls the subprogram's caller.\n\nAs a result, a tailcall infinite loop comes up. And the loop would halt\nthe machine.\n\nAs we know, in tail call context, the tail_call_cnt propagates by stack\nand rax register between BPF subprograms. So do in trampolines.\n\nHow did I discover the bug?\n\nFrom commit 7f6e4312e15a5c37 (\"bpf: Limit caller's stack depth 256 for\nsubprogs with tailcalls\"), the total stack size limits to around 8KiB.\nThen, I write some bpf progs to validate the stack consuming, that are\ntailcalls running in bpf2bpf and FENTRY/FEXIT tracing on bpf2bpf.\n\nAt that time, accidently, I made a tailcall loop. And then the loop halted\nmy VM. Without the loop, the bpf progs would consume over 8KiB stack size.\nBut the _stack-overflow_ did not halt my VM.\n\nWith bpf_printk(), I confirmed that the tailcall count limit did not work\nexpectedly. Next, read the code and fix it.\n\nThank Ilya Leoshkevich, this bug on s390x has been fixed.\n\nHopefully, this bug on arm64 will be fixed in near future.\n====================\n\nLink: https://lore.kernel.org/r/20230912150442.2009-1-hffilwlqm@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-09-12 13:06:12 -0700",
      "commits": [
        {
          "hash": "2bee9770f3c6be736a28725cb0f93775ed22e720",
          "subject": "bpf, x64: Comment tail_call_cnt initialisation",
          "message": "Without understanding emit_prologue(), it is really hard to figure out\nwhere does tail_call_cnt come from, even though searching tail_call_cnt\nin the whole kernel repo.\n\nBy adding these comments, it is a little bit easier to understand\ntail_call_cnt initialisation.\n\nSigned-off-by: Leon Hwang <hffilwlqm@gmail.com>\nLink: https://lore.kernel.org/r/20230912150442.2009-2-hffilwlqm@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Leon Hwang <hffilwlqm@gmail.com>",
          "date": "2023-09-12 13:06:12 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c"
          ]
        },
        {
          "hash": "2b5dcb31a19a2e0acd869b12c9db9b2d696ef544",
          "subject": "bpf, x64: Fix tailcall infinite loop",
          "message": "From commit ebf7d1f508a73871 (\"bpf, x64: rework pro/epilogue and tailcall\nhandling in JIT\"), the tailcall on x64 works better than before.\n\nFrom commit e411901c0b775a3a (\"bpf: allow for tailcalls in BPF subprograms\nfor x64 JIT\"), tailcall is able to run in BPF subprograms on x64.\n\nFrom commit 5b92a28aae4dd0f8 (\"bpf: Support attaching tracing BPF program\nto other BPF programs\"), BPF program is able to trace other BPF programs.\n\nHow about combining them all together?\n\n1. FENTRY/FEXIT on a BPF subprogram.\n2. A tailcall runs in the BPF subprogram.\n3. The tailcall calls the subprogram's caller.\n\nAs a result, a tailcall infinite loop comes up. And the loop would halt\nthe machine.\n\nAs we know, in tail call context, the tail_call_cnt propagates by stack\nand rax register between BPF subprograms. So do in trampolines.\n\nFixes: ebf7d1f508a7 (\"bpf, x64: rework pro/epilogue and tailcall handling in JIT\")\nFixes: e411901c0b77 (\"bpf: allow for tailcalls in BPF subprograms for x64 JIT\")\nReviewed-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>\nSigned-off-by: Leon Hwang <hffilwlqm@gmail.com>\nLink: https://lore.kernel.org/r/20230912150442.2009-3-hffilwlqm@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Leon Hwang <hffilwlqm@gmail.com>",
          "date": "2023-09-12 13:06:12 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/bpf.h",
            "kernel/bpf/trampoline.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "e13b5f2f3ba3df1ca31824d2fdbd182250fa10c7",
          "subject": "selftests/bpf: Add testcases for tailcall infinite loop fixing",
          "message": "Add 4 test cases to confirm the tailcall infinite loop bug has been fixed.\n\nLike tailcall_bpf2bpf cases, do fentry/fexit on the bpf2bpf, and then\ncheck the final count result.\n\ntools/testing/selftests/bpf/test_progs -t tailcalls\n226/13  tailcalls/tailcall_bpf2bpf_fentry:OK\n226/14  tailcalls/tailcall_bpf2bpf_fexit:OK\n226/15  tailcalls/tailcall_bpf2bpf_fentry_fexit:OK\n226/16  tailcalls/tailcall_bpf2bpf_fentry_entry:OK\n226     tailcalls:OK\nSummary: 1/16 PASSED, 0 SKIPPED, 0 FAILED\n\nSigned-off-by: Leon Hwang <hffilwlqm@gmail.com>\nLink: https://lore.kernel.org/r/20230912150442.2009-4-hffilwlqm@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Leon Hwang <hffilwlqm@gmail.com>",
          "date": "2023-09-12 13:06:12 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/tailcalls.c",
            "tools/testing/selftests/bpf/progs/tailcall_bpf2bpf_fentry.c",
            "tools/testing/selftests/bpf/progs/tailcall_bpf2bpf_fexit.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "1e4a6d975e5cd114509aa447750d68d295a501a7",
      "merge_subject": "Merge branch 'bpf-add-support-for-local-percpu-kptr'",
      "merge_body": "Yonghong Song says:\n\n====================\nbpf: Add support for local percpu kptr\n\nPatch set [1] implemented cgroup local storage BPF_MAP_TYPE_CGRP_STORAGE\nsimilar to sk/task/inode local storage and old BPF_MAP_TYPE_CGROUP_STORAGE\nmap is marked as deprecated since old BPF_MAP_TYPE_CGROUP_STORAGE map can\nonly work with current cgroup.\n\nSimilarly, the existing BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE map\nis a percpu version of BPF_MAP_TYPE_CGROUP_STORAGE and only works\nwith current cgroup. But there is no replacement which can work\nwith arbitrary cgroup.\n\nThis patch set solved this problem but adding support for local\npercpu kptr. The map value can have a percpu kptr field which holds\na bpf prog allocated percpu data. The below is an example,\n\n  struct percpu_val_t {\n    ... fields ...\n  }\n\n  struct map_value_t {\n    struct percpu_val_t __percpu_kptr *percpu_data_ptr;\n  }\n\nIn the above, 'map_value_t' is the map value type for a\nBPF_MAP_TYPE_CGRP_STORAGE map. User can access 'percpu_data_ptr'\nand then read/write percpu data. This covers BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE\nand more. So BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE map type\nis marked as deprecated.\n\nIn additional, local percpu kptr supports the same map type\nas other kptrs including hash, lru_hash, array, sk/inode/task/cgrp\nlocal storage. Currently, percpu data structure does not support\nnon-scalars or special fields (e.g., bpf_spin_lock, bpf_rb_root, etc.).\nThey can be supported in the future if there exist use cases.\n\nPlease for individual patches for details.\n\n  [1] https://lore.kernel.org/all/20221026042835.672317-1-yhs@fb.com/\n\nChangelog:\n  v2 -> v3:\n    - fix libbpf_str test failure.\n  v1 -> v2:\n    - does not support special fields in percpu data structure.\n    - rename __percpu attr to __percpu_kptr attr.\n    - rename BPF_KPTR_PERCPU_REF to BPF_KPTR_PERCPU.\n    - better code to handle bpf_{this,per}_cpu_ptr() helpers.\n    - add more negative tests.\n    - fix a bpftool related test failure.\n====================\n\nLink: https://lore.kernel.org/r/20230827152729.1995219-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-09-08 08:42:18 -0700",
      "commits": [
        {
          "hash": "41a5db8d8161457b121a03fde999ff6e00090ee2",
          "subject": "bpf: Add support for non-fix-size percpu mem allocation",
          "message": "This is needed for later percpu mem allocation when the\nallocation is done by bpf program. For such cases, a global\nbpf_global_percpu_ma is added where a flexible allocation\nsize is needed.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152734.1995725-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:17 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/core.c",
            "kernel/bpf/memalloc.c"
          ]
        },
        {
          "hash": "55db92f42fe4a4ef7b4c2b4960c6212c8512dd53",
          "subject": "bpf: Add BPF_KPTR_PERCPU as a field type",
          "message": "BPF_KPTR_PERCPU represents a percpu field type like below\n\n  struct val_t {\n    ... fields ...\n  };\n  struct t {\n    ...\n    struct val_t __percpu_kptr *percpu_data_ptr;\n    ...\n  };\n\nwhere\n  #define __percpu_kptr __attribute__((btf_type_tag(\"percpu_kptr\")))\n\nWhile BPF_KPTR_REF points to a trusted kernel object or a trusted\nlocal object, BPF_KPTR_PERCPU points to a trusted local\npercpu object.\n\nThis patch added basic support for BPF_KPTR_PERCPU\nrelated to percpu_kptr field parsing, recording and free operations.\nBPF_KPTR_PERCPU also supports the same map types\nas BPF_KPTR_REF does.\n\nNote that unlike a local kptr, it is possible that\na BPF_KTPR_PERCPU struct may not contain any\nspecial fields like other kptr, bpf_spin_lock, bpf_list_head, etc.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152739.1996391-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:17 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "36d8bdf75a93190e5669b9d1d95994e13e15ba1d",
          "subject": "bpf: Add alloc/xchg/direct_access support for local percpu kptr",
          "message": "Add two new kfunc's, bpf_percpu_obj_new_impl() and\nbpf_percpu_obj_drop_impl(), to allocate a percpu obj.\nTwo functions are very similar to bpf_obj_new_impl()\nand bpf_obj_drop_impl(). The major difference is related\nto percpu handling.\n\n    bpf_rcu_read_lock()\n    struct val_t __percpu_kptr *v = map_val->percpu_data;\n    ...\n    bpf_rcu_read_unlock()\n\nFor a percpu data map_val like above 'v', the reg->type\nis set as\n\tPTR_TO_BTF_ID | MEM_PERCPU | MEM_RCU\nif inside rcu critical section.\n\nMEM_RCU marking here is similar to NON_OWN_REF as 'v'\nis not a owning reference. But NON_OWN_REF is\ntrusted and typically inside the spinlock while\nMEM_RCU is under rcu read lock. RCU is preferred here\nsince percpu data structures mean potential concurrent\naccess into its contents.\n\nAlso, bpf_percpu_obj_new_impl() is restricted such that\nno pointers or special fields are allowed. Therefore,\nthe bpf_list_head and bpf_rb_root will not be supported\nin this patch set to avoid potential memory leak issue\ndue to racing between bpf_obj_free_fields() and another\nbpf_kptr_xchg() moving an allocated object to\nbpf_list_head and bpf_rb_root.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152744.1996739-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:17 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "01cc55af93884f1ff5a883426e1924378dfcc62a",
          "subject": "bpf: Add bpf_this_cpu_ptr/bpf_per_cpu_ptr support for allocated percpu obj",
          "message": "The bpf helpers bpf_this_cpu_ptr() and bpf_per_cpu_ptr() are re-purposed\nfor allocated percpu objects. For an allocated percpu obj,\nthe reg type is 'PTR_TO_BTF_ID | MEM_PERCPU | MEM_RCU'.\n\nThe return type for these two re-purposed helpera is\n'PTR_TO_MEM | MEM_RCU | MEM_ALLOC'.\nThe MEM_ALLOC allows that the per-cpu data can be read and written.\n\nSince the memory allocator bpf_mem_alloc() returns\na ptr to a percpu ptr for percpu data, the first argument\nof bpf_this_cpu_ptr() and bpf_per_cpu_ptr() is patched\nwith a dereference before passing to the helper func.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152749.1997202-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:17 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "96fc99d3d56ff094db7fc5d211183bb3d5c2caaa",
          "subject": "selftests/bpf: Update error message in negative linked_list test",
          "message": "Some error messages are changed due to the addition of\npercpu kptr support. Fix linked_list test with changed\nerror messages.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152754.1997769-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:17 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/linked_list.c"
          ]
        },
        {
          "hash": "ed5285a1482f81f031183286e98edfe76fd9ac3b",
          "subject": "libbpf: Add __percpu_kptr macro definition",
          "message": "Add __percpu_kptr macro definition in bpf_helpers.h.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152800.1998492-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:18 -0700",
          "modified_files": [
            "tools/lib/bpf/bpf_helpers.h"
          ]
        },
        {
          "hash": "968c76cb3dc6cc86e8099ecaa5c30dc0d4738a30",
          "subject": "selftests/bpf: Add bpf_percpu_obj_{new,drop}() macro in bpf_experimental.h",
          "message": "The new macro bpf_percpu_obj_{new/drop}() is very similar to bpf_obj_{new,drop}()\nas they both take a type as the argument.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152805.1999417-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:18 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "6adf82a4398d774398b4538dad561958c2c9521e",
          "subject": "selftests/bpf: Add tests for array map with local percpu kptr",
          "message": "Add non-sleepable and sleepable tests with percpu kptr. For\nnon-sleepable test, four programs are executed in the order of:\n  1. allocate percpu data.\n  2. assign values to percpu data.\n  3. retrieve percpu data.\n  4. de-allocate percpu data.\n\nThe sleepable prog tried to exercise all above 4 steps in a\nsingle prog. Also for sleepable prog, rcu_read_lock is needed\nto protect direct percpu ptr access (from map value) and\nfollowing bpf_this_cpu_ptr() and bpf_per_cpu_ptr() helpers.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152811.2000125-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:18 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/percpu_alloc.c",
            "tools/testing/selftests/bpf/progs/percpu_alloc_array.c"
          ]
        },
        {
          "hash": "5b221ecb3a9e48013d7b4ad7960af3adba23d1d1",
          "subject": "bpf: Mark OBJ_RELEASE argument as MEM_RCU when possible",
          "message": "In previous selftests/bpf patch, we have\n  p = bpf_percpu_obj_new(struct val_t);\n  if (!p)\n          goto out;\n\n  p1 = bpf_kptr_xchg(&e->pc, p);\n  if (p1) {\n          /* race condition */\n          bpf_percpu_obj_drop(p1);\n  }\n\n  p = e->pc;\n  if (!p)\n          goto out;\n\nAfter bpf_kptr_xchg(), we need to re-read e->pc into 'p'.\nThis is due to that the second argument of bpf_kptr_xchg() is marked\nOBJ_RELEASE and it will be marked as invalid after the call.\nSo after bpf_kptr_xchg(), 'p' is an unknown scalar,\nand the bpf program needs to reread from the map value.\n\nThis patch checks if the 'p' has type MEM_ALLOC and MEM_PERCPU,\nand if 'p' is RCU protected. If this is the case, 'p' can be marked\nas MEM_RCU. MEM_ALLOC needs to be removed since 'p' is not\nan owning reference any more. Such a change makes re-read\nfrom the map value unnecessary.\n\nNote that re-reading 'e->pc' after bpf_kptr_xchg() might get\na different value from 'p' if immediately before 'p = e->pc',\nanother cpu may do another bpf_kptr_xchg() and swap in another value\ninto 'e->pc'. If this is the case, then 'p = e->pc' may\nget either 'p' or another value, and race condition already exists.\nSo removing direct re-reading seems fine too.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152816.2000760-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:18 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "46200d6da544a624ad4a6f5745defed7e318f73d",
          "subject": "selftests/bpf: Remove unnecessary direct read of local percpu kptr",
          "message": "For the second argument of bpf_kptr_xchg(), if the reg type contains\nMEM_ALLOC and MEM_PERCPU, which means a percpu allocation,\nafter bpf_kptr_xchg(), the argument is marked as MEM_RCU and MEM_PERCPU\nif in rcu critical section. This way, re-reading from the map value\nis not needed. Remove it from the percpu_alloc_array.c selftest.\n\nWithout previous kernel change, the test will fail like below:\n\n  0: R1=ctx(off=0,imm=0) R10=fp0\n  ; int BPF_PROG(test_array_map_10, int a)\n  0: (b4) w1 = 0                        ; R1_w=0\n  ; int i, index = 0;\n  1: (63) *(u32 *)(r10 -4) = r1         ; R1_w=0 R10=fp0 fp-8=0000????\n  2: (bf) r2 = r10                      ; R2_w=fp0 R10=fp0\n  ;\n  3: (07) r2 += -4                      ; R2_w=fp-4\n  ; e = bpf_map_lookup_elem(&array, &index);\n  4: (18) r1 = 0xffff88810e771800       ; R1_w=map_ptr(off=0,ks=4,vs=16,imm=0)\n  6: (85) call bpf_map_lookup_elem#1    ; R0_w=map_value_or_null(id=1,off=0,ks=4,vs=16,imm=0)\n  7: (bf) r6 = r0                       ; R0_w=map_value_or_null(id=1,off=0,ks=4,vs=16,imm=0) R6_w=map_value_or_null(id=1,off=0,ks=4,vs=16,imm=0)\n  ; if (!e)\n  8: (15) if r6 == 0x0 goto pc+81       ; R6_w=map_value(off=0,ks=4,vs=16,imm=0)\n  ; bpf_rcu_read_lock();\n  9: (85) call bpf_rcu_read_lock#87892          ;\n  ; p = e->pc;\n  10: (bf) r7 = r6                      ; R6=map_value(off=0,ks=4,vs=16,imm=0) R7_w=map_value(off=0,ks=4,vs=16,imm=0)\n  11: (07) r7 += 8                      ; R7_w=map_value(off=8,ks=4,vs=16,imm=0)\n  12: (79) r6 = *(u64 *)(r6 +8)         ; R6_w=percpu_rcu_ptr_or_null_val_t(id=2,off=0,imm=0)\n  ; if (!p) {\n  13: (55) if r6 != 0x0 goto pc+13      ; R6_w=0\n  ; p = bpf_percpu_obj_new(struct val_t);\n  14: (18) r1 = 0x12                    ; R1_w=18\n  16: (b7) r2 = 0                       ; R2_w=0\n  17: (85) call bpf_percpu_obj_new_impl#87883   ; R0_w=percpu_ptr_or_null_val_t(id=4,ref_obj_id=4,off=0,imm=0) refs=4\n  18: (bf) r6 = r0                      ; R0=percpu_ptr_or_null_val_t(id=4,ref_obj_id=4,off=0,imm=0) R6=percpu_ptr_or_null_val_t(id=4,ref_obj_id=4,off=0,imm=0) refs=4\n  ; if (!p)\n  19: (15) if r6 == 0x0 goto pc+69      ; R6=percpu_ptr_val_t(ref_obj_id=4,off=0,imm=0) refs=4\n  ; p1 = bpf_kptr_xchg(&e->pc, p);\n  20: (bf) r1 = r7                      ; R1_w=map_value(off=8,ks=4,vs=16,imm=0) R7=map_value(off=8,ks=4,vs=16,imm=0) refs=4\n  21: (bf) r2 = r6                      ; R2_w=percpu_ptr_val_t(ref_obj_id=4,off=0,imm=0) R6=percpu_ptr_val_t(ref_obj_id=4,off=0,imm=0) refs=4\n  22: (85) call bpf_kptr_xchg#194       ; R0_w=percpu_ptr_or_null_val_t(id=6,ref_obj_id=6,off=0,imm=0) refs=6\n  ; if (p1) {\n  23: (15) if r0 == 0x0 goto pc+3       ; R0_w=percpu_ptr_val_t(ref_obj_id=6,off=0,imm=0) refs=6\n  ; bpf_percpu_obj_drop(p1);\n  24: (bf) r1 = r0                      ; R0_w=percpu_ptr_val_t(ref_obj_id=6,off=0,imm=0) R1_w=percpu_ptr_val_t(ref_obj_id=6,off=0,imm=0) refs=6\n  25: (b7) r2 = 0                       ; R2_w=0 refs=6\n  26: (85) call bpf_percpu_obj_drop_impl#87882          ;\n  ; v = bpf_this_cpu_ptr(p);\n  27: (bf) r1 = r6                      ; R1_w=scalar(id=7) R6=scalar(id=7)\n  28: (85) call bpf_this_cpu_ptr#154\n  R1 type=scalar expected=percpu_ptr_, percpu_rcu_ptr_, percpu_trusted_ptr_\n\nThe R1 which gets its value from R6 is a scalar. But before insn 22, R6 is\n  R6=percpu_ptr_val_t(ref_obj_id=4,off=0,imm=0)\nIts type is changed to a scalar at insn 22 without previous patch.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152821.2001129-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:18 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/percpu_alloc_array.c"
          ]
        },
        {
          "hash": "dfae1eeee9baa12e27f24a223d699326133e366b",
          "subject": "selftests/bpf: Add tests for cgrp_local_storage with local percpu kptr",
          "message": "Add a non-sleepable cgrp_local_storage test with percpu kptr. The\ntest does allocation of percpu data, assigning values to percpu\ndata and retrieval of percpu data. The de-allocation of percpu\ndata is done when the map is freed.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152827.2001784-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:18 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/percpu_alloc.c",
            "tools/testing/selftests/bpf/progs/percpu_alloc_cgrp_local_storage.c"
          ]
        },
        {
          "hash": "1bd7931728718bc463c43b78ab74954452e099e3",
          "subject": "selftests/bpf: Add some negative tests",
          "message": "Add a few negative tests for common mistakes with using percpu kptr\nincluding:\n  - store to percpu kptr.\n  - type mistach in bpf_kptr_xchg arguments.\n  - sleepable prog with untrusted arg for bpf_this_cpu_ptr().\n  - bpf_percpu_obj_new && bpf_obj_drop, and bpf_obj_new && bpf_percpu_obj_drop\n  - struct with ptr for bpf_percpu_obj_new\n  - struct with special field (e.g., bpf_spin_lock) for bpf_percpu_obj_new\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152832.2002421-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:18 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/percpu_alloc.c",
            "tools/testing/selftests/bpf/progs/percpu_alloc_fail.c"
          ]
        },
        {
          "hash": "9bc95a95abbe91e9315c1fe27dc124019bd2592c",
          "subject": "bpf: Mark BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE deprecated",
          "message": "Now 'BPF_MAP_TYPE_CGRP_STORAGE + local percpu ptr'\ncan cover all BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE functionality\nand more. So mark BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE deprecated.\nAlso make changes in selftests/bpf/test_bpftool_synctypes.py\nand selftest libbpf_str to fix otherwise test errors.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152837.2003563-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-09-08 08:42:18 -0700",
          "modified_files": [
            "include/uapi/linux/bpf.h",
            "tools/include/uapi/linux/bpf.h",
            "tools/testing/selftests/bpf/prog_tests/libbpf_str.c",
            "tools/testing/selftests/bpf/test_bpftool_synctypes.py"
          ]
        }
      ]
    },
    {
      "merge_hash": "ec0ded2e02822ee6a7acb655d186af91854112cb",
      "merge_subject": "Merge branch 'bpf-refcount-followups-3-bpf_mem_free_rcu-refcounted-nodes'",
      "merge_body": "Dave Marchevsky says:\n\n====================\nBPF Refcount followups 3: bpf_mem_free_rcu refcounted nodes\n\nThis series is the third of three (or more) followups to address issues\nin the bpf_refcount shared ownership implementation discovered by Kumar.\nThis series addresses the use-after-free scenario described in [0]. The\nfirst followup series ([1]) also attempted to address the same\nuse-after-free, but only got rid of the splat without addressing the\nunderlying issue. After this series the underyling issue is fixed and\nbpf_refcount_acquire can be re-enabled.\n\nThe main fix here is migration of bpf_obj_drop to use\nbpf_mem_free_rcu. To understand why this fixes the issue, let us consider\nthe example interleaving provided by Kumar in [0]:\n\nCPU 0                                   CPU 1\nn = bpf_obj_new\nlock(lock1)\nbpf_rbtree_add(rbtree1, n)\nm = bpf_rbtree_acquire(n)\nunlock(lock1)\n\nkptr_xchg(map, m) // move to map\n// at this point, refcount = 2\n\t\t\t\t\tm = kptr_xchg(map, NULL)\n\t\t\t\t\tlock(lock2)\nlock(lock1)\t\t\t\tbpf_rbtree_add(rbtree2, m)\np = bpf_rbtree_first(rbtree1)\t\t\tif (!RB_EMPTY_NODE) bpf_obj_drop_impl(m) // A\nbpf_rbtree_remove(rbtree1, p)\nunlock(lock1)\nbpf_obj_drop(p) // B\n\t\t\t\t\tbpf_refcount_acquire(m) // use-after-free\n\t\t\t\t\t...\n\nBefore this series, bpf_obj_drop returns memory to the allocator using\nbpf_mem_free. At this point (B in the example) there might be some\nnon-owning references to that memory which the verifier believes are valid,\nbut where the underlying memory was reused for some other allocation.\nCommit 7793fc3babe9 (\"bpf: Make bpf_refcount_acquire fallible for\nnon-owning refs\") attempted to fix this by doing refcount_inc_non_zero\non refcount_acquire in instead of refcount_inc under the assumption that\npreventing erroneous incr-on-0 would be sufficient. This isn't true,\nthough: refcount_inc_non_zero must *check* if the refcount is zero, and\nthe memory it's checking could have been reused, so the check may look\nat and incr random reused bytes.\n\nIf we wait to reuse this memory until all non-owning refs that could\npoint to it are gone, there is no possibility of this scenario\nhappening. Migrating bpf_obj_drop to use bpf_mem_free_rcu for refcounted\nnodes accomplishes this.\n\nFor such nodes, the validity of their underlying memory is now tied to\nRCU critical section. This matches MEM_RCU trustedness\nexpectations, so the series takes the opportunity to more explicitly\nmark this trustedness state.\n\nThe functional effects of trustedness changes here are rather small.\nThis is largely due to local kptrs having separate verifier handling -\nwith implicit trustedness assumptions - than arbitrary kptrs.\nRegardless, let's take the opportunity to move towards a world where\ntrustedness is more explicitly handled.\n\nChangelog:\n\nv1 -> v2: https://lore.kernel.org/bpf/20230801203630.3581291-1-davemarchevsky@fb.com/\n\nPatch 1 (\"bpf: Ensure kptr_struct_meta is non-NULL for collection insert and refcount_acquire\")\n  * Spent some time experimenting with a better approach as per convo w/\n    Yonghong on v1's patch. It started getting too complex, so left unchanged\n    for now. Yonghong was fine with this approach being shipped.\n\nPatch 2 (\"bpf: Consider non-owning refs trusted\")\n  * Add Yonghong ack\nPatch 3 (\"bpf: Use bpf_mem_free_rcu when bpf_obj_dropping refcounted nodes\")\n  * Add Yonghong ack\nPatch 4 (\"bpf: Reenable bpf_refcount_acquire\")\n  * Add Yonghong ack\n\nPatch 5 (\"bpf: Consider non-owning refs to refcounted nodes RCU protected\")\n  * Undo a nonfunctional whitespace change that shouldn't have been included\n    (Yonghong)\n  * Better logging message when complaining about rcu_read_{lock,unlock} in\n    rbtree cb (Alexei)\n  * Don't invalidate_non_owning_refs when processing bpf_rcu_read_unlock\n    (Yonghong, Alexei)\n\nPatch 6 (\"[RFC] bpf: Allow bpf_spin_{lock,unlock} in sleepable prog's RCU CS\")\n  * preempt_{disable,enable} in __bpf_spin_{lock,unlock} (Alexei)\n    * Due to this we can consider spin_lock CS an RCU-sched read-side CS (per\n      RCU/Design/Requirements/Requirements.rst). Modify in_rcu_cs accordingly.\n  * no need to check for !in_rcu_cs before allowing bpf_spin_{lock,unlock}\n    (Alexei)\n  * RFC tag removed and renamed to \"bpf: Allow bpf_spin_{lock,unlock} in\n    sleepable progs\"\n\nPatch 7 (\"selftests/bpf: Add tests for rbtree API interaction in sleepable progs\")\n  * Remove \"no explicit bpf_rcu_read_lock\" failure test, add similar success\n    test (Alexei)\n\nSummary of patch contents, with sub-bullets being leading questions and\ncomments I think are worth reviewer attention:\n\n  * Patches 1 and 2 are moreso documententation - and\n    enforcement, in patch 1's case - of existing semantics / expectations\n\n  * Patch 3 changes bpf_obj_drop behavior for refcounted nodes such that\n    their underlying memory is not reused until RCU grace period elapses\n    * Perhaps it makes sense to move to mem_free_rcu for _all_\n      non-owning refs in the future, not just refcounted. This might\n      allow custom non-owning ref lifetime + invalidation logic to be\n      entirely subsumed by MEM_RCU handling. IMO this needs a bit more\n      thought and should be tackled outside of a fix series, so it's not\n      attempted here.\n\n  * Patch 4 re-enables bpf_refcount_acquire as changes in patch 3 fix\n    the remaining use-after-free\n    * One might expect this patch to be last in the series, or last\n      before selftest changes. Patches 5 and 6 don't change\n      verification or runtime behavior for existing BPF progs, though.\n\n  * Patch 5 brings the verifier's understanding of refcounted node\n    trustedness in line with Patch 4's changes\n\n  * Patch 6 allows some bpf_spin_{lock, unlock} calls in sleepable\n    progs. Marked RFC for a few reasons:\n    * bpf_spin_{lock,unlock} haven't been usable in sleepable progs\n      since before the introduction of bpf linked list and rbtree. As\n      such this feels more like a new feature that may not belong in\n      this fixes series.\n\n  * Patch 7 adds tests\n\n  [0]: https://lore.kernel.org/bpf/atfviesiidev4hu53hzravmtlau3wdodm2vqs7rd7tnwft34e3@xktodqeqevir/\n  [1]: https://lore.kernel.org/bpf/20230602022647.1571784-1-davemarchevsky@fb.com/\n====================\n\nLink: https://lore.kernel.org/r/20230821193311.3290257-1-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-08-25 09:23:23 -0700",
      "commits": [
        {
          "hash": "f0d991a070750ada4f4397304b580ed6f68d3187",
          "subject": "bpf: Ensure kptr_struct_meta is non-NULL for collection insert and refcount_acquire",
          "message": "It's straightforward to prove that kptr_struct_meta must be non-NULL for\nany valid call to these kfuncs:\n\n  * btf_parse_struct_metas in btf.c creates a btf_struct_meta for any\n    struct in user BTF with a special field (e.g. bpf_refcount,\n    {rb,list}_node). These are stored in that BTF's struct_meta_tab.\n\n  * __process_kf_arg_ptr_to_graph_node in verifier.c ensures that nodes\n    have {rb,list}_node field and that it's at the correct offset.\n    Similarly, check_kfunc_args ensures bpf_refcount field existence for\n    node param to bpf_refcount_acquire.\n\n  * So a btf_struct_meta must have been created for the struct type of\n    node param to these kfuncs\n\n  * That BTF and its struct_meta_tab are guaranteed to still be around.\n    Any arbitrary {rb,list} node the BPF program interacts with either:\n    came from bpf_obj_new or a collection removal kfunc in the same\n    program, in which case the BTF is associated with the program and\n    still around; or came from bpf_kptr_xchg, in which case the BTF was\n    associated with the map and is still around\n\nInstead of silently continuing with NULL struct_meta, which caused\nconfusing bugs such as those addressed by commit 2140a6e3422d (\"bpf: Set\nkptr_struct_meta for node param to list and rbtree insert funcs\"), let's\nerror out. Then, at runtime, we can confidently say that the\nimplementations of these kfuncs were given a non-NULL kptr_struct_meta,\nmeaning that special-field-specific functionality like\nbpf_obj_free_fields and the bpf_obj_drop change introduced later in this\nseries are guaranteed to execute.\n\nThis patch doesn't change functionality, just makes it easier to reason\nabout existing functionality.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230821193311.3290257-2-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-08-25 09:23:16 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "2a6d50b50d6d589d43a90d6ca990b8b811e67701",
          "subject": "bpf: Consider non-owning refs trusted",
          "message": "Recent discussions around default kptr \"trustedness\" led to changes such\nas commit 6fcd486b3a0a (\"bpf: Refactor RCU enforcement in the\nverifier.\"). One of the conclusions of those discussions, as expressed\nin code and comments in that patch, is that we'd like to move away from\n'raw' PTR_TO_BTF_ID without some type flag or other register state\nindicating trustedness. Although PTR_TRUSTED and PTR_UNTRUSTED flags mark\nthis state explicitly, the verifier currently considers trustedness\nimplied by other register state. For example, owning refs to graph\ncollection nodes must have a nonzero ref_obj_id, so they pass the\nis_trusted_reg check despite having no explicit PTR_{UN}TRUSTED flag.\nThis patch makes trustedness of non-owning refs to graph collection\nnodes explicit as well.\n\nBy definition, non-owning refs are currently trusted. Although the ref\nhas no control over pointee lifetime, due to non-owning ref clobbering\nrules (see invalidate_non_owning_refs) dereferencing a non-owning ref is\nsafe in the critical section controlled by bpf_spin_lock associated with\nits owning collection.\n\nNote that the previous statement does not hold true for nodes with shared\nownership due to the use-after-free issue that this series is\naddressing. True shared ownership was disabled by commit 7deca5eae833\n(\"bpf: Disable bpf_refcount_acquire kfunc calls until race conditions are fixed\"),\nthough, so the statement holds for now. Further patches in the series will change\nthe trustedness state of non-owning refs before re-enabling\nbpf_refcount_acquire.\n\nLet's add NON_OWN_REF type flag to BPF_REG_TRUSTED_MODIFIERS such that a\nnon-owning ref reg state would pass is_trusted_reg check. Somewhat\nsurprisingly, this doesn't result in any change to user-visible\nfunctionality elsewhere in the verifier: graph collection nodes are all\nmarked MEM_ALLOC, which tends to be handled in separate codepaths from\n\"raw\" PTR_TO_BTF_ID. Regardless, let's be explicit here and document the\ncurrent state of things before changing it elsewhere in the series.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230821193311.3290257-3-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-08-25 09:23:16 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h"
          ]
        },
        {
          "hash": "7e26cd12ad1c8f3e55d32542c7e4708a9e6a3c02",
          "subject": "bpf: Use bpf_mem_free_rcu when bpf_obj_dropping refcounted nodes",
          "message": "This is the final fix for the use-after-free scenario described in\ncommit 7793fc3babe9 (\"bpf: Make bpf_refcount_acquire fallible for\nnon-owning refs\"). That commit, by virtue of changing\nbpf_refcount_acquire's refcount_inc to a refcount_inc_not_zero, fixed\nthe \"refcount incr on 0\" splat. The not_zero check in\nrefcount_inc_not_zero, though, still occurs on memory that could have\nbeen free'd and reused, so the commit didn't properly fix the root\ncause.\n\nThis patch actually fixes the issue by free'ing using the recently-added\nbpf_mem_free_rcu, which ensures that the memory is not reused until\nRCU grace period has elapsed. If that has happened then\nthere are no non-owning references alive that point to the\nrecently-free'd memory, so it can be safely reused.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230821193311.3290257-4-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-08-25 09:23:16 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "ba2464c86f182c6fdb69fe2f77a3d04c19a72357",
          "subject": "bpf: Reenable bpf_refcount_acquire",
          "message": "Now that all reported issues are fixed, bpf_refcount_acquire can be\nturned back on. Also reenable all bpf_refcount-related tests which were\ndisabled.\n\nThis a revert of:\n * commit f3514a5d6740 (\"selftests/bpf: Disable newly-added 'owner' field test until refcount re-enabled\")\n * commit 7deca5eae833 (\"bpf: Disable bpf_refcount_acquire kfunc calls until race conditions are fixed\")\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230821193311.3290257-5-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-08-25 09:23:16 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/prog_tests/refcounted_kptr.c"
          ]
        },
        {
          "hash": "0816b8c6bf7fc87cec4273dc199e8f0764b9e7b1",
          "subject": "bpf: Consider non-owning refs to refcounted nodes RCU protected",
          "message": "An earlier patch in the series ensures that the underlying memory of\nnodes with bpf_refcount - which can have multiple owners - is not reused\nuntil RCU grace period has elapsed. This prevents\nuse-after-free with non-owning references that may point to\nrecently-freed memory. While RCU read lock is held, it's safe to\ndereference such a non-owning ref, as by definition RCU GP couldn't have\nelapsed and therefore underlying memory couldn't have been reused.\n\nFrom the perspective of verifier \"trustedness\" non-owning refs to\nrefcounted nodes are now trusted only in RCU CS and therefore should no\nlonger pass is_trusted_reg, but rather is_rcu_reg. Let's mark them\nMEM_RCU in order to reflect this new state.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230821193311.3290257-6-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-08-25 09:23:16 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "5861d1e8dbc4e1a03ebffb96ac041026cdd34c07",
          "subject": "bpf: Allow bpf_spin_{lock,unlock} in sleepable progs",
          "message": "Commit 9e7a4d9831e8 (\"bpf: Allow LSM programs to use bpf spin locks\")\ndisabled bpf_spin_lock usage in sleepable progs, stating:\n\n Sleepable LSM programs can be preempted which means that allowng spin\n locks will need more work (disabling preemption and the verifier\n ensuring that no sleepable helpers are called when a spin lock is\n held).\n\nThis patch disables preemption before grabbing bpf_spin_lock. The second\nrequirement above \"no sleepable helpers are called when a spin lock is\nheld\" is implicitly enforced by current verifier logic due to helper\ncalls in spin_lock CS being disabled except for a few exceptions, none\nof which sleep.\n\nDue to above preemption changes, bpf_spin_lock CS can also be considered\na RCU CS, so verifier's in_rcu_cs check is modified to account for this.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230821193311.3290257-7-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-08-25 09:23:17 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "312aa5bde8985dd2aef99d3e20abc0889c6f2a3e",
          "subject": "selftests/bpf: Add tests for rbtree API interaction in sleepable progs",
          "message": "Confirm that the following sleepable prog states fail verification:\n  * bpf_rcu_read_unlock before bpf_spin_unlock\n     * RCU CS will last at least as long as spin_lock CS\n\nAlso confirm that correct usage passes verification, specifically:\n  * Explicit use of bpf_rcu_read_{lock, unlock} in sleepable test prog\n  * Implied RCU CS due to spin_lock CS\n\nNone of the selftest progs actually attach to bpf_testmod's\nbpf_testmod_test_read.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230821193311.3290257-8-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-08-25 09:23:17 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/refcounted_kptr.c",
            "tools/testing/selftests/bpf/progs/refcounted_kptr_fail.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "f586a77030b38f1b7258aaea44d0ab52b1963859",
      "merge_subject": "Merge branch 'bpf-fix-an-issue-in-verifing-allow_ptr_leaks'",
      "merge_body": "Yafang Shao says:\n\n====================\nbpf: Fix an issue in verifing allow_ptr_leaks\n\nPatch #1: An issue found in our local 6.1 kernel.\n          This issue also exists in bpf-next.\nPatch #2: Selftess for #1\n\nv1->v2:\n  - Add acked-by from Eduard\n  - Fix build error reported by Alexei\n====================\n\nLink: https://lore.kernel.org/r/20230823020703.3790-1-laoar.shao@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-08-23 09:37:29 -0700",
      "commits": [
        {
          "hash": "d75e30dddf73449bc2d10bb8e2f1a2c446bc67a2",
          "subject": "bpf: Fix issue in verifying allow_ptr_leaks",
          "message": "After we converted the capabilities of our networking-bpf program from\ncap_sys_admin to cap_net_admin+cap_bpf, our networking-bpf program\nfailed to start. Because it failed the bpf verifier, and the error log\nis \"R3 pointer comparison prohibited\".\n\nA simple reproducer as follows,\n\nSEC(\"cls-ingress\")\nint ingress(struct __sk_buff *skb)\n{\n\tstruct iphdr *iph = (void *)(long)skb->data + sizeof(struct ethhdr);\n\n\tif ((long)(iph + 1) > (long)skb->data_end)\n\t\treturn TC_ACT_STOLEN;\n\treturn TC_ACT_OK;\n}\n\nPer discussion with Yonghong and Alexei [1], comparison of two packet\npointers is not a pointer leak. This patch fixes it.\n\nOur local kernel is 6.1.y and we expect this fix to be backported to\n6.1.y, so stable is CCed.\n\n[1]. https://lore.kernel.org/bpf/CAADnVQ+Nmspr7Si+pxWn8zkE7hX-7s93ugwC+94aXSy4uQ9vBg@mail.gmail.com/\n\nSuggested-by: Yonghong Song <yonghong.song@linux.dev>\nSuggested-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nSigned-off-by: Yafang Shao <laoar.shao@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/r/20230823020703.3790-2-laoar.shao@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yafang Shao <laoar.shao@gmail.com>",
          "date": "2023-08-23 09:37:29 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "0072e3624b463636c842ad8e261f1dc91deb8c78",
          "subject": "selftests/bpf: Add selftest for allow_ptr_leaks",
          "message": "- Without prev commit\n\n  $ tools/testing/selftests/bpf/test_progs --name=tc_bpf\n  #232/1   tc_bpf/tc_bpf_root:OK\n  test_tc_bpf_non_root:PASS:set_cap_bpf_cap_net_admin 0 nsec\n  test_tc_bpf_non_root:PASS:disable_cap_sys_admin 0 nsec\n  0: R1=ctx(off=0,imm=0) R10=fp0\n  ; if ((long)(iph + 1) > (long)skb->data_end)\n  0: (61) r2 = *(u32 *)(r1 +80)         ; R1=ctx(off=0,imm=0) R2_w=pkt_end(off=0,imm=0)\n  ; struct iphdr *iph = (void *)(long)skb->data + sizeof(struct ethhdr);\n  1: (61) r1 = *(u32 *)(r1 +76)         ; R1_w=pkt(off=0,r=0,imm=0)\n  ; if ((long)(iph + 1) > (long)skb->data_end)\n  2: (07) r1 += 34                      ; R1_w=pkt(off=34,r=0,imm=0)\n  3: (b4) w0 = 1                        ; R0_w=1\n  4: (2d) if r1 > r2 goto pc+1\n  R2 pointer comparison prohibited\n  processed 5 insns (limit 1000000) max_states_per_insn 0 total_states 0 peak_states 0 mark_read 0\n  test_tc_bpf_non_root:FAIL:test_tc_bpf__open_and_load unexpected error: -13\n  #233/2   tc_bpf_non_root:FAIL\n\n- With prev commit\n\n  $ tools/testing/selftests/bpf/test_progs --name=tc_bpf\n  #232/1   tc_bpf/tc_bpf_root:OK\n  #232/2   tc_bpf/tc_bpf_non_root:OK\n  #232     tc_bpf:OK\n  Summary: 1/2 PASSED, 0 SKIPPED, 0 FAILED\n\nSigned-off-by: Yafang Shao <laoar.shao@gmail.com>\nLink: https://lore.kernel.org/r/20230823020703.3790-3-laoar.shao@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yafang Shao <laoar.shao@gmail.com>",
          "date": "2023-08-23 09:37:29 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/tc_bpf.c",
            "tools/testing/selftests/bpf/progs/test_tc_bpf.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "87680ac7979177a34ca39b5a35a2ed94209cd20f",
      "merge_subject": "Merge branch 'fix-for-check_func_arg_reg_off'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nFix for check_func_arg_reg_off\n\nRemove a leftover hunk in check_func_arg_reg_off that incorrectly\nbypasses reg->off == 0 requirement for release kfuncs and helpers.\n====================\n\nLink: https://lore.kernel.org/r/20230822175140.1317749-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-08-22 12:52:48 -0700",
      "commits": [
        {
          "hash": "6785b2edf48c6b1c3ea61fe3b0d2e02b8fbf90c0",
          "subject": "bpf: Fix check_func_arg_reg_off bug for graph root/node",
          "message": "The commit being fixed introduced a hunk into check_func_arg_reg_off\nthat bypasses reg->off == 0 enforcement when offset points to a graph\nnode or root. This might possibly be done for treating bpf_rbtree_remove\nand others as KF_RELEASE and then later check correct reg->off in helper\nargument checks.\n\nBut this is not the case, those helpers are already not KF_RELEASE and\npermit non-zero reg->off and verify it later to match the subobject in\nBTF type.\n\nHowever, this logic leads to bpf_obj_drop permitting free of register\narguments with non-zero offset when they point to a graph root or node\nwithin them, which is not ok.\n\nFor instance:\n\nstruct foo {\n\tint i;\n\tint j;\n\tstruct bpf_rb_node node;\n};\n\nstruct foo *f = bpf_obj_new(typeof(*f));\nif (!f) ...\nbpf_obj_drop(f); // OK\nbpf_obj_drop(&f->i); // still ok from verifier PoV\nbpf_obj_drop(&f->node); // Not OK, but permitted right now\n\nFix this by dropping the whole part of code altogether.\n\nFixes: 6a3cd3318ff6 (\"bpf: Migrate release_on_unlock logic to non-owning ref semantics\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230822175140.1317749-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-08-22 12:52:48 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "fbc5bc4c8e6ca6f5720798c96107307906dc49c0",
          "subject": "selftests/bpf: Add test for bpf_obj_drop with bad reg->off",
          "message": "Add a selftest for the fix provided in the previous commit. Without the\nfix, the selftest passes the verifier while it should fail. The special\nlogic for detecting graph root or node for reg->off and bypassing\nreg->off == 0 guarantee for release helpers/kfuncs has been dropped.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230822175140.1317749-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-08-22 12:52:48 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/local_kptr_stash_fail.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "648880e9331c68b2008430fd90f3648d1795399d",
      "merge_subject": "Merge branch 'net: struct netdev_rx_queue and xdp.h reshuffling'",
      "merge_body": "Jakub Kicinski says:\n\n====================\nWhile poking at struct netdev_rx_queue I got annoyed by\nthe huge rebuild times. I split it out from netdevice.h\nand then realized that it was the main reason we included\nxdp.h in there. So I removed that dependency as well.\n\nThis gives us very pleasant build times for both xdp.h\nand struct netdev_rx_queue changes.\n\nI'm sending this for bpf-next because I think it'd be easiest\nif it goes in there, and then bpf-next gets flushed soon after?\nI can also make a branch on merge-base for net-next and bpf-next..\n\nv2:\n - build fix\n - reorder some includes\nv1: https://lore.kernel.org/all/20230802003246.2153774-1-kuba@kernel.org/\n====================\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_date": "2023-08-03 08:38:53 -0700",
      "commits": [
        {
          "hash": "92272ec4107ef4f826b694a1338562c007e09821",
          "subject": "eth: add missing xdp.h includes in drivers",
          "message": "Handful of drivers currently expect to get xdp.h by virtue\nof including netdevice.h. This will soon no longer be the case\nso add explicit includes.\n\nReviewed-by: Wei Fang <wei.fang@nxp.com>\nReviewed-by: Gerhard Engleder <gerhard@engleder-embedded.com>\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\nAcked-by: Jesper Dangaard Brouer <hawk@kernel.org>\nLink: https://lore.kernel.org/r/20230803010230.1755386-2-kuba@kernel.org\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Jakub Kicinski <kuba@kernel.org>",
          "date": "2023-08-03 08:38:07 -0700",
          "modified_files": [
            "drivers/net/bonding/bond_main.c",
            "drivers/net/ethernet/amazon/ena/ena_netdev.h",
            "drivers/net/ethernet/engleder/tsnep.h",
            "drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.h",
            "drivers/net/ethernet/freescale/enetc/enetc.h",
            "drivers/net/ethernet/freescale/fec.h",
            "drivers/net/ethernet/fungible/funeth/funeth_txrx.h",
            "drivers/net/ethernet/google/gve/gve.h",
            "drivers/net/ethernet/intel/igc/igc.h",
            "drivers/net/ethernet/microchip/lan966x/lan966x_main.h",
            "drivers/net/ethernet/microsoft/mana/mana_en.c",
            "drivers/net/ethernet/stmicro/stmmac/stmmac.h",
            "drivers/net/ethernet/ti/cpsw_priv.h",
            "drivers/net/hyperv/hyperv_net.h",
            "drivers/net/tap.c",
            "include/net/mana/mana.h"
          ]
        },
        {
          "hash": "49e47a5b6145d86c30022fe0e949bbb24bae28ba",
          "subject": "net: move struct netdev_rx_queue out of netdevice.h",
          "message": "struct netdev_rx_queue is touched in only a few places\nand having it defined in netdevice.h brings in the dependency\non xdp.h, because struct xdp_rxq_info gets embedded in\nstruct netdev_rx_queue.\n\nIn prep for removal of xdp.h from netdevice.h move all\nthe netdev_rx_queue stuff to a new header.\n\nWe could technically break the new header up to avoid\nthe sysfs.h include but it's so rarely included it\ndoesn't seem to be worth it at this point.\n\nReviewed-by: Amritha Nambiar <amritha.nambiar@intel.com>\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\nAcked-by: Jesper Dangaard Brouer <hawk@kernel.org>\nLink: https://lore.kernel.org/r/20230803010230.1755386-3-kuba@kernel.org\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Jakub Kicinski <kuba@kernel.org>",
          "date": "2023-08-03 08:38:07 -0700",
          "modified_files": [
            "drivers/net/virtio_net.c",
            "include/linux/netdevice.h",
            "include/net/netdev_rx_queue.h",
            "net/bpf/test_run.c",
            "net/core/dev.c",
            "net/core/net-sysfs.c",
            "net/xdp/xsk.c"
          ]
        },
        {
          "hash": "680ee0456a5712309db9ec2692e908ea1d6b1644",
          "subject": "net: invert the netdevice.h vs xdp.h dependency",
          "message": "xdp.h is far more specific and is included in only 67 other\nfiles vs netdevice.h's 1538 include sites.\nMake xdp.h include netdevice.h, instead of the other way around.\nThis decreases the incremental allmodconfig builds size when\nxdp.h is touched from 5947 to 662 objects.\n\nMove bpf_prog_run_xdp() to xdp.h, seems appropriate and filter.h\nis a mega-header in its own right so it's nice to avoid xdp.h\ngetting included there as well.\n\nThe only unfortunate part is that the typedef for xdp_features_t\nhas to move to netdevice.h, since its embedded in struct netdevice.\n\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\nAcked-by: Jesper Dangaard Brouer <hawk@kernel.org>\nLink: https://lore.kernel.org/r/20230803010230.1755386-4-kuba@kernel.org\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Jakub Kicinski <kuba@kernel.org>",
          "date": "2023-08-03 08:38:07 -0700",
          "modified_files": [
            "include/linux/filter.h",
            "include/linux/netdevice.h",
            "include/net/busy_poll.h",
            "include/net/xdp.h",
            "include/trace/events/xdp.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/offload.c",
            "kernel/bpf/verifier.c",
            "net/netfilter/nf_conntrack_bpf.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "f7e6bd33d1d404608515addcd84cf25ac5289314",
      "merge_subject": "Merge branch 'bpf-support-new-insns-from-cpu-v4'",
      "merge_body": "Yonghong Song says:\n\n====================\nbpf: Support new insns from cpu v4\n\nIn previous discussion ([1]), it is agreed that we should introduce\ncpu version 4 (llvm flag -mcpu=v4) which contains some instructions\nwhich can simplify code, make code easier to understand, fix the\nexisting problem, or simply for feature completeness. More specifically,\nthe following new insns are proposed:\n  . sign extended load\n  . sign extended mov\n  . bswap\n  . signed div/mod\n  . ja with 32-bit offset\n\nThis patch set added kernel support for insns proposed in [1] except\nBPF_ST which already has full kernel support. Beside the above proposed\ninsns, LLVM will generate BPF_ST insn as well under -mcpu=v4.\nThe llvm patch ([2]) has been merged into llvm-project 'main' branch.\n\nThe patchset implements interpreter, jit and verifier support for these new\ninsns.\n\nFor this patch set, I tested cpu v2/v3/v4 and the selftests are all passed.\nI also tested selftests introduced in this patch set with additional changes\nbeside normal jit testing (bpf_jit_enable = 1 and bpf_jit_harden = 0)\n  - bpf_jit_enable = 0\n  - bpf_jit_enable = 1 and bpf_jit_harden = 1\nand both testing passed.\n\n  [1] https://lore.kernel.org/bpf/4bfe98be-5333-1c7e-2f6d-42486c8ec039@meta.com/\n  [2] https://reviews.llvm.org/D144829\n\nChangelogs:\n  v4 -> v5:\n   . for v4, patch 8/17 missed in mailing list and patchwork, so resend.\n   . rebase on top of master\n  v3 -> v4:\n   . some minor asm syntax adjustment based on llvm change.\n   . add clang version and target arch guard for new tests\n     so they can still compile with old llvm compilers.\n   . some changes to the bpf doc.\n  v2 -> v3:\n   . add missed disasm change from v2.\n   . handle signed load of ctx fields properly.\n   . fix some interpreter sdiv/smod error when bpf_jit_enable = 0.\n   . fix some verifier range bounding errors.\n   . add more C tests.\n  RFCv1 -> v2:\n   . add more verifier supports for signed extend load and mov insns.\n   . rename some insn names to be more consistent with intel practice.\n   . add cpuv4 test runner for test progs.\n   . add more unit and C tests.\n   . add documentation.\n====================\n\nLink: https://lore.kernel.org/r/20230728011143.3710005-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-07-27 18:54:17 -0700",
      "commits": [
        {
          "hash": "1f9a1ea821ff25353a0e80d971e7958cd55b47a3",
          "subject": "bpf: Support new sign-extension load insns",
          "message": "Add interpreter/jit support for new sign-extension load insns\nwhich adds a new mode (BPF_MEMSX).\nAlso add verifier support to recognize these insns and to\ndo proper verification with new insns. In verifier, besides\nto deduce proper bounds for the dst_reg, probed memory access\nis also properly handled.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011156.3711870-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:52:33 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "include/linux/filter.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "8100928c881482a73ed8bd499d602bab0fe55608",
          "subject": "bpf: Support new sign-extension mov insns",
          "message": "Add interpreter/jit support for new sign-extension mov insns.\nThe original 'MOV' insn is extended to support reg-to-reg\nsigned version for both ALU and ALU64 operations. For ALU mode,\nthe insn->off value of 8 or 16 indicates sign-extension\nfrom 8- or 16-bit value to 32-bit value. For ALU64 mode,\nthe insn->off value of 8/16/32 indicates sign-extension\nfrom 8-, 16- or 32-bit value to 64-bit value.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011202.3712300-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:52:33 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "1f1e864b65554e33fe74e3377e58b12f4302f2eb",
          "subject": "bpf: Handle sign-extenstin ctx member accesses",
          "message": "Currently, if user accesses a ctx member with signed types,\nthe compiler will generate an unsigned load followed by\nnecessary left and right shifts.\n\nWith the introduction of sign-extension load, compiler may\njust emit a ldsx insn instead. Let us do a final movsx sign\nextension to the final unsigned ctx load result to\nsatisfy original sign extension requirement.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011207.3712528-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:52:33 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "0845c3db7bf5c4ceb7100bcd8fd594d9ccf3c29a",
          "subject": "bpf: Support new unconditional bswap instruction",
          "message": "The existing 'be' and 'le' insns will do conditional bswap\ndepends on host endianness. This patch implements\nunconditional bswap insns.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011213.3712808-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:52:33 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "ec0e2da95f72d4a46050a4d994e4fe471474fd80",
          "subject": "bpf: Support new signed div/mod instructions.",
          "message": "Add interpreter/jit support for new signed div/mod insns.\nThe new signed div/mod instructions are encoded with\nunsigned div/mod instructions plus insn->off == 1.\nAlso add basic verifier support to ensure new insns get\naccepted.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011219.3714605-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:52:33 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "7058e3a31ee4b9240cccab5bc13c1afbfa3d16a0",
          "subject": "bpf: Fix jit blinding with new sdiv/smov insns",
          "message": "Handle new insns properly in bpf_jit_blind_insn() function.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011225.3715812-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:52:33 -0700",
          "modified_files": [
            "include/linux/filter.h",
            "kernel/bpf/core.c"
          ]
        },
        {
          "hash": "4cd58e9af8b9d9fff6b7145e742abbfcda0af4af",
          "subject": "bpf: Support new 32bit offset jmp instruction",
          "message": "Add interpreter/jit/verifier support for 32bit offset jmp instruction.\nIf a conditional jmp instruction needs more than 16bit offset,\nit can be simulated with a conditional jmp + a 32bit jmp insn.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011231.3716103-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:52:33 -0700",
          "modified_files": [
            "arch/x86/net/bpf_jit_comp.c",
            "kernel/bpf/core.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "f835bb6222998c8655bc4e85287d42b57c17b208",
          "subject": "bpf: Add kernel/bpftool asm support for new instructions",
          "message": "Add asm support for new instructions so kernel verifier and bpftool\nxlated insn dumps can have proper asm syntax for new instructions.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Quentin Monnet <quentin@isovalent.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:54:02 -0700",
          "modified_files": [
            "kernel/bpf/disasm.c"
          ]
        },
        {
          "hash": "86180493a2ef572d703ed3a486ab347b194ce4d9",
          "subject": "selftests/bpf: Fix a test_verifier failure",
          "message": "The following test_verifier subtest failed due to\nnew encoding for BSWAP.\n\n  $ ./test_verifier\n  ...\n  #99/u invalid 64-bit BPF_END FAIL\n  Unexpected success to load!\n  verification time 215 usec\n  stack depth 0\n  processed 3 insns (limit 1000000) max_states_per_insn 0 total_states 0 peak_states 0 mark_read 0\n  #99/p invalid 64-bit BPF_END FAIL\n  Unexpected success to load!\n  verification time 198 usec\n  stack depth 0\n  processed 3 insns (limit 1000000) max_states_per_insn 0 total_states 0 peak_states 0 mark_read 0\n\nTighten the test so it still reports a failure.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011244.3717464-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:54:16 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/verifier/basic_instr.c"
          ]
        },
        {
          "hash": "a5d0c26a2784890d803fca5e0dd27c590472b43d",
          "subject": "selftests/bpf: Add a cpuv4 test runner for cpu=v4 testing",
          "message": "Similar to no-alu32 runner, if clang compiler supports -mcpu=v4,\na cpuv4 runner is created to test bpf programs compiled with\n-mcpu=v4.\n\nThe following are some num-of-insn statistics for each newer\ninstructions based on existing selftests, excluding subsequent\ncpuv4 insn specific tests.\n\n   insn pattern                # of instructions\n   reg = (s8)reg               4\n   reg = (s16)reg              4\n   reg = (s32)reg              144\n   reg = *(s8 *)(reg + off)    13\n   reg = *(s16 *)(reg + off)   14\n   reg = *(s32 *)(reg + off)   15215\n   reg = bswap16 reg           142\n   reg = bswap32 reg           38\n   reg = bswap64 reg           14\n   reg s/= reg                 0\n   reg s%= reg                 0\n   gotol <offset>              58\n\nNote that in llvm -mcpu=v4 implementation, the compiler is a little\nbit conservative about generating 'gotol' insn (32-bit branch offset)\nas it didn't precise count the number of insns (e.g., some insns are\ndebug insns, etc.). Compared to old 'goto' insn, newer 'gotol' insn\nshould have comparable verification states to 'goto' insn.\n\nWith current patch set, all selftests passed with -mcpu=v4\nwhen running test_progs-cpuv4 binary. The -mcpu=v3 and -mcpu=v2 run\nare also successful.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011250.3718252-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:54:16 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/.gitignore",
            "tools/testing/selftests/bpf/Makefile"
          ]
        },
        {
          "hash": "147c8f4470eeae14c566984bd81b33334866ce10",
          "subject": "selftests/bpf: Add unit tests for new sign-extension load insns",
          "message": "Add unit tests for new ldsx insns. The test includes sign-extension\nwith a single value or with a value range.\n\nIf cpuv4 is not supported due to\n  (1) older compiler, e.g., less than clang version 18, or\n  (2) test runner test_progs and test_progs-no_alu32 which tests\n      cpu v2 and v3, or\n  (3) non-x86_64 arch not supporting new insns in jit yet,\na dummy program is added with below output:\n  #318/1   verifier_ldsx/cpuv4 is not supported by compiler or jit, use a dummy test:OK\n  #318     verifier_ldsx:OK\nto indicate the test passed with a dummy test instead of actually\ntesting cpuv4. I am using a dummy prog to avoid changing the\nverifier testing infrastructure. Once clang 18 is widely available\nand other architectures support cpuv4, at least for CI run,\nthe dummy program can be removed.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011304.3719139-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:54:16 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_ldsx.c"
          ]
        },
        {
          "hash": "f02ec3ff3f09b191a6fde8dcf805da6a01baea97",
          "subject": "selftests/bpf: Add unit tests for new sign-extension mov insns",
          "message": "Add unit tests for movsx insns.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011309.3719295-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:54:17 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_movsx.c"
          ]
        },
        {
          "hash": "79dbabc175408f7f6a7d156ab8fd68539703459c",
          "subject": "selftests/bpf: Add unit tests for new bswap insns",
          "message": "Add unit tests for bswap insns.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011314.3720109-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:54:17 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_bswap.c"
          ]
        },
        {
          "hash": "de1c26809ec37b632ad7bcec3bba8b849eb44d43",
          "subject": "selftests/bpf: Add unit tests for new sdiv/smod insns",
          "message": "Add unit tests for sdiv/smod insns.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011321.3720500-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:54:17 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_sdiv.c"
          ]
        },
        {
          "hash": "613dad498072f20d9bfd996422f0fd8e9a2f6c0d",
          "subject": "selftests/bpf: Add unit tests for new gotol insn",
          "message": "Add unit tests for gotol insn.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011329.3721881-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:54:17 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_gotol.c"
          ]
        },
        {
          "hash": "0c606571ae07568b18c112d011dc8cd01d6ae346",
          "subject": "selftests/bpf: Test ldsx with more complex cases",
          "message": "The following ldsx cases are tested:\n  - signed readonly map value\n  - read/write map value\n  - probed memory\n  - not-narrowed ctx field access\n  - narrowed ctx field access.\n\nWithout previous proper verifier/git handling, the test will fail.\n\nIf cpuv4 is not supported either by compiler or by jit,\nthe test will be skipped.\n\n  # ./test_progs -t ldsx_insn\n  #113/1   ldsx_insn/map_val and probed_memory:SKIP\n  #113/2   ldsx_insn/ctx_member_sign_ext:SKIP\n  #113/3   ldsx_insn/ctx_member_narrow_sign_ext:SKIP\n  #113     ldsx_insn:SKIP\n  Summary: 1/0 PASSED, 3 SKIPPED, 0 FAILED\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011336.3723434-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:54:17 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/prog_tests/test_ldsx_insn.c",
            "tools/testing/selftests/bpf/progs/test_ldsx_insn.c"
          ]
        },
        {
          "hash": "245d4c40c09bd8d5a71640950eeb074880925b9a",
          "subject": "docs/bpf: Add documentation for new instructions",
          "message": "Add documentation in instruction-set.rst for new instruction encoding\nand their corresponding operations. Also removed the question\nrelated to 'no BPF_SDIV' in bpf_design_QA.rst since we have\nBPF_SDIV insn now.\n\nCc: bpf@ietf.org\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011342.3724411-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yonghong.song@linux.dev>",
          "date": "2023-07-27 18:54:17 -0700",
          "modified_files": [
            "Documentation/bpf/bpf_design_QA.rst",
            "Documentation/bpf/standardization/instruction-set.rst"
          ]
        }
      ]
    },
    {
      "merge_hash": "9df76fe0c5ac7622dfd16c47f98f3d48e890890e",
      "merge_subject": "Merge branch 'allow-bpf_map_sum_elem_count-for-all-program-types'",
      "merge_body": "Anton Protopopov says:\n\n====================\nallow bpf_map_sum_elem_count for all program types\n\nThis series is a follow up to the recent change [1] which added\nper-cpu insert/delete statistics for maps. The bpf_map_sum_elem_count\nkfunc presented in the original series was only available to tracing\nprograms, so let's make it available to all.\n\nThe first patch makes types listed in the reg2btf_ids[] array to be\nconsidered trusted by kfuncs.\n\nThe second patch allows to treat CONST_PTR_TO_MAP as trusted pointers from\nkfunc's point of view by adding it to the reg2btf_ids[] array.\n\nThe third patch adds missing const to the map argument of the\nbpf_map_sum_elem_count kfunc.\n\nThe fourth patch registers the bpf_map_sum_elem_count for all programs,\nand patches selftests correspondingly.\n\n  [1] https://lore.kernel.org/bpf/20230705160139.19967-1-aspsk@isovalent.com/\n\nv1 -> v2:\n  * treat the whole reg2btf_ids array as trusted (Alexei)\n====================\n\nLink: https://lore.kernel.org/r/20230719092952.41202-1-aspsk@isovalent.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-07-19 09:48:53 -0700",
      "commits": [
        {
          "hash": "831deb2976de4458adae4daee56aa6f740ed4acc",
          "subject": "bpf: consider types listed in reg2btf_ids as trusted",
          "message": "The reg2btf_ids array contains a list of types for which we can (and need)\nto find a corresponding static BTF id. All the types in the list can be\nconsidered as trusted for purposes of kfuncs.\n\nSigned-off-by: Anton Protopopov <aspsk@isovalent.com>\nLink: https://lore.kernel.org/r/20230719092952.41202-2-aspsk@isovalent.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Anton Protopopov <aspsk@isovalent.com>",
          "date": "2023-07-19 09:48:52 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "5ba190c29cf92f157bd63c9909c7050d6dc43df7",
          "subject": "bpf: consider CONST_PTR_TO_MAP as trusted pointer to struct bpf_map",
          "message": "Add the BTF id of struct bpf_map to the reg2btf_ids array. This makes the\nvalues of the CONST_PTR_TO_MAP type to be considered as trusted by kfuncs.\nThis, in turn, allows users to execute trusted kfuncs which accept `struct\nbpf_map *` arguments from non-tracing programs.\n\nWhile exporting the btf_bpf_map_id variable, save some bytes by defining\nit as BTF_ID_LIST_GLOBAL_SINGLE (which is u32[1]) and not as BTF_ID_LIST\n(which is u32[64]).\n\nSigned-off-by: Anton Protopopov <aspsk@isovalent.com>\nLink: https://lore.kernel.org/r/20230719092952.41202-3-aspsk@isovalent.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Anton Protopopov <aspsk@isovalent.com>",
          "date": "2023-07-19 09:48:52 -0700",
          "modified_files": [
            "include/linux/btf_ids.h",
            "kernel/bpf/map_iter.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "9c29804961c1bbf5c879a1879fe5fcac6364736f",
          "subject": "bpf: make an argument const in the bpf_map_sum_elem_count kfunc",
          "message": "We use the map pointer only to read the counter values, no locking\ninvolved, so mark the argument as const.\n\nSigned-off-by: Anton Protopopov <aspsk@isovalent.com>\nLink: https://lore.kernel.org/r/20230719092952.41202-4-aspsk@isovalent.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Anton Protopopov <aspsk@isovalent.com>",
          "date": "2023-07-19 09:48:52 -0700",
          "modified_files": [
            "kernel/bpf/map_iter.c"
          ]
        },
        {
          "hash": "72829b1c1f1601015cd7332b968befcf5e636c24",
          "subject": "bpf: allow any program to use the bpf_map_sum_elem_count kfunc",
          "message": "Register the bpf_map_sum_elem_count func for all programs, and update the\nmap_ptr subtest of the test_progs test to test the new functionality.\n\nThe usage is allowed as long as the pointer to the map is trusted (when\nusing tracing programs) or is a const pointer to map, as in the following\nexample:\n\n    struct {\n            __uint(type, BPF_MAP_TYPE_HASH);\n            ...\n    } hash SEC(\".maps\");\n\n    ...\n\n    static inline int some_bpf_prog(void)\n    {\n            struct bpf_map *map = (struct bpf_map *)&hash;\n            __s64 count;\n\n            count = bpf_map_sum_elem_count(map);\n\n            ...\n    }\n\nSigned-off-by: Anton Protopopov <aspsk@isovalent.com>\nLink: https://lore.kernel.org/r/20230719092952.41202-5-aspsk@isovalent.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Anton Protopopov <aspsk@isovalent.com>",
          "date": "2023-07-19 09:48:53 -0700",
          "modified_files": [
            "kernel/bpf/map_iter.c",
            "tools/testing/selftests/bpf/progs/map_ptr_kern.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "a8237cc87e3de1adb3f2f6a8056621e7e578cc00",
      "merge_subject": "Merge branch 'two-more-fixes-for-check_max_stack_depth'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nTwo more fixes for check_max_stack_depth\n\nI noticed two more bugs while reviewing the code, description and\nexamples available in the patches.\n\nOne leads to incorrect subprog index to be stored in the frame stack\nmaintained by the function (leading to incorrect tail_call_reachable\nmarks, among other things).\n\nThe other problem is missing exploration pass of other async callbacks\nwhen they are not called from the main prog. Call chains rooted at them\ncan thus bypass the stack limits (32 call frames * max permitted stack\ndepth per function).\n\nChangelog:\n----------\nv1 -> v2\nv1: https://lore.kernel.org/bpf/20230713003118.1327943-1-memxor@gmail.com\n\n * Fix commit message for patch 2 (Alexei)\n====================\n\nLink: https://lore.kernel.org/r/20230717161530.1238-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-07-18 15:21:10 -0700",
      "commits": [
        {
          "hash": "ba7b3e7d5f9014be65879ede8fd599cb222901c9",
          "subject": "bpf: Fix subprog idx logic in check_max_stack_depth",
          "message": "The assignment to idx in check_max_stack_depth happens once we see a\nbpf_pseudo_call or bpf_pseudo_func. This is not an issue as the rest of\nthe code performs a few checks and then pushes the frame to the frame\nstack, except the case of async callbacks. If the async callback case\ncauses the loop iteration to be skipped, the idx assignment will be\nincorrect on the next iteration of the loop. The value stored in the\nframe stack (as the subprogno of the current subprog) will be incorrect.\n\nThis leads to incorrect checks and incorrect tail_call_reachable\nmarking. Save the target subprog in a new variable and only assign to\nidx once we are done with the is_async_cb check which may skip pushing\nof frame to the frame stack and subsequent stack depth checks and tail\ncall markings.\n\nFixes: 7ddc80a476c2 (\"bpf: Teach stack depth check about async callbacks.\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230717161530.1238-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-07-18 15:21:09 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b5e9ad522c4ccd32d322877515cff8d47ed731b9",
          "subject": "bpf: Repeat check_max_stack_depth for async callbacks",
          "message": "While the check_max_stack_depth function explores call chains emanating\nfrom the main prog, which is typically enough to cover all possible call\nchains, it doesn't explore those rooted at async callbacks unless the\nasync callback will have been directly called, since unlike non-async\ncallbacks it skips their instruction exploration as they don't\ncontribute to stack depth.\n\nIt could be the case that the async callback leads to a callchain which\nexceeds the stack depth, but this is never reachable while only\nexploring the entry point from main subprog. Hence, repeat the check for\nthe main subprog *and* all async callbacks marked by the symbolic\nexecution pass of the verifier, as execution of the program may begin at\nany of them.\n\nConsider functions with following stack depths:\nmain: 256\nasync: 256\nfoo: 256\n\nmain:\n    rX = async\n    bpf_timer_set_callback(...)\n\nasync:\n    foo()\n\nHere, async is not descended as it does not contribute to stack depth of\nmain (since it is referenced using bpf_pseudo_func and not\nbpf_pseudo_call). However, when async is invoked asynchronously, it will\nend up breaching the MAX_BPF_STACK limit by calling foo.\n\nHence, in addition to main, we also need to explore call chains\nbeginning at all async callback subprogs in a program.\n\nFixes: 7ddc80a476c2 (\"bpf: Teach stack depth check about async callbacks.\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230717161530.1238-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-07-18 15:21:09 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "824adae4530b4db1d06987d8dd31a0adef37044f",
          "subject": "selftests/bpf: Add more tests for check_max_stack_depth bug",
          "message": "Another test which now exercies the path of the verifier where it will\nexplore call chains rooted at the async callback. Without the prior\nfixes, this program loads successfully, which is incorrect.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230717161530.1238-4-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-07-18 15:21:09 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/async_stack_depth.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "496720b7cfb6574a8f6f4d434f23e3d1e6cfaeb9",
      "merge_subject": "Merge branch 'Fix for check_max_stack_depth'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\nFix for a bug in check_max_stack_depth which allows bypassing the\n512-byte stack limit.\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-07-05 19:15:01 -0700",
      "commits": [
        {
          "hash": "5415ccd50a8620c8cbaa32d6f18c946c453566f5",
          "subject": "bpf: Fix max stack depth check for async callbacks",
          "message": "The check_max_stack_depth pass happens after the verifier's symbolic\nexecution, and attempts to walk the call graph of the BPF program,\nensuring that the stack usage stays within bounds for all possible call\nchains. There are two cases to consider: bpf_pseudo_func and\nbpf_pseudo_call. In the former case, the callback pointer is loaded into\na register, and is assumed that it is passed to some helper later which\ncalls it (however there is no way to be sure), but the check remains\nconservative and accounts the stack usage anyway. For this particular\ncase, asynchronous callbacks are skipped as they execute asynchronously\nwhen their corresponding event fires.\n\nThe case of bpf_pseudo_call is simpler and we know that the call is\ndefinitely made, hence the stack depth of the subprog is accounted for.\n\nHowever, the current check still skips an asynchronous callback even if\na bpf_pseudo_call was made for it. This is erroneous, as it will miss\naccounting for the stack usage of the asynchronous callback, which can\nbe used to breach the maximum stack depth limit.\n\nFix this by only skipping asynchronous callbacks when the instruction is\nnot a pseudo call to the subprog.\n\nFixes: 7ddc80a476c2 (\"bpf: Teach stack depth check about async callbacks.\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230705144730.235802-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-07-05 19:14:54 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "906bd22a44c7c381ae92996129b075ea7beba8f6",
          "subject": "selftests/bpf: Add selftest for check_stack_max_depth bug",
          "message": "Use the bpf_timer_set_callback helper to mark timer_cb as an async\ncallback, and put a direct call to timer_cb in the main subprog.\n\nAs the check_stack_max_depth happens after the do_check pass, the order\ndoes not matter. Without the previous fix, the test passes successfully.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230705144730.235802-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-07-05 19:14:54 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/async_stack_depth.c",
            "tools/testing/selftests/bpf/progs/async_stack_depth.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "c03531e087b550d975bc1eb32f56f9da47aaa77e",
      "merge_subject": "Merge branch 'verify scalar ids mapping in regsafe()'",
      "merge_body": "Eduard Zingerman says:\n\n====================\nUpdate regsafe() to use check_ids() for scalar values.\nOtherwise the following unsafe pattern is accepted by verifier:\n\n  1: r9 = ... some pointer with range X ...\n  2: r6 = ... unbound scalar ID=a ...\n  3: r7 = ... unbound scalar ID=b ...\n  4: if (r6 > r7) goto +1\n  5: r6 = r7\n  6: if (r6 > X) goto ...\n  --- checkpoint ---\n  7: r9 += r7\n  8: *(u64 *)r9 = Y\n\nThis example is unsafe because not all execution paths verify r7 range.\nBecause of the jump at (4) the verifier would arrive at (6) in two states:\nI.  r6{.id=b}, r7{.id=b} via path 1-6;\nII. r6{.id=a}, r7{.id=b} via path 1-4, 6.\n\nCurrently regsafe() does not call check_ids() for scalar registers,\nthus from POV of regsafe() states (I) and (II) are identical.\n\nThe change is split in two parts:\n- patches #1,2: update for mark_chain_precision() to propagate\n  precision marks through scalar IDs.\n- patches #3,4: update for regsafe() to use a special version of\n  check_ids() for precise scalar values.\n\nChangelog:\n- V5 -> V6:\n  - check_ids() is modified to disallow mapping different 'old_id' to\n    the same 'cur_id', check_scalar_ids() simplified (Andrii);\n  - idset_push() updated to return -EFAULT instead of -1 (Andrii);\n  - comments fixed in check_ids_in_regsafe() test case\n    (Maxim Mikityanskiy);\n  - fixed memset warning in states_equal() reported in [4].\n- V4 -> V5 (all changes are based on feedback for V4 from Andrii):\n  - mark_precise_scalar_ids() error code is updated to EFAULT;\n  - bpf_verifier_env::idmap_scratch field type is changed to struct\n    bpf_idmap to encapsulate temporary ID generation counter;\n  - regsafe() is updated to call scalar_regs_exact() only for\n    env->explore_alu_limits case (this had no measurable impact on\n    verification duration when tested using veristat).\n- V3 -> V4:\n  - check_ids() in regsafe() is replaced by check_scalar_ids(),\n    as discussed with Andrii in [3],\n    Note: I did not transfer Andrii's ack for patch #3 from V3 because\n          of the changes to the algorithm.\n  - reg_id_scratch is renamed to idset_scratch;\n  - mark_precise_scalar_ids() is modified to propagate error from\n    idset_push();\n  - test cases adjusted according to feedback from Andrii for V3.\n- V2 -> V3:\n  - u32_hashset for IDs used for range transfer is removed;\n  - mark_chain_precision() is updated as discussed with Andrii in [2].\n- V1 -> v2:\n  - 'rold->precise' and 'rold->id' checks are dropped as unsafe\n    (thanks to discussion with Yonghong);\n  - patches #3,4 adding tracking of ids used for range transfer in\n    order to mitigate performance impact.\n- RFC -> V1:\n  - Function verifier.c:mark_equal_scalars_as_read() is dropped,\n    as it was an incorrect fix for problem solved by commit [3].\n  - check_ids() is called only for precise scalar values.\n  - Test case updated to use inline assembly.\n\n[V1]  https://lore.kernel.org/bpf/20230526184126.3104040-1-eddyz87@gmail.com/\n[V2]  https://lore.kernel.org/bpf/20230530172739.447290-1-eddyz87@gmail.com/\n[V3]  https://lore.kernel.org/bpf/20230606222411.1820404-1-eddyz87@gmail.com/\n[V4]  https://lore.kernel.org/bpf/20230609210143.2625430-1-eddyz87@gmail.com/\n[V5]  https://lore.kernel.org/bpf/20230612160801.2804666-1-eddyz87@gmail.com/\n[RFC] https://lore.kernel.org/bpf/20221128163442.280187-1-eddyz87@gmail.com/\n[1]   https://gist.github.com/eddyz87/a32ea7e62a27d3c201117c9a39ab4286\n[2]   https://lore.kernel.org/bpf/20230530172739.447290-1-eddyz87@gmail.com/T/#mc21009dcd8574b195c1860a98014bb037f16f450\n[3]   https://lore.kernel.org/bpf/20230606222411.1820404-1-eddyz87@gmail.com/T/#m89da8eeb2fa8c9ca1202c5d0b6660e1f72e45e04\n[4]   https://lore.kernel.org/oe-kbuild-all/202306131550.U3M9AJGm-lkp@intel.com/\n====================\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2023-06-13 15:15:30 -0700",
      "commits": [
        {
          "hash": "904e6ddf4133c52fdb9654c2cd2ad90f320d48b9",
          "subject": "bpf: Use scalar ids in mark_chain_precision()",
          "message": "Change mark_chain_precision() to track precision in situations\nlike below:\n\n    r2 = unknown value\n    ...\n  --- state #0 ---\n    ...\n    r1 = r2                 // r1 and r2 now share the same ID\n    ...\n  --- state #1 {r1.id = A, r2.id = A} ---\n    ...\n    if (r2 > 10) goto exit; // find_equal_scalars() assigns range to r1\n    ...\n  --- state #2 {r1.id = A, r2.id = A} ---\n    r3 = r10\n    r3 += r1                // need to mark both r1 and r2\n\nAt the beginning of the processing of each state, ensure that if a\nregister with a scalar ID is marked as precise, all registers sharing\nthis ID are also marked as precise.\n\nThis property would be used by a follow-up change in regsafe().\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230613153824.3324830-2-eddyz87@gmail.com",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-06-13 15:14:27 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/verifier/precise.c"
          ]
        },
        {
          "hash": "dec020280373c60d6df48d1954e72dd6c5640282",
          "subject": "selftests/bpf: Check if mark_chain_precision() follows scalar ids",
          "message": "Check __mark_chain_precision() log to verify that scalars with same\nIDs are marked as precise. Use several scenarios to test that\nprecision marks are propagated through:\n- registers of scalar type with the same ID within one state;\n- registers of scalar type with the same ID cross several states;\n- registers of scalar type  with the same ID cross several stack frames;\n- stack slot of scalar type with the same ID;\n- multiple scalar IDs are tracked independently.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230613153824.3324830-3-eddyz87@gmail.com",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-06-13 15:14:27 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_scalar_ids.c"
          ]
        },
        {
          "hash": "1ffc85d9298e0ca0137ba65c93a786143fe167b8",
          "subject": "bpf: Verify scalar ids mapping in regsafe() using check_ids()",
          "message": "Make sure that the following unsafe example is rejected by verifier:\n\n1: r9 = ... some pointer with range X ...\n2: r6 = ... unbound scalar ID=a ...\n3: r7 = ... unbound scalar ID=b ...\n4: if (r6 > r7) goto +1\n5: r6 = r7\n6: if (r6 > X) goto ...\n--- checkpoint ---\n7: r9 += r7\n8: *(u64 *)r9 = Y\n\nThis example is unsafe because not all execution paths verify r7 range.\nBecause of the jump at (4) the verifier would arrive at (6) in two states:\nI.  r6{.id=b}, r7{.id=b} via path 1-6;\nII. r6{.id=a}, r7{.id=b} via path 1-4, 6.\n\nCurrently regsafe() does not call check_ids() for scalar registers,\nthus from POV of regsafe() states (I) and (II) are identical. If the\npath 1-6 is taken by verifier first, and checkpoint is created at (6)\nthe path [1-4, 6] would be considered safe.\n\nChanges in this commit:\n- check_ids() is modified to disallow mapping multiple old_id to the\n  same cur_id.\n- check_scalar_ids() is added, unlike check_ids() it treats ID zero as\n  a unique scalar ID.\n- check_scalar_ids() needs to generate temporary unique IDs, field\n  'tmp_id_gen' is added to bpf_verifier_env::idmap_scratch to\n  facilitate this.\n- regsafe() is updated to:\n  - use check_scalar_ids() for precise scalar registers.\n  - compare scalar registers using memcmp only for explore_alu_limits\n    branch. This simplifies control flow for scalar case, and has no\n    measurable performance impact.\n- check_alu_op() is updated to avoid generating bpf_reg_state::id for\n  constant scalar values when processing BPF_MOV. ID is needed to\n  propagate range information for identical values, but there is\n  nothing to propagate for constants.\n\nFixes: 75748837b7e5 (\"bpf: Propagate scalar ranges through register assignments.\")\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230613153824.3324830-4-eddyz87@gmail.com",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-06-13 15:15:08 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "18b89265572b5c899522b6c1f8698e87edfad369",
          "subject": "selftests/bpf: Verify that check_ids() is used for scalars in regsafe()",
          "message": "Verify that the following example is rejected by verifier:\n\n  r9 = ... some pointer with range X ...\n  r6 = ... unbound scalar ID=a ...\n  r7 = ... unbound scalar ID=b ...\n  if (r6 > r7) goto +1\n  r7 = r6\n  if (r7 > X) goto exit\n  r9 += r6\n  *(u64 *)r9 = Y\n\nAlso add test cases to:\n- check that check_alu_op() for BPF_MOV instruction does not allocate\n  scalar ID if source register is a constant;\n- check that unique scalar IDs are ignored when new verifier state is\n  compared to cached verifier state;\n- check that two different scalar IDs in a verified state can't be\n  mapped to the same scalar ID in current state.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230613153824.3324830-5-eddyz87@gmail.com",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-06-13 15:15:13 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_scalar_ids.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "b78b34c6043ec811ab03bf77a7808a2df522d549",
      "merge_subject": "Merge branch 'bpf: fix NULL dereference during extable search'",
      "merge_body": "Krister Johansen says:\n\n====================\nHi,\nEnclosed are a pair of patches for an oops that can occur if an exception is\ngenerated while a bpf subprogram is running.  One of the bpf_prog_aux entries\nfor the subprograms are missing an extable.  This can lead to an exception that\nwould otherwise be handled turning into a NULL pointer bug.\n\nThese changes were tested via the verifier and progs selftests and no\nregressions were observed.\n\nChanges from v4:\n- Ensure that num_exentries is copied to prog->aux from func[0] (Feedback from\n  Ilya Leoshkevich)\n\nChanges from v3:\n- Selftest style fixups (Feedback from Yonghong Song)\n- Selftest needs to assert that test bpf program executed (Feedback from\n  Yonghong Song)\n- Selftest should combine open and load using open_and_load (Feedback from\n  Yonghong Song)\n\nChanges from v2:\n- Insert only the main program's kallsyms (Feedback from Yonghong Song and\n  Alexei Starovoitov)\n- Selftest should use ASSERT instead of CHECK (Feedback from Yonghong Song)\n- Selftest needs some cleanup (Feedback from Yonghong Song)\n- Switch patch order (Feedback from Alexei Starovoitov)\n\nChanges from v1:\n- Add a selftest (Feedback From Alexei Starovoitov)\n- Move to a 1-line verifier change instead of searching multiple extables\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-06-13 15:13:59 -0700",
      "commits": [
        {
          "hash": "0108a4e9f3584a7a2c026d1601b0682ff7335d95",
          "subject": "bpf: ensure main program has an extable",
          "message": "When subprograms are in use, the main program is not jit'd after the\nsubprograms because jit_subprogs sets a value for prog->bpf_func upon\nsuccess.  Subsequent calls to the JIT are bypassed when this value is\nnon-NULL.  This leads to a situation where the main program and its\nfunc[0] counterpart are both in the bpf kallsyms tree, but only func[0]\nhas an extable.  Extables are only created during JIT.  Now there are\ntwo nearly identical program ksym entries in the tree, but only one has\nan extable.  Depending upon how the entries are placed, there's a chance\nthat a fault will call search_extable on the aux with the NULL entry.\n\nSince jit_subprogs already copies state from func[0] to the main\nprogram, include the extable pointer in this state duplication.\nAdditionally, ensure that the copy of the main program in func[0] is not\nadded to the bpf_prog_kallsyms table. Instead, let the main program get\nadded later in bpf_prog_load().  This ensures there is only a single\ncopy of the main program in the kallsyms table, and that its tag matches\nthe tag observed by tooling like bpftool.\n\nCc: stable@vger.kernel.org\nFixes: 1c2a088a6626 (\"bpf: x64: add JIT support for multi-function programs\")\nSigned-off-by: Krister Johansen <kjlx@templeofstupid.com>\nAcked-by: Yonghong Song <yhs@fb.com>\nAcked-by: Ilya Leoshkevich <iii@linux.ibm.com>\nTested-by: Ilya Leoshkevich <iii@linux.ibm.com>\nLink: https://lore.kernel.org/r/6de9b2f4b4724ef56efbb0339daaa66c8b68b1e7.1686616663.git.kjlx@templeofstupid.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Krister Johansen <kjlx@templeofstupid.com>",
          "date": "2023-06-13 15:13:52 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "84a62b445c86d0f2c9831290ffecee7269f18254",
          "subject": "selftests/bpf: add a test for subprogram extables",
          "message": "In certain situations a program with subprograms may have a NULL\nextable entry.  This should not happen, and when it does, it turns a\nsingle trap into multiple.  Add a test case for further debugging and to\nprevent regressions.\n\nThe test-case contains three essentially identical versions of the same\ntest because just one program may not be sufficient to trigger the oops.\nThis is due to the fact that the items are stored in a binary tree and\nhave identical values so it's possible to sometimes find the ksym with\nthe extable.  With 3 copies, this has been reliable on this author's\ntest systems.\n\nWhen triggered out of this test case, the oops looks like this:\n\n   BUG: kernel NULL pointer dereference, address: 000000000000000c\n   #PF: supervisor read access in kernel mode\n   #PF: error_code(0x0000) - not-present page\n   PGD 0 P4D 0\n   Oops: 0000 [#1] PREEMPT SMP NOPTI\n   CPU: 0 PID: 1132 Comm: test_progs Tainted: G           OE      6.4.0-rc3+ #2\n   RIP: 0010:cmp_ex_search+0xb/0x30\n   Code: cc cc cc cc e8 36 cb 03 00 66 0f 1f 44 00 00 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 f3 0f 1e fa 55 48 89 e5 48 8b 07 <48> 63 0e 48 01 f1 31 d2 48 39 c8 19 d2 48 39 c8 b8 01 00 00 00 0f\n   RSP: 0018:ffffb30c4291f998 EFLAGS: 00010006\n   RAX: ffffffffc00b49da RBX: 0000000000000002 RCX: 000000000000000c\n   RDX: 0000000000000002 RSI: 000000000000000c RDI: ffffb30c4291f9e8\n   RBP: ffffb30c4291f998 R08: ffffffffab1a42d0 R09: 0000000000000001\n   R10: 0000000000000000 R11: ffffffffab1a42d0 R12: ffffb30c4291f9e8\n   R13: 000000000000000c R14: 000000000000000c R15: 0000000000000000\n   FS:  00007fb5d9e044c0(0000) GS:ffff92e95ee00000(0000) knlGS:0000000000000000\n   CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n   CR2: 000000000000000c CR3: 000000010c3a2005 CR4: 00000000007706f0\n   DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n   DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n   PKRU: 55555554\n   Call Trace:\n    <TASK>\n    bsearch+0x41/0x90\n    ? __pfx_cmp_ex_search+0x10/0x10\n    ? bpf_prog_45a7907e7114d0ff_handle_fexit_ret_subprogs3+0x2a/0x6c\n    search_extable+0x3b/0x60\n    ? bpf_prog_45a7907e7114d0ff_handle_fexit_ret_subprogs3+0x2a/0x6c\n    search_bpf_extables+0x10d/0x190\n    ? bpf_prog_45a7907e7114d0ff_handle_fexit_ret_subprogs3+0x2a/0x6c\n    search_exception_tables+0x5d/0x70\n    fixup_exception+0x3f/0x5b0\n    ? look_up_lock_class+0x61/0x110\n    ? __lock_acquire+0x6b8/0x3560\n    ? __lock_acquire+0x6b8/0x3560\n    ? __lock_acquire+0x6b8/0x3560\n    kernelmode_fixup_or_oops+0x46/0x110\n    __bad_area_nosemaphore+0x68/0x2b0\n    ? __lock_acquire+0x6b8/0x3560\n    bad_area_nosemaphore+0x16/0x20\n    do_kern_addr_fault+0x81/0xa0\n    exc_page_fault+0xd6/0x210\n    asm_exc_page_fault+0x2b/0x30\n   RIP: 0010:bpf_prog_45a7907e7114d0ff_handle_fexit_ret_subprogs3+0x2a/0x6c\n   Code: f3 0f 1e fa 0f 1f 44 00 00 66 90 55 48 89 e5 f3 0f 1e fa 48 8b 7f 08 49 bb 00 00 00 00 00 80 00 00 4c 39 df 73 04 31 f6 eb 04 <48> 8b 77 00 49 bb 00 00 00 00 00 80 00 00 48 81 c7 7c 00 00 00 4c\n   RSP: 0018:ffffb30c4291fcb8 EFLAGS: 00010282\n   RAX: 0000000000000001 RBX: 0000000000000001 RCX: 0000000000000000\n   RDX: 00000000cddf1af1 RSI: 000000005315a00d RDI: ffffffffffffffea\n   RBP: ffffb30c4291fcb8 R08: ffff92e644bf38a8 R09: 0000000000000000\n   R10: 0000000000000000 R11: 0000800000000000 R12: ffff92e663652690\n   R13: 00000000000001c8 R14: 00000000000001c8 R15: 0000000000000003\n    bpf_trampoline_251255721842_2+0x63/0x1000\n    bpf_testmod_return_ptr+0x9/0xb0 [bpf_testmod]\n    ? bpf_testmod_test_read+0x43/0x2d0 [bpf_testmod]\n    sysfs_kf_bin_read+0x60/0x90\n    kernfs_fop_read_iter+0x143/0x250\n    vfs_read+0x240/0x2a0\n    ksys_read+0x70/0xe0\n    __x64_sys_read+0x1f/0x30\n    do_syscall_64+0x68/0xa0\n    ? syscall_exit_to_user_mode+0x77/0x1f0\n    ? do_syscall_64+0x77/0xa0\n    ? irqentry_exit+0x35/0xa0\n    ? sysvec_apic_timer_interrupt+0x4d/0x90\n    entry_SYSCALL_64_after_hwframe+0x72/0xdc\n   RIP: 0033:0x7fb5da00a392\n   Code: ac 00 00 f7 d8 64 89 02 48 c7 c0 ff ff ff ff eb be 0f 1f 80 00 00 00 00 f3 0f 1e fa 64 8b 04 25 18 00 00 00 85 c0 75 10 0f 05 <48> 3d 00 f0 ff ff 77 56 c3 0f 1f 44 00 00 48 83 ec 28 48 89 54 24\n   RSP: 002b:00007ffc5b3cab68 EFLAGS: 00000246 ORIG_RAX: 0000000000000000\n   RAX: ffffffffffffffda RBX: 000055bee7b8b100 RCX: 00007fb5da00a392\n   RDX: 00000000000001c8 RSI: 0000000000000000 RDI: 0000000000000009\n   RBP: 00007ffc5b3caba0 R08: 0000000000000000 R09: 0000000000000037\n   R10: 000055bee7b8c2a7 R11: 0000000000000246 R12: 000055bee78f1f60\n   R13: 00007ffc5b3cae90 R14: 0000000000000000 R15: 0000000000000000\n    </TASK>\n   Modules linked in: bpf_testmod(OE) nls_iso8859_1 dm_multipath scsi_dh_rdac scsi_dh_emc scsi_dh_alua intel_rapl_msr intel_rapl_common intel_uncore_frequency_common ppdev nfit crct10dif_pclmul crc32_pclmul psmouse ghash_clmulni_intel sha512_ssse3 aesni_intel parport_pc crypto_simd cryptd input_leds parport rapl ena i2c_piix4 mac_hid serio_raw ramoops reed_solomon pstore_blk drm pstore_zone efi_pstore autofs4 [last unloaded: bpf_testmod(OE)]\n   CR2: 000000000000000c\n\nThough there may be some variation, depending on which suprogram\ntriggers the bug.\n\nSigned-off-by: Krister Johansen <kjlx@templeofstupid.com>\nAcked-by: Yonghong Song <yhs@fb.com>\nLink: https://lore.kernel.org/r/4ebf95ec857cd785b81db69f3e408c039ad8408b.1686616663.git.kjlx@templeofstupid.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Krister Johansen <kjlx@templeofstupid.com>",
          "date": "2023-06-13 15:13:52 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/subprogs_extable.c",
            "tools/testing/selftests/bpf/progs/test_subprogs_extable.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "18f558876ff0361e8ceb537cdf6fec8936ff6f72",
      "merge_subject": "Merge branch 'bpf: Add socket destroy capability'",
      "merge_body": "Aditi Ghag says:\n\n====================\n\nThis patch set adds the capability to destroy sockets in BPF. We plan to\nuse the capability in Cilium to force client sockets to reconnect when\ntheir remote load-balancing backends are deleted. The other use case is\non-the-fly policy enforcement where existing socket connections\nprevented by policies need to be terminated.\n\nThe use cases, and more details around\nthe selected approach were presented at LPC 2022 -\nhttps://lpc.events/event/16/contributions/1358/.\nRFC discussion -\nhttps://lore.kernel.org/netdev/CABG=zsBEh-P4NXk23eBJw7eajB5YJeRS7oPXnTAzs=yob4EMoQ@mail.gmail.com/T/#u.\nv8 patch series -\nhttps://lore.kernel.org/bpf/20230517175359.527917-1-aditi.ghag@isovalent.com/\n\nv9 highlights:\nAddress review comments:\nMartin:\n- Rearranged the kfunc filter patch, and added the missing break\n  statement.\n- Squashed the extended selftest/bpf patch.\nYonghong:\n- Revised commit message for patch 1.\n\n(Below notes are same as v8 patch series that are still relevant. Refer to\nearlier patch series versions for other notes.)\n- I hit a snag while writing the kfunc where verifier complained about the\n  `sock_common` type passed from TCP iterator. With kfuncs, there don't\n  seem to be any options available to pass BTF type hints to the verifier\n  (equivalent of `ARG_PTR_TO_BTF_ID_SOCK_COMMON`, as was the case with the\n  helper).  As a result, I changed the argument type of the sock_destory\n  kfunc to `sock_common`.\n====================\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_date": "2023-05-19 22:44:28 -0700",
      "commits": [
        {
          "hash": "9378096e8a656fb5c4099b26b1370c56f056eab9",
          "subject": "bpf: tcp: Avoid taking fast sock lock in iterator",
          "message": "This is a preparatory commit to replace `lock_sock_fast` with\n`lock_sock`,and facilitate BPF programs executed from the TCP sockets\niterator to be able to destroy TCP sockets using the bpf_sock_destroy\nkfunc (implemented in follow-up commits).\n\nPreviously, BPF TCP iterator was acquiring the sock lock with BH\ndisabled. This led to scenarios where the sockets hash table bucket lock\ncan be acquired with BH enabled in some path versus disabled in other.\nIn such situation, kernel issued a warning since it thinks that in the\nBH enabled path the same bucket lock *might* be acquired again in the\nsoftirq context (BH disabled), which will lead to a potential dead lock.\nSince bpf_sock_destroy also happens in a process context, the potential\ndeadlock warning is likely a false alarm.\n\nHere is a snippet of annotated stack trace that motivated this change:\n\n```\n\nPossible interrupt unsafe locking scenario:\n\n      CPU0                    CPU1\n      ----                    ----\n lock(&h->lhash2[i].lock);\n                              local_bh_disable();\n                              lock(&h->lhash2[i].lock);\nkernel imagined possible scenario:\n  local_bh_disable();  /* Possible softirq */\n  lock(&h->lhash2[i].lock);\n*** Potential Deadlock ***\n\nprocess context:\n\nlock_acquire+0xcd/0x330\n_raw_spin_lock+0x33/0x40\n------> Acquire (bucket) lhash2.lock with BH enabled\n__inet_hash+0x4b/0x210\ninet_csk_listen_start+0xe6/0x100\ninet_listen+0x95/0x1d0\n__sys_listen+0x69/0xb0\n__x64_sys_listen+0x14/0x20\ndo_syscall_64+0x3c/0x90\nentry_SYSCALL_64_after_hwframe+0x72/0xdc\n\nbpf_sock_destroy run from iterator:\n\nlock_acquire+0xcd/0x330\n_raw_spin_lock+0x33/0x40\n------> Acquire (bucket) lhash2.lock with BH disabled\ninet_unhash+0x9a/0x110\ntcp_set_state+0x6a/0x210\ntcp_abort+0x10d/0x200\nbpf_prog_6793c5ca50c43c0d_iter_tcp6_server+0xa4/0xa9\nbpf_iter_run_prog+0x1ff/0x340\n------> lock_sock_fast that acquires sock lock with BH disabled\nbpf_iter_tcp_seq_show+0xca/0x190\nbpf_seq_read+0x177/0x450\n\n```\n\nAlso, Yonghong reported a deadlock for non-listening TCP sockets that\nthis change resolves. Previously, `lock_sock_fast` held the sock spin\nlock with BH which was again being acquired in `tcp_abort`:\n\n```\nwatchdog: BUG: soft lockup - CPU#0 stuck for 86s! [test_progs:2331]\nRIP: 0010:queued_spin_lock_slowpath+0xd8/0x500\nCall Trace:\n <TASK>\n _raw_spin_lock+0x84/0x90\n tcp_abort+0x13c/0x1f0\n bpf_prog_88539c5453a9dd47_iter_tcp6_client+0x82/0x89\n bpf_iter_run_prog+0x1aa/0x2c0\n ? preempt_count_sub+0x1c/0xd0\n ? from_kuid_munged+0x1c8/0x210\n bpf_iter_tcp_seq_show+0x14e/0x1b0\n bpf_seq_read+0x36c/0x6a0\n\nbpf_iter_tcp_seq_show\n   lock_sock_fast\n     __lock_sock_fast\n       spin_lock_bh(&sk->sk_lock.slock);\n\t/* * Fast path return with bottom halves disabled and * sock::sk_lock.slock held.* */\n\n ...\n tcp_abort\n   local_bh_disable();\n   spin_lock(&((sk)->sk_lock.slock)); // from bh_lock_sock(sk)\n\n```\n\nWith the switch to `lock_sock`, it calls `spin_unlock_bh` before returning:\n\n```\nlock_sock\n    lock_sock_nested\n       spin_lock_bh(&sk->sk_lock.slock);\n       :\n       spin_unlock_bh(&sk->sk_lock.slock);\n```\n\nAcked-by: Yonghong Song <yhs@meta.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Aditi Ghag <aditi.ghag@isovalent.com>\nLink: https://lore.kernel.org/r/20230519225157.760788-2-aditi.ghag@isovalent.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Aditi Ghag <aditi.ghag@isovalent.com>",
          "date": "2023-05-19 17:45:46 -0700",
          "modified_files": [
            "net/ipv4/tcp_ipv4.c"
          ]
        },
        {
          "hash": "f44b1c515833c59701c86f92d47b4edd478fb0f3",
          "subject": "udp: seq_file: Helper function to match socket attributes",
          "message": "This is a preparatory commit to refactor code that matches socket\nattributes in iterators to a helper function, and use it in the\nproc fs iterator.\n\nSigned-off-by: Aditi Ghag <aditi.ghag@isovalent.com>\nLink: https://lore.kernel.org/r/20230519225157.760788-3-aditi.ghag@isovalent.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Aditi Ghag <aditi.ghag@isovalent.com>",
          "date": "2023-05-19 17:45:46 -0700",
          "modified_files": [
            "net/ipv4/udp.c"
          ]
        },
        {
          "hash": "7625d2e9741c1f6e08ee79c28a1e27bbb5071805",
          "subject": "bpf: udp: Encapsulate logic to get udp table",
          "message": "This is a preparatory commit that encapsulates the logic\nto get udp table in iterator inside udp_get_table_afinfo, and\nrenames the function to `udp_get_table_seq` accordingly.\n\nSuggested-by: Martin KaFai Lau <martin.lau@kernel.org>\nSigned-off-by: Aditi Ghag <aditi.ghag@isovalent.com>\nLink: https://lore.kernel.org/r/20230519225157.760788-4-aditi.ghag@isovalent.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Aditi Ghag <aditi.ghag@isovalent.com>",
          "date": "2023-05-19 17:45:46 -0700",
          "modified_files": [
            "net/ipv4/udp.c"
          ]
        },
        {
          "hash": "e4fe1bf13e09019578b9b93b942fff3d76ed5793",
          "subject": "udp: seq_file: Remove bpf_seq_afinfo from udp_iter_state",
          "message": "This is a preparatory commit to remove the field. The field was\npreviously shared between proc fs and BPF UDP socket iterators. As the\nfollow-up commits will decouple the implementation for the iterators,\nremove the field. As for BPF socket iterator, filtering of sockets is\nexepected to be done in BPF programs.\n\nSuggested-by: Martin KaFai Lau <martin.lau@kernel.org>\nSigned-off-by: Aditi Ghag <aditi.ghag@isovalent.com>\nLink: https://lore.kernel.org/r/20230519225157.760788-5-aditi.ghag@isovalent.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Aditi Ghag <aditi.ghag@isovalent.com>",
          "date": "2023-05-19 17:45:47 -0700",
          "modified_files": [
            "include/net/udp.h",
            "net/ipv4/udp.c"
          ]
        },
        {
          "hash": "c96dac8d369ffd713a45f4e5c30f23c47a1671f0",
          "subject": "bpf: udp: Implement batching for sockets iterator",
          "message": "Batch UDP sockets from BPF iterator that allows for overlapping locking\nsemantics in BPF/kernel helpers executed in BPF programs.  This facilitates\nBPF socket destroy kfunc (introduced by follow-up patches) to execute from\nBPF iterator programs.\n\nPreviously, BPF iterators acquired the sock lock and sockets hash table\nbucket lock while executing BPF programs. This prevented BPF helpers that\nagain acquire these locks to be executed from BPF iterators.  With the\nbatching approach, we acquire a bucket lock, batch all the bucket sockets,\nand then release the bucket lock. This enables BPF or kernel helpers to\nskip sock locking when invoked in the supported BPF contexts.\n\nThe batching logic is similar to the logic implemented in TCP iterator:\nhttps://lore.kernel.org/bpf/20210701200613.1036157-1-kafai@fb.com/.\n\nSuggested-by: Martin KaFai Lau <martin.lau@kernel.org>\nSigned-off-by: Aditi Ghag <aditi.ghag@isovalent.com>\nLink: https://lore.kernel.org/r/20230519225157.760788-6-aditi.ghag@isovalent.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Aditi Ghag <aditi.ghag@isovalent.com>",
          "date": "2023-05-19 17:45:47 -0700",
          "modified_files": [
            "net/ipv4/udp.c"
          ]
        },
        {
          "hash": "e924e80ee6a39bc28d2ef8f51e19d336a98e3be0",
          "subject": "bpf: Add kfunc filter function to 'struct btf_kfunc_id_set'",
          "message": "This commit adds the ability to filter kfuncs to certain BPF program\ntypes. This is required to limit bpf_sock_destroy kfunc implemented in\nfollow-up commits to programs with attach type 'BPF_TRACE_ITER'.\n\nThe commit adds a callback filter to 'struct btf_kfunc_id_set'.  The\nfilter has access to the `bpf_prog` construct including its properties\nsuch as `expected_attached_type`.\n\nSigned-off-by: Aditi Ghag <aditi.ghag@isovalent.com>\nLink: https://lore.kernel.org/r/20230519225157.760788-7-aditi.ghag@isovalent.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Aditi Ghag <aditi.ghag@isovalent.com>",
          "date": "2023-05-19 22:44:14 -0700",
          "modified_files": [
            "include/linux/btf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "4ddbcb886268af8d12a23e6640b39d1d9c652b1b",
          "subject": "bpf: Add bpf_sock_destroy kfunc",
          "message": "The socket destroy kfunc is used to forcefully terminate sockets from\ncertain BPF contexts. We plan to use the capability in Cilium\nload-balancing to terminate client sockets that continue to connect to\ndeleted backends.  The other use case is on-the-fly policy enforcement\nwhere existing socket connections prevented by policies need to be\nforcefully terminated.  The kfunc also allows terminating sockets that may\nor may not be actively sending traffic.\n\nThe kfunc can currently be called only from BPF TCP and UDP iterators\nwhere users can filter, and terminate selected sockets. More\nspecifically, it can only be called from  BPF contexts that ensure\nsocket locking in order to allow synchronous execution of protocol\nspecific `diag_destroy` handlers. The previous commit that batches UDP\nsockets during iteration facilitated a synchronous invocation of the UDP\ndestroy callback from BPF context by skipping socket locks in\n`udp_abort`. TCP iterator already supported batching of sockets being\niterated. To that end, `tracing_iter_filter` callback filter is added so\nthat verifier can restrict the kfunc to programs with `BPF_TRACE_ITER`\nattach type, and reject other programs.\n\nThe kfunc takes `sock_common` type argument, even though it expects, and\ncasts them to a `sock` pointer. This enables the verifier to allow the\nsock_destroy kfunc to be called for TCP with `sock_common` and UDP with\n`sock` structs. Furthermore, as `sock_common` only has a subset of\ncertain fields of `sock`, casting pointer to the latter type might not\nalways be safe for certain sockets like request sockets, but these have a\nspecial handling in the diag_destroy handlers.\n\nAdditionally, the kfunc is defined with `KF_TRUSTED_ARGS` flag to avoid the\ncases where a `PTR_TO_BTF_ID` sk is obtained by following another pointer.\neg. getting a sk pointer (may be even NULL) by following another sk\npointer. The pointer socket argument passed in TCP and UDP iterators is\ntagged as `PTR_TRUSTED` in {tcp,udp}_reg_info.  The TRUSTED arg changes\nare contributed by Martin KaFai Lau <martin.lau@kernel.org>.\n\nSigned-off-by: Aditi Ghag <aditi.ghag@isovalent.com>\nLink: https://lore.kernel.org/r/20230519225157.760788-8-aditi.ghag@isovalent.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Aditi Ghag <aditi.ghag@isovalent.com>",
          "date": "2023-05-19 22:44:28 -0700",
          "modified_files": [
            "net/core/filter.c",
            "net/ipv4/tcp.c",
            "net/ipv4/tcp_ipv4.c",
            "net/ipv4/udp.c"
          ]
        },
        {
          "hash": "176ba657e6aaa61df637558a57acd8b7bf043cb4",
          "subject": "selftests/bpf: Add helper to get port using getsockname",
          "message": "The helper will be used to programmatically retrieve\nand pass ports in userspace and kernel selftest programs.\n\nSuggested-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Aditi Ghag <aditi.ghag@isovalent.com>\nLink: https://lore.kernel.org/r/20230519225157.760788-9-aditi.ghag@isovalent.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Aditi Ghag <aditi.ghag@isovalent.com>",
          "date": "2023-05-19 22:44:28 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/network_helpers.c",
            "tools/testing/selftests/bpf/network_helpers.h"
          ]
        },
        {
          "hash": "1a8bc2299f4028e9bac36020ffaaec27a0dfb9c1",
          "subject": "selftests/bpf: Test bpf_sock_destroy",
          "message": "The test cases for destroying sockets mirror the intended usages of the\nbpf_sock_destroy kfunc using iterators.\n\nThe destroy helpers set `ECONNABORTED` error code that we can validate\nin the test code with client sockets. But UDP sockets have an overriding\nerror code from `disconnect()` called during abort, so the error code\nvalidation is only done for TCP sockets.\n\nThe failure test cases validate that the `bpf_sock_destroy` kfunc is not\nallowed from program attach types other than BPF trace iterator, and\nsuch programs fail to load.\n\nSigned-off-by: Aditi Ghag <aditi.ghag@isovalent.com>\nLink: https://lore.kernel.org/r/20230519225157.760788-10-aditi.ghag@isovalent.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Aditi Ghag <aditi.ghag@isovalent.com>",
          "date": "2023-05-19 22:44:28 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/sock_destroy.c",
            "tools/testing/selftests/bpf/progs/sock_destroy_prog.c",
            "tools/testing/selftests/bpf/progs/sock_destroy_prog_fail.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "6e61c5fa4d43d4c3f780f74ba6b08dba80bd653a",
      "merge_subject": "Merge branch 'Dynptr Verifier Adjustments'",
      "merge_body": "Daniel Rosenberg says:\n\n====================\nThese patches relax a few verifier requirements around dynptrs.\nPatches 1-3 are unchanged from v2, apart from rebasing\nPatch 4 is the same as in v1, see\nhttps://lore.kernel.org/bpf/CA+PiJmST4WUH061KaxJ4kRL=fqy3X6+Wgb2E2rrLT5OYjUzxfQ@mail.gmail.com/\nPatch 5 adds a test for the change in Patch 4\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-05-06 16:43:10 -0700",
      "commits": [
        {
          "hash": "3bda08b63670c39be390fcb00e7718775508e673",
          "subject": "bpf: Allow NULL buffers in bpf_dynptr_slice(_rw)",
          "message": "bpf_dynptr_slice(_rw) uses a user provided buffer if it can not provide\na pointer to a block of contiguous memory. This buffer is unused in the\ncase of local dynptrs, and may be unused in other cases as well. There\nis no need to require the buffer, as the kfunc can just return NULL if\nit was needed and not provided.\n\nThis adds another kfunc annotation, __opt, which combines with __sz and\n__szk to allow the buffer associated with the size to be NULL. If the\nbuffer is NULL, the verifier does not check that the buffer is of\nsufficient size.\n\nSigned-off-by: Daniel Rosenberg <drosen@google.com>\nLink: https://lore.kernel.org/r/20230506013134.2492210-2-drosen@google.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Rosenberg <drosen@google.com>",
          "date": "2023-05-06 16:42:57 -0700",
          "modified_files": [
            "Documentation/bpf/kfuncs.rst",
            "include/linux/skbuff.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "1ce33b6c846fbe0439eeee477b767de4bc3ad35f",
          "subject": "selftests/bpf: Test allowing NULL buffer in dynptr slice",
          "message": "bpf_dynptr_slice(_rw) no longer requires a buffer for verification. If the\nbuffer is needed, but not present, the function will return NULL.\n\nSigned-off-by: Daniel Rosenberg <drosen@google.com>\nLink: https://lore.kernel.org/r/20230506013134.2492210-3-drosen@google.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Rosenberg <drosen@google.com>",
          "date": "2023-05-06 16:42:57 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/dynptr.c",
            "tools/testing/selftests/bpf/progs/dynptr_success.c"
          ]
        },
        {
          "hash": "3881fdfed21ff129a23979c0a92df6d3c5f49aa9",
          "subject": "selftests/bpf: Check overflow in optional buffer",
          "message": "This ensures we still reject invalid memory accesses in buffers that are\nmarked optional.\n\nSigned-off-by: Daniel Rosenberg <drosen@google.com>\nLink: https://lore.kernel.org/r/20230506013134.2492210-4-drosen@google.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Rosenberg <drosen@google.com>",
          "date": "2023-05-06 16:42:57 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/dynptr_fail.c"
          ]
        },
        {
          "hash": "2012c867c8005d72c949e274133df429ece78808",
          "subject": "bpf: verifier: Accept dynptr mem as mem in helpers",
          "message": "This allows using memory retrieved from dynptrs with helper functions\nthat accept ARG_PTR_TO_MEM. For instance, results from bpf_dynptr_data\ncan be passed along to bpf_strncmp.\n\nSigned-off-by: Daniel Rosenberg <drosen@google.com>\nLink: https://lore.kernel.org/r/20230506013134.2492210-5-drosen@google.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Rosenberg <drosen@google.com>",
          "date": "2023-05-06 16:42:57 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "798e48fc28fa64aa4eca6e8a404fa20ac8f7c09e",
          "subject": "selftests/bpf: Accept mem from dynptr in helper funcs",
          "message": "This ensures that buffers retrieved from dynptr_data are allowed to be\npassed in to helpers that take mem, like bpf_strncmp\n\nSigned-off-by: Daniel Rosenberg <drosen@google.com>\nLink: https://lore.kernel.org/r/20230506013134.2492210-6-drosen@google.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Daniel Rosenberg <drosen@google.com>",
          "date": "2023-05-06 16:42:58 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/dynptr.c",
            "tools/testing/selftests/bpf/progs/dynptr_success.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "fbc0b0253001c397a481d258a88ce5f08996574f",
      "merge_subject": "Merge branch 'Add precision propagation for subprogs and callbacks'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nAs more and more real-world BPF programs become more complex\nand increasingly use subprograms (both static and global), scalar precision\ntracking and its (previously weak) support for BPF subprograms (and callbacks\nas a special case of that) is becoming more and more of an issue and\nlimitation. Couple that with increasing reliance on state equivalence (BPF\nopen-coded iterators have a hard requirement for state equivalence to converge\nand successfully validate loops), and it becomes pretty critical to address\nthis limitation and make precision tracking universally supported for BPF\nprograms of any complexity and composition.\n\nThis patch set teaches BPF verifier to support SCALAR precision\nbackpropagation across multiple frames (for subprogram calls and callback\nsimulations) and addresses most practical situations (SCALAR stack\nloads/stores using registers other than r10 being the last remaining\nlimitation, though thankfully rarely used in practice).\n\nMain logic is explained in details in patch #8. The rest are preliminary\npreparations, refactorings, clean ups, and fixes. See respective patches for\ndetails.\n\nPatch #8 has also veristat comparison of results for selftests, Cilium, and\nsome of Meta production BPF programs before and after these changes.\n\nv2->v3:\n  - drop bitcnt and ifs from bt_xxx() helpers (Alexei);\nv1->v2:\n  - addressed review feedback form Alexei, adjusted commit messages, comments,\n    added verbose(), WARN_ONCE(), etc;\n  - re-ran all the tests and veristat on selftests, cilium, and meta-internal\n    code: no new changes and no kernel warnings.\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-05-04 22:35:54 -0700",
      "commits": [
        {
          "hash": "5956f3011604f03be073cba0fbe5f399b4d779ec",
          "subject": "veristat: add -t flag for adding BPF_F_TEST_STATE_FREQ program flag",
          "message": "Sometimes during debugging it's important that BPF program is loaded\nwith BPF_F_TEST_STATE_FREQ flag set to force verifier to do frequent\nstate checkpointing. Teach veristat to do this when -t (\"test state\")\nflag is specified.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-05-04 22:35:34 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/veristat.c"
          ]
        },
        {
          "hash": "e0bf462276b6ee23203365eacb5c599f42a5a084",
          "subject": "bpf: mark relevant stack slots scratched for register read instructions",
          "message": "When handling instructions that read register slots, mark relevant stack\nslots as scratched so that verifier log would contain those slots' states, in\naddition to currently emitted registers with stack slot offsets.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-05-04 22:35:34 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "407958a0e980b9e1842ab87b5a1040521e1e24e9",
          "subject": "bpf: encapsulate precision backtracking bookkeeping",
          "message": "Add struct backtrack_state and straightforward API around it to keep\ntrack of register and stack masks used and maintained during precision\nbacktracking process. Having this logic separately allow to keep\nhigh-level backtracking algorithm cleaner, but also it sets us up to\ncleanly keep track of register and stack masks per frame, allowing (with\nsome further logic adjustments) to perform precision backpropagation\nacross multiple frames (i.e., subprog calls).\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-05-04 22:35:35 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "d9439c21a9e4769bfd83a03ab39056164d44ac31",
          "subject": "bpf: improve precision backtrack logging",
          "message": "Add helper to format register and stack masks in more human-readable\nformat. Adjust logging a bit during backtrack propagation and especially\nduring forcing precision fallback logic to make it clearer what's going\non (with log_level=2, of course), and also start reporting affected\nframe depth. This is in preparation for having more than one active\nframe later when precision propagation between subprog calls is added.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-05-04 22:35:35 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/verifier/precise.c"
          ]
        },
        {
          "hash": "1ef22b6865a73a8aed36d43375fe8c7b30869326",
          "subject": "bpf: maintain bitmasks across all active frames in __mark_chain_precision",
          "message": "Teach __mark_chain_precision logic to maintain register/stack masks\nacross all active frames when going from child state to parent state.\nCurrently this should be mostly no-op, as precision backtracking usually\nbails out when encountering subprog entry/exit.\n\nIt's not very apparent from the diff due to increased indentation, but\nthe logic remains the same, except everything is done on specific `fr`\nframe index. Calls to bt_clear_reg() and bt_clear_slot() are replaced\nwith frame-specific bt_clear_frame_reg() and bt_clear_frame_slot(),\nwhere frame index is passed explicitly, instead of using current frame\nnumber.\n\nWe also adjust logging to emit affected frame number. And we also add\nbetter logging of human-readable register and stack slot masks, similar\nto previous patch.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-05-04 22:35:35 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/verifier/precise.c"
          ]
        },
        {
          "hash": "f655badf2a8fc028433d9583bf86a6b473721f09",
          "subject": "bpf: fix propagate_precision() logic for inner frames",
          "message": "Fix propagate_precision() logic to perform propagation of all necessary\nregisters and stack slots across all active frames *in one batch step*.\n\nDoing this for each register/slot in each individual frame is wasteful,\nbut the main problem is that backtracking of instruction in any frame\nexcept the deepest one just doesn't work. This is due to backtracking\nlogic relying on jump history, and available jump history always starts\n(or ends, depending how you view it) in current frame. So, if\nprog A (frame #0) called subprog B (frame #1) and we need to propagate\nprecision of, say, register R6 (callee-saved) within frame #0, we\nactually don't even know where jump history that corresponds to prog\nA even starts. We'd need to skip subprog part of jump history first to\nbe able to do this.\n\nLuckily, with struct backtrack_state and __mark_chain_precision()\nhandling bitmasks tracking/propagation across all active frames at the\nsame time (added in previous patch), propagate_precision() can be both\nfixed and sped up by setting all the necessary bits across all frames\nand then performing one __mark_chain_precision() pass. This makes it\nunnecessary to skip subprog parts of jump history.\n\nWe also improve logging along the way, to clearly specify which\nregisters' and slots' precision markings are propagated within which\nframe. Each frame will have dedicated line and all registers and stack\nslots from that frame will be reported in format similar to precision\nbacktrack regs/stack logging. E.g.:\n\nframe 1: propagating r1,r2,r3,fp-8,fp-16\nframe 0: propagating r3,r9,fp-120\n\nFixes: 529409ea92d5 (\"bpf: propagate precision across all frames, not just the last one\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-05-04 22:35:35 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "c50c0b57a515826b5d2e1ce85cd85f24f0da10c2",
          "subject": "bpf: fix mark_all_scalars_precise use in mark_chain_precision",
          "message": "When precision backtracking bails out due to some unsupported sequence\nof instructions (e.g., stack access through register other than r10), we\nneed to mark all SCALAR registers as precise to be safe. Currently,\nthough, we mark SCALARs precise only starting from the state we detected\nunsupported condition, which could be one of the parent states of the\nactual current state. This will leave some registers potentially not\nmarked as precise, even though they should. So make sure we start\nmarking scalars as precise from current state (env->cur_state).\n\nFurther, we don't currently detect a situation when we end up with some\nstack slots marked as needing precision, but we ran out of available\nstates to find the instructions that populate those stack slots. This is\nakin the `i >= func->allocated_stack / BPF_REG_SIZE` check and should be\nhandled similarly by falling back to marking all SCALARs precise. Add\nthis check when we run out of states.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-05-04 22:35:35 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/verifier/precise.c"
          ]
        },
        {
          "hash": "fde2a3882bd07876c144f2e00f7ae6893c378180",
          "subject": "bpf: support precision propagation in the presence of subprogs",
          "message": "Add support precision backtracking in the presence of subprogram frames in\njump history.\n\nThis means supporting a few different kinds of subprogram invocation\nsituations, all requiring a slightly different handling in precision\nbacktracking handling logic:\n  - static subprogram calls;\n  - global subprogram calls;\n  - callback-calling helpers/kfuncs.\n\nFor each of those we need to handle a few precision propagation cases:\n  - what to do with precision of subprog returns (r0);\n  - what to do with precision of input arguments;\n  - for all of them callee-saved registers in caller function should be\n    propagated ignoring subprog/callback part of jump history.\n\nN.B. Async callback-calling helpers (currently only\nbpf_timer_set_callback()) are transparent to all this because they set\na separate async callback environment and thus callback's history is not\nshared with main program's history. So as far as all the changes in this\ncommit goes, such helper is just a regular helper.\n\nLet's look at all these situation in more details. Let's start with\nstatic subprogram being called, using an exxerpt of a simple main\nprogram and its static subprog, indenting subprog's frame slightly to\nmake everything clear.\n\nframe 0\t\t\t\tframe 1\t\t\tprecision set\n=======\t\t\t\t=======\t\t\t=============\n\n 9: r6 = 456;\n10: r1 = 123;\t\t\t\t\t\tfr0: r6\n11: call pc+10;\t\t\t\t\t\tfr0: r1, r6\n\t\t\t\t22: r0 = r1;\t\tfr0: r6;     fr1: r1\n\t\t\t\t23: exit\t\tfr0: r6;     fr1: r0\n12: r1 = <map_pointer>\t\t\t\t\tfr0: r0, r6\n13: r1 += r0;\t\t\t\t\t\tfr0: r0, r6\n14: r1 += r6;\t\t\t\t\t\tfr0: r6\n15: exit\n\nAs can be seen above main function is passing 123 as single argument to\nan identity (`return x;`) subprog. Returned value is used to adjust map\npointer offset, which forces r0 to be marked as precise. Then\ninstruction #14 does the same for callee-saved r6, which will have to be\nbacktracked all the way to instruction #9. For brevity, precision sets\nfor instruction #13 and #14 are combined in the diagram above.\n\nFirst, for subprog calls, r0 returned from subprog (in frame 0) has to\ngo into subprog's frame 1, and should be cleared from frame 0. So we go\nback into subprog's frame knowing we need to mark r0 precise. We then\nsee that insn #22 sets r0 from r1, so now we care about marking r1\nprecise.  When we pop up from subprog's frame back into caller at\ninsn #11 we keep r1, as it's an argument-passing register, so we eventually\nfind `10: r1 = 123;` and satify precision propagation chain for insn #13.\n\nThis example demonstrates two sets of rules:\n  - r0 returned after subprog call has to be moved into subprog's r0 set;\n  - *static* subprog arguments (r1-r5) are moved back to caller precision set.\n\nLet's look at what happens with callee-saved precision propagation. Insn #14\nmark r6 as precise. When we get into subprog's frame, we keep r6 in\nframe 0's precision set *only*. Subprog itself has its own set of\nindependent r6-r10 registers and is not affected. When we eventually\nmade our way out of subprog frame we keep r6 in precision set until we\nreach `9: r6 = 456;`, satisfying propagation. r6-r10 propagation is\nperhaps the simplest aspect, it always stays in its original frame.\n\nThat's pretty much all we have to do to support precision propagation\nacross *static subprog* invocation.\n\nLet's look at what happens when we have global subprog invocation.\n\nframe 0\t\t\t\tframe 1\t\t\tprecision set\n=======\t\t\t\t=======\t\t\t=============\n\n 9: r6 = 456;\n10: r1 = 123;\t\t\t\t\t\tfr0: r6\n11: call pc+10; # global subprog\t\t\tfr0: r6\n12: r1 = <map_pointer>\t\t\t\t\tfr0: r0, r6\n13: r1 += r0;\t\t\t\t\t\tfr0: r0, r6\n14: r1 += r6;\t\t\t\t\t\tfr0: r6;\n15: exit\n\nStarting from insn #13, r0 has to be precise. We backtrack all the way\nto insn #11 (call pc+10) and see that subprog is global, so was already\nvalidated in isolation. As opposed to static subprog, global subprog\nalways returns unknown scalar r0, so that satisfies precision\npropagation and we drop r0 from precision set. We are done for insns #13.\n\nNow for insn #14. r6 is in precision set, we backtrack to `call pc+10;`.\nHere we need to recognize that this is effectively both exit and entry\nto global subprog, which means we stay in caller's frame. So we carry on\nwith r6 still in precision set, until we satisfy it at insn #9. The only\nhard part with global subprogs is just knowing when it's a global func.\n\nLastly, callback-calling helpers and kfuncs do simulate subprog calls,\nso jump history will have subprog instructions in between caller\nprogram's instructions, but the rules of propagating r0 and r1-r5\ndiffer, because we don't actually directly call callback. We actually\ncall helper/kfunc, which at runtime will call subprog, so the only\ndifference between normal helper/kfunc handling is that we need to make\nsure to skip callback simulatinog part of jump history.\nLet's look at an example to make this clearer.\n\nframe 0\t\t\t\tframe 1\t\t\tprecision set\n=======\t\t\t\t=======\t\t\t=============\n\n 8: r6 = 456;\n 9: r1 = 123;\t\t\t\t\t\tfr0: r6\n10: r2 = &callback;\t\t\t\t\tfr0: r6\n11: call bpf_loop;\t\t\t\t\tfr0: r6\n\t\t\t\t22: r0 = r1;\t\tfr0: r6      fr1:\n\t\t\t\t23: exit\t\tfr0: r6      fr1:\n12: r1 = <map_pointer>\t\t\t\t\tfr0: r0, r6\n13: r1 += r0;\t\t\t\t\t\tfr0: r0, r6\n14: r1 += r6;\t\t\t\t\t\tfr0: r6;\n15: exit\n\nAgain, insn #13 forces r0 to be precise. As soon as we get to `23: exit`\nwe see that this isn't actually a static subprog call (it's `call\nbpf_loop;` helper call instead). So we clear r0 from precision set.\n\nFor callee-saved register, there is no difference: it stays in frame 0's\nprecision set, we go through insn #22 and #23, ignoring them until we\nget back to caller frame 0, eventually satisfying precision backtrack\nlogic at insn #8 (`r6 = 456;`).\n\nAssuming callback needed to set r0 as precise at insn #23, we'd\nbacktrack to insn #22, switching from r0 to r1, and then at the point\nwhen we pop back to frame 0 at insn #11, we'll clear r1-r5 from\nprecision set, as we don't really do a subprog call directly, so there\nis no input argument precision propagation.\n\nThat's pretty much it. With these changes, it seems like the only still\nunsupported situation for precision backpropagation is the case when\nprogram is accessing stack through registers other than r10. This is\nstill left as unsupported (though rare) case for now.\n\nAs for results. For selftests, few positive changes for bigger programs,\ncls_redirect in dynptr variant benefitting the most:\n\n[vmuser@archvm bpf]$ ./veristat -C ~/subprog-precise-before-results.csv ~/subprog-precise-after-results.csv -f @veristat.cfg -e file,prog,insns -f 'insns_diff!=0'\nFile                                      Program        Insns (A)  Insns (B)  Insns     (DIFF)\n----------------------------------------  -------------  ---------  ---------  ----------------\npyperf600_bpf_loop.bpf.linked1.o          on_event            2060       2002      -58 (-2.82%)\ntest_cls_redirect_dynptr.bpf.linked1.o    cls_redirect       15660       2914  -12746 (-81.39%)\ntest_cls_redirect_subprogs.bpf.linked1.o  cls_redirect       61620      59088    -2532 (-4.11%)\nxdp_synproxy_kern.bpf.linked1.o           syncookie_tc      109980      86278  -23702 (-21.55%)\nxdp_synproxy_kern.bpf.linked1.o           syncookie_xdp      97716      85147  -12569 (-12.86%)\n\nCilium progress don't really regress. They don't use subprogs and are\nmostly unaffected, but some other fixes and improvements could have\nchanged something. This doesn't appear to be the case:\n\n[vmuser@archvm bpf]$ ./veristat -C ~/subprog-precise-before-results-cilium.csv ~/subprog-precise-after-results-cilium.csv -e file,prog,insns -f 'insns_diff!=0'\nFile           Program                         Insns (A)  Insns (B)  Insns (DIFF)\n-------------  ------------------------------  ---------  ---------  ------------\nbpf_host.o     tail_nodeport_nat_ingress_ipv6       4983       5003  +20 (+0.40%)\nbpf_lxc.o      tail_nodeport_nat_ingress_ipv6       4983       5003  +20 (+0.40%)\nbpf_overlay.o  tail_nodeport_nat_ingress_ipv6       4983       5003  +20 (+0.40%)\nbpf_xdp.o      tail_handle_nat_fwd_ipv6            12475      12504  +29 (+0.23%)\nbpf_xdp.o      tail_nodeport_nat_ingress_ipv6       6363       6371   +8 (+0.13%)\n\nLooking at (somewhat anonymized) Meta production programs, we see mostly\ninsignificant variation in number of instructions, with one program\n(syar_bind6_protect6) benefitting the most at -17%.\n\n[vmuser@archvm bpf]$ ./veristat -C ~/subprog-precise-before-results-fbcode.csv ~/subprog-precise-after-results-fbcode.csv -e prog,insns -f 'insns_diff!=0'\nProgram                   Insns (A)  Insns (B)  Insns     (DIFF)\n------------------------  ---------  ---------  ----------------\non_request_context_event        597        585      -12 (-2.01%)\nread_async_py_stack           43789      43657     -132 (-0.30%)\nread_sync_py_stack            35041      37599    +2558 (+7.30%)\nrrm_usdt                        946        940       -6 (-0.63%)\nsysarmor_inet6_bind           28863      28249     -614 (-2.13%)\nsysarmor_inet_bind            28845      28240     -605 (-2.10%)\nsyar_bind4_protect4          154145     147640    -6505 (-4.22%)\nsyar_bind6_protect6          165242     137088  -28154 (-17.04%)\nsyar_task_exit_setgid         21289      19720    -1569 (-7.37%)\nsyar_task_exit_setuid         21290      19721    -1569 (-7.37%)\ndo_uprobe                     19967      19413     -554 (-2.77%)\ntw_twfw_ingress              215877     204833   -11044 (-5.12%)\ntw_twfw_tc_in                215877     204833   -11044 (-5.12%)\n\nBut checking duration (wall clock) differences, that is the actual time taken\nby verifier to validate programs, we see a sometimes dramatic improvements, all\nthe way to about 16x improvements:\n\n[vmuser@archvm bpf]$ ./veristat -C ~/subprog-precise-before-results-meta.csv ~/subprog-precise-after-results-meta.csv -e prog,duration -s duration_diff^ | head -n20\nProgram                                   Duration (us) (A)  Duration (us) (B)  Duration (us) (DIFF)\n----------------------------------------  -----------------  -----------------  --------------------\ntw_twfw_ingress                                     4488374             272836    -4215538 (-93.92%)\ntw_twfw_tc_in                                       4339111             268175    -4070936 (-93.82%)\ntw_twfw_egress                                      3521816             270751    -3251065 (-92.31%)\ntw_twfw_tc_eg                                       3472878             284294    -3188584 (-91.81%)\nbalancer_ingress                                     343119             291391      -51728 (-15.08%)\nsyar_bind6_protect6                                   78992              64782      -14210 (-17.99%)\nttls_tc_ingress                                       11739               8176       -3563 (-30.35%)\nkprobe__security_inode_link                           13864              11341       -2523 (-18.20%)\nread_sync_py_stack                                    21927              19442       -2485 (-11.33%)\nread_async_py_stack                                   30444              28136        -2308 (-7.58%)\nsyar_task_exit_setuid                                 10256               8440       -1816 (-17.71%)\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-05-04 22:35:35 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "3ef3d2177b1a5484908c53d19269b964c488c20f",
          "subject": "selftests/bpf: add precision propagation tests in the presence of subprogs",
          "message": "Add a bunch of tests validating verifier's precision backpropagation\nlogic in the presence of subprog calls and/or callback-calling\nhelpers/kfuncs.\n\nWe validate the following conditions:\n  - subprog_result_precise: static subprog r0 result precision handling;\n  - global_subprog_result_precise: global subprog r0 precision\n    shortcutting, similar to BPF helper handling;\n  - callback_result_precise: similarly r0 marking precise for\n    callback-calling helpers;\n  - parent_callee_saved_reg_precise, parent_callee_saved_reg_precise_global:\n    propagation of precision for callee-saved registers bypassing\n    static/global subprogs;\n  - parent_callee_saved_reg_precise_with_callback: same as above, but in\n    the presence of callback-calling helper;\n  - parent_stack_slot_precise, parent_stack_slot_precise_global:\n    similar to above, but instead propagating precision of stack slot\n    (spilled SCALAR reg);\n  - parent_stack_slot_precise_with_callback: same as above, but in the\n    presence of callback-calling helper;\n  - subprog_arg_precise: propagation of precision of static subprog's\n    input argument back to caller;\n  - subprog_spill_into_parent_stack_slot_precise: negative test\n    validating that verifier currently can't support backtracking of stack\n    access with non-r10 register, we validate that we fallback to\n    forcing precision for all SCALARs.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-10-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-05-04 22:35:35 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/bpf_misc.h",
            "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c"
          ]
        },
        {
          "hash": "c91ab90cea7ae61334c7026daf310f5875dfdee7",
          "subject": "selftests/bpf: revert iter test subprog precision workaround",
          "message": "Now that precision propagation is supported fully in the presence of\nsubprogs, there is no need to work around iter test. Revert original\nworkaround.\n\nThis reverts be7dbd275dc6 (\"selftests/bpf: avoid mark_all_scalars_precise() trigger in one of iter tests\").\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-11-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-05-04 22:35:35 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/iters.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "d7a799ec782b6ae9158f8d587c9f49cf34a6c5f4",
      "merge_subject": "Merge branch 'bpf: add netfilter program type'",
      "merge_body": "Florian Westphal says:\n\n====================\nChanges since last version:\n- rework test case in last patch wrt. ctx->skb dereference etc (Alexei)\n- pacify bpf ci tests, netfilter program type missed string translation\n  in libbpf helper.\n\nThis still uses runtime btf walk rather than extending\nthe btf trace array as Alexei suggested, I would do this later (or someone else can).\n\nv1 cover letter:\n\nAdd minimal support to hook bpf programs to netfilter hooks, e.g.\nPREROUTING or FORWARD.\n\nFor this the most relevant parts for registering a netfilter\nhook via the in-kernel api are exposed to userspace via bpf_link.\n\nThe new program type is 'tracing style', i.e. there is no context\naccess rewrite done by verifier, the function argument (struct bpf_nf_ctx)\nisn't stable.\nThere is no support for direct packet access, dynptr api should be used\ninstead.\n\nWith this its possible to build a small test program such as:\n\n #include \"vmlinux.h\"\nextern int bpf_dynptr_from_skb(struct __sk_buff *skb, __u64 flags,\n                               struct bpf_dynptr *ptr__uninit) __ksym;\nextern void *bpf_dynptr_slice(const struct bpf_dynptr *ptr, uint32_t offset,\n                                   void *buffer, uint32_t buffer__sz) __ksym;\nSEC(\"netfilter\")\nint nf_test(struct bpf_nf_ctx *ctx)\n{\n\tstruct nf_hook_state *state = ctx->state;\n\tstruct sk_buff *skb = ctx->skb;\n\tconst struct iphdr *iph, _iph;\n\tconst struct tcphdr *th, _th;\n\tstruct bpf_dynptr ptr;\n\n\tif (bpf_dynptr_from_skb(skb, 0, &ptr))\n\t\treturn NF_DROP;\n\n\tiph = bpf_dynptr_slice(&ptr, 0, &_iph, sizeof(_iph));\n\tif (!iph)\n\t\treturn NF_DROP;\n\n\tth = bpf_dynptr_slice(&ptr, iph->ihl << 2, &_th, sizeof(_th));\n\tif (!th)\n\t\treturn NF_DROP;\n\n\tbpf_printk(\"accept %x:%d->%x:%d, hook %d ifin %d\\n\",\n\t\t   iph->saddr, bpf_ntohs(th->source), iph->daddr,\n\t\t   bpf_ntohs(th->dest), state->hook, state->in->ifindex);\n        return NF_ACCEPT;\n}\n\nThen, tail /sys/kernel/tracing/trace_pipe.\n\nChanges since v3:\n- uapi: remove 'reserved' struct member, s/prio/priority (Alexei)\n- add ctx access test cases (Alexei, see last patch)\n- some arm32 can only handle cmpxchg on u32 (build bot)\n- Fix kdoc annotations (Simon Horman)\n- bpftool: prefer p_err, not fprintf (Quentin)\n- add test cases in separate patch\n\nChanges since v2:\n1. don't WARN when user calls 'bpftool loink detach' twice\n   restrict attachment to ip+ip6 families, lets relax this\n   later in case arp/bridge/netdev are needed too.\n2. show netfilter links in 'bpftool net' output as well.\n\nChanges since v1:\n1. Don't fail to link when CONFIG_NETFILTER=n (build bot)\n2. Use test_progs instead of test_verifier (Alexei)\n\nChanges since last RFC version:\n1. extend 'bpftool link show' to print prio/hooknum etc\n2. extend 'nft list hooks' so it can print the bpf program id\n3. Add an extra patch to artificially restrict bpf progs with\n   same priority.  Its fine from a technical pov but it will\n   cause ordering issues (most recent one comes first).\n   Can be removed later.\n4. Add test_run support for netfilter prog type and a small\n   extension to verifier tests to make sure we can't return\n   verdicts like NF_STOLEN.\n5. Alter the netfilter part of the bpf_link uapi struct:\n   - add flags/reserved members.\n  Not used here except returning errors when they are nonzero.\n  Plan is to allow the bpf_link users to enable netfilter\n  defrag or conntrack engine by setting feature flags at\n  link create time in the future.\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-04-21 11:37:41 -0700",
      "commits": [
        {
          "hash": "84601d6ee68ae820dec97450934797046d62db4b",
          "subject": "bpf: add bpf_link support for BPF_NETFILTER programs",
          "message": "Add bpf_link support skeleton.  To keep this reviewable, no bpf program\ncan be invoked yet, if a program is attached only a c-stub is called and\nnot the actual bpf program.\n\nDefaults to 'y' if both netfilter and bpf syscall are enabled in kconfig.\n\nUapi example usage:\n\tunion bpf_attr attr = { };\n\n\tattr.link_create.prog_fd = progfd;\n\tattr.link_create.attach_type = 0; /* unused */\n\tattr.link_create.netfilter.pf = PF_INET;\n\tattr.link_create.netfilter.hooknum = NF_INET_LOCAL_IN;\n\tattr.link_create.netfilter.priority = -128;\n\n\terr = bpf(BPF_LINK_CREATE, &attr, sizeof(attr));\n\n... this would attach progfd to ipv4:input hook.\n\nSuch hook gets removed automatically if the calling program exits.\n\nBPF_NETFILTER program invocation is added in followup change.\n\nNF_HOOK_OP_BPF enum will eventually be read from nfnetlink_hook, it\nallows to tell userspace which program is attached at the given hook\nwhen user runs 'nft hook list' command rather than just the priority\nand not-very-helpful 'this hook runs a bpf prog but I can't tell which\none'.\n\nWill also be used to disallow registration of two bpf programs with\nsame priority in a followup patch.\n\nv4: arm32 cmpxchg only supports 32bit operand\n    s/prio/priority/\nv3: restrict prog attachment to ip/ip6 for now, lets lift restrictions if\n    more use cases pop up (arptables, ebtables, netdev ingress/egress etc).\n\nSigned-off-by: Florian Westphal <fw@strlen.de>\nLink: https://lore.kernel.org/r/20230421170300.24115-2-fw@strlen.de\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Florian Westphal <fw@strlen.de>",
          "date": "2023-04-21 11:34:14 -0700",
          "modified_files": [
            "include/linux/netfilter.h",
            "include/net/netfilter/nf_bpf_link.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/syscall.c",
            "net/netfilter/Kconfig",
            "net/netfilter/Makefile",
            "net/netfilter/nf_bpf_link.c"
          ]
        },
        {
          "hash": "fd9c663b9ad67dedfc9a3fd3429ddd3e83782b4d",
          "subject": "bpf: minimal support for programs hooked into netfilter framework",
          "message": "This adds minimal support for BPF_PROG_TYPE_NETFILTER bpf programs\nthat will be invoked via the NF_HOOK() points in the ip stack.\n\nInvocation incurs an indirect call.  This is not a necessity: Its\npossible to add 'DEFINE_BPF_DISPATCHER(nf_progs)' and handle the\nprogram invocation with the same method already done for xdp progs.\n\nThis isn't done here to keep the size of this chunk down.\n\nVerifier restricts verdicts to either DROP or ACCEPT.\n\nSigned-off-by: Florian Westphal <fw@strlen.de>\nLink: https://lore.kernel.org/r/20230421170300.24115-3-fw@strlen.de\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Florian Westphal <fw@strlen.de>",
          "date": "2023-04-21 11:34:14 -0700",
          "modified_files": [
            "include/linux/bpf_types.h",
            "include/net/netfilter/nf_bpf_link.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c",
            "net/core/filter.c",
            "net/netfilter/nf_bpf_link.c"
          ]
        },
        {
          "hash": "506a74db7e019a277e987fa65654bdd953859d5b",
          "subject": "netfilter: nfnetlink hook: dump bpf prog id",
          "message": "This allows userspace (\"nft list hooks\") to show which bpf program\nis attached to which hook.\n\nWithout this, user only knows bpf prog is attached at prio\nx, y, z at INPUT and FORWARD, but can't tell which program is where.\n\nv4: kdoc fixups (Simon Horman)\n\nLink: https://lore.kernel.org/bpf/ZEELzpNCnYJuZyod@corigine.com/\nSigned-off-by: Florian Westphal <fw@strlen.de>\nLink: https://lore.kernel.org/r/20230421170300.24115-4-fw@strlen.de\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Florian Westphal <fw@strlen.de>",
          "date": "2023-04-21 11:34:14 -0700",
          "modified_files": [
            "include/uapi/linux/netfilter/nfnetlink_hook.h",
            "net/netfilter/nfnetlink_hook.c"
          ]
        },
        {
          "hash": "0bdc6da88f5bac0f3f272cb6f545c2cc70e8c66a",
          "subject": "netfilter: disallow bpf hook attachment at same priority",
          "message": "This is just to avoid ordering issues between multiple bpf programs,\nthis could be removed later in case it turns out to be too cautious.\n\nbpf prog could still be shared with non-bpf hook, otherwise we'd have to\nmake conntrack hook registration fail just because a bpf program has\nsame priority.\n\nSigned-off-by: Florian Westphal <fw@strlen.de>\nLink: https://lore.kernel.org/r/20230421170300.24115-5-fw@strlen.de\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Florian Westphal <fw@strlen.de>",
          "date": "2023-04-21 11:34:14 -0700",
          "modified_files": [
            "net/netfilter/core.c"
          ]
        },
        {
          "hash": "d0fe92fb5e3df6991c640fb9205d880b68603259",
          "subject": "tools: bpftool: print netfilter link info",
          "message": "Dump protocol family, hook and priority value:\n$ bpftool link\n2: netfilter  prog 14\n        ip input prio -128\n        pids install(3264)\n5: netfilter  prog 14\n        ip6 forward prio 21\n        pids a.out(3387)\n9: netfilter  prog 14\n        ip prerouting prio 123\n        pids a.out(5700)\n10: netfilter  prog 14\n        ip input prio 21\n        pids test2(5701)\n\nv2: Quentin Monnet suggested to also add 'bpftool net' support:\n\n$ bpftool net\nxdp:\n\ntc:\n\nflow_dissector:\n\nnetfilter:\n\n        ip prerouting prio 21 prog_id 14\n        ip input prio -128 prog_id 14\n        ip input prio 21 prog_id 14\n        ip forward prio 21 prog_id 14\n        ip output prio 21 prog_id 14\n        ip postrouting prio 21 prog_id 14\n\n'bpftool net' only dumps netfilter link type, links are sorted by protocol\nfamily, hook and priority.\n\nv5: fix bpf ci failure: libbpf needs small update to prog_type_name[]\n    and probe_prog_load helper.\nv4: don't fail with -EOPNOTSUPP in libbpf probe_prog_load, update\n    prog_type_name[] with \"netfilter\" entry (bpf ci)\nv3: fix bpf.h copy, 'reserved' member was removed (Alexei)\n    use p_err, not fprintf (Quentin)\n\nSuggested-by: Quentin Monnet <quentin@isovalent.com>\nLink: https://lore.kernel.org/bpf/eeeaac99-9053-90c2-aa33-cc1ecb1ae9ca@isovalent.com/\nReviewed-by: Quentin Monnet <quentin@isovalent.com>\nSigned-off-by: Florian Westphal <fw@strlen.de>\nLink: https://lore.kernel.org/r/20230421170300.24115-6-fw@strlen.de\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Florian Westphal <fw@strlen.de>",
          "date": "2023-04-21 11:34:49 -0700",
          "modified_files": [
            "tools/bpf/bpftool/link.c",
            "tools/bpf/bpftool/main.h",
            "tools/bpf/bpftool/net.c",
            "tools/include/uapi/linux/bpf.h",
            "tools/lib/bpf/libbpf.c",
            "tools/lib/bpf/libbpf_probes.c"
          ]
        },
        {
          "hash": "2b99ef22e0d237e08bfc437e7d051f78f352aeb2",
          "subject": "bpf: add test_run support for netfilter program type",
          "message": "add glue code so a bpf program can be run using userspace-provided\nnetfilter state and packet/skb.\n\nDefault is to use ipv4:output hook point, but this can be overridden by\nuserspace.  Userspace provided netfilter state is restricted, only hook and\nprotocol families can be overridden and only to ipv4/ipv6.\n\nSigned-off-by: Florian Westphal <fw@strlen.de>\nLink: https://lore.kernel.org/r/20230421170300.24115-7-fw@strlen.de\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Florian Westphal <fw@strlen.de>",
          "date": "2023-04-21 11:34:50 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "net/bpf/test_run.c",
            "net/netfilter/nf_bpf_link.c"
          ]
        },
        {
          "hash": "006c0e44ed924140d44bc756e6ea36301fcea68d",
          "subject": "selftests/bpf: add missing netfilter return value and ctx access tests",
          "message": "Extend prog_tests with two test cases:\n\n # ./test_progs --allow=verifier_netfilter_retcode\n #278/1   verifier_netfilter_retcode/bpf_exit with invalid return code. test1:OK\n #278/2   verifier_netfilter_retcode/bpf_exit with valid return code. test2:OK\n #278/3   verifier_netfilter_retcode/bpf_exit with valid return code. test3:OK\n #278/4   verifier_netfilter_retcode/bpf_exit with invalid return code. test4:OK\n #278     verifier_netfilter_retcode:OK\n\nThis checks that only accept and drop (0,1) are permitted.\n\nNF_QUEUE could be implemented later if we can guarantee that attachment\nof such programs can be rejected if they get attached to a pf/hook that\ndoesn't support async reinjection.\n\nNF_STOLEN could be implemented via trusted helpers that can guarantee\nthat the skb will eventually be free'd.\n\nv4: test case for bpf_nf_ctx access checks, requested by Alexei Starovoitov.\nv5: also check ctx->{state,skb} can be dereferenced (Alexei).\n\n # ./test_progs --allow=verifier_netfilter_ctx\n #281/1   verifier_netfilter_ctx/netfilter invalid context access, size too short:OK\n #281/2   verifier_netfilter_ctx/netfilter invalid context access, size too short:OK\n #281/3   verifier_netfilter_ctx/netfilter invalid context access, past end of ctx:OK\n #281/4   verifier_netfilter_ctx/netfilter invalid context, write:OK\n #281/5   verifier_netfilter_ctx/netfilter valid context read and invalid write:OK\n #281/6   verifier_netfilter_ctx/netfilter test prog with skb and state read access:OK\n #281/7   verifier_netfilter_ctx/netfilter test prog with skb and state read access @unpriv:OK\n #281     verifier_netfilter_ctx:OK\nSummary: 1/7 PASSED, 0 SKIPPED, 0 FAILED\n\nThis checks:\n1/2: partial reads of ctx->{skb,state} are rejected\n3. read access past sizeof(ctx) is rejected\n4. write to ctx content, e.g. 'ctx->skb = NULL;' is rejected\n5. ctx->state content cannot be altered\n6. ctx->state and ctx->skb can be dereferenced\n7. ... same program fails for unpriv (CAP_NET_ADMIN needed).\n\nLink: https://lore.kernel.org/bpf/20230419021152.sjq4gttphzzy6b5f@dhcp-172-26-102-232.dhcp.thefacebook.com/\nLink: https://lore.kernel.org/bpf/20230420201655.77kkgi3dh7fesoll@MacBook-Pro-6.local/\nSigned-off-by: Florian Westphal <fw@strlen.de>\nLink: https://lore.kernel.org/r/20230421170300.24115-8-fw@strlen.de\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Florian Westphal <fw@strlen.de>",
          "date": "2023-04-21 11:34:50 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_netfilter_ctx.c",
            "tools/testing/selftests/bpf/progs/verifier_netfilter_retcode.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "d40f4f68132e9f6d4b1743c8eca0d6194ea1712f",
      "merge_subject": "Merge branch 'Remove KF_KPTR_GET kfunc flag'",
      "merge_body": "David Vernet says:\n\n====================\n\nWe've managed to improve the UX for kptrs significantly over the last 9\nmonths. All of the existing use cases which previously had KF_KPTR_GET\nkfuncs (struct bpf_cpumask *, struct task_struct *, and struct cgroup *)\nhave all been updated to be synchronized using RCU. In other words,\ntheir KF_KPTR_GET kfuncs have been removed in favor of KF_RCU |\nKF_ACQUIRE kfuncs, with the pointers themselves also being readable from\nmaps in an RCU read region thanks to the types being RCU safe.\n\nWhile KF_KPTR_GET was a logical starting point for kptrs, it's become\nclear that they're not the correct abstraction. KF_KPTR_GET is a flag\nthat essentially does nothing other than enforcing that the argument to\na function is a pointer to a referenced kptr map value. At first glance,\nthat's a useful thing to guarantee to a kfunc. It gives kfuncs the\nability to try and acquire a reference on that kptr without requiring\nthe BPF prog to do something like this:\n\nstruct kptr_type *in_map, *new = NULL;\n\nin_map = bpf_kptr_xchg(&map->value, NULL);\nif (in_map) {\n\tnew = bpf_kptr_type_acquire(in_map);\n\tin_map = bpf_kptr_xchg(&map->value, in_map);\n\tif (in_map)\n\t\tbpf_kptr_type_release(in_map);\n}\n\nThat's clearly a pretty ugly (and racy) UX, and if using KF_KPTR_GET is\nthe only alternative, it's better than nothing. However, the problem\nwith any KF_KPTR_GET kfunc lies in the fact that it always requires some\nkind of synchronization in order to safely do an opportunistic acquire\nof the kptr in the map. This is because a BPF program running on another\nCPU could do a bpf_kptr_xchg() on that map value, and free the kptr\nafter it's been read by the KF_KPTR_GET kfunc. For example, the\nnow-removed bpf_task_kptr_get() kfunc did the following:\n\nstruct task_struct *bpf_task_kptr_get(struct task_struct **pp)\n{\n\t    struct task_struct *p;\n\n\trcu_read_lock();\n\tp = READ_ONCE(*pp);\n\t/* If p is non-NULL, it could still be freed by another CPU,\n \t * so we have to do an opportunistic refcount_inc_not_zero()\n\t * and return NULL if the task will be freed after the\n\t * current RCU read region.\n\t */\n\t|f (p && !refcount_inc_not_zero(&p->rcu_users))\n\t\tp = NULL;\n\trcu_read_unlock();\n\n\treturn p;\n}\n\nIn other words, the kfunc uses RCU to ensure that the task remains valid\nafter it's been peeked from the map. However, this is completely\nredundant with just defining a KF_RCU kfunc that itself does a\nrefcount_inc_not_zero(), which is exactly what bpf_task_acquire() now\ndoes.\n\nSo, the question of whether KF_KPTR_GET is useful is actually, \"Are\nthere any synchronization mechanisms / safety flags that are required by\ncertain kptrs, but which are not provided by the verifier to kfuncs?\"\nThe answer to that question today is \"No\", because every kptr we\ncurrently care about is RCU protected.\n\nEven if the answer ever became \"yes\", the proper way to support that\nreferenced kptr type would be to add support for whatever\nsynchronization mechanism it requires in the verifier, rather than\ngiving kfuncs a flag that says, \"Here's a pointer to a referenced kptr\nin a map, do whatever you need to do.\"\n\nWith all that said -- so as to allow us to consolidate the kfunc API,\nand simplify the verifier, this patchset removes the KF_KPTR_GET kfunc\nflag.\n---\n\nThis is v2 of this patchset\n\nv1: https://lore.kernel.org/all/20230415103231.236063-1-void@manifault.com/\n\nChangelog:\n----------\n\nv1 -> v2:\n- Fix KF_RU -> KF_RCU typo in commit summary for patch 2/3, and in cover\n  letter (Alexei)\n- In order to reduce churn, don't shift all KF_* flags down by 1. We'll\n  just fill the now-empty slot the next time we add a flag (Alexei)\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-04-16 08:51:24 -0700",
      "commits": [
        {
          "hash": "09b501d905217a38f03c0f07d5a66e0b5c8c1644",
          "subject": "bpf: Remove bpf_kfunc_call_test_kptr_get() test kfunc",
          "message": "We've managed to improve the UX for kptrs significantly over the last 9\nmonths. All of the prior main use cases, struct bpf_cpumask *, struct\ntask_struct *, and struct cgroup *, have all been updated to be\nsynchronized mainly using RCU. In other words, their KF_ACQUIRE kfunc\ncalls are all KF_RCU, and the pointers themselves are MEM_RCU and can be\naccessed in an RCU read region in BPF.\n\nIn a follow-on change, we'll be removing the KF_KPTR_GET kfunc flag.\nThis patch prepares for that by removing the\nbpf_kfunc_call_test_kptr_get() kfunc, and all associated selftests.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230416084928.326135-2-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-04-16 08:51:24 -0700",
          "modified_files": [
            "net/bpf/test_run.c",
            "tools/testing/selftests/bpf/progs/map_kptr.c",
            "tools/testing/selftests/bpf/progs/map_kptr_fail.c",
            "tools/testing/selftests/bpf/verifier/map_kptr.c"
          ]
        },
        {
          "hash": "7b4ddf3920d247c2949073b9c274301c8131332a",
          "subject": "bpf: Remove KF_KPTR_GET kfunc flag",
          "message": "We've managed to improve the UX for kptrs significantly over the last 9\nmonths. All of the existing use cases which previously had KF_KPTR_GET\nkfuncs (struct bpf_cpumask *, struct task_struct *, and struct cgroup *)\nhave all been updated to be synchronized using RCU. In other words,\ntheir KF_KPTR_GET kfuncs have been removed in favor of KF_RCU |\nKF_ACQUIRE kfuncs, with the pointers themselves also being readable from\nmaps in an RCU read region thanks to the types being RCU safe.\n\nWhile KF_KPTR_GET was a logical starting point for kptrs, it's become\nclear that they're not the correct abstraction. KF_KPTR_GET is a flag\nthat essentially does nothing other than enforcing that the argument to\na function is a pointer to a referenced kptr map value. At first glance,\nthat's a useful thing to guarantee to a kfunc. It gives kfuncs the\nability to try and acquire a reference on that kptr without requiring\nthe BPF prog to do something like this:\n\nstruct kptr_type *in_map, *new = NULL;\n\nin_map = bpf_kptr_xchg(&map->value, NULL);\nif (in_map) {\n        new = bpf_kptr_type_acquire(in_map);\n        in_map = bpf_kptr_xchg(&map->value, in_map);\n        if (in_map)\n                bpf_kptr_type_release(in_map);\n}\n\nThat's clearly a pretty ugly (and racy) UX, and if using KF_KPTR_GET is\nthe only alternative, it's better than nothing. However, the problem\nwith any KF_KPTR_GET kfunc lies in the fact that it always requires some\nkind of synchronization in order to safely do an opportunistic acquire\nof the kptr in the map. This is because a BPF program running on another\nCPU could do a bpf_kptr_xchg() on that map value, and free the kptr\nafter it's been read by the KF_KPTR_GET kfunc. For example, the\nnow-removed bpf_task_kptr_get() kfunc did the following:\n\nstruct task_struct *bpf_task_kptr_get(struct task_struct **pp)\n{\n            struct task_struct *p;\n\n        rcu_read_lock();\n        p = READ_ONCE(*pp);\n        /* If p is non-NULL, it could still be freed by another CPU,\n         * so we have to do an opportunistic refcount_inc_not_zero()\n         * and return NULL if the task will be freed after the\n         * current RCU read region.\n         */\n        |f (p && !refcount_inc_not_zero(&p->rcu_users))\n                p = NULL;\n        rcu_read_unlock();\n\n        return p;\n}\n\nIn other words, the kfunc uses RCU to ensure that the task remains valid\nafter it's been peeked from the map. However, this is completely\nredundant with just defining a KF_RCU kfunc that itself does a\nrefcount_inc_not_zero(), which is exactly what bpf_task_acquire() now\ndoes.\n\nSo, the question of whether KF_KPTR_GET is useful is actually, \"Are\nthere any synchronization mechanisms / safety flags that are required by\ncertain kptrs, but which are not provided by the verifier to kfuncs?\"\nThe answer to that question today is \"No\", because every kptr we\ncurrently care about is RCU protected.\n\nEven if the answer ever became \"yes\", the proper way to support that\nreferenced kptr type would be to add support for whatever\nsynchronization mechanism it requires in the verifier, rather than\ngiving kfuncs a flag that says, \"Here's a pointer to a referenced kptr\nin a map, do whatever you need to do.\"\n\nWith all that said -- so as to allow us to consolidate the kfunc API,\nand simplify the verifier a bit, this patch removes KF_KPTR_GET, and all\nrelevant logic from the verifier.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230416084928.326135-3-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-04-16 08:51:24 -0700",
          "modified_files": [
            "include/linux/btf.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "530474e6d044d07b179dc4d3392fb853c47446d0",
          "subject": "bpf,docs: Remove KF_KPTR_GET from documentation",
          "message": "A prior patch removed KF_KPTR_GET from the kernel. Now that it's no\nlonger accessible to kfunc authors, this patch removes it from the BPF\nkfunc documentation.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230416084928.326135-4-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-04-16 08:51:24 -0700",
          "modified_files": [
            "Documentation/bpf/kfuncs.rst"
          ]
        }
      ]
    },
    {
      "merge_hash": "7a0788fe835f98391b8fcb03e3cd29c1296b3280",
      "merge_subject": "Merge branch 'Shared ownership for local kptrs'",
      "merge_body": "Dave Marchevsky says:\n\n====================\n\nThis series adds support for refcounted local kptrs to the verifier. A local\nkptr is 'refcounted' if its type contains a struct bpf_refcount field:\n\n  struct refcounted_node {\n    long data;\n    struct bpf_list_node ll;\n    struct bpf_refcount ref;\n  };\n\nbpf_refcount is used to implement shared ownership for local kptrs.\n\nMotivating usecase\n==================\n\nIf a struct has two collection node fields, e.g.:\n\n  struct node {\n    long key;\n    long val;\n    struct bpf_rb_node rb;\n    struct bpf_list_node ll;\n  };\n\nIt's not currently possible to add a node to both the list and rbtree:\n\n  long bpf_prog(void *ctx)\n  {\n    struct node *n = bpf_obj_new(typeof(*n));\n    if (!n) { /* ... */ }\n\n    bpf_spin_lock(&lock);\n\n    bpf_list_push_back(&head, &n->ll);\n    bpf_rbtree_add(&root, &n->rb, less); /* Assume a resonable less() */\n    bpf_spin_unlock(&lock);\n  }\n\nThe above program will fail verification due to current owning / non-owning ref\nlogic: after bpf_list_push_back, n is a non-owning reference and thus cannot be\npassed to bpf_rbtree_add. The only way to get an owning reference for the node\nthat was added is to bpf_list_pop_{front,back} it.\n\nMore generally, verifier ownership semantics expect that a node has one\nowner (program, collection, or stashed in map) with exclusive ownership\nof the node's lifetime. The owner free's the node's underlying memory when it\nitself goes away.\n\nWithout a shared ownership concept it's impossible to express many real-world\nusecases such that they pass verification.\n\nSemantic Changes\n================\n\nBefore this series, the verifier could make this statement: \"whoever has the\nowning reference has exclusive ownership of the referent's lifetime\". As\ndemonstrated in the previous section, this implies that a BPF program can't\nhave an owning reference to some node if that node is in a collection. If\nsuch a state were possible, the node would have multiple owners, each thinking\nthey have exclusive ownership. In order to support shared ownership it's\nnecessary to modify the exclusive ownership semantic.\n\nAfter this series' changes, an owning reference has ownership of the referent's\nlifetime, but it's not necessarily exclusive. The referent's underlying memory\nis guaranteed to be valid (i.e. not free'd) until the reference is dropped or\nused for collection insert.\n\nThis change doesn't affect UX of owning or non-owning references much:\n\n  * insert kfuncs (bpf_rbtree_add, bpf_list_push_{front,back}) still require\n    an owning reference arg, as ownership still must be passed to the\n    collection in a shared-ownership world.\n\n  * non-owning references still refer to valid memory without claiming\n    any ownership.\n\nOne important conclusion that followed from \"exclusive ownership\" statement\nis no longer valid, though. In exclusive-ownership world, if a BPF prog has\nan owning reference to a node, the verifier can conclude that no collection has\nownership of it. This conclusion was used to avoid runtime checking in the\nimplementations of insert and remove operations (\"\"has the node already been\n{inserted, removed}?\").\n\nIn a shared-ownership world the aforementioned conclusion is no longer valid,\nwhich necessitates doing runtime checking in insert and remove operation\nkfuncs, and those functions possibly failing to insert or remove anything.\n\nLuckily the verifier changes necessary to go from exclusive to shared ownership\nwere fairly minimal. Patches in this series which do change verifier semantics\ngenerally have some summary dedicated to explaining why certain usecases\nJust Work for shared ownership without verifier changes.\n\nImplementation\n==============\n\nThe changes in this series can be categorized as follows:\n\n  * struct bpf_refcount opaque field + plumbing\n  * support for refcounted kptrs in bpf_obj_new and bpf_obj_drop\n  * bpf_refcount_acquire kfunc\n    * enables shared ownershp by bumping refcount + acquiring owning ref\n  * support for possibly-failing collection insertion and removal\n    * insertion changes are more complex\n\nIf a patch's changes have some nuance to their effect - or lack of effect - on\nverifier behavior, the patch summary talks about it at length.\n\nPatch contents:\n  * Patch 1 removes btf_field_offs struct\n  * Patch 2 adds struct bpf_refcount and associated plumbing\n  * Patch 3 modifies semantics of bpf_obj_drop and bpf_obj_new to handle\n    refcounted kptrs\n  * Patch 4 adds bpf_refcount_acquire\n  * Patches 5-7 add support for possibly-failing collection insert and remove\n  * Patch 8 centralizes constructor-like functionality for local kptr types\n  * Patch 9 adds tests for new functionality\n\nbase-commit: 4a1e885c6d143ff1b557ec7f3fc6ddf39c51502f\n\nChangelog:\n\nv1 -> v2: lore.kernel.org/bpf/20230410190753.2012798-1-davemarchevsky@fb.com\n\nPatch #s used below refer to the patch's position in v1 unless otherwise\nspecified.\n\n  * General\n    * Rebase onto latest bpf-next (base-commit updated above)\n\n  * Patch 4 - \"bpf: Add bpf_refcount_acquire kfunc\"\n    * Fix typo in summary (Alexei)\n  * Patch 7 - \"Migrate bpf_rbtree_remove to possibly fail\"\n    * Modify a paragraph in patch summary to more clearly state that only\n      bpf_rbtree_remove's non-owning ref clobbering behavior is changed by the\n      patch (Alexei)\n    * refcount_off == -1 -> refcount_off < 0  in \"node type w/ both list\n      and rb_node fields\" check, since any negative value means \"no\n      bpf_refcount field found\", and furthermore refcount_off is never\n      explicitly set to -1, but rather -EINVAL. (Alexei)\n    * Instead of just changing \"btf: list_node and rb_node in same struct\" test\n      expectation to pass instead of fail, do some refactoring to test both\n      \"list_node, rb_node, and bpf_refcount\" (success) and \"list_node, rb_node,\n      _no_ bpf_refcount\" (failure) cases. This ensures that logic change in\n      previous bullet point is correct.\n      * v1's \"btf: list_node and rb_node in same struct\" test changes didn't\n        add bpf_refcount, so the fact that btf load succeeded w/ list and\n        rb_nodes but no bpf_refcount field is further proof that this logic\n        was incorrect in v1.\n  * Patch 8 - \"bpf: Centralize btf_field-specific initialization logic\"\n    * Instead of doing __init_field_infer_size in kfuncs when taking\n      bpf_list_head type input which might've been 0-initialized in map, go\n      back to simple oneliner initialization. Add short comment explaining why\n      this is necessary. (Alexei)\n  * Patch 9 - \"selftests/bpf: Add refcounted_kptr tests\"\n    * Don't __always_inline helper fns in progs/refcounted_kptr.c (Alexei)\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-04-15 17:36:51 -0700",
      "commits": [
        {
          "hash": "cd2a8079014aced27da9b2e669784f31680f1351",
          "subject": "bpf: Remove btf_field_offs, use btf_record's fields instead",
          "message": "The btf_field_offs struct contains (offset, size) for btf_record fields,\nsorted by offset. btf_field_offs is always used in conjunction with\nbtf_record, which has btf_field 'fields' array with (offset, type), the\nlatter of which btf_field_offs' size is derived from via\nbtf_field_type_size.\n\nThis patch adds a size field to struct btf_field and sorts btf_record's\nfields by offset, making it possible to get rid of btf_field_offs. Less\ndata duplication and less code complexity results.\n\nSince btf_field_offs' lifetime closely followed the btf_record used to\npopulate it, most complexity wins are from removal of initialization\ncode like:\n\n  if (btf_record_successfully_initialized) {\n    foffs = btf_parse_field_offs(rec);\n    if (IS_ERR_OR_NULL(foffs))\n      // free the btf_record and return err\n  }\n\nOther changes in this patch are pretty mechanical:\n\n  * foffs->field_off[i] -> rec->fields[i].offset\n  * foffs->field_sz[i] -> rec->fields[i].size\n  * Sort rec->fields in btf_parse_fields before returning\n    * It's possible that this is necessary independently of other\n      changes in this patch. btf_record_find in syscall.c expects\n      btf_record's fields to be sorted by offset, yet there's no\n      explicit sorting of them before this patch, record's fields are\n      populated in the order they're read from BTF struct definition.\n      BTF docs don't say anything about the sortedness of struct fields.\n  * All functions taking struct btf_field_offs * input now instead take\n    struct btf_record *. All callsites of these functions already have\n    access to the correct btf_record.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-2-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-04-15 17:36:49 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/btf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/helpers.c",
            "kernel/bpf/map_in_map.c",
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "d54730b50bae1f3119bd686d551d66f0fcc387ca",
          "subject": "bpf: Introduce opaque bpf_refcount struct and add btf_record plumbing",
          "message": "A 'struct bpf_refcount' is added to the set of opaque uapi/bpf.h types\nmeant for use in BPF programs. Similarly to other opaque types like\nbpf_spin_lock and bpf_rbtree_node, the verifier needs to know where in\nuser-defined struct types a bpf_refcount can be located, so necessary\nbtf_record plumbing is added to enable this. bpf_refcount is sized to\nhold a refcount_t.\n\nSimilarly to bpf_spin_lock, the offset of a bpf_refcount is cached in\nbtf_record as refcount_off in addition to being in the field array.\nCaching refcount_off makes sense for this field because further patches\nin the series will modify functions that take local kptrs (e.g.\nbpf_obj_drop) to change their behavior if the type they're operating on\nis refcounted. So enabling fast \"is this type refcounted?\" checks is\ndesirable.\n\nNo such verifier behavior changes are introduced in this patch, just\nlogic to recognize 'struct bpf_refcount' in btf_record.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-3-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-04-15 17:36:49 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/syscall.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "1512217c47f0e8ea076dd0e67262e5a668a78f01",
          "subject": "bpf: Support refcounted local kptrs in existing semantics",
          "message": "A local kptr is considered 'refcounted' when it is of a type that has a\nbpf_refcount field. When such a kptr is created, its refcount should be\ninitialized to 1; when destroyed, the object should be free'd only if a\nrefcount decr results in 0 refcount.\n\nExisting logic always frees the underlying memory when destroying a\nlocal kptr, and 0-initializes all btf_record fields. This patch adds\nchecks for \"is local kptr refcounted?\" and new logic for that case in\nthe appropriate places.\n\nThis patch focuses on changing existing semantics and thus conspicuously\ndoes _not_ provide a way for BPF programs in increment refcount. That\nfollows later in the series.\n\n__bpf_obj_drop_impl is modified to do the right thing when it sees a\nrefcounted type. Container types for graph nodes (list, tree, stashed in\nmap) are migrated to use __bpf_obj_drop_impl as a destructor for their\nnodes instead of each having custom destruction code in their _free\npaths. Now that \"drop\" isn't a synonym for \"free\" when the type is\nrefcounted it makes sense to centralize this logic.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-4-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-04-15 17:36:49 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "7c50b1cb76aca4540aa917db5f2a302acddcadff",
          "subject": "bpf: Add bpf_refcount_acquire kfunc",
          "message": "Currently, BPF programs can interact with the lifetime of refcounted\nlocal kptrs in the following ways:\n\n  bpf_obj_new  - Initialize refcount to 1 as part of new object creation\n  bpf_obj_drop - Decrement refcount and free object if it's 0\n  collection add - Pass ownership to the collection. No change to\n                   refcount but collection is responsible for\n\t\t   bpf_obj_dropping it\n\nIn order to be able to add a refcounted local kptr to multiple\ncollections we need to be able to increment the refcount and acquire a\nnew owning reference. This patch adds a kfunc, bpf_refcount_acquire,\nimplementing such an operation.\n\nbpf_refcount_acquire takes a refcounted local kptr and returns a new\nowning reference to the same underlying memory as the input. The input\ncan be either owning or non-owning. To reinforce why this is safe,\nconsider the following code snippets:\n\n  struct node *n = bpf_obj_new(typeof(*n)); // A\n  struct node *m = bpf_refcount_acquire(n); // B\n\nIn the above snippet, n will be alive with refcount=1 after (A), and\nsince nothing changes that state before (B), it's obviously safe. If\nn is instead added to some rbtree, we can still safely refcount_acquire\nit:\n\n  struct node *n = bpf_obj_new(typeof(*n));\n  struct node *m;\n\n  bpf_spin_lock(&glock);\n  bpf_rbtree_add(&groot, &n->node, less);   // A\n  m = bpf_refcount_acquire(n);              // B\n  bpf_spin_unlock(&glock);\n\nIn the above snippet, after (A) n is a non-owning reference, and after\n(B) m is an owning reference pointing to the same memory as n. Although\nn has no ownership of that memory's lifetime, it's guaranteed to be\nalive until the end of the critical section, and n would be clobbered if\nwe were past the end of the critical section, so it's safe to bump\nrefcount.\n\nImplementation details:\n\n* From verifier's perspective, bpf_refcount_acquire handling is similar\n  to bpf_obj_new and bpf_obj_drop. Like the former, it returns a new\n  owning reference matching input type, although like the latter, type\n  can be inferred from concrete kptr input. Verifier changes in\n  {check,fixup}_kfunc_call and check_kfunc_args are largely copied from\n  aforementioned functions' verifier changes.\n\n* An exception to the above is the new KF_ARG_PTR_TO_REFCOUNTED_KPTR\n  arg, indicated by new \"__refcounted_kptr\" kfunc arg suffix. This is\n  necessary in order to handle both owning and non-owning input without\n  adding special-casing to \"__alloc\" arg handling. Also a convenient\n  place to confirm that input type has bpf_refcount field.\n\n* The implemented kfunc is actually bpf_refcount_acquire_impl, with\n  'hidden' second arg that the verifier sets to the type's struct_meta\n  in fixup_kfunc_call.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-5-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-04-15 17:36:50 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "d2dcc67df910dd85253a701b6a5b747f955d28f5",
          "subject": "bpf: Migrate bpf_rbtree_add and bpf_list_push_{front,back} to possibly fail",
          "message": "Consider this code snippet:\n\n  struct node {\n    long key;\n    bpf_list_node l;\n    bpf_rb_node r;\n    bpf_refcount ref;\n  }\n\n  int some_bpf_prog(void *ctx)\n  {\n    struct node *n = bpf_obj_new(/*...*/), *m;\n\n    bpf_spin_lock(&glock);\n\n    bpf_rbtree_add(&some_tree, &n->r, /* ... */);\n    m = bpf_refcount_acquire(n);\n    bpf_rbtree_add(&other_tree, &m->r, /* ... */);\n\n    bpf_spin_unlock(&glock);\n\n    /* ... */\n  }\n\nAfter bpf_refcount_acquire, n and m point to the same underlying memory,\nand that node's bpf_rb_node field is being used by the some_tree insert,\nso overwriting it as a result of the second insert is an error. In order\nto properly support refcounted nodes, the rbtree and list insert\nfunctions must be allowed to fail. This patch adds such support.\n\nThe kfuncs bpf_rbtree_add, bpf_list_push_{front,back} are modified to\nreturn an int indicating success/failure, with 0 -> success, nonzero ->\nfailure.\n\nbpf_obj_drop on failure\n=======================\n\nCurrently the only reason an insert can fail is the example above: the\nbpf_{list,rb}_node is already in use. When such a failure occurs, the\ninsert kfuncs will bpf_obj_drop the input node. This allows the insert\noperations to logically fail without changing their verifier owning ref\nbehavior, namely the unconditional release_reference of the input\nowning ref.\n\nWith insert that always succeeds, ownership of the node is always passed\nto the collection, since the node always ends up in the collection.\n\nWith a possibly-failed insert w/ bpf_obj_drop, ownership of the node\nis always passed either to the collection (success), or to bpf_obj_drop\n(failure). Regardless, it's correct to continue unconditionally\nreleasing the input owning ref, as something is always taking ownership\nfrom the calling program on insert.\n\nKeeping owning ref behavior unchanged results in a nice default UX for\ninsert functions that can fail. If the program's reaction to a failed\ninsert is \"fine, just get rid of this owning ref for me and let me go\non with my business\", then there's no reason to check for failure since\nthat's default behavior. e.g.:\n\n  long important_failures = 0;\n\n  int some_bpf_prog(void *ctx)\n  {\n    struct node *n, *m, *o; /* all bpf_obj_new'd */\n\n    bpf_spin_lock(&glock);\n    bpf_rbtree_add(&some_tree, &n->node, /* ... */);\n    bpf_rbtree_add(&some_tree, &m->node, /* ... */);\n    if (bpf_rbtree_add(&some_tree, &o->node, /* ... */)) {\n      important_failures++;\n    }\n    bpf_spin_unlock(&glock);\n  }\n\nIf we instead chose to pass ownership back to the program on failed\ninsert - by returning NULL on success or an owning ref on failure -\nprograms would always have to do something with the returned ref on\nfailure. The most likely action is probably \"I'll just get rid of this\nowning ref and go about my business\", which ideally would look like:\n\n  if (n = bpf_rbtree_add(&some_tree, &n->node, /* ... */))\n    bpf_obj_drop(n);\n\nBut bpf_obj_drop isn't allowed in a critical section and inserts must\noccur within one, so in reality error handling would become a\nhard-to-parse mess.\n\nFor refcounted nodes, we can replicate the \"pass ownership back to\nprogram on failure\" logic with this patch's semantics, albeit in an ugly\nway:\n\n  struct node *n = bpf_obj_new(/* ... */), *m;\n\n  bpf_spin_lock(&glock);\n\n  m = bpf_refcount_acquire(n);\n  if (bpf_rbtree_add(&some_tree, &n->node, /* ... */)) {\n    /* Do something with m */\n  }\n\n  bpf_spin_unlock(&glock);\n  bpf_obj_drop(m);\n\nbpf_refcount_acquire is used to simulate \"return owning ref on failure\".\nThis should be an uncommon occurrence, though.\n\nAddition of two verifier-fixup'd args to collection inserts\n===========================================================\n\nThe actual bpf_obj_drop kfunc is\nbpf_obj_drop_impl(void *, struct btf_struct_meta *), with bpf_obj_drop\nmacro populating the second arg with 0 and the verifier later filling in\nthe arg during insn fixup.\n\nBecause bpf_rbtree_add and bpf_list_push_{front,back} now might do\nbpf_obj_drop, these kfuncs need a btf_struct_meta parameter that can be\npassed to bpf_obj_drop_impl.\n\nSimilarly, because the 'node' param to those insert functions is the\nbpf_{list,rb}_node within the node type, and bpf_obj_drop expects a\npointer to the beginning of the node, the insert functions need to be\nable to find the beginning of the node struct. A second\nverifier-populated param is necessary: the offset of {list,rb}_node within the\nnode type.\n\nThese two new params allow the insert kfuncs to correctly call\n__bpf_obj_drop_impl:\n\n  beginning_of_node = bpf_rb_node_ptr - offset\n  if (already_inserted)\n    __bpf_obj_drop_impl(beginning_of_node, btf_struct_meta->record);\n\nSimilarly to other kfuncs with \"hidden\" verifier-populated params, the\ninsert functions are renamed with _impl prefix and a macro is provided\nfor common usage. For example, bpf_rbtree_add kfunc is now\nbpf_rbtree_add_impl and bpf_rbtree_add is now a macro which sets\n\"hidden\" args to 0.\n\nDue to the two new args BPF progs will need to be recompiled to work\nwith the new _impl kfuncs.\n\nThis patch also rewrites the \"hidden argument\" explanation to more\ndirectly say why the BPF program writer doesn't need to populate the\narguments with anything meaningful.\n\nHow does this new logic affect non-owning references?\n=====================================================\n\nCurrently, non-owning refs are valid until the end of the critical\nsection in which they're created. We can make this guarantee because, if\na non-owning ref exists, the referent was added to some collection. The\ncollection will drop() its nodes when it goes away, but it can't go away\nwhile our program is accessing it, so that's not a problem. If the\nreferent is removed from the collection in the same CS that it was added\nin, it can't be bpf_obj_drop'd until after CS end. Those are the only\ntwo ways to free the referent's memory and neither can happen until\nafter the non-owning ref's lifetime ends.\n\nOn first glance, having these collection insert functions potentially\nbpf_obj_drop their input seems like it breaks the \"can't be\nbpf_obj_drop'd until after CS end\" line of reasoning. But we care about\nthe memory not being _freed_ until end of CS end, and a previous patch\nin the series modified bpf_obj_drop such that it doesn't free refcounted\nnodes until refcount == 0. So the statement can be more accurately\nrewritten as \"can't be free'd until after CS end\".\n\nWe can prove that this rewritten statement holds for any non-owning\nreference produced by collection insert functions:\n\n* If the input to the insert function is _not_ refcounted\n  * We have an owning reference to the input, and can conclude it isn't\n    in any collection\n    * Inserting a node in a collection turns owning refs into\n      non-owning, and since our input type isn't refcounted, there's no\n      way to obtain additional owning refs to the same underlying\n      memory\n  * Because our node isn't in any collection, the insert operation\n    cannot fail, so bpf_obj_drop will not execute\n  * If bpf_obj_drop is guaranteed not to execute, there's no risk of\n    memory being free'd\n\n* Otherwise, the input to the insert function is refcounted\n  * If the insert operation fails due to the node's list_head or rb_root\n    already being in some collection, there was some previous successful\n    insert which passed refcount to the collection\n  * We have an owning reference to the input, it must have been\n    acquired via bpf_refcount_acquire, which bumped the refcount\n  * refcount must be >= 2 since there's a valid owning reference and the\n    node is already in a collection\n  * Insert triggering bpf_obj_drop will decr refcount to >= 1, never\n    resulting in a free\n\nSo although we may do bpf_obj_drop during the critical section, this\nwill never result in memory being free'd, and no changes to non-owning\nref logic are needed in this patch.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-6-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-04-15 17:36:50 -0700",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/bpf_experimental.h"
          ]
        },
        {
          "hash": "de67ba3968fa1455e8020b21e5ccc2bb48b9a852",
          "subject": "selftests/bpf: Modify linked_list tests to work with macro-ified inserts",
          "message": "The linked_list tests use macros and function pointers to reduce code\nduplication. Earlier in the series, bpf_list_push_{front,back} were\nmodified to be macros, expanding to invoke actual kfuncs\nbpf_list_push_{front,back}_impl. Due to this change, a code snippet\nlike:\n\n  void (*p)(void *, void *) = (void *)&bpf_list_##op;\n  p(hexpr, nexpr);\n\nmeant to do bpf_list_push_{front,back}(hexpr, nexpr), will no longer\nwork as it's no longer valid to do &bpf_list_push_{front,back} since\nthey're no longer functions.\n\nThis patch fixes issues of this type, along with two other minor changes\n- one improvement and one fix - both related to the node argument to\nlist_push_{front,back}.\n\n  * The fix: migration of list_push tests away from (void *, void *)\n    func ptr uncovered that some tests were incorrectly passing pointer\n    to node, not pointer to struct bpf_list_node within the node. This\n    patch fixes such issues (CHECK(..., f) -> CHECK(..., &f->node))\n\n  * The improvement: In linked_list tests, the struct foo type has two\n    list_node fields: node and node2, at byte offsets 0 and 40 within\n    the struct, respectively. Currently node is used in ~all tests\n    involving struct foo and lists. The verifier needs to do some work\n    to account for the offset of bpf_list_node within the node type, so\n    using node2 instead of node exercises that logic more in the tests.\n    This patch migrates linked_list tests to use node2 instead of node.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-7-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-04-15 17:36:50 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/linked_list.c",
            "tools/testing/selftests/bpf/progs/linked_list.c",
            "tools/testing/selftests/bpf/progs/linked_list.h",
            "tools/testing/selftests/bpf/progs/linked_list_fail.c"
          ]
        },
        {
          "hash": "404ad75a36fb1a1008e9fe803aa7d0212df9e240",
          "subject": "bpf: Migrate bpf_rbtree_remove to possibly fail",
          "message": "This patch modifies bpf_rbtree_remove to account for possible failure\ndue to the input rb_node already not being in any collection.\nThe function can now return NULL, and does when the aforementioned\nscenario occurs. As before, on successful removal an owning reference to\nthe removed node is returned.\n\nAdding KF_RET_NULL to bpf_rbtree_remove's kfunc flags - now KF_RET_NULL |\nKF_ACQUIRE - provides the desired verifier semantics:\n\n  * retval must be checked for NULL before use\n  * if NULL, retval's ref_obj_id is released\n  * retval is a \"maybe acquired\" owning ref, not a non-owning ref,\n    so it will live past end of critical section (bpf_spin_unlock), and\n    thus can be checked for NULL after the end of the CS\n\nBPF programs must add checks\n============================\n\nThis does change bpf_rbtree_remove's verifier behavior. BPF program\nwriters will need to add NULL checks to their programs, but the\nresulting UX looks natural:\n\n  bpf_spin_lock(&glock);\n\n  n = bpf_rbtree_first(&ghead);\n  if (!n) { /* ... */}\n  res = bpf_rbtree_remove(&ghead, &n->node);\n\n  bpf_spin_unlock(&glock);\n\n  if (!res)  /* Newly-added check after this patch */\n    return 1;\n\n  n = container_of(res, /* ... */);\n  /* Do something else with n */\n  bpf_obj_drop(n);\n  return 0;\n\nThe \"if (!res)\" check above is the only addition necessary for the above\nprogram to pass verification after this patch.\n\nbpf_rbtree_remove no longer clobbers non-owning refs\n====================================================\n\nAn issue arises when bpf_rbtree_remove fails, though. Consider this\nexample:\n\n  struct node_data {\n    long key;\n    struct bpf_list_node l;\n    struct bpf_rb_node r;\n    struct bpf_refcount ref;\n  };\n\n  long failed_sum;\n\n  void bpf_prog()\n  {\n    struct node_data *n = bpf_obj_new(/* ... */);\n    struct bpf_rb_node *res;\n    n->key = 10;\n\n    bpf_spin_lock(&glock);\n\n    bpf_list_push_back(&some_list, &n->l); /* n is now a non-owning ref */\n    res = bpf_rbtree_remove(&some_tree, &n->r, /* ... */);\n    if (!res)\n      failed_sum += n->key;  /* not possible */\n\n    bpf_spin_unlock(&glock);\n    /* if (res) { do something useful and drop } ... */\n  }\n\nThe bpf_rbtree_remove in this example will always fail. Similarly to\nbpf_spin_unlock, bpf_rbtree_remove is a non-owning reference\ninvalidation point. The verifier clobbers all non-owning refs after a\nbpf_rbtree_remove call, so the \"failed_sum += n->key\" line will fail\nverification, and in fact there's no good way to get information about\nthe node which failed to add after the invalidation. This patch removes\nnon-owning reference invalidation from bpf_rbtree_remove to allow the\nabove usecase to pass verification. The logic for why this is now\npossible is as follows:\n\nBefore this series, bpf_rbtree_add couldn't fail and thus assumed that\nits input, a non-owning reference, was in the tree. But it's easy to\nconstruct an example where two non-owning references pointing to the same\nunderlying memory are acquired and passed to rbtree_remove one after\nanother (see rbtree_api_release_aliasing in\nselftests/bpf/progs/rbtree_fail.c).\n\nSo it was necessary to clobber non-owning refs to prevent this\ncase and, more generally, to enforce \"non-owning ref is definitely\nin some collection\" invariant. This series removes that invariant and\nthe failure / runtime checking added in this patch provide a clean way\nto deal with the aliasing issue - just fail to remove.\n\nBecause the aliasing issue prevented by clobbering non-owning refs is no\nlonger an issue, this patch removes the invalidate_non_owning_refs\ncall from verifier handling of bpf_rbtree_remove. Note that\nbpf_spin_unlock - the other caller of invalidate_non_owning_refs -\nclobbers non-owning refs for a different reason, so its clobbering\nbehavior remains unchanged.\n\nNo BPF program changes are necessary for programs to remain valid as a\nresult of this clobbering change. A valid program before this patch\npassed verification with its non-owning refs having shorter (or equal)\nlifetimes due to more aggressive clobbering.\n\nAlso, update existing tests to check bpf_rbtree_remove retval for NULL\nwhere necessary, and move rbtree_api_release_aliasing from\nprogs/rbtree_fail.c to progs/rbtree.c since it's now expected to pass\nverification.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-8-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-04-15 17:36:50 -0700",
          "modified_files": [
            "kernel/bpf/btf.c",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/prog_tests/linked_list.c",
            "tools/testing/selftests/bpf/prog_tests/rbtree.c",
            "tools/testing/selftests/bpf/progs/rbtree.c",
            "tools/testing/selftests/bpf/progs/rbtree_fail.c"
          ]
        },
        {
          "hash": "3e81740a90626024a9d9c6f9bfa3d36204dafefb",
          "subject": "bpf: Centralize btf_field-specific initialization logic",
          "message": "All btf_fields in an object are 0-initialized by memset in\nbpf_obj_init. This might not be a valid initial state for some field\ntypes, in which case kfuncs that use the type will properly initialize\ntheir input if it's been 0-initialized. Some BPF graph collection types\nand kfuncs do this: bpf_list_{head,node} and bpf_rb_node.\n\nAn earlier patch in this series added the bpf_refcount field, for which\nthe 0 state indicates that the refcounted object should be free'd.\nbpf_obj_init treats this field specially, setting refcount to 1 instead\nof relying on scattered \"refcount is 0? Must have just been initialized,\nlet's set to 1\" logic in kfuncs.\n\nThis patch extends this treatment to list and rbtree field types,\nallowing most scattered initialization logic in kfuncs to be removed.\n\nNote that bpf_{list_head,rb_root} may be inside a BPF map, in which case\nthey'll be 0-initialized without passing through the newly-added logic,\nso scattered initialization logic must remain for these collection root\ntypes.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-9-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-04-15 17:36:50 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "6147f15131e2df544a5449815f456da48c0c88e7",
          "subject": "selftests/bpf: Add refcounted_kptr tests",
          "message": "Test refcounted local kptr functionality added in previous patches in\nthe series.\n\nUsecases which pass verification:\n\n* Add refcounted local kptr to both tree and list. Then, read and -\n  possibly, depending on test variant - delete from tree, then list.\n  * Also test doing read-and-maybe-delete in opposite order\n* Stash a refcounted local kptr in a map_value, then add it to a\n  rbtree. Read from both, possibly deleting after tree read.\n* Add refcounted local kptr to both tree and list. Then, try reading and\n  deleting twice from one of the collections.\n* bpf_refcount_acquire of just-added non-owning ref should work, as\n  should bpf_refcount_acquire of owning ref just out of bpf_obj_new\n\nUsecases which fail verification:\n\n* The simple successful bpf_refcount_acquire cases from above should\n  both fail to verify if the newly-acquired owning ref is not dropped\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-10-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-04-15 17:36:50 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/refcounted_kptr.c",
            "tools/testing/selftests/bpf/progs/refcounted_kptr.c",
            "tools/testing/selftests/bpf/progs/refcounted_kptr_fail.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "255f0e14b9b0f8afe53dd6607c7b8ba489100a66",
      "merge_subject": "Merge branch 'bpf-verifier-log-rotation'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\nThis patch set changes BPF verifier log behavior to behave as a rotating log,\nby default. If user-supplied log buffer is big enough to contain entire\nverifier log output, there is no effective difference. But where previously\nuser supplied too small log buffer and would get -ENOSPC error result and the\nbeginning part of the verifier log, now there will be no error and user will\nget ending part of verifier log filling up user-supplied log buffer.  Which\nis, in absolute majority of cases, is exactly what's useful, relevant, and\nwhat users want and need, as the ending of the verifier log is containing\ndetails of verifier failure and relevant state that got us to that failure.\nSo this rotating mode is made default, but for some niche advanced debugging\nscenarios it's possible to request old behavior by specifying additional\nBPF_LOG_FIXED (8) flag.\n\nThis patch set adjusts libbpf to allow specifying flags beyond 1 | 2 | 4. We\nalso add --log-size and --log-fixed options to veristat to be able to both\ntest this functionality manually, but also to be used in various debugging\nscenarios. We also add selftests that tries many variants of log buffer size\nto stress-test correctness of internal verifier log bookkeeping code.\n\nFurther, this patch set is merged with log_size_actual v1 patchset ([0]),\nwhich adds ability to get required log buffer size to fit entire verifier\nlog output.\n\nThis addresses a long-standing limitation, which causes users and BPF loader\nlibrary writers to guess and pre-size log buffer, often allocating unnecessary\nextra memory for this or doing extra program verifications just to size logs\nbetter, ultimately wasting resources. This was requested most recently by Go\nBPF library maintainers ([1]).\n\nSee respective patches for details. A bunch of them some drive-by fixes\ndetecting during working with the code. Some other further refactor and\ncompratmentalize verifier log handling code into kernel/bpf/log.c, which\nshould also make it simpler to integrate such verbose log for other\ncomplicated bpf() syscall commands, if necessary. The rest are actual logic\nto calculate maximum log buffer size needed and return it to user-space.\nFew patches wire this on libbpf side, and the rest add selftests to test\nproper log truncation and log_buf==NULL handling.\n\nThis turned into a pretty sizable patch set with lots of arithmetics, but\nhopefully the set of features added to verifier log in this patch set are\nboth useful for BPF users and are self-contained and isolated enough to not\ncause troubles going forward.\n\nv3->v4:\n  - s/log_size_actual/log_true_size/ (Alexei);\n  - log_buf==NULL && log_size==0 don't trigger -ENOSPC (Lorenz);\n  - added WARN_ON_ONCE if we try bpf_vlog_reset() forward (Lorenz);\n  - added selftests for truncation in BPF_LOG_FIXED mode;\n  - fixed edge case in BPF_LOG_FIXED when log_size==1, leaving buf not zero\n    terminated;\nv2->v3:\n  - typos and comment improvement (Lorenz);\n  - merged with log_size_actual v1 ([0]) patch set (Alexei);\n  - added log_buf==NULL condition allowed (Lorenz);\n  - added BPF_BTF_LOAD logs tests (Lorenz);\n  - more clean up and refactoring of internal verifier log API;\nv1->v2:\n  - return -ENOSPC even in rotating log mode for preserving backwards\n    compatibility (Lorenz);\n\n  [0] https://patchwork.kernel.org/project/netdevbpf/list/?series=735213&state=*\n  [1] https://lore.kernel.org/bpf/CAN+4W8iNoEbQzQVbB_o1W0MWBDV4xCJAq7K3f6psVE-kkCfMqg@mail.gmail.com/\n====================\n\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>",
      "merge_author": "Daniel Borkmann <daniel@iogearbox.net>",
      "merge_date": "2023-04-11 18:06:03 +0200",
      "commits": [
        {
          "hash": "4294a0a7ab6282c3d92f03de84e762dda993c93d",
          "subject": "bpf: Split off basic BPF verifier log into separate file",
          "message": "kernel/bpf/verifier.c file is large and growing larger all the time. So\nit's good to start splitting off more or less self-contained parts into\nseparate files to keep source code size (somewhat) somewhat under\ncontrol.\n\nThis patch is a one step in this direction, moving some of BPF verifier log\nroutines into a separate kernel/bpf/log.c. Right now it's most low-level\nand isolated routines to append data to log, reset log to previous\nposition, etc. Eventually we could probably move verifier state\nprinting logic here as well, but this patch doesn't attempt to do that\nyet.\n\nSubsequent patches will add more logic to verifier log management, so\nhaving basics in a separate file will make sure verifier.c doesn't grow\nmore with new changes.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-2-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:42 +0200",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/Makefile",
            "kernel/bpf/log.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "03cc3aa6a53394481f01c16231f99298332066f9",
          "subject": "bpf: Remove minimum size restrictions on verifier log buffer",
          "message": "It's not clear why we have 128 as minimum size, but it makes testing\nharder and seems unnecessary, as we carefully handle truncation\nscenarios and use proper snprintf variants. So remove this limitation\nand just enforce positive length for log buffer.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-3-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:42 +0200",
          "modified_files": [
            "kernel/bpf/log.c"
          ]
        },
        {
          "hash": "1216640938035e63bdbd32438e91c9bcc1fd8ee1",
          "subject": "bpf: Switch BPF verifier log to be a rotating log by default",
          "message": "Currently, if user-supplied log buffer to collect BPF verifier log turns\nout to be too small to contain full log, bpf() syscall returns -ENOSPC,\nfails BPF program verification/load, and preserves first N-1 bytes of\nthe verifier log (where N is the size of user-supplied buffer).\n\nThis is problematic in a bunch of common scenarios, especially when\nworking with real-world BPF programs that tend to be pretty complex as\nfar as verification goes and require big log buffers. Typically, it's\nwhen debugging tricky cases at log level 2 (verbose). Also, when BPF program\nis successfully validated, log level 2 is the only way to actually see\nverifier state progression and all the important details.\n\nEven with log level 1, it's possible to get -ENOSPC even if the final\nverifier log fits in log buffer, if there is a code path that's deep\nenough to fill up entire log, even if normally it would be reset later\non (there is a logic to chop off successfully validated portions of BPF\nverifier log).\n\nIn short, it's not always possible to pre-size log buffer. Also, what's\nworse, in practice, the end of the log most often is way more important\nthan the beginning, but verifier stops emitting log as soon as initial\nlog buffer is filled up.\n\nThis patch switches BPF verifier log behavior to effectively behave as\nrotating log. That is, if user-supplied log buffer turns out to be too\nshort, verifier will keep overwriting previously written log,\neffectively treating user's log buffer as a ring buffer. -ENOSPC is\nstill going to be returned at the end, to notify user that log contents\nwas truncated, but the important last N bytes of the log would be\nreturned, which might be all that user really needs. This consistent\n-ENOSPC behavior, regardless of rotating or fixed log behavior, allows\nto prevent backwards compatibility breakage. The only user-visible\nchange is which portion of verifier log user ends up seeing *if buffer\nis too small*. Given contents of verifier log itself is not an ABI,\nthere is no breakage due to this behavior change. Specialized tools that\nrely on specific contents of verifier log in -ENOSPC scenario are\nexpected to be easily adapted to accommodate old and new behaviors.\n\nImportantly, though, to preserve good user experience and not require\nevery user-space application to adopt to this new behavior, before\nexiting to user-space verifier will rotate log (in place) to make it\nstart at the very beginning of user buffer as a continuous\nzero-terminated string. The contents will be a chopped off N-1 last\nbytes of full verifier log, of course.\n\nGiven beginning of log is sometimes important as well, we add\nBPF_LOG_FIXED (which equals 8) flag to force old behavior, which allows\ntools like veristat to request first part of verifier log, if necessary.\nBPF_LOG_FIXED flag is also a simple and straightforward way to check if\nBPF verifier supports rotating behavior.\n\nOn the implementation side, conceptually, it's all simple. We maintain\n64-bit logical start and end positions. If we need to truncate the log,\nstart position will be adjusted accordingly to lag end position by\nN bytes. We then use those logical positions to calculate their matching\nactual positions in user buffer and handle wrap around the end of the\nbuffer properly. Finally, right before returning from bpf_check(), we\nrotate user log buffer contents in-place as necessary, to make log\ncontents contiguous. See comments in relevant functions for details.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nReviewed-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-4-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:43 +0200",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/log.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/prog_tests/log_fixup.c"
          ]
        },
        {
          "hash": "e0aee1facccf9f12136600031be4ce21eb810a78",
          "subject": "libbpf: Don't enforce unnecessary verifier log restrictions on libbpf side",
          "message": "This basically prevents any forward compatibility. And we either way\njust return -EINVAL, which would otherwise be returned from bpf()\nsyscall anyways.\n\nSimilarly, drop enforcement of non-NULL log_buf when log_level > 0. This\nwon't be true anymore soon.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-5-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:43 +0200",
          "modified_files": [
            "tools/lib/bpf/bpf.c"
          ]
        },
        {
          "hash": "d0d75c67c45abd3930967dcafc82fd4505400665",
          "subject": "veristat: Add more veristat control over verifier log options",
          "message": "Add --log-size to be able to customize log buffer sent to bpf() syscall\nfor BPF program verification logging.\n\nAdd --log-fixed to enforce BPF_LOG_FIXED behavior for BPF verifier log.\nThis is useful in unlikely event that beginning of truncated verifier\nlog is more important than the end of it (which with rotating verifier\nlog behavior is the default now).\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-6-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:43 +0200",
          "modified_files": [
            "tools/testing/selftests/bpf/veristat.c"
          ]
        },
        {
          "hash": "b1a7a480a1120d4f70305f5e8859f527e0efe4a5",
          "subject": "selftests/bpf: Add fixed vs rotating verifier log tests",
          "message": "Add selftests validating BPF_LOG_FIXED behavior, which used to be the\nonly behavior, and now default rotating BPF verifier log, which returns\njust up to last N bytes of full verifier log, instead of returning\n-ENOSPC.\n\nTo stress test correctness of in-kernel verifier log logic, we force it\nto truncate program's verifier log to all lengths from 1 all the way to\nits full size (about 450 bytes today). This was a useful stress test\nwhile developing the feature.\n\nFor both fixed and rotating log modes we expect -ENOSPC if log contents\ndoesn't fit in user-supplied log buffer.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-7-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:43 +0200",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier_log.c"
          ]
        },
        {
          "hash": "24bc80887adb4d6fc0057d4f14fabeaa4502b2a0",
          "subject": "bpf: Ignore verifier log reset in BPF_LOG_KERNEL mode",
          "message": "Verifier log position reset is meaningless in BPF_LOG_KERNEL mode, so\njust exit early in bpf_vlog_reset() if log->level is BPF_LOG_KERNEL.\n\nThis avoid meaningless put_user() into NULL log->ubuf.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-8-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:43 +0200",
          "modified_files": [
            "kernel/bpf/log.c"
          ]
        },
        {
          "hash": "971fb5057d787d0a7e7c8cb910207c82e2db920e",
          "subject": "bpf: Fix missing -EFAULT return on user log buf error in btf_parse()",
          "message": "btf_parse() is missing -EFAULT error return if log->ubuf was NULL-ed out\ndue to error while copying data into user-provided buffer. Add it, but\nhandle a special case of BPF_LOG_KERNEL in which log->ubuf is always NULL.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-9-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:43 +0200",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "cbedb42a0da3bb48819b2200af4b4cb5d922c518",
          "subject": "bpf: Avoid incorrect -EFAULT error in BPF_LOG_KERNEL mode",
          "message": "If verifier log is in BPF_LOG_KERNEL mode, no log->ubuf is expected and\nit stays NULL throughout entire verification process. Don't erroneously\nreturn -EFAULT in such case.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-10-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:43 +0200",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "8a6ca6bc553e3c878fa53c506bc6ec281efdc039",
          "subject": "bpf: Simplify logging-related error conditions handling",
          "message": "Move log->level == 0 check into bpf_vlog_truncated() instead of doing it\nexplicitly. Also remove unnecessary goto in kernel/bpf/verifier.c.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-11-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:43 +0200",
          "modified_files": [
            "kernel/bpf/btf.c",
            "kernel/bpf/log.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "fa1c7d5cc404ac3b6e6b4ab6d00b07c76bd819be",
          "subject": "bpf: Keep track of total log content size in both fixed and rolling modes",
          "message": "Change how we do accounting in BPF_LOG_FIXED mode and adopt log->end_pos\nas *logical* log position. This means that we can go beyond physical log\nbuffer size now and be able to tell what log buffer size should be to\nfit entire log contents without -ENOSPC.\n\nTo do this for BPF_LOG_FIXED mode, we need to remove a short-circuiting\nlogic of not vsnprintf()'ing further log content once we filled up\nuser-provided buffer, which is done by bpf_verifier_log_needed() checks.\nWe modify these checks to always keep going if log->level is non-zero\n(i.e., log is requested), even if log->ubuf was NULL'ed out due to\ncopying data to user-space, or if entire log buffer is physically full.\nWe adopt bpf_verifier_vlog() routine to work correctly with\nlog->ubuf == NULL condition, performing log formatting into temporary\nkernel buffer, doing all the necessary accounting, but just avoiding\ncopying data out if buffer is full or NULL'ed out.\n\nWith these changes, it's now possible to do this sort of determination of\nlog contents size in both BPF_LOG_FIXED and default rolling log mode.\nWe need to keep in mind bpf_vlog_reset(), though, which shrinks log\ncontents after successful verification of a particular code path. This\nlog reset means that log->end_pos isn't always increasing, so to return\nback to users what should be the log buffer size to fit all log content\nwithout causing -ENOSPC even in the presence of log resetting, we need\nto keep maximum over \"lifetime\" of logging. We do this accounting in\nbpf_vlog_update_len_max() helper.\n\nA related and subtle aspect is that with this logical log->end_pos even in\nBPF_LOG_FIXED mode we could temporary \"overflow\" buffer, but then reset\nit back with bpf_vlog_reset() to a position inside user-supplied\nlog_buf. In such situation we still want to properly maintain\nterminating zero. We will eventually return -ENOSPC even if final log\nbuffer is small (we detect this through log->len_max check). This\nbehavior is simpler to reason about and is consistent with current\nbehavior of verifier log. Handling of this required a small addition to\nbpf_vlog_reset() logic to avoid doing put_user() beyond physical log\nbuffer dimensions.\n\nAnother issue to keep in mind is that we limit log buffer size to 32-bit\nvalue and keep such log length as u32, but theoretically verifier could\nproduce huge log stretching beyond 4GB. Instead of keeping (and later\nreturning) 64-bit log length, we cap it at UINT_MAX. Current UAPI makes\nit impossible to specify log buffer size bigger than 4GB anyways, so we\ndon't really loose anything here and keep everything consistently 32-bit\nin UAPI. This property will be utilized in next patch.\n\nDoing the same determination of maximum log buffer for rolling mode is\ntrivial, as log->end_pos and log->start_pos are already logical\npositions, so there is nothing new there.\n\nThese changes do incidentally fix one small issue with previous logging\nlogic. Previously, if use provided log buffer of size N, and actual log\noutput was exactly N-1 bytes + terminating \\0, kernel logic coun't\ndistinguish this condition from log truncation scenario which would end\nup with truncated log contents of N-1 bytes + terminating \\0 as well.\n\nBut now with log->end_pos being logical position that could go beyond\nactual log buffer size, we can distinguish these two conditions, which\nwe do in this patch. This plays nicely with returning log_size_actual\n(implemented in UAPI in the next patch), as we can now guarantee that if\nuser takes such log_size_actual and provides log buffer of that exact\nsize, they will not get -ENOSPC in return.\n\nAll in all, all these changes do conceptually unify fixed and rolling\nlog modes much better, and allow a nice feature requested by users:\nknowing what should be the size of the buffer to avoid -ENOSPC.\n\nWe'll plumb this through the UAPI and the code in the next patch.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-12-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:43 +0200",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/log.c"
          ]
        },
        {
          "hash": "47a71c1f9af0a334c9dfa97633c41de4feda4287",
          "subject": "bpf: Add log_true_size output field to return necessary log buffer size",
          "message": "Add output-only log_true_size and btf_log_true_size field to\nBPF_PROG_LOAD and BPF_BTF_LOAD commands, respectively. It will return\nthe size of log buffer necessary to fit in all the log contents at\nspecified log_level. This is very useful for BPF loader libraries like\nlibbpf to be able to size log buffer correctly, but could be used by\nusers directly, if necessary, as well.\n\nThis patch plumbs all this through the code, taking into account actual\nbpf_attr size provided by user to determine if these new fields are\nexpected by users. And if they are, set them from kernel on return.\n\nWe refactory btf_parse() function to accommodate this, moving attr and\nuattr handling inside it. The rest is very straightforward code, which\nis split from the logging accounting changes in the previous patch to\nmake it simpler to review logic vs UAPI changes.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-13-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:43 +0200",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/btf.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "bdcab4144f5da97cc0fa7e1dd63b8475e10c8f0a",
          "subject": "bpf: Simplify internal verifier log interface",
          "message": "Simplify internal verifier log API down to bpf_vlog_init() and\nbpf_vlog_finalize(). The former handles input arguments validation in\none place and makes it easier to change it. The latter subsumes -ENOSPC\n(truncation) and -EFAULT handling and simplifies both caller's code\n(bpf_check() and btf_parse()).\n\nFor btf_parse(), this patch also makes sure that verifier log\nfinalization happens even if there is some error condition during BTF\nverification process prior to normal finalization step.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-14-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:44 +0200",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/log.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "fac08d45e2531f91d8fb3d11fc6576f588049476",
          "subject": "bpf: Relax log_buf NULL conditions when log_level>0 is requested",
          "message": "Drop the log_size>0 and log_buf!=NULL condition when log_level>0. This\nallows users to request log_true_size of a full log without providing\nactual (even if small) log buffer. Verifier log handling code was mostly\nready to handle NULL log->ubuf, so only few small changes were necessary\nto prevent NULL log->ubuf from causing problems.\n\nNote, that if user provided NULL log_buf with log_level>0 we don't\nconsider this a log truncation, and thus won't return -ENOSPC.\n\nWe also enforce that either (log_buf==NULL && log_size==0) or\n(log_buf!=NULL && log_size>0).\n\nSuggested-by: Lorenz Bauer <lmb@isovalent.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nReviewed-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-15-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:44 +0200",
          "modified_files": [
            "kernel/bpf/log.c"
          ]
        },
        {
          "hash": "94e55c0fdaf4268bdda2d3b375bc61daba056e85",
          "subject": "libbpf: Wire through log_true_size returned from kernel for BPF_PROG_LOAD",
          "message": "Add output-only log_true_size field to bpf_prog_load_opts to return\nbpf_attr->log_true_size value back from bpf() syscall.\n\nNote, that we have to drop const modifier from opts in bpf_prog_load().\nThis could potentially cause compilation error for some users. But\nthe usual practice is to define bpf_prog_load_ops\nas a local variable next to bpf_prog_load() call and pass pointer to it,\nso const vs non-const makes no difference and won't even come up in most\n(if not all) cases.\n\nThere are no runtime and ABI backwards/forward compatibility issues at all.\nIf user provides old struct bpf_prog_load_opts, libbpf won't set new\nfields. If old libbpf is provided new bpf_prog_load_opts, nothing will\nhappen either as old libbpf doesn't yet know about this new field.\n\nAdding a new variant of bpf_prog_load() just for this seems like a big\nand unnecessary overkill. As a corroborating evidence is the fact that\nentire selftests/bpf code base required not adjustment whatsoever.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-16-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:44 +0200",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/bpf.h"
          ]
        },
        {
          "hash": "097d8002b754a865beef880e5c1cdc3ef7c2163d",
          "subject": "libbpf: Wire through log_true_size for bpf_btf_load() API",
          "message": "Similar to what we did for bpf_prog_load() in previous patch, wire\nreturning of log_true_size value from kernel back to the user through\nOPTS out field.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-17-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:44 +0200",
          "modified_files": [
            "tools/lib/bpf/bpf.c",
            "tools/lib/bpf/bpf.h"
          ]
        },
        {
          "hash": "5787540827a9e2cdecf38166e648b2924a57443f",
          "subject": "selftests/bpf: Add tests to validate log_true_size feature",
          "message": "Add additional test cases validating that log_true_size is consistent\nbetween fixed and rotating log modes, and that log_true_size can be\nused *exactly* without causing -ENOSPC, while using just 1 byte shorter\nlog buffer would cause -ENOSPC.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-18-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:44 +0200",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier_log.c"
          ]
        },
        {
          "hash": "be983f44274f575e42025130e3c62b8718b0a29a",
          "subject": "selftests/bpf: Add testing of log_buf==NULL condition for BPF_PROG_LOAD",
          "message": "Add few extra test conditions to validate that it's ok to pass\nlog_buf==NULL and log_size==0 to BPF_PROG_LOAD command with the intent\nto get log_true_size without providing a buffer.\n\nTest that log_buf==NULL condition *does not* return -ENOSPC.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-19-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:44 +0200",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier_log.c"
          ]
        },
        {
          "hash": "054b6c7866c7a2537fffd4aa12d88aac47db60f9",
          "subject": "selftests/bpf: Add verifier log tests for BPF_BTF_LOAD command",
          "message": "Add verifier log tests for BPF_BTF_LOAD command, which are very similar,\nconceptually, to BPF_PROG_LOAD tests. These are two separate commands\ndealing with verbose verifier log, so should be both tested separately.\n\nTest that log_buf==NULL condition *does not* return -ENOSPC.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-20-andrii@kernel.org",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-04-11 18:05:44 +0200",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier_log.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "4daf0b327f2032ecacd2fb00c46999619f530972",
      "merge_subject": "Merge branch 'bpf: Improve verifier for cond_op and spilled loop index variables'",
      "merge_body": "Yonghong Song says:\n\n====================\n\nLLVM commit [1] introduced hoistMinMax optimization like\n  (i < VIRTIO_MAX_SGS) && (i < out_sgs)\nto\n  upper = MIN(VIRTIO_MAX_SGS, out_sgs)\n  ... i < upper ...\nand caused the verification failure. Commit [2] workarounded the issue by\nadding some bpf assembly code to prohibit the above optimization.\nThis patch improved verifier such that verification can succeed without\nthe above workaround.\n\nWithout [2], the current verifier will hit the following failures:\n  ...\n  119: (15) if r1 == 0x0 goto pc+1\n  The sequence of 8193 jumps is too complex.\n  verification time 525829 usec\n  stack depth 64\n  processed 156616 insns (limit 1000000) max_states_per_insn 8 total_states 1754 peak_states 1712 mark_read 12\n  -- END PROG LOAD LOG --\n  libbpf: prog 'trace_virtqueue_add_sgs': failed to load: -14\n  libbpf: failed to load object 'loop6.bpf.o'\n  ...\nThe failure is due to verifier inadequately handling '<const> <cond_op> <non_const>' which will\ngo through both pathes and generate the following verificaiton states:\n  ...\n  89: (07) r2 += 1                      ; R2_w=5\n  90: (79) r8 = *(u64 *)(r10 -48)       ; R8_w=scalar() R10=fp0\n  91: (79) r1 = *(u64 *)(r10 -56)       ; R1_w=scalar(umax=5,var_off=(0x0; 0x7)) R10=fp0\n  92: (ad) if r2 < r1 goto pc+41        ; R0_w=scalar() R1_w=scalar(umin=6,umax=5,var_off=(0x4; 0x3))\n      R2_w=5 R6_w=scalar(id=385) R7_w=0 R8_w=scalar() R9_w=scalar(umax=21474836475,var_off=(0x0; 0x7ffffffff))\n      R10=fp0 fp-8=mmmmmmmm fp-16=mmmmmmmm fp-24=mmmm???? fp-32= fp-40_w=4 fp-48=mmmmmmmm fp-56= fp-64=mmmmmmmm\n  ...\n  89: (07) r2 += 1                      ; R2_w=6\n  90: (79) r8 = *(u64 *)(r10 -48)       ; R8_w=scalar() R10=fp0\n  91: (79) r1 = *(u64 *)(r10 -56)       ; R1_w=scalar(umax=5,var_off=(0x0; 0x7)) R10=fp0\n  92: (ad) if r2 < r1 goto pc+41        ; R0_w=scalar() R1_w=scalar(umin=7,umax=5,var_off=(0x4; 0x3))\n      R2_w=6 R6=scalar(id=388) R7=0 R8_w=scalar() R9_w=scalar(umax=25769803770,var_off=(0x0; 0x7ffffffff))\n      R10=fp0 fp-8=mmmmmmmm fp-16=mmmmmmmm fp-24=mmmm???? fp-32= fp-40=5 fp-48=mmmmmmmm fp-56= fp-64=mmmmmmmm\n    ...\n  89: (07) r2 += 1                      ; R2_w=4088\n  90: (79) r8 = *(u64 *)(r10 -48)       ; R8_w=scalar() R10=fp0\n  91: (79) r1 = *(u64 *)(r10 -56)       ; R1_w=scalar(umax=5,var_off=(0x0; 0x7)) R10=fp0\n  92: (ad) if r2 < r1 goto pc+41        ; R0=scalar() R1=scalar(umin=4089,umax=5,var_off=(0x0; 0x7))\n      R2=4088 R6=scalar(id=12634) R7=0 R8=scalar() R9=scalar(umax=17557826301960,var_off=(0x0; 0xfffffffffff))\n      R10=fp0 fp-8=mmmmmmmm fp-16=mmmmmmmm fp-24=mmmm???? fp-32= fp-40=4087 fp-48=mmmmmmmm fp-56= fp-64=mmmmmmmm\n\nPatch 3 fixed the above issue by handling '<const> <cond_op> <non_const>' properly.\nDuring developing selftests for Patch 3, I found some issues with bound deduction with\nBPF_EQ/BPF_NE and fixed the issue in Patch 1.\n\nAfter the above issue is fixed, the second issue shows up.\n  ...\n  67: (07) r1 += -16                    ; R1_w=fp-16\n  ; bpf_probe_read_kernel(&sgp, sizeof(sgp), sgs + i);\n  68: (b4) w2 = 8                       ; R2_w=8\n  69: (85) call bpf_probe_read_kernel#113       ; R0_w=scalar() fp-16=mmmmmmmm\n  ; return sgp;\n  70: (79) r6 = *(u64 *)(r10 -16)       ; R6=scalar() R10=fp0\n  ; for (n = 0, sgp = get_sgp(sgs, i); sgp && (n < SG_MAX);\n  71: (15) if r6 == 0x0 goto pc-49      ; R6=scalar()\n  72: (b4) w1 = 0                       ; R1_w=0\n  73: (05) goto pc-46\n  ; for (i = 0; (i < VIRTIO_MAX_SGS) && (i < out_sgs); i++) {\n  28: (bc) w7 = w1                      ; R1_w=0 R7_w=0\n  ; bpf_probe_read_kernel(&len, sizeof(len), &sgp->length);\n  ...\n  23: (79) r3 = *(u64 *)(r10 -40)       ; R3_w=2 R10=fp0\n  ; for (i = 0; (i < VIRTIO_MAX_SGS) && (i < out_sgs); i++) {\n  24: (07) r3 += 1                      ; R3_w=3\n  ; for (i = 0; (i < VIRTIO_MAX_SGS) && (i < out_sgs); i++) {\n  25: (79) r1 = *(u64 *)(r10 -56)       ; R1_w=scalar(umax=5,var_off=(0x0; 0x7)) R10=fp0\n  26: (ad) if r3 < r1 goto pc+34 61: R0=scalar() R1_w=scalar(umin=4,umax=5,var_off=(0x4; 0x1)) R3_w=3 R6=scalar(id=1658)\n     R7=0 R8=scalar(id=1653) R9=scalar(umax=4294967295,var_off=(0x0; 0xffffffff)) R10=fp0 fp-8=mmmmmmmm fp-16=mmmmmmmm\n     fp-24=mmmm???? fp-32= fp-40=2 fp-56= fp-64=mmmmmmmm\n  ; if (sg_is_chain(&sg))\n  61: (7b) *(u64 *)(r10 -40) = r3       ; R3_w=3 R10=fp0 fp-40_w=3\n    ...\n  67: (07) r1 += -16                    ; R1_w=fp-16\n  ; bpf_probe_read_kernel(&sgp, sizeof(sgp), sgs + i);\n  68: (b4) w2 = 8                       ; R2_w=8\n  69: (85) call bpf_probe_read_kernel#113       ; R0_w=scalar() fp-16=mmmmmmmm\n  ; return sgp;\n  70: (79) r6 = *(u64 *)(r10 -16)\n  ; for (n = 0, sgp = get_sgp(sgs, i); sgp && (n < SG_MAX);\n  infinite loop detected at insn 71\n  verification time 90800 usec\n  stack depth 64\n  processed 25017 insns (limit 1000000) max_states_per_insn 20 total_states 491 peak_states 169 mark_read 12\n  -- END PROG LOAD LOG --\n  libbpf: prog 'trace_virtqueue_add_sgs': failed to load: -22\n\nFurther analysis found the index variable 'i' is spilled but since it is not marked as precise.\nThis is more tricky as identifying induction variable is not easy in verifier. Although a heuristic\nis possible, let us leave it for now.\n\n  [1] https://reviews.llvm.org/D143726\n  [2] Commit 3c2611bac08a (\"selftests/bpf: Fix trace_virtqueue_add_sgs test issue with LLVM 17.\")\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-04-06 15:26:08 -0700",
      "commits": [
        {
          "hash": "13fbcee55706db45ce047a7cea14811d68f94ee3",
          "subject": "bpf: Improve verifier JEQ/JNE insn branch taken checking",
          "message": "Currently, for BPF_JEQ/BPF_JNE insn, verifier determines\nwhether the branch is taken or not only if both operands\nare constants. Therefore, for the following code snippet,\n  0: (85) call bpf_ktime_get_ns#5       ; R0_w=scalar()\n  1: (a5) if r0 < 0x3 goto pc+2         ; R0_w=scalar(umin=3)\n  2: (b7) r2 = 2                        ; R2_w=2\n  3: (1d) if r0 == r2 goto pc+2 6\n\nAt insn 3, since r0 is not a constant, verifier assumes both branch\ncan be taken which may lead inproper verification failure.\n\nAdd comparing umin/umax value and the constant. If the umin value\nis greater than the constant, or umax value is smaller than the constant,\nfor JEQ the branch must be not-taken, and for JNE the branch must be taken.\nThe jmp32 mode JEQ/JNE branch taken checking is also handled similarly.\n\nThe following lists the veristat result w.r.t. changed number\nof processes insns during verification:\n\nFile                                                   Program                                               Insns (A)  Insns (B)  Insns    (DIFF)\n-----------------------------------------------------  ----------------------------------------------------  ---------  ---------  ---------------\ntest_cls_redirect.bpf.linked3.o                        cls_redirect                                              64980      73472  +8492 (+13.07%)\ntest_seg6_loop.bpf.linked3.o                           __add_egr_x                                               12425      12423      -2 (-0.02%)\ntest_tcp_hdr_options.bpf.linked3.o                     estab                                                      2634       2558     -76 (-2.89%)\ntest_parse_tcp_hdr_opt.bpf.linked3.o                   xdp_ingress_v6                                             1421       1420      -1 (-0.07%)\ntest_parse_tcp_hdr_opt_dynptr.bpf.linked3.o            xdp_ingress_v6                                             1238       1237      -1 (-0.08%)\ntest_tc_dtime.bpf.linked3.o                            egress_fwdns_prio100                                        414        411      -3 (-0.72%)\n\nMostly a small improvement but test_cls_redirect.bpf.linked3.o has a 13% regression.\nI checked with verifier log and found it this is due to pruning.\nFor some JEQ/JNE branches impacted by this patch,\none branch is explored and the other has state equivalence and\npruned.\n\nSigned-off-by: Yonghong Song <yhs@fb.com>\nAcked-by: Dave Marchevsky <davemarchevsky@fb.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230406164455.1045294-1-yhs@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yhs@fb.com>",
          "date": "2023-04-06 15:26:08 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "aec08d677b4d0adeb7412fa98547cf07bfce6fea",
          "subject": "selftests/bpf: Add tests for non-constant cond_op NE/EQ bound deduction",
          "message": "Add various tests for code pattern '<non-const> NE/EQ <const>' implemented\nin the previous verifier patch. Without the verifier patch, these new\ntests will fail.\n\nSigned-off-by: Yonghong Song <yhs@fb.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230406164500.1045715-1-yhs@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yhs@fb.com>",
          "date": "2023-04-06 15:26:08 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_bounds_deduction_non_const.c"
          ]
        },
        {
          "hash": "953d9f5beaf75e88c69a13d70ce424cd606a29f5",
          "subject": "bpf: Improve handling of pattern '<const> <cond_op> <non_const>' in verifier",
          "message": "Currently, the verifier does not handle '<const> <cond_op> <non_const>' well.\nFor example,\n  ...\n  10: (79) r1 = *(u64 *)(r10 -16)       ; R1_w=scalar() R10=fp0\n  11: (b7) r2 = 0                       ; R2_w=0\n  12: (2d) if r2 > r1 goto pc+2\n  13: (b7) r0 = 0\n  14: (95) exit\n  15: (65) if r1 s> 0x1 goto pc+3\n  16: (0f) r0 += r1\n  ...\nAt insn 12, verifier decides both true and false branch are possible, but\nactually only false branch is possible.\n\nCurrently, the verifier already supports patterns '<non_const> <cond_op> <const>.\nAdd support for patterns '<const> <cond_op> <non_const>' in a similar way.\n\nAlso fix selftest 'verifier_bounds_mix_sign_unsign/bounds checks mixing signed and unsigned, variant 10'\ndue to this change.\n\nSigned-off-by: Yonghong Song <yhs@fb.com>\nAcked-by: Dave Marchevsky <davemarchevsky@fb.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230406164505.1046801-1-yhs@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yhs@fb.com>",
          "date": "2023-04-06 15:26:08 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/progs/verifier_bounds_mix_sign_unsign.c"
          ]
        },
        {
          "hash": "23a88fae9f20d47bb3aed99b1e08d0d6cf65cf0c",
          "subject": "selftests/bpf: Add verifier tests for code pattern '<const> <cond_op> <non_const>'",
          "message": "Add various tests for code pattern '<const> <cond_op> <non_const>' to\nexercise the previous verifier patch.\n\nThe following are veristat changed number of processed insns stat\ncomparing the previous patch vs. this patch:\n\nFile                                                   Program                                               Insns (A)  Insns (B)  Insns  (DIFF)\n-----------------------------------------------------  ----------------------------------------------------  ---------  ---------  -------------\ntest_seg6_loop.bpf.linked3.o                           __add_egr_x                                               12423      12314  -109 (-0.88%)\n\nOnly one program is affected with minor change.\n\nSigned-off-by: Yonghong Song <yhs@fb.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230406164510.1047757-1-yhs@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Yonghong Song <yhs@fb.com>",
          "date": "2023-04-06 15:26:08 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/verifier_bounds_deduction_non_const.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "e8f59d84f43deab59bb86ff9e12cd4d542a4bb34",
      "merge_subject": "Merge branch 'bpf: Follow up to RCU enforcement in the verifier.'",
      "merge_body": "Alexei Starovoitov says:\n\n====================\n\nFrom: Alexei Starovoitov <ast@kernel.org>\n\nThe patch set is addressing a fallout from\ncommit 6fcd486b3a0a (\"bpf: Refactor RCU enforcement in the verifier.\")\nIt was too aggressive with PTR_UNTRUSTED marks.\nPatches 1-6 are cleanup and adding verifier smartness to address real\nuse cases in bpf programs that broke with too aggressive PTR_UNTRUSTED.\nThe partial revert is done in patch 7 anyway.\n====================\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2023-04-04 16:57:30 -0700",
      "commits": [
        {
          "hash": "7d64c513284408fee5178a0953a686e9410f2399",
          "subject": "bpf: Invoke btf_struct_access() callback only for writes.",
          "message": "Remove duplicated if (atype == BPF_READ) btf_struct_access() from\nbtf_struct_access() callback and invoke it only for writes. This is\npossible to do because currently btf_struct_access() custom callback\nalways delegates to generic btf_struct_access() helper for BPF_READ\naccesses.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-2-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-04-04 16:57:03 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c",
            "net/bpf/bpf_dummy_struct_ops.c",
            "net/core/filter.c",
            "net/ipv4/bpf_tcp_ca.c"
          ]
        },
        {
          "hash": "b7e852a9ec96635168c04204fb7cf1f7390b9a8c",
          "subject": "bpf: Remove unused arguments from btf_struct_access().",
          "message": "Remove unused arguments from btf_struct_access() callback.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-3-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-04-04 16:57:10 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/filter.h",
            "kernel/bpf/verifier.c",
            "net/bpf/bpf_dummy_struct_ops.c",
            "net/core/filter.c",
            "net/ipv4/bpf_tcp_ca.c",
            "net/netfilter/nf_conntrack_bpf.c"
          ]
        },
        {
          "hash": "63260df1396578226ac3134cf7f764690002e70e",
          "subject": "bpf: Refactor btf_nested_type_is_trusted().",
          "message": "btf_nested_type_is_trusted() tries to find a struct member at corresponding offset.\nIt works for flat structures and falls apart in more complex structs with nested structs.\nThe offset->member search is already performed by btf_struct_walk() including nested structs.\nReuse this work and pass {field name, field btf id} into btf_nested_type_is_trusted()\ninstead of offset to make BTF_TYPE_SAFE*() logic more robust.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-4-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-04-04 16:57:14 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "91571a515d1bcdc280bb46423bb697ea7eb42ff3",
          "subject": "bpf: Teach verifier that certain helpers accept NULL pointer.",
          "message": "bpf_[sk|inode|task|cgrp]_storage_[get|delete]() and bpf_get_socket_cookie() helpers\nperform run-time check that sk|inode|task|cgrp pointer != NULL.\nTeach verifier about this fact and allow bpf programs to pass\nPTR_TO_BTF_ID | PTR_MAYBE_NULL into such helpers.\nIt will be used in the subsequent patch that will do\nbpf_sk_storage_get(.., skb->sk, ...);\nEven when 'skb' pointer is trusted the 'sk' pointer may be NULL.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-5-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-04-04 16:57:16 -0700",
          "modified_files": [
            "kernel/bpf/bpf_cgrp_storage.c",
            "kernel/bpf/bpf_inode_storage.c",
            "kernel/bpf/bpf_task_storage.c",
            "net/core/bpf_sk_storage.c",
            "net/core/filter.c"
          ]
        },
        {
          "hash": "add68b843f33d4e5dcbdc7ba6dffe7750a964159",
          "subject": "bpf: Refactor NULL-ness check in check_reg_type().",
          "message": "check_reg_type() unconditionally disallows PTR_TO_BTF_ID | PTR_MAYBE_NULL.\nIt's problematic for helpers that allow ARG_PTR_TO_BTF_ID_OR_NULL like\nbpf_sk_storage_get(). Allow passing PTR_TO_BTF_ID | PTR_MAYBE_NULL into such\nhelpers. That technically includes bpf_kptr_xchg() helper, but in practice:\n  bpf_kptr_xchg(..., bpf_cpumask_create());\nis still disallowed because bpf_cpumask_create() returns ref counted pointer\nwith ref_obj_id > 0.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-6-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-04-04 16:57:18 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "30ee9821f9430c34a6254134a5f0e8db227510be",
          "subject": "bpf: Allowlist few fields similar to __rcu tag.",
          "message": "Allow bpf program access cgrp->kn, mm->exe_file, skb->sk, req->sk.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-7-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-04-04 16:57:21 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "afeebf9f57a4965e0be90e4dc87f8f3a1417376d",
          "subject": "bpf: Undo strict enforcement for walking untagged fields.",
          "message": "The commit 6fcd486b3a0a (\"bpf: Refactor RCU enforcement in the verifier.\")\nbroke several tracing bpf programs. Even in clang compiled kernels there are\nmany fields that are not marked with __rcu that are safe to read and pass into\nhelpers, but the verifier doesn't know that they're safe. Aggressively marking\nthem as PTR_UNTRUSTED was premature.\n\nFixes: 6fcd486b3a0a (\"bpf: Refactor RCU enforcement in the verifier.\")\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-8-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-04-04 16:57:24 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "69f41a787761633b752d71166786eb642bad4913",
          "subject": "selftests/bpf: Add tracing tests for walking skb and req.",
          "message": "Add tracing tests for walking skb->sk and req->sk.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-9-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-04-04 16:57:27 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/test_sk_storage_tracing.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "a033907e7b34505ab281b6207750f55c98c69156",
      "merge_subject": "Merge branch 'Enable RCU semantics for task kptrs'",
      "merge_body": "David Vernet says:\n\n====================\n\nIn commit 22df776a9a86 (\"tasks: Extract rcu_users out of union\"), the\n'refcount_t rcu_users' field was extracted out of a union with the\n'struct rcu_head rcu' field. This allows us to use the field for\nrefcounting struct task_struct with RCU protection, as the RCU callback\nno longer flips rcu_users to be nonzero after the callback is scheduled.\n\nThis patch set leverages this to do a few things:\n\n1. Marks struct task_struct as RCU safe in the verifier, allowing\n   referenced kptr tasks stored in maps to be accessed in an RCU\n   read region without acquiring a reference (with just a NULL check).\n2. Makes bpf_task_acquire() a KF_ACQUIRE | KF_RCU | KF_RET_NULL kfunc.\n3. Removes bpf_task_kptr_get() and bpf_task_acquire_not_zero(), as\n   they're now redundant with the above two changes.\n4. Updates selftests and documentation accordingly.\n---\nChangelog:\nv1: https://lore.kernel.org/all/20230331005733.406202-1-void@manifault.com/\nv1 -> v2:\n- Remove testcases validating nested trust inheritance. The first\n  version used 'struct task_struct __rcu *parent', but because that\n  field has the __rcu tag it functions differently on gcc and llvm and\n  causes gcc selftests to fail. Alexei is reworking nested trust,\n  anyways so let's leave it off for now (Alexei).\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-04-01 09:07:21 -0700",
      "commits": [
        {
          "hash": "d02c48fa113953aba0b330ec6c35f50c7d1d7986",
          "subject": "bpf: Make struct task_struct an RCU-safe type",
          "message": "struct task_struct objects are a bit interesting in terms of how their\nlifetime is protected by refcounts. task structs have two refcount\nfields:\n\n1. refcount_t usage: Protects the memory backing the task struct. When\n   this refcount drops to 0, the task is immediately freed, without\n   waiting for an RCU grace period to elapse. This is the field that\n   most callers in the kernel currently use to ensure that a task\n   remains valid while it's being referenced, and is what's currently\n   tracked with bpf_task_acquire() and bpf_task_release().\n\n2. refcount_t rcu_users: A refcount field which, when it drops to 0,\n   schedules an RCU callback that drops a reference held on the 'usage'\n   field above (which is acquired when the task is first created). This\n   field therefore provides a form of RCU protection on the task by\n   ensuring that at least one 'usage' refcount will be held until an RCU\n   grace period has elapsed. The qualifier \"a form of\" is important\n   here, as a task can remain valid after task->rcu_users has dropped to\n   0 and the subsequent RCU gp has elapsed.\n\nIn terms of BPF, we want to use task->rcu_users to protect tasks that\nfunction as referenced kptrs, and to allow tasks stored as referenced\nkptrs in maps to be accessed with RCU protection.\n\nLet's first determine whether we can safely use task->rcu_users to\nprotect tasks stored in maps. All of the bpf_task* kfuncs can only be\ncalled from tracepoint, struct_ops, or BPF_PROG_TYPE_SCHED_CLS, program\ntypes. For tracepoint and struct_ops programs, the struct task_struct\npassed to a program handler will always be trusted, so it will always be\nsafe to call bpf_task_acquire() with any task passed to a program.\nNote, however, that we must update bpf_task_acquire() to be KF_RET_NULL,\nas it is possible that the task has exited by the time the program is\ninvoked, even if the pointer is still currently valid because the main\nkernel holds a task->usage refcount. For BPF_PROG_TYPE_SCHED_CLS, tasks\nshould never be passed as an argument to the any program handlers, so it\nshould not be relevant.\n\nThe second question is whether it's safe to use RCU to access a task\nthat was acquired with bpf_task_acquire(), and stored in a map. Because\nbpf_task_acquire() now uses task->rcu_users, it follows that if the task\nis present in the map, that it must have had at least one\ntask->rcu_users refcount by the time the current RCU cs was started.\nTherefore, it's safe to access that task until the end of the current\nRCU cs.\n\nWith all that said, this patch makes struct task_struct is an\nRCU-protected object. In doing so, we also change bpf_task_acquire() to\nbe KF_ACQUIRE | KF_RCU | KF_RET_NULL, and adjust any selftests as\nnecessary. A subsequent patch will remove bpf_task_kptr_get(), and\nbpf_task_acquire_not_zero() respectively.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230331195733.699708-2-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-04-01 09:07:20 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/prog_tests/task_kfunc.c",
            "tools/testing/selftests/bpf/progs/task_kfunc_common.h",
            "tools/testing/selftests/bpf/progs/task_kfunc_failure.c",
            "tools/testing/selftests/bpf/progs/task_kfunc_success.c"
          ]
        },
        {
          "hash": "f85671c6ef46d490a90dac719e0c0e0adbacfd9b",
          "subject": "bpf: Remove now-defunct task kfuncs",
          "message": "In commit 22df776a9a86 (\"tasks: Extract rcu_users out of union\"), the\n'refcount_t rcu_users' field was extracted out of a union with the\n'struct rcu_head rcu' field. This allows us to safely perform a\nrefcount_inc_not_zero() on task->rcu_users when acquiring a reference on\na task struct. A prior patch leveraged this by making struct task_struct\nan RCU-protected object in the verifier, and by bpf_task_acquire() to\nuse the task->rcu_users field for synchronization.\n\nNow that we can use RCU to protect tasks, we no longer need\nbpf_task_kptr_get(), or bpf_task_acquire_not_zero(). bpf_task_kptr_get()\nis truly completely unnecessary, as we can just use RCU to get the\nobject. bpf_task_acquire_not_zero() is now equivalent to\nbpf_task_acquire().\n\nIn addition to these changes, this patch also updates the associated\nselftests to no longer use these kfuncs.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230331195733.699708-3-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-04-01 09:07:20 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c",
            "tools/testing/selftests/bpf/prog_tests/task_kfunc.c",
            "tools/testing/selftests/bpf/progs/rcu_read_lock.c",
            "tools/testing/selftests/bpf/progs/task_kfunc_common.h",
            "tools/testing/selftests/bpf/progs/task_kfunc_failure.c",
            "tools/testing/selftests/bpf/progs/task_kfunc_success.c"
          ]
        },
        {
          "hash": "db9d479ab59b21d719486e6bf673f83f129dae32",
          "subject": "bpf,docs: Update documentation to reflect new task kfuncs",
          "message": "Now that struct task_struct objects are RCU safe, and bpf_task_acquire()\ncan return NULL, we should update the BPF task kfunc documentation to\nreflect the current state of the API.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230331195733.699708-4-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-04-01 09:07:20 -0700",
          "modified_files": [
            "Documentation/bpf/kfuncs.rst"
          ]
        }
      ]
    },
    {
      "merge_hash": "496f4f1b0f8e01baea22e6573f60af8cfd84df48",
      "merge_subject": "Merge branch 'Don't invoke KPTR_REF destructor on NULL xchg'",
      "merge_body": "David Vernet says:\n\n====================\n\nWhen a map value is being freed, we loop over all of the fields of the\ncorresponding BPF object and issue the appropriate cleanup calls\ncorresponding to the field's type. If the field is a referenced kptr, we\natomically xchg the value out of the map, and invoke the kptr's\ndestructor on whatever was there before.\n\nCurrently, we always invoke the destructor (or bpf_obj_drop() for a\nlocal kptr) on any kptr, including if no value was xchg'd out of the\nmap. This means that any function serving as the kptr's KF_RELEASE\ndestructor must always treat the argument as possibly NULL, and we\ninvoke unnecessary (and seemingly unsafe) cleanup logic for the local\nkptr path as well.\n\nThis is an odd requirement -- KF_RELEASE kfuncs that are invoked by BPF\nprograms do not have this restriction, and the verifier will fail to\nload the program if the register containing the to-be-released type has\nany untrusted modifiers (e.g. PTR_UNTRUSTED or PTR_MAYBE_NULL). So as to\nsimplify the expectations required for a KF_RELEASE kfunc, this patch\nset updates the KPTR_REF destructor logic to only be invoked when a\nnon-NULL value is xchg'd out of the map.\n\nAdditionally, the patch removes now-unnecessary KF_RELEASE calls from\nseveral kfuncs, and finally, updates the verifier to have KF_RELEASE\nautomatically imply KF_TRUSTED_ARGS. This restriction was already\nimplicitly happening because of the aforementioned logic in the verifier\nto reject any regs with untrusted modifiers, and to enforce that\nKF_RELEASE args are passed with a 0 offset. This change just updates the\nbehavior to match that of other trusted args. This patch is left to the\nend of the series in case it happens to be controversial, as it arguably\nis slightly orthogonal to the purpose of the rest of the series.\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-03-25 16:56:22 -0700",
      "commits": [
        {
          "hash": "1431d0b584a673ea690c88a5f7e1aedd9caf0e84",
          "subject": "bpf: Only invoke kptr dtor following non-NULL xchg",
          "message": "When a map value is being freed, we loop over all of the fields of the\ncorresponding BPF object and issue the appropriate cleanup calls\ncorresponding to the field's type. If the field is a referenced kptr, we\natomically xchg the value out of the map, and invoke the kptr's\ndestructor on whatever was there before (or bpf_obj_drop() it if it was\na local kptr).\n\nCurrently, we always invoke the destructor (either bpf_obj_drop() or the\nkptr's registered destructor) on any KPTR_REF-type field in a map, even\nif there wasn't a value in the map. This means that any function serving\nas the kptr's KF_RELEASE destructor must always treat the argument as\npossibly NULL, as the following can and regularly does happen:\n\nvoid *xchgd_field;\n\n/* No value was in the map, so xchgd_field is NULL */\nxchgd_field = (void *)xchg(unsigned long *field_ptr, 0);\nfield->kptr.dtor(xchgd_field);\n\nThese are odd semantics to impose on KF_RELEASE kfuncs -- BPF programs\nare prohibited by the verifier from passing NULL pointers to KF_RELEASE\nkfuncs, so it doesn't make sense to require this of BPF programs, but\nnot the main kernel destructor path. It's also unnecessary to invoke any\ncleanup logic for local kptrs. If there is no object there, there's\nnothing to drop.\n\nSo as to allow KF_RELEASE kfuncs to fully assume that an argument is\nnon-NULL, this patch updates a KPTR_REF's destructor to only be invoked\nwhen a non-NULL value is xchg'd out of the kptr map field.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230325213144.486885-2-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-03-25 16:56:22 -0700",
          "modified_files": [
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "fb2211a57c110b4ced3cb7f8570bd7246acf2d04",
          "subject": "bpf: Remove now-unnecessary NULL checks for KF_RELEASE kfuncs",
          "message": "Now that we're not invoking kfunc destructors when the kptr in a map was\nNULL, we no longer require NULL checks in many of our KF_RELEASE kfuncs.\nThis patch removes those NULL checks.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230325213144.486885-3-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-03-25 16:56:22 -0700",
          "modified_files": [
            "drivers/hid/bpf/hid_bpf_dispatch.c",
            "kernel/bpf/cpumask.c",
            "kernel/bpf/helpers.c",
            "net/bpf/test_run.c",
            "net/netfilter/nf_conntrack_bpf.c"
          ]
        },
        {
          "hash": "6c831c4684124a544f73f7c9b83bc7b2eb0b23d3",
          "subject": "bpf: Treat KF_RELEASE kfuncs as KF_TRUSTED_ARGS",
          "message": "KF_RELEASE kfuncs are not currently treated as having KF_TRUSTED_ARGS,\neven though they have a superset of the requirements of KF_TRUSTED_ARGS.\nLike KF_TRUSTED_ARGS, KF_RELEASE kfuncs require a 0-offset argument, and\ndon't allow NULL-able arguments. Unlike KF_TRUSTED_ARGS which require\n_either_ an argument with ref_obj_id > 0, _or_ (ref->type &\nBPF_REG_TRUSTED_MODIFIERS) (and no unsafe modifiers allowed), KF_RELEASE\nonly allows for ref_obj_id > 0.  Because KF_RELEASE today doesn't\nautomatically imply KF_TRUSTED_ARGS, some of these requirements are\nenforced in different ways that can make the behavior of the verifier\nfeel unpredictable. For example, a KF_RELEASE kfunc with a NULL-able\nargument will currently fail in the verifier with a message like, \"arg#0\nis ptr_or_null_ expected ptr_ or socket\" rather than \"Possibly NULL\npointer passed to trusted arg0\". Our intention is the same, but the\nsemantics are different due to implemenetation details that kfunc authors\nand BPF program writers should not need to care about.\n\nLet's make the behavior of the verifier more consistent and intuitive by\nhaving KF_RELEASE kfuncs imply the presence of KF_TRUSTED_ARGS. Our\neventual goal is to have all kfuncs assume KF_TRUSTED_ARGS by default\nanyways, so this takes us a step in that direction.\n\nNote that it does not make sense to assume KF_TRUSTED_ARGS for all\nKF_ACQUIRE kfuncs. KF_ACQUIRE kfuncs can have looser semantics than\nKF_RELEASE, with e.g. KF_RCU | KF_RET_NULL. We may want to have\nKF_ACQUIRE imply KF_TRUSTED_ARGS _unless_ KF_RCU is specified, but that\ncan be left to another patch set, and there are no such subtleties to\naddress for KF_RELEASE.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230325213144.486885-4-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-03-25 16:56:22 -0700",
          "modified_files": [
            "Documentation/bpf/kfuncs.rst",
            "kernel/bpf/cpumask.c",
            "kernel/bpf/verifier.c",
            "net/bpf/test_run.c",
            "tools/testing/selftests/bpf/progs/cgrp_kfunc_failure.c",
            "tools/testing/selftests/bpf/progs/task_kfunc_failure.c",
            "tools/testing/selftests/bpf/verifier/calls.c",
            "tools/testing/selftests/bpf/verifier/ref_tracking.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "02adf9e9bec115962424d45d45d0d65d6b6477fa",
      "merge_subject": "Merge branch 'error checking where helpers call bpf_map_ops'",
      "merge_body": "JP Kobryn says:\n\n====================\n\nWithin bpf programs, the bpf helper functions can make inline calls to\nkernel functions. In this scenario there can be a disconnect between the\nregister the kernel function writes a return value to and the register the\nbpf program uses to evaluate that return value.\n\nAs an example, this bpf code:\n\nlong err = bpf_map_update_elem(...);\nif (err && err != -EEXIST)\n\t// got some error other than -EEXIST\n\n...can result in the bpf assembly:\n\n; err = bpf_map_update_elem(&mymap, &key, &val, BPF_NOEXIST);\n  37:\tmovabs $0xffff976a10730400,%rdi\n  41:\tmov    $0x1,%ecx\n  46:\tcall   0xffffffffe103291c\t; htab_map_update_elem\n; if (err && err != -EEXIST) {\n  4b:\tcmp    $0xffffffffffffffef,%rax ; cmp -EEXIST,%rax\n  4f:\tje     0x000000000000008e\n  51:\ttest   %rax,%rax\n  54:\tje     0x000000000000008e\n\nThe compare operation here evaluates %rax, while in the preceding call to\nhtab_map_update_elem the corresponding assembly returns -EEXIST via %eax\n(the lower 32 bits of %rax):\n\nmovl $0xffffffef, %r9d\n...\nmovl %r9d, %eax\n\n...since it's returning int (32-bit). So the resulting comparison becomes:\n\ncmp $0xffffffffffffffef, $0x00000000ffffffef\n\n...making it not possible to check for negative errors or specific errors,\nsince the sign value is left at the 32nd bit. It means in the original\nexample, the conditional branch will be entered even when the error is\n-EEXIST, which was not intended.\n\nThe selftests added cover these cases for the different bpf_map_ops\nfunctions. When the second patch is applied, changing the return type of\nthose functions to long, the comparison works as intended and the tests\npass.\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-03-22 15:12:03 -0700",
      "commits": [
        {
          "hash": "830154cdc57971b06f81d4ffc39b868e3d7693de",
          "subject": "bpf/selftests: coverage for bpf_map_ops errors",
          "message": "These tests expose the issue of being unable to properly check for errors\nreturned from inlined bpf map helpers that make calls to the bpf_map_ops\nfunctions. At best, a check for zero or non-zero can be done but these\ntests show it is not possible to check for a negative value or for a\nspecific error value.\n\nSigned-off-by: JP Kobryn <inwardvessel@gmail.com>\nTested-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20230322194754.185781-2-inwardvessel@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "JP Kobryn <inwardvessel@gmail.com>",
          "date": "2023-03-22 15:11:06 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/map_ops.c",
            "tools/testing/selftests/bpf/progs/test_map_ops.c"
          ]
        },
        {
          "hash": "d7ba4cc900bf1eea2d8c807c6b1fc6bd61f41237",
          "subject": "bpf: return long from bpf_map_ops funcs",
          "message": "This patch changes the return types of bpf_map_ops functions to long, where\npreviously int was returned. Using long allows for bpf programs to maintain\nthe sign bit in the absence of sign extension during situations where\ninlined bpf helper funcs make calls to the bpf_map_ops funcs and a negative\nerror is returned.\n\nThe definitions of the helper funcs are generated from comments in the bpf\nuapi header at `include/uapi/linux/bpf.h`. The return type of these\nhelpers was previously changed from int to long in commit bdb7b79b4ce8. For\nany case where one of the map helpers call the bpf_map_ops funcs that are\nstill returning 32-bit int, a compiler might not include sign extension\ninstructions to properly convert the 32-bit negative value a 64-bit\nnegative value.\n\nFor example:\nbpf assembly excerpt of an inlined helper calling a kernel function and\nchecking for a specific error:\n\n; err = bpf_map_update_elem(&mymap, &key, &val, BPF_NOEXIST);\n  ...\n  46:\tcall   0xffffffffe103291c\t; htab_map_update_elem\n; if (err && err != -EEXIST) {\n  4b:\tcmp    $0xffffffffffffffef,%rax ; cmp -EEXIST,%rax\n\nkernel function assembly excerpt of return value from\n`htab_map_update_elem` returning 32-bit int:\n\nmovl $0xffffffef, %r9d\n...\nmovl %r9d, %eax\n\n...results in the comparison:\ncmp $0xffffffffffffffef, $0x00000000ffffffef\n\nFixes: bdb7b79b4ce8 (\"bpf: Switch most helper return values from 32-bit int to 64-bit long\")\nTested-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: JP Kobryn <inwardvessel@gmail.com>\nLink: https://lore.kernel.org/r/20230322194754.185781-3-inwardvessel@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "JP Kobryn <inwardvessel@gmail.com>",
          "date": "2023-03-22 15:11:30 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/filter.h",
            "kernel/bpf/arraymap.c",
            "kernel/bpf/bloom_filter.c",
            "kernel/bpf/bpf_cgrp_storage.c",
            "kernel/bpf/bpf_inode_storage.c",
            "kernel/bpf/bpf_struct_ops.c",
            "kernel/bpf/bpf_task_storage.c",
            "kernel/bpf/cpumap.c",
            "kernel/bpf/devmap.c",
            "kernel/bpf/hashtab.c",
            "kernel/bpf/local_storage.c",
            "kernel/bpf/lpm_trie.c",
            "kernel/bpf/queue_stack_maps.c",
            "kernel/bpf/reuseport_array.c",
            "kernel/bpf/ringbuf.c",
            "kernel/bpf/stackmap.c",
            "kernel/bpf/verifier.c",
            "net/core/bpf_sk_storage.c",
            "net/core/sock_map.c",
            "net/xdp/xskmap.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "d9d93f3b61434bc18ec905eaad224407cce1a9e2",
      "merge_subject": "Merge branch 'bpf: Support ksym detection in light skeleton.'",
      "merge_body": "Alexei Starovoitov says:\n\n====================\n\nFrom: Alexei Starovoitov <ast@kernel.org>\n\nv1->v2: update denylist on s390\n\nPatch 1: Cleanup internal libbpf names.\nPatch 2: Teach the verifier that rdonly_mem != NULL.\nPatch 3: Fix gen_loader to support ksym detection.\nPatch 4: Selftest and update denylist.\n====================\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2023-03-22 09:31:06 -0700",
      "commits": [
        {
          "hash": "a18f721415b4d4edea23cb8c7d6276330302af14",
          "subject": "libbpf: Rename RELO_EXTERN_VAR/FUNC.",
          "message": "RELO_EXTERN_VAR/FUNC names are not correct anymore. RELO_EXTERN_VAR represent\nksym symbol in ld_imm64 insn. It can point to kernel variable or kfunc.\nRename RELO_EXTERN_VAR->RELO_EXTERN_LD64 and RELO_EXTERN_FUNC->RELO_EXTERN_CALL\nto match what they actually represent.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230321203854.3035-2-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-22 09:31:05 -0700",
          "modified_files": [
            "tools/lib/bpf/libbpf.c"
          ]
        },
        {
          "hash": "1057d299459657b85e593a4b6294a000f920672a",
          "subject": "bpf: Teach the verifier to recognize rdonly_mem as not null.",
          "message": "Teach the verifier to recognize PTR_TO_MEM | MEM_RDONLY as not NULL\notherwise if (!bpf_ksym_exists(known_kfunc)) doesn't go through\ndead code elimination.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230321203854.3035-3-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-22 09:31:05 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "708cdc5706a4701be9e5f81cb2e2b60b57f34c42",
          "subject": "libbpf: Support kfunc detection in light skeleton.",
          "message": "Teach gen_loader to find {btf_id, btf_obj_fd} of kernel variables and kfuncs\nand populate corresponding ld_imm64 and bpf_call insns.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230321203854.3035-4-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-22 09:31:05 -0700",
          "modified_files": [
            "tools/lib/bpf/bpf_gen_internal.h",
            "tools/lib/bpf/gen_loader.c",
            "tools/lib/bpf/libbpf.c"
          ]
        },
        {
          "hash": "3b2ec2140fa27febb21034943d656898b659dc02",
          "subject": "selftests/bpf: Add light skeleton test for kfunc detection.",
          "message": "Add light skeleton test for kfunc detection and denylist it for s390.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230321203854.3035-5-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-22 09:31:05 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/DENYLIST.s390x",
            "tools/testing/selftests/bpf/progs/test_ksyms_weak.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "6cae5a7106e272e7908a1facdf519a307f93872d",
      "merge_subject": "Merge branch 'bpf: Add detection of kfuncs.'",
      "merge_body": "Alexei Starovoitov says:\n\n====================\n\nFrom: Alexei Starovoitov <ast@kernel.org>\n\nAllow BPF programs detect at load time whether particular kfunc exists.\n\nPatch 1: Allow ld_imm64 to point to kfunc in the kernel.\nPatch 2: Fix relocation of kfunc in ld_imm64 insn when kfunc is in kernel module.\nPatch 3: Introduce bpf_ksym_exists() macro.\nPatch 4: selftest.\n\nNOTE: detection of kfuncs from light skeleton is not supported yet.\n====================\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "merge_author": "Andrii Nakryiko <andrii@kernel.org>",
      "merge_date": "2023-03-17 15:46:03 -0700",
      "commits": [
        {
          "hash": "58aa2afbb1e61fcf35bfcc819952a3c13d9f9203",
          "subject": "bpf: Allow ld_imm64 instruction to point to kfunc.",
          "message": "Allow ld_imm64 insn with BPF_PSEUDO_BTF_ID to hold the address of kfunc. The\nld_imm64 pointing to a valid kfunc will be seen as non-null PTR_TO_MEM by\nis_branch_taken() logic of the verifier, while libbpf will resolve address to\nunknown kfunc as ld_imm64 reg, 0 which will also be recognized by\nis_branch_taken() and the verifier will proceed dead code elimination. BPF\nprograms can use this logic to detect at load time whether kfunc is present in\nthe kernel with bpf_ksym_exists() macro that is introduced in the next patches.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nReviewed-by: Martin KaFai Lau <martin.lau@kernel.org>\nReviewed-by: Toke H\u00f8iland-J\u00f8rgensen <toke@redhat.com>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20230317201920.62030-2-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-17 15:44:26 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "5fc13ad59b60708e52932188854d4d5bf2b0e10e",
          "subject": "libbpf: Fix relocation of kfunc ksym in ld_imm64 insn.",
          "message": "void *p = kfunc; -> generates ld_imm64 insn.\nkfunc() -> generates bpf_call insn.\n\nlibbpf patches bpf_call insn correctly while only btf_id part of ld_imm64 is\nset in the former case. Which means that pointers to kfuncs in modules are not\npatched correctly and the verifier rejects load of such programs due to btf_id\nbeing out of range. Fix libbpf to patch ld_imm64 for kfunc.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230317201920.62030-3-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-17 15:44:27 -0700",
          "modified_files": [
            "tools/lib/bpf/libbpf.c"
          ]
        },
        {
          "hash": "5cbd3fe3a91df46ea201cc5d8ab4e390332ec26e",
          "subject": "libbpf: Introduce bpf_ksym_exists() macro.",
          "message": "Introduce bpf_ksym_exists() macro that can be used by BPF programs\nto detect at load time whether particular ksym (either variable or kfunc)\nis present in the kernel.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230317201920.62030-4-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-17 15:46:00 -0700",
          "modified_files": [
            "tools/lib/bpf/bpf_helpers.h"
          ]
        },
        {
          "hash": "95fdf6e313a981b0729886f86916190cb418b04c",
          "subject": "selftests/bpf: Add test for bpf_ksym_exists().",
          "message": "Add load and run time test for bpf_ksym_exists() and check that the verifier\nperforms dead code elimination for non-existing kfunc.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nReviewed-by: Martin KaFai Lau <martin.lau@kernel.org>\nReviewed-by: Toke H\u00f8iland-J\u00f8rgensen <toke@redhat.com>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20230317201920.62030-5-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-17 15:46:02 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/task_kfunc_success.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "deb9fd64d145b42c0a15193507b4fea27514a559",
      "merge_subject": "Merge branch 'Make struct bpf_cpumask RCU safe'",
      "merge_body": "David Vernet says:\n\n====================\n\nThe struct bpf_cpumask type is currently not RCU safe. It uses the\nbpf_mem_cache_{alloc,free}() APIs to allocate and release cpumasks, and\nthose allocations may be reused before an RCU grace period has elapsed.\nWe want to be able to enable using this pattern in BPF programs:\n\nprivate(MASK) static struct bpf_cpumask __kptr *global;\n\nint BPF_PROG(prog, ...)\n{\n\tstruct bpf_cpumask *cpumask;\n\n\tbpf_rcu_read_lock();\n\tcpumask = global;\n\tif (!cpumask) {\n\t\tbpf_rcu_read_unlock();\n\t\treturn -1;\n\t}\n\tbpf_cpumask_setall(cpumask);\n\t...\n\tbpf_rcu_read_unlock();\n}\n\nIn other words, to be able to pass a kptr to KF_RCU bpf_cpumask kfuncs\nwithout requiring the acquisition and release of refcounts using\nbpf_cpumask_kptr_get(). This patchset enables this by making the struct\nbpf_cpumask type RCU safe, and removing the bpf_cpumask_kptr_get()\nfunction.\n---\nv1: https://lore.kernel.org/all/20230316014122.678082-2-void@manifault.com/\n\nChangelog:\n----------\nv1 -> v2:\n- Add doxygen comment for new @rcu field in struct bpf_cpumask.\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-03-16 12:28:31 -0700",
      "commits": [
        {
          "hash": "77473d1a962f3d4f7ba48324502b6d27b8ef2591",
          "subject": "bpf: Free struct bpf_cpumask in call_rcu handler",
          "message": "The struct bpf_cpumask type uses the bpf_mem_cache_{alloc,free}() APIs\nto allocate and free its cpumasks. The bpf_mem allocator may currently\nimmediately reuse some memory when its freed, without waiting for an RCU\nread cycle to elapse. We want to be able to treat struct bpf_cpumask\nobjects as completely RCU safe.\n\nThis is necessary for two reasons:\n\n1. bpf_cpumask_kptr_get() currently does an RCU-protected\n   refcnt_inc_not_zero(). This of course assumes that the underlying\n   memory is not reused, and is therefore unsafe in its current form.\n\n2. We want to be able to get rid of bpf_cpumask_kptr_get() entirely, and\n   intead use the superior kptr RCU semantics now afforded by the\n   verifier.\n\nThis patch fixes (1), and enables (2), by making struct bpf_cpumask RCU\nsafe. A subsequent patch will update the verifier to allow struct\nbpf_cpumask * pointers to be passed to KF_RCU kfuncs, and then a latter\npatch will remove bpf_cpumask_kptr_get().\n\nFixes: 516f4d3397c9 (\"bpf: Enable cpumasks to be queried and used as kptrs\")\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230316054028.88924-2-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-03-16 12:28:30 -0700",
          "modified_files": [
            "kernel/bpf/cpumask.c"
          ]
        },
        {
          "hash": "63d2d83d21a6e2c6f019da5b2d5cdabe6d1cb951",
          "subject": "bpf: Mark struct bpf_cpumask as rcu protected",
          "message": "struct bpf_cpumask is a BPF-wrapper around the struct cpumask type which\ncan be instantiated by a BPF program, and then queried as a cpumask in\nsimilar fashion to normal kernel code. The previous patch in this series\nmakes the type fully RCU safe, so the type can be included in the\nrcu_protected_type BTF ID list.\n\nA subsequent patch will remove bpf_cpumask_kptr_get(), as it's no longer\nuseful now that we can just treat the type as RCU safe by default and do\nour own if check.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230316054028.88924-3-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-03-16 12:28:30 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "a5a197df58c44ce32a86b57e970da4bd7b71b399",
          "subject": "bpf/selftests: Test using global cpumask kptr with RCU",
          "message": "Now that struct bpf_cpumask * is considered an RCU-safe type according\nto the verifier, we should add tests that validate its common usages.\nThis patch adds those tests to the cpumask test suite. A subsequent\nchanges will remove bpf_cpumask_kptr_get(), and will adjust the selftest\nand BPF documentation accordingly.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230316054028.88924-4-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-03-16 12:28:30 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/cpumask.c",
            "tools/testing/selftests/bpf/progs/cpumask_common.h",
            "tools/testing/selftests/bpf/progs/cpumask_failure.c",
            "tools/testing/selftests/bpf/progs/cpumask_success.c"
          ]
        },
        {
          "hash": "1b403ce77dfbf234723a91bc411dfb03a0499d6e",
          "subject": "bpf: Remove bpf_cpumask_kptr_get() kfunc",
          "message": "Now that struct bpf_cpumask is RCU safe, there's no need for this kfunc.\nRather than doing the following:\n\nprivate(MASK) static struct bpf_cpumask __kptr *global;\n\nint BPF_PROG(prog, s32 cpu, ...)\n{\n\tstruct bpf_cpumask *cpumask;\n\n\tbpf_rcu_read_lock();\n\tcpumask = bpf_cpumask_kptr_get(&global);\n\tif (!cpumask) {\n\t\tbpf_rcu_read_unlock();\n\t\treturn -1;\n\t}\n\tbpf_cpumask_setall(cpumask);\n\t...\n\tbpf_cpumask_release(cpumask);\n\tbpf_rcu_read_unlock();\n}\n\nPrograms can instead simply do (assume same global cpumask):\n\nint BPF_PROG(prog, ...)\n{\n\tstruct bpf_cpumask *cpumask;\n\n\tbpf_rcu_read_lock();\n\tcpumask = global;\n\tif (!cpumask) {\n\t\tbpf_rcu_read_unlock();\n\t\treturn -1;\n\t}\n\tbpf_cpumask_setall(cpumask);\n\t...\n\tbpf_rcu_read_unlock();\n}\n\nIn other words, no extra atomic acquire / release, and less boilerplate\ncode.\n\nThis patch removes both the kfunc, as well as its selftests and\ndocumentation.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230316054028.88924-5-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-03-16 12:28:30 -0700",
          "modified_files": [
            "kernel/bpf/cpumask.c",
            "tools/testing/selftests/bpf/prog_tests/cpumask.c",
            "tools/testing/selftests/bpf/progs/cpumask_common.h",
            "tools/testing/selftests/bpf/progs/cpumask_failure.c",
            "tools/testing/selftests/bpf/progs/cpumask_success.c"
          ]
        },
        {
          "hash": "fec2c6d14fd5001e7d24a2ae44f0e9aea82a6149",
          "subject": "bpf,docs: Remove bpf_cpumask_kptr_get() from documentation",
          "message": "Now that the kfunc no longer exists, we can remove it and instead\ndescribe how RCU can be used to get a struct bpf_cpumask from a map\nvalue. This patch updates the BPF documentation accordingly.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230316054028.88924-6-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "David Vernet <void@manifault.com>",
          "date": "2023-03-16 12:28:30 -0700",
          "modified_files": [
            "Documentation/bpf/cpumasks.rst"
          ]
        }
      ]
    },
    {
      "merge_hash": "72fe61d745cb0e199448e9125d894e59508b2bb3",
      "merge_subject": "Merge branch 'Fix attaching fentry/fexit/fmod_ret/lsm to modules'",
      "merge_body": "Viktor Malik says:\n\n====================\n\nI noticed that the verifier behaves incorrectly when attaching to fentry\nof multiple functions of the same name located in different modules (or\nin vmlinux). The reason for this is that if the target program is not\nspecified, the verifier will search kallsyms for the trampoline address\nto attach to. The entire kallsyms is always searched, not respecting the\nmodule in which the function to attach to is located.\n\nAs Yonghong correctly pointed out, there is yet another issue - the\ntrampoline acquires the module reference in register_fentry which means\nthat if the module is unloaded between the place where the address is\nfound in the verifier and register_fentry, it is possible that another\nmodule is loaded to the same address in the meantime, which may lead to\nerrors.\n\nThis patch fixes the above issues by extracting the module name from the\nBTF of the attachment target (which must be specified) and by doing the\nsearch in kallsyms of the correct module. At the same time, the module\nreference is acquired right after the address is found and only released\nright before the program itself is unloaded.\n---\nChanges in v10:\n- added the new test to DENYLIST.aarch64 (suggested by Andrii)\n- renamed the test source file to match the test name\n\nChanges in v9:\n- two small changes suggested by Jiri Olsa and Jiri's ack\n\nChanges in v8:\n- added module_put to error paths in bpf_check_attach_target after the\n  module reference is acquired\n\nChanges in v7:\n- refactored the module reference manipulation (comments by Jiri Olsa)\n- cleaned up the test (comments by Andrii Nakryiko)\n\nChanges in v6:\n- storing the module reference inside bpf_prog_aux instead of\n  bpf_trampoline and releasing it when the program is unloaded\n  (suggested by Jiri Olsa)\n\nChanges in v5:\n- fixed acquiring and releasing of module references by trampolines to\n  prevent modules being unloaded between address lookup and trampoline\n  allocation\n\nChanges in v4:\n- reworked module kallsyms lookup approach using existing functions,\n  verifier now calls btf_try_get_module to retrieve the module and\n  find_kallsyms_symbol_value to get the symbol address (suggested by\n  Alexei)\n- included Jiri Olsa's comments\n- improved description of the new test and added it as a comment into\n  the test source\n\nChanges in v3:\n- added trivial implementation for kallsyms_lookup_name_in_module() for\n  !CONFIG_MODULES (noticed by test robot, fix suggested by Hao Luo)\n\nChanges in v2:\n- introduced and used more space-efficient kallsyms lookup function,\n  suggested by Jiri Olsa\n- included Hao Luo's comments\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-03-15 18:38:30 -0700",
      "commits": [
        {
          "hash": "31bf1dbccfb0a9861d4846755096b3fff5687f8a",
          "subject": "bpf: Fix attaching fentry/fexit/fmod_ret/lsm to modules",
          "message": "This resolves two problems with attachment of fentry/fexit/fmod_ret/lsm\nto functions located in modules:\n\n1. The verifier tries to find the address to attach to in kallsyms. This\n   is always done by searching the entire kallsyms, not respecting the\n   module in which the function is located. Such approach causes an\n   incorrect attachment address to be computed if the function to attach\n   to is shadowed by a function of the same name located earlier in\n   kallsyms.\n\n2. If the address to attach to is located in a module, the module\n   reference is only acquired in register_fentry. If the module is\n   unloaded between the place where the address is found\n   (bpf_check_attach_target in the verifier) and register_fentry, it is\n   possible that another module is loaded to the same address which may\n   lead to potential errors.\n\nSince the attachment must contain the BTF of the program to attach to,\nwe extract the module from it and search for the function address in the\ncorrect module (resolving problem no. 1). Then, the module reference is\ntaken directly in bpf_check_attach_target and stored in the bpf program\n(in bpf_prog_aux). The reference is only released when the program is\nunloaded (resolving problem no. 2).\n\nSigned-off-by: Viktor Malik <vmalik@redhat.com>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nReviewed-by: Luis Chamberlain <mcgrof@kernel.org>\nLink: https://lore.kernel.org/r/3f6a9d8ae850532b5ef864ef16327b0f7a669063.1678432753.git.vmalik@redhat.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Viktor Malik <vmalik@redhat.com>",
          "date": "2023-03-15 18:38:21 -0700",
          "modified_files": [
            "include/linux/bpf.h",
            "kernel/bpf/syscall.c",
            "kernel/bpf/trampoline.c",
            "kernel/bpf/verifier.c",
            "kernel/module/internal.h"
          ]
        },
        {
          "hash": "aa3d65de4b9004d799f97700751a86d3ebd7d5f9",
          "subject": "bpf/selftests: Test fentry attachment to shadowed functions",
          "message": "Adds a new test that tries to attach a program to fentry of two\nfunctions of the same name, one located in vmlinux and the other in\nbpf_testmod.\n\nTo avoid conflicts with existing tests, a new function\n\"bpf_fentry_shadow_test\" was created both in vmlinux and in bpf_testmod.\n\nThe previous commit fixed a bug which caused this test to fail. The\nverifier would always use the vmlinux function's address as the target\ntrampoline address, hence trying to create two trampolines for a single\naddress, which is forbidden.\n\nThe test (similarly to other fentry/fexit tests) is not working on arm64\nat the moment.\n\nSigned-off-by: Viktor Malik <vmalik@redhat.com>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nLink: https://lore.kernel.org/r/5fe2f364190b6f79b085066ed7c5989c5bc475fa.1678432753.git.vmalik@redhat.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Viktor Malik <vmalik@redhat.com>",
          "date": "2023-03-15 18:38:30 -0700",
          "modified_files": [
            "net/bpf/test_run.c",
            "tools/testing/selftests/bpf/DENYLIST.aarch64",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/prog_tests/module_fentry_shadow.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "283b40c52d9ad850d204c447df69faaaf9d177f0",
      "merge_subject": "Merge branch 'bpf: Allow helpers access ptr_to_btf_id.'",
      "merge_body": "Alexei Starovoitov says:\n\n====================\n\nFrom: Alexei Starovoitov <ast@kernel.org>\n\nAllow code like:\nbpf_strncmp(task->comm, 16, \"foo\");\n====================\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "merge_date": "2023-03-13 23:08:21 -0700",
      "commits": [
        {
          "hash": "c9267aa8b794c2188d49c7d7bd2990e98b2d6b84",
          "subject": "bpf: Fix bpf_strncmp proto.",
          "message": "bpf_strncmp() doesn't write into its first argument.\nMake sure that the verifier knows about it.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230313235845.61029-2-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-13 23:08:21 -0700",
          "modified_files": [
            "kernel/bpf/helpers.c"
          ]
        },
        {
          "hash": "3e30be4288b31702d4898487a74e80ba14150a9f",
          "subject": "bpf: Allow helpers access trusted PTR_TO_BTF_ID.",
          "message": "The verifier rejects the code:\n  bpf_strncmp(task->comm, 16, \"my_task\");\nwith the message:\n  16: (85) call bpf_strncmp#182\n  R1 type=trusted_ptr_ expected=fp, pkt, pkt_meta, map_key, map_value, mem, ringbuf_mem, buf\n\nTeach the verifier that such access pattern is safe.\nDo not allow untrusted and legacy ptr_to_btf_id to be passed into helpers.\n\nReported-by: David Vernet <void@manifault.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230313235845.61029-3-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-13 23:08:21 -0700",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "f25fd6088216bd257902e5c212177cddcb291218",
          "subject": "selftests/bpf: Add various tests to check helper access into ptr_to_btf_id.",
          "message": "Add various tests to check helper access into ptr_to_btf_id.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230313235845.61029-4-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-13 23:08:21 -0700",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/task_kfunc_failure.c",
            "tools/testing/selftests/bpf/progs/task_kfunc_success.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "49b5300f1f8f2b542b31d63043c2febd13edbe3a",
      "merge_subject": "Merge branch 'Support stashing local kptrs with bpf_kptr_xchg'",
      "merge_body": "Dave Marchevsky says:\n\n====================\n\nLocal kptrs are kptrs allocated via bpf_obj_new with a type specified in program\nBTF. A BPF program which creates a local kptr has exclusive control of the\nlifetime of the kptr, and, prior to terminating, must:\n\n  * free the kptr via bpf_obj_drop\n  * If the kptr is a {list,rbtree} node, add the node to a {list, rbtree},\n    thereby passing control of the lifetime to the collection\n\nThis series adds a third option:\n\n  * stash the kptr in a map value using bpf_kptr_xchg\n\nAs indicated by the use of \"stash\" to describe this behavior, the intended use\nof this feature is temporary storage of local kptrs. For example, a sched_ext\n([0]) scheduler may want to create an rbtree node for each new cgroup on cgroup\ninit, but to add that node to the rbtree as part of a separate program which\nruns on enqueue. Stashing the node in a map_value allows its lifetime to outlive\nthe execution of the cgroup_init program.\n\nBehavior:\n\nThere is no semantic difference between adding a kptr to a graph collection and\n\"stashing\" it in a map. In both cases exclusive ownership of the kptr's lifetime\nis passed to some containing data structure, which is responsible for\nbpf_obj_drop'ing it when the container goes away.\n\nSince graph collections also expect exclusive ownership of the nodes they\ncontain, graph nodes cannot be both stashed in a map_value and contained by\ntheir corresponding collection.\n\nImplementation:\n\nTwo observations simplify the verifier changes for this feature. First, kptrs\n(\"referenced kptrs\" until a recent renaming) require registration of a\ndtor function as part of their acquire/release semantics, so that a referenced\nkptr which is placed in a map_value is properly released when the map goes away.\nWe want this exact behavior for local kptrs, but with bpf_obj_drop as the dtor\ninstead of a per-btf_id dtor.\n\nThe second observation is that, in terms of identification, \"referenced kptr\"\nand \"local kptr\" already don't interfere with one another. Consider the\nfollowing example:\n\n  struct node_data {\n          long key;\n          long data;\n          struct bpf_rb_node node;\n  };\n\n  struct map_value {\n          struct node_data __kptr *node;\n  };\n\n  struct {\n          __uint(type, BPF_MAP_TYPE_ARRAY);\n          __type(key, int);\n          __type(value, struct map_value);\n          __uint(max_entries, 1);\n  } some_nodes SEC(\".maps\");\n\n  struct map_value *mapval;\n  struct node_data *res;\n  int key = 0;\n\n  res = bpf_obj_new(typeof(*res));\n  if (!res) { /* err handling */ }\n\n  mapval = bpf_map_lookup_elem(&some_nodes, &key);\n  if (!mapval) { /* err handling */ }\n\n  res = bpf_kptr_xchg(&mapval->node, res);\n  if (res)\n          bpf_obj_drop(res);\n\nThe __kptr tag identifies map_value's node as a referenced kptr, while the\nPTR_TO_BTF_ID which bpf_obj_new returns - a type in some non-vmlinux,\nnon-module BTF - identifies res as a local kptr. Type tag on the pointer\nindicates referenced kptr, while the type of the pointee indicates local kptr.\nSo using existing facilities we can tell the verifier about a \"referenced kptr\"\npointer to a \"local kptr\" pointee.\n\nWhen kptr_xchg'ing a kptr into a map_value, the verifier can recognize local\nkptr types and treat them like referenced kptrs with a properly-typed\nbpf_obj_drop as a dtor.\n\nOther implementation notes:\n  * We don't need to do anything special to enforce \"graph nodes cannot be\n    both stashed in a map_value and contained by their corresponding collection\"\n    * bpf_kptr_xchg both returns and takes as input a (possibly-null) owning\n      reference. It does not accept non-owning references as input by virtue\n      of requiring a ref_obj_id. By definition, if a program has an owning\n      ref to a node, the node isn't in a collection, so it's safe to pass\n      ownership via bpf_kptr_xchg.\n\nSummary of patches:\n\n  * Patch 1 modifies BTF plumbing to support using bpf_obj_drop as a dtor\n  * Patch 2 adds verifier plumbing to support MEM_ALLOC-flagged param for\n    bpf_kptr_xchg\n  * Patch 3 adds selftests exercising the new behavior\n\nChangelog:\n\nv1 -> v2: https://lore.kernel.org/bpf/20230309180111.1618459-1-davemarchevsky@fb.com/\n\nPatch #s used below refer to the patch's position in v1 unless otherwise\nspecified.\n\nPatches 1-3 were applied and are not included in v2.\nRebase onto latest bpf-next: \"libbpf: Revert poisoning of strlcpy\"\n\nPatch 4: \"bpf: Support __kptr to local kptrs\"\n  * Remove !btf_is_kernel(btf) check, WARN_ON_ONCE instead (Alexei)\n\nPatch 6: \"selftests/bpf: Add local kptr stashing test\"\n  * Add test which stashes 2 nodes and later unstashes one of them using a\n    separate BPF program (Alexei)\n  * Fix incorrect runner subtest name for original test (was\n    \"rbtree_add_nodes\")\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-03-10 16:38:06 -0800",
      "commits": [
        {
          "hash": "c8e18754091479fac3f5b6c053c6bc4be0b7fb11",
          "subject": "bpf: Support __kptr to local kptrs",
          "message": "If a PTR_TO_BTF_ID type comes from program BTF - not vmlinux or module\nBTF - it must have been allocated by bpf_obj_new and therefore must be\nfree'd with bpf_obj_drop. Such a PTR_TO_BTF_ID is considered a \"local\nkptr\" and is tagged with MEM_ALLOC type tag by bpf_obj_new.\n\nThis patch adds support for treating __kptr-tagged pointers to \"local\nkptrs\" as having an implicit bpf_obj_drop destructor for referenced kptr\nacquire / release semantics. Consider the following example:\n\n  struct node_data {\n          long key;\n          long data;\n          struct bpf_rb_node node;\n  };\n\n  struct map_value {\n          struct node_data __kptr *node;\n  };\n\n  struct {\n          __uint(type, BPF_MAP_TYPE_ARRAY);\n          __type(key, int);\n          __type(value, struct map_value);\n          __uint(max_entries, 1);\n  } some_nodes SEC(\".maps\");\n\nIf struct node_data had a matching definition in kernel BTF, the verifier would\nexpect a destructor for the type to be registered. Since struct node_data does\nnot match any type in kernel BTF, the verifier knows that there is no kfunc\nthat provides a PTR_TO_BTF_ID to this type, and that such a PTR_TO_BTF_ID can\nonly come from bpf_obj_new. So instead of searching for a registered dtor,\na bpf_obj_drop dtor can be assumed.\n\nThis allows the runtime to properly destruct such kptrs in\nbpf_obj_free_fields, which enables maps to clean up map_vals w/ such\nkptrs when going away.\n\nImplementation notes:\n  * \"kernel_btf\" variable is renamed to \"kptr_btf\" in btf_parse_kptr.\n    Before this patch, the variable would only ever point to vmlinux or\n    module BTFs, but now it can point to some program BTF for local kptr\n    type. It's later used to populate the (btf, btf_id) pair in kptr btf\n    field.\n  * It's necessary to btf_get the program BTF when populating btf_field\n    for local kptr. btf_record_free later does a btf_put.\n  * Behavior for non-local referenced kptrs is not modified, as\n    bpf_find_btf_id helper only searches vmlinux and module BTFs for\n    matching BTF type. If such a type is found, btf_field_kptr's btf will\n    pass btf_is_kernel check, and the associated release function is\n    some one-argument dtor. If btf_is_kernel check fails, associated\n    release function is two-arg bpf_obj_drop_impl. Before this patch\n    only btf_field_kptr's w/ kernel or module BTFs were created.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230310230743.2320707-2-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-03-10 16:38:05 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/btf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/helpers.c",
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "738c96d5e2e3700b370f02ac84a9dc159ca81f25",
          "subject": "bpf: Allow local kptrs to be exchanged via bpf_kptr_xchg",
          "message": "The previous patch added necessary plumbing for verifier and runtime to\nknow what to do with non-kernel PTR_TO_BTF_IDs in map values, but didn't\nprovide any way to get such local kptrs into a map value. This patch\nmodifies verifier handling of bpf_kptr_xchg to allow MEM_ALLOC kptr\ntypes.\n\ncheck_reg_type is modified accept MEM_ALLOC-flagged input to\nbpf_kptr_xchg despite such types not being in btf_ptr_types. This could\nhave been done with a MAYBE_MEM_ALLOC equivalent to MAYBE_NULL, but\nbpf_kptr_xchg is the only helper that I can forsee using\nMAYBE_MEM_ALLOC, so keep it special-cased for now.\n\nThe verifier tags bpf_kptr_xchg retval MEM_ALLOC if and only if the BTF\nassociated with the retval is not kernel BTF.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230310230743.2320707-3-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-03-10 16:38:05 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "5d8d6634cccf1ebd0db4e220e52e7128b030c7b4",
          "subject": "selftests/bpf: Add local kptr stashing test",
          "message": "Add a new selftest, local_kptr_stash, which uses bpf_kptr_xchg to stash\na bpf_obj_new-allocated object in a map. Test the following scenarios:\n\n  * Stash two rb_nodes in an arraymap, don't unstash them, rely on map\n    free to destruct them\n  * Stash two rb_nodes in an arraymap, unstash the second one in a\n    separate program, rely on map free to destruct first\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230310230743.2320707-4-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Dave Marchevsky <davemarchevsky@fb.com>",
          "date": "2023-03-10 16:38:05 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/local_kptr_stash.c",
            "tools/testing/selftests/bpf/progs/local_kptr_stash.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "23e403b326786c05f84515760886c2aedec84964",
      "merge_subject": "Merge branch 'BPF open-coded iterators'",
      "merge_body": "Andrii Nakryiko says:\n\n====================\n\nAdd support for open-coded (aka inline) iterators in BPF world. This is a next\nevolution of gradually allowing more powerful and less restrictive looping and\niteration capabilities to BPF programs.\n\nWe set up a framework for implementing all kinds of iterators (e.g., cgroup,\ntask, file, etc, iterators), but this patch set only implements numbers\niterator, which is used to implement ergonomic bpf_for() for-like construct\n(see patches #4-#5). We also add bpf_for_each(), which is a generic\nforeach-like construct that will work with any kind of open-coded iterator\nimplementation, as long as we stick with bpf_iter_<type>_{new,next,destroy}()\nnaming pattern (which we now enforce on the kernel side).\n\nPatch #1 is preparatory refactoring for easier way to check for special kfunc\ncalls. Patch #2 is adding iterator kfunc registration and validation logic,\nwhich is mostly independent from the rest of open-coded iterator logic, so is\nseparated out for easier reviewing.\n\nThe meat of verifier-side logic is in patch #3. Patch #4 implements numbers\niterator. I kept them separate to have clean reference for how to integrate\nnew iterator types (now even simpler to do than in v1 of this patch set).\nPatch #5 adds bpf_for(), bpf_for_each(), and bpf_repeat() macros to\nbpf_misc.h, and also adds yet another pyperf test variant, now with bpf_for()\nloop. Patch #6 is verification tests, based on numbers iterator (as the only\navailable right now). Patch #7 actually tests runtime behavior of numbers\niterator.\n\nFinally, with changes in v2, it's possible and trivial to implement custom\niterators completely in kernel modules, which we showcase and test by adding\na simple iterator returning same number a given number of times to\nbpf_testmod. Patch #8 is where all this happens and is tested.\n\nMost of the relevant details are in corresponding commit messages or code\ncomments.\n\nv4->v5:\n  - fixing missed inner for() in is_iter_reg_valid_uninit, and fixed return\n    false (kernel test robot);\n  - typo fixes and comment/commit description improvements throughout the\n    patch set;\nv3->v4:\n  - remove unused variable from is_iter_reg_valid_init (kernel test robot);\nv2->v3:\n  - remove special kfunc leftovers for bpf_iter_num_{new,next,destroy};\n  - add iters/testmod_seq* to DENYLIST.s390x, it doesn't support kfuncs in\n    modules yet (CI);\nv1->v2:\n  - rebased on latest, dropping previously landed preparatory patches;\n  - each iterator type now have its own `struct bpf_iter_<type>` which allows\n    each iterator implementation to use exactly as much stack space as\n    necessary, allowing to avoid runtime allocations (Alexei);\n  - reworked how iterator kfuncs are defined, no verifier changes are required\n    when adding new iterator type;\n  - added bpf_testmod-based iterator implementation;\n  - address the rest of feedback, comments, commit message adjustment, etc.\n\nCc: Tejun Heo <tj@kernel.org>\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-03-08 16:19:51 -0800",
      "commits": [
        {
          "hash": "07236eab7a3139da97aef9f5f21f403be82a82ea",
          "subject": "bpf: factor out fetching basic kfunc metadata",
          "message": "Factor out logic to fetch basic kfunc metadata based on struct bpf_insn.\nThis is not exactly short or trivial code to just copy/paste and this\ninformation is sometimes necessary in other parts of the verifier logic.\nSubsequent patches will rely on this to determine if an instruction is\na kfunc call to iterator next method.\n\nNo functional changes intended, including that verbose() warning\nbehavior when kfunc is not allowed for a particular program type.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230308184121.1165081-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-03-08 16:19:50 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "215bf4962f6c9605710012fad222a5fec001b3ad",
          "subject": "bpf: add iterator kfuncs registration and validation logic",
          "message": "Add ability to register kfuncs that implement BPF open-coded iterator\ncontract and enforce naming and function proto convention. Enforcement\nhappens at the time of kfunc registration and significantly simplifies\nthe rest of iterators logic in the verifier.\n\nMore details follow in subsequent patches, but we enforce the following\nconditions.\n\nAll kfuncs (constructor, next, destructor) have to be named consistenly\nas bpf_iter_<type>_{new,next,destroy}(), respectively. <type> represents\niterator type, and iterator state should be represented as a matching\n`struct bpf_iter_<type>` state type. Also, all iter kfuncs should have\na pointer to this `struct bpf_iter_<type>` as the very first argument.\n\nAdditionally:\n  - Constructor, i.e., bpf_iter_<type>_new(), can have arbitrary extra\n  number of arguments. Return type is not enforced either.\n  - Next method, i.e., bpf_iter_<type>_next(), has to return a pointer\n  type and should have exactly one argument: `struct bpf_iter_<type> *`\n  (const/volatile/restrict and typedefs are ignored).\n  - Destructor, i.e., bpf_iter_<type>_destroy(), should return void and\n  should have exactly one argument, similar to the next method.\n  - struct bpf_iter_<type> size is enforced to be positive and\n  a multiple of 8 bytes (to fit stack slots correctly).\n\nSuch strictness and consistency allows to build generic helpers\nabstracting important, but boilerplate, details to be able to use\nopen-coded iterators effectively and ergonomically (see bpf_for_each()\nin subsequent patches). It also simplifies the verifier logic in some\nplaces. At the same time, this doesn't hurt generality of possible\niterator implementations. Win-win.\n\nConstructor kfunc is marked with a new KF_ITER_NEW flags, next method is\nmarked with KF_ITER_NEXT (and should also have KF_RET_NULL, of course),\nwhile destructor kfunc is marked as KF_ITER_DESTROY.\n\nAdditionally, we add a trivial kfunc name validation: it should be\na valid non-NULL and non-empty string.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230308184121.1165081-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-03-08 16:19:50 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "include/linux/btf.h",
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "06accc8779c1d558a5b5a21f2ac82b0c95827ddd",
          "subject": "bpf: add support for open-coded iterator loops",
          "message": "Teach verifier about the concept of the open-coded (or inline) iterators.\n\nThis patch adds generic iterator loop verification logic, new STACK_ITER\nstack slot type to contain iterator state, and necessary kfunc plumbing\nfor iterator's constructor, destructor and next methods. Next patch\nimplements first specific iterator (numbers iterator for implementing\nfor() loop logic). Such split allows to have more focused commits for\nverifier logic and separate commit that we could point later to\ndemonstrating  what does it take to add a new kind of iterator.\n\nEach kind of iterator has its own associated struct bpf_iter_<type>,\nwhere <type> denotes a specific type of iterator. struct bpf_iter_<type>\nstate is supposed to live on BPF program stack, so there will be no way\nto change its size later on without breaking backwards compatibility, so\nchoose wisely! But given this struct is specific to a given <type> of\niterator, this allows a lot of flexibility: simple iterators could be\nfine with just one stack slot (8 bytes), like numbers iterator in the\nnext patch, while some other more complicated iterators might need way\nmore to keep their iterator state. Either way, such design allows to\navoid runtime memory allocations, which otherwise would be necessary if\nwe fixed on-the-stack size and it turned out to be too small for a given\niterator implementation.\n\nThe way BPF verifier logic is implemented, there are no artificial\nrestrictions on a number of active iterators, it should work correctly\nusing multiple active iterators at the same time. This also means you\ncan have multiple nested iteration loops. struct bpf_iter_<type>\nreference can be safely passed to subprograms as well.\n\nGeneral flow is easiest to demonstrate with a simple example using\nnumber iterator implemented in next patch. Here's the simplest possible\nloop:\n\n  struct bpf_iter_num it;\n  int *v;\n\n  bpf_iter_num_new(&it, 2, 5);\n  while ((v = bpf_iter_num_next(&it))) {\n      bpf_printk(\"X = %d\", *v);\n  }\n  bpf_iter_num_destroy(&it);\n\nAbove snippet should output \"X = 2\", \"X = 3\", \"X = 4\". Note that 5 is\nexclusive and is not returned. This matches similar APIs (e.g., slices\nin Go or Rust) that implement a range of elements, where end index is\nnon-inclusive.\n\nIn the above example, we see a trio of function:\n  - constructor, bpf_iter_num_new(), which initializes iterator state\n  (struct bpf_iter_num it) on the stack. If any of the input arguments\n  are invalid, constructor should make sure to still initialize it such\n  that subsequent bpf_iter_num_next() calls will return NULL. I.e., on\n  error, return error and construct empty iterator.\n  - next method, bpf_iter_num_next(), which accepts pointer to iterator\n  state and produces an element. Next method should always return\n  a pointer. The contract between BPF verifier is that next method will\n  always eventually return NULL when elements are exhausted. Once NULL is\n  returned, subsequent next calls should keep returning NULL. In the\n  case of numbers iterator, bpf_iter_num_next() returns a pointer to an int\n  (storage for this integer is inside the iterator state itself),\n  which can be dereferenced after corresponding NULL check.\n  - once done with the iterator, it's mandated that user cleans up its\n  state with the call to destructor, bpf_iter_num_destroy() in this\n  case. Destructor frees up any resources and marks stack space used by\n  struct bpf_iter_num as usable for something else.\n\nAny other iterator implementation will have to implement at least these\nthree methods. It is enforced that for any given type of iterator only\napplicable constructor/destructor/next are callable. I.e., verifier\nensures you can't pass number iterator state into, say, cgroup\niterator's next method.\n\nIt is important to keep the naming pattern consistent to be able to\ncreate generic macros to help with BPF iter usability. E.g., one\nof the follow up patches adds generic bpf_for_each() macro to bpf_misc.h\nin selftests, which allows to utilize iterator \"trio\" nicely without\nhaving to code the above somewhat tedious loop explicitly every time.\nThis is enforced at kfunc registration point by one of the previous\npatches in this series.\n\nAt the implementation level, iterator state tracking for verification\npurposes is very similar to dynptr. We add STACK_ITER stack slot type,\nreserve necessary number of slots, depending on\nsizeof(struct bpf_iter_<type>), and keep track of necessary extra state\nin the \"main\" slot, which is marked with non-zero ref_obj_id. Other\nslots are also marked as STACK_ITER, but have zero ref_obj_id. This is\nsimpler than having a separate \"is_first_slot\" flag.\n\nAnother big distinction is that STACK_ITER is *always refcounted*, which\nsimplifies implementation without sacrificing usability. So no need for\nextra \"iter_id\", no need to anticipate reuse of STACK_ITER slots for new\nconstructors, etc. Keeping it simple here.\n\nAs far as the verification logic goes, there are two extensive comments:\nin process_iter_next_call() and iter_active_depths_differ() explaining\nsome important and sometimes subtle aspects. Please refer to them for\ndetails.\n\nBut from 10,000-foot point of view, next methods are the points of\nforking a verification state, which are conceptually similar to what\nverifier is doing when validating conditional jump. We branch out at\na `call bpf_iter_<type>_next` instruction and simulate two outcomes:\nNULL (iteration is done) and non-NULL (new element is returned). NULL is\nsimulated first and is supposed to reach exit without looping. After\nthat non-NULL case is validated and it either reaches exit (for trivial\nexamples with no real loop), or reaches another `call bpf_iter_<type>_next`\ninstruction with the state equivalent to already (partially) validated\none. State equivalency at that point means we technically are going to\nbe looping forever without \"breaking out\" out of established \"state\nenvelope\" (i.e., subsequent iterations don't add any new knowledge or\nconstraints to the verifier state, so running 1, 2, 10, or a million of\nthem doesn't matter). But taking into account the contract stating that\niterator next method *has to* return NULL eventually, we can conclude\nthat loop body is safe and will eventually terminate. Given we validated\nlogic outside of the loop (NULL case), and concluded that loop body is\nsafe (though potentially looping many times), verifier can claim safety\nof the overall program logic.\n\nThe rest of the patch is necessary plumbing for state tracking, marking,\nvalidation, and necessary further kfunc plumbing to allow implementing\niterator constructor, destructor, and next methods.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230308184121.1165081-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-03-08 16:19:50 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "6018e1f407cccf39b804d1f75ad4de7be4e6cc45",
          "subject": "bpf: implement numbers iterator",
          "message": "Implement the first open-coded iterator type over a range of integers.\n\nIt's public API consists of:\n  - bpf_iter_num_new() constructor, which accepts [start, end) range\n    (that is, start is inclusive, end is exclusive).\n  - bpf_iter_num_next() which will keep returning read-only pointer to int\n    until the range is exhausted, at which point NULL will be returned.\n    If bpf_iter_num_next() is kept calling after this, NULL will be\n    persistently returned.\n  - bpf_iter_num_destroy() destructor, which needs to be called at some\n    point to clean up iterator state. BPF verifier enforces that iterator\n    destructor is called at some point before BPF program exits.\n\nNote that `start = end = X` is a valid combination to setup an empty\niterator. bpf_iter_num_new() will return 0 (success) for any such\ncombination.\n\nIf bpf_iter_num_new() detects invalid combination of input arguments, it\nreturns error, resets iterator state to, effectively, empty iterator, so\nany subsequent call to bpf_iter_num_next() will keep returning NULL.\n\nBPF verifier has no knowledge that returned integers are in the\n[start, end) value range, as both `start` and `end` are not statically\nknown and enforced: they are runtime values.\n\nWhile the implementation is pretty trivial, some care needs to be taken\nto avoid overflows and underflows. Subsequent selftests will validate\ncorrectness of [start, end) semantics, especially around extremes\n(INT_MIN and INT_MAX).\n\nSimilarly to bpf_loop(), we enforce that no more than BPF_MAX_LOOPS can\nbe specified.\n\nbpf_iter_num_{new,next,destroy}() is a logical evolution from bounded\nBPF loops and bpf_loop() helper and is the basis for implementing\nergonomic BPF loops with no statically known or verified bounds.\nSubsequent patches implement bpf_for() macro, demonstrating how this can\nbe wrapped into something that works and feels like a normal for() loop\nin C language.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230308184121.1165081-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-03-08 16:19:51 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/bpf_iter.c",
            "kernel/bpf/helpers.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "8c2b5e90505e474f36ecc3b7f3f8298b59d72e91",
          "subject": "selftests/bpf: add bpf_for_each(), bpf_for(), and bpf_repeat() macros",
          "message": "Add bpf_for_each(), bpf_for(), and bpf_repeat() macros that make writing\nopen-coded iterator-based loops much more convenient and natural. These\nmacros utilize cleanup attribute to ensure proper destruction of the\niterator and thanks to that manage to provide the ergonomics that is\nvery close to C language's for() construct. Typical loop would look like:\n\n  int i;\n  int arr[N];\n\n  bpf_for(i, 0, N) {\n      /* verifier will know that i >= 0 && i < N, so could be used to\n       * directly access array elements with no extra checks\n       */\n       arr[i] = i;\n  }\n\nbpf_repeat() is very similar, but it doesn't expose iteration number and\nis meant as a simple \"repeat action N times\" loop:\n\n  bpf_repeat(N) { /* whatever, N times */ }\n\nNote that `break` and `continue` statements inside the {} block work as\nexpected.\n\nbpf_for_each() is a generalization over any kind of BPF open-coded\niterator allowing to use for-each-like approach instead of calling\nlow-level bpf_iter_<type>_{new,next,destroy}() APIs explicitly. E.g.:\n\n  struct cgroup *cg;\n\n  bpf_for_each(cgroup, cg, some, input, args) {\n      /* do something with each cg */\n  }\n\nwould call (not-yet-implemented) bpf_iter_cgroup_{new,next,destroy}()\nfunctions to form a loop over cgroups, where `some, input, args` are\npassed verbatim into constructor as\n\n  bpf_iter_cgroup_new(&it, some, input, args).\n\nAs a first demonstration, add pyperf variant based on the bpf_for() loop.\n\nAlso clean up a few tests that either included bpf_misc.h header\nunnecessarily from the user-space, which is unsupported, or included it\nbefore any common types are defined (and thus leading to unnecessary\ncompilation warnings, potentially).\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230308184121.1165081-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-03-08 16:19:51 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/bpf_verif_scale.c",
            "tools/testing/selftests/bpf/prog_tests/uprobe_autoattach.c",
            "tools/testing/selftests/bpf/progs/bpf_misc.h",
            "tools/testing/selftests/bpf/progs/lsm.c",
            "tools/testing/selftests/bpf/progs/pyperf.h",
            "tools/testing/selftests/bpf/progs/pyperf600_iter.c",
            "tools/testing/selftests/bpf/progs/pyperf600_nounroll.c"
          ]
        },
        {
          "hash": "57400dcce6c2cf3985120c4ee28b37a1f4238dbb",
          "subject": "selftests/bpf: add iterators tests",
          "message": "Add various tests for open-coded iterators. Some of them excercise\nvarious possible coding patterns in C, some go down to low-level\nassembly for more control over various conditions, especially invalid\nones.\n\nWe also make use of bpf_for(), bpf_for_each(), bpf_repeat() macros in\nsome of these tests.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230308184121.1165081-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-03-08 16:19:51 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/iters.c",
            "tools/testing/selftests/bpf/progs/bpf_misc.h",
            "tools/testing/selftests/bpf/progs/iters.c",
            "tools/testing/selftests/bpf/progs/iters_looping.c",
            "tools/testing/selftests/bpf/progs/iters_state_safety.c"
          ]
        },
        {
          "hash": "f59b146092653bcf014ccdc9bd8bc94e79065ce3",
          "subject": "selftests/bpf: add number iterator tests",
          "message": "Add number iterator (bpf_iter_num_{new,next,destroy}()) tests,\nvalidating the correct handling of various corner and common cases\n*at runtime*.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230308184121.1165081-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-03-08 16:19:51 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/iters.c",
            "tools/testing/selftests/bpf/progs/iters_num.c"
          ]
        },
        {
          "hash": "7e86a8c4ac8d5dcf7dd58f5a4779d1a6ff0a827d",
          "subject": "selftests/bpf: implement and test custom testmod_seq iterator",
          "message": "Implement a trivial iterator returning same specified integer value\nN times as part of bpf_testmod kernel module. Add selftests to validate\neverything works end to end.\n\nWe also reuse these tests as \"verification-only\" tests to validate that\nkernel prints the state of custom kernel module-defined iterator correctly:\n\n  fp-16=iter_testmod_seq(ref_id=1,state=drained,depth=0)\n\n\"testmod_seq\" part is an iterator type, and is coming from module's BTF\ndata dynamically at runtime.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230308184121.1165081-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Andrii Nakryiko <andrii@kernel.org>",
          "date": "2023-03-08 16:19:51 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/DENYLIST.s390x",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.c",
            "tools/testing/selftests/bpf/bpf_testmod/bpf_testmod.h",
            "tools/testing/selftests/bpf/prog_tests/iters.c",
            "tools/testing/selftests/bpf/progs/iters_testmod_seq.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "2564a031ab7be975fb907197227cc5ae0aecff0f",
      "merge_subject": "Merge branch 'bpf: allow ctx writes using BPF_ST_MEM instruction'",
      "merge_body": "Eduard Zingerman says:\n\n====================\n\nChanges v1 -> v2, suggested by Alexei:\n- Resolved conflict with recent commit:\n  6fcd486b3a0a (\"bpf: Refactor RCU enforcement in the verifier\");\n- Variable `ctx_access` removed in function `convert_ctx_accesses()`;\n- Macro `BPF_COPY_STORE` renamed to `BPF_EMIT_STORE` and fixed to\n  correctly extract original store instruction class from code.\n\nOriginal message follows:\n\nThe function verifier.c:convert_ctx_access() applies some rewrites to BPF\ninstructions that read from or write to the BPF program context.\nFor example, the write instruction for the `struct bpf_sockopt::retval`\nfield:\n\n    *(u32 *)(r1 + offsetof(struct bpf_sockopt, retval)) = r2\n\nIs transformed to:\n\n    *(u64 *)(r1 + offsetof(struct bpf_sockopt_kern, tmp_reg)) = r9\n    r9 = *(u64 *)(r1 + offsetof(struct bpf_sockopt_kern, current_task))\n    r9 = *(u64 *)(r9 + offsetof(struct task_struct, bpf_ctx))\n    *(u32 *)(r9 + offsetof(struct bpf_cg_run_ctx, retval)) = r2\n    r9 = *(u64 *)(r1 + offsetof(struct bpf_sockopt_kern, tmp_reg))\n\nCurrently, the verifier only supports such transformations for LDX\n(memory-to-register read) and STX (register-to-memory write) instructions.\nError is reported for ST instructions (immediate-to-memory write).\nThis is fine because clang does not currently emit ST instructions.\n\nHowever, new `-mcpu=v4` clang flag is planned, which would allow to emit\nST instructions (discussed in [1]).\n\nThis patch-set adjusts the verifier to support ST instructions in\n`verifier.c:convert_ctx_access()`.\n\nThe patches #1 and #2 were previously shared as part of RFC [2]. The\nchanges compared to that RFC are:\n- In patch #1, a bug in the handling of the\n  `struct __sk_buff::queue_mapping` field was fixed.\n- Patch #3 is added, which is a set of disassembler-based test cases for\n  context access rewrites. The test cases cover all fields for which the\n  handling code is modified in patch #1.\n\n[1] Propose some new instructions for -mcpu=v4\n    https://lore.kernel.org/bpf/4bfe98be-5333-1c7e-2f6d-42486c8ec039@meta.com/\n[2] RFC Support for BPF_ST instruction in LLVM C compiler\n    https://lore.kernel.org/bpf/20221231163122.1360813-1-eddyz87@gmail.com/\n[3] v1\n    https://lore.kernel.org/bpf/20230302225507.3413720-1-eddyz87@gmail.com/\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-03-03 21:41:46 -0800",
      "commits": [
        {
          "hash": "0d80a619c113d0e216dbffa56b2d5ccc079ee520",
          "subject": "bpf: allow ctx writes using BPF_ST_MEM instruction",
          "message": "Lift verifier restriction to use BPF_ST_MEM instructions to write to\ncontext data structures. This requires the following changes:\n - verifier.c:do_check() for BPF_ST updated to:\n   - no longer forbid writes to registers of type PTR_TO_CTX;\n   - track dst_reg type in the env->insn_aux_data[...].ptr_type field\n     (same way it is done for BPF_STX and BPF_LDX instructions).\n - verifier.c:convert_ctx_access() and various callbacks invoked by\n   it are updated to handled BPF_ST instruction alongside BPF_STX.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20230304011247.566040-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-03-03 21:41:46 -0800",
          "modified_files": [
            "kernel/bpf/cgroup.c",
            "kernel/bpf/verifier.c",
            "net/core/filter.c",
            "tools/testing/selftests/bpf/verifier/ctx.c"
          ]
        },
        {
          "hash": "806f81cd1ee30c66a3d2a4cd18b13c97429397a0",
          "subject": "selftests/bpf: test if pointer type is tracked for BPF_ST_MEM",
          "message": "Check that verifier tracks pointer types for BPF_ST_MEM instructions\nand reports error if pointer types do not match for different\nexecution branches.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20230304011247.566040-3-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-03-03 21:41:46 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/verifier/unpriv.c"
          ]
        },
        {
          "hash": "71cf4d027ad53a1e2847191ac14e50132d35a6a7",
          "subject": "selftests/bpf: Disassembler tests for verifier.c:convert_ctx_access()",
          "message": "Function verifier.c:convert_ctx_access() applies some rewrites to BPF\ninstructions that read or write BPF program context. This commit adds\nmachinery to allow test cases that inspect BPF program after these\nrewrites are applied.\n\nAn example of a test case:\n\n  {\n        // Shorthand for field offset and size specification\n\tN(CGROUP_SOCKOPT, struct bpf_sockopt, retval),\n\n        // Pattern generated for field read\n\t.read  = \"$dst = *(u64 *)($ctx + bpf_sockopt_kern::current_task);\"\n\t\t \"$dst = *(u64 *)($dst + task_struct::bpf_ctx);\"\n\t\t \"$dst = *(u32 *)($dst + bpf_cg_run_ctx::retval);\",\n\n        // Pattern generated for field write\n\t.write = \"*(u64 *)($ctx + bpf_sockopt_kern::tmp_reg) = r9;\"\n\t\t \"r9 = *(u64 *)($ctx + bpf_sockopt_kern::current_task);\"\n\t\t \"r9 = *(u64 *)(r9 + task_struct::bpf_ctx);\"\n\t\t \"*(u32 *)(r9 + bpf_cg_run_ctx::retval) = $src;\"\n\t\t \"r9 = *(u64 *)($ctx + bpf_sockopt_kern::tmp_reg);\" ,\n  },\n\nFor each test case, up to three programs are created:\n- One that uses BPF_LDX_MEM to read the context field.\n- One that uses BPF_STX_MEM to write to the context field.\n- One that uses BPF_ST_MEM to write to the context field.\n\nThe disassembly of each program is compared with the pattern specified\nin the test case.\n\nKernel code for disassembly is reused (as is in the bpftool).\nTo keep Makefile changes to the minimum, symbolic links to\n`kernel/bpf/disasm.c` and `kernel/bpf/disasm.h ` are added.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20230304011247.566040-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Eduard Zingerman <eddyz87@gmail.com>",
          "date": "2023-03-03 21:41:46 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/Makefile",
            "tools/testing/selftests/bpf/disasm.c",
            "tools/testing/selftests/bpf/disasm.h",
            "tools/testing/selftests/bpf/prog_tests/ctx_rewrite.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "db55174d05ee6bed9d0583ba08e99c891ef0ed05",
      "merge_subject": "Merge branch 'bpf-kptr-rcu'",
      "merge_body": "Alexei Starovoitov says:\n\n====================\nv4->v5:\nfix typos, add acks.\n\nv3->v4:\n- patch 3 got much cleaner after BPF_KPTR_RCU was removed as suggested by David.\n\n- make KF_RCU stronger and require that bpf program checks for NULL\nbefore passing such pointers into kfunc. The prog has to do that anyway\nto access fields and it aligns with BTF_TYPE_SAFE_RCU allowlist.\n\n- New patch 6: refactor RCU enforcement in the verifier.\nThe patches 2,3,6 are part of one feature.\nThe 2 and 3 alone are incomplete, since RCU pointers are barely useful\nwithout bpf_rcu_read_lock/unlock in GCC compiled kernel.\nEven if GCC lands support for btf_type_tag today it will take time\nto mandate that version for kernel builds. Hence go with allow list\napproach. See patch 6 for details.\nThis allows to start strict enforcement of TRUSTED | UNTRUSTED\nin one part of PTR_TO_BTF_ID accesses.\nOne step closer to KF_TRUSTED_ARGS by default.\n\nv2->v3:\n- Instead of requiring bpf progs to tag fields with __kptr_rcu\nteach the verifier to infer RCU properties based on the type.\nBPF_KPTR_RCU becomes kernel internal type of struct btf_field.\n- Add patch 2 to tag cgroups and dfl_cgrp as trusted.\nThat bug was spotted by BPF CI on clang compiler kernels,\nsince patch 3 is doing:\nstatic bool in_rcu_cs(struct bpf_verifier_env *env)\n{\n        return env->cur_state->active_rcu_lock || !env->prog->aux->sleepable;\n}\nwhich makes all non-sleepable programs behave like they have implicit\nrcu_read_lock around them. Which is the case in practice.\nIt was fine on gcc compiled kernels where task->cgroup deference was producing\nPTR_TO_BTF_ID, but on clang compiled kernels task->cgroup deference was\nproducing PTR_TO_BTF_ID | MEM_RCU | MAYBE_NULL, which is more correct,\nbut selftests were failing. Patch 2 fixes this discrepancy.\nWith few more patches like patch 2 we can make KF_TRUSTED_ARGS default\nfor kfuncs and helpers.\n- Add comment in selftest patch 5 that it's verifier only check.\n\nv1->v2:\nInstead of agressively allow dereferenced kptr_rcu pointers into KF_TRUSTED_ARGS\nkfuncs only allow them into KF_RCU funcs.\nThe KF_RCU flag is a weaker version of KF_TRUSTED_ARGS. The kfuncs marked with\nKF_RCU expect either PTR_TRUSTED or MEM_RCU arguments. The verifier guarantees\nthat the objects are valid and there is no use-after-free, but the pointers\nmaybe NULL and pointee object's reference count could have reached zero, hence\nkfuncs must do != NULL check and consider refcnt==0 case when accessing such\narguments.\nNo changes in patch 1.\nPatches 2,3,4 adjusted with above behavior.\n\nv1:\nThe __kptr_ref turned out to be too limited, since any \"trusted\" pointer access\nrequires bpf_kptr_xchg() which is impractical when the same pointer needs\nto be dereferenced by multiple cpus.\nThe __kptr \"untrusted\" only access isn't very useful in practice.\nRename __kptr to __kptr_untrusted with eventual goal to deprecate it,\nand rename __kptr_ref to __kptr, since that looks to be more common use of kptrs.\nIntroduce __kptr_rcu that can be directly dereferenced and used similar\nto native kernel C code.\nOnce bpf_cpumask and task_struct kfuncs are converted to observe RCU GP\nwhen refcnt goes to zero, both __kptr and __kptr_untrusted can be deprecated\nand __kptr_rcu can become the only __kptr tag.\n====================\n\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>",
      "merge_author": "Daniel Borkmann <daniel@iogearbox.net>",
      "merge_date": "2023-03-03 17:42:29 +0100",
      "commits": [
        {
          "hash": "03b77e17aeb22a5935ea20d585ca6a1f2947e62b",
          "subject": "bpf: Rename __kptr_ref -> __kptr and __kptr -> __kptr_untrusted.",
          "message": "__kptr meant to store PTR_UNTRUSTED kernel pointers inside bpf maps.\nThe concept felt useful, but didn't get much traction,\nsince bpf_rdonly_cast() was added soon after and bpf programs received\na simpler way to access PTR_UNTRUSTED kernel pointers\nwithout going through restrictive __kptr usage.\n\nRename __kptr_ref -> __kptr and __kptr -> __kptr_untrusted to indicate\nits intended usage.\nThe main goal of __kptr_untrusted was to read/write such pointers\ndirectly while bpf_kptr_xchg was a mechanism to access refcnted\nkernel pointers. The next patch will allow RCU protected __kptr access\nwith direct read. At that point __kptr_untrusted will be deprecated.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230303041446.3630-2-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-03 17:42:20 +0100",
          "modified_files": [
            "Documentation/bpf/bpf_design_QA.rst",
            "Documentation/bpf/cpumasks.rst",
            "Documentation/bpf/kfuncs.rst",
            "kernel/bpf/btf.c",
            "tools/lib/bpf/bpf_helpers.h",
            "tools/testing/selftests/bpf/progs/cb_refs.c",
            "tools/testing/selftests/bpf/progs/cgrp_kfunc_common.h",
            "tools/testing/selftests/bpf/progs/cpumask_common.h",
            "tools/testing/selftests/bpf/progs/jit_probe_mem.c",
            "tools/testing/selftests/bpf/progs/lru_bug.c",
            "tools/testing/selftests/bpf/progs/map_kptr.c",
            "tools/testing/selftests/bpf/progs/map_kptr_fail.c",
            "tools/testing/selftests/bpf/progs/task_kfunc_common.h",
            "tools/testing/selftests/bpf/test_verifier.c"
          ]
        },
        {
          "hash": "8d093b4e95a2a16a2cfcd36869b348a17112fabe",
          "subject": "bpf: Mark cgroups and dfl_cgrp fields as trusted.",
          "message": "bpf programs sometimes do:\nbpf_cgrp_storage_get(&map, task->cgroups->dfl_cgrp, ...);\nIt is safe to do, because cgroups->dfl_cgrp pointer is set diring init and\nnever changes. The task->cgroups is also never NULL. It is also set during init\nand will change when task switches cgroups. For any trusted task pointer\ndereference of cgroups and dfl_cgrp should yield trusted pointers. The verifier\nwasn't aware of this. Hence in gcc compiled kernels task->cgroups dereference\nwas producing PTR_TO_BTF_ID without modifiers while in clang compiled kernels\nthe verifier recognizes __rcu tag in cgroups field and produces\nPTR_TO_BTF_ID | MEM_RCU | MAYBE_NULL.\nTag cgroups and dfl_cgrp as trusted to equalize clang and gcc behavior.\nWhen GCC supports btf_type_tag such tagging will done directly in the type.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: David Vernet <void@manifault.com>\nAcked-by: Tejun Heo <tj@kernel.org>\nLink: https://lore.kernel.org/bpf/20230303041446.3630-3-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-03 17:42:20 +0100",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "20c09d92faeefb8536f705d3a4629e0dc314c8a1",
          "subject": "bpf: Introduce kptr_rcu.",
          "message": "The life time of certain kernel structures like 'struct cgroup' is protected by RCU.\nHence it's safe to dereference them directly from __kptr tagged pointers in bpf maps.\nThe resulting pointer is MEM_RCU and can be passed to kfuncs that expect KF_RCU.\nDerefrence of other kptr-s returns PTR_UNTRUSTED.\n\nFor example:\nstruct map_value {\n   struct cgroup __kptr *cgrp;\n};\n\nSEC(\"tp_btf/cgroup_mkdir\")\nint BPF_PROG(test_cgrp_get_ancestors, struct cgroup *cgrp_arg, const char *path)\n{\n  struct cgroup *cg, *cg2;\n\n  cg = bpf_cgroup_acquire(cgrp_arg); // cg is PTR_TRUSTED and ref_obj_id > 0\n  bpf_kptr_xchg(&v->cgrp, cg);\n\n  cg2 = v->cgrp; // This is new feature introduced by this patch.\n  // cg2 is PTR_MAYBE_NULL | MEM_RCU.\n  // When cg2 != NULL, it's a valid cgroup, but its percpu_ref could be zero\n\n  if (cg2)\n    bpf_cgroup_ancestor(cg2, level); // safe to do.\n}\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Tejun Heo <tj@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230303041446.3630-4-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-03 17:42:20 +0100",
          "modified_files": [
            "Documentation/bpf/kfuncs.rst",
            "include/linux/btf.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c",
            "net/bpf/test_run.c",
            "tools/testing/selftests/bpf/progs/cgrp_kfunc_failure.c",
            "tools/testing/selftests/bpf/progs/map_kptr_fail.c",
            "tools/testing/selftests/bpf/verifier/calls.c",
            "tools/testing/selftests/bpf/verifier/map_kptr.c"
          ]
        },
        {
          "hash": "838bd4ac9aa35bdf43bf0199fa8eef9d3a004611",
          "subject": "selftests/bpf: Add a test case for kptr_rcu.",
          "message": "Tweak existing map_kptr test to check kptr_rcu.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230303041446.3630-5-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-03 17:42:20 +0100",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/map_kptr.c"
          ]
        },
        {
          "hash": "0047d8343f6042c4feea24072ef254d47b8a33b3",
          "subject": "selftests/bpf: Tweak cgroup kfunc test.",
          "message": "Adjust cgroup kfunc test to dereference RCU protected cgroup pointer\nas PTR_TRUSTED and pass into KF_TRUSTED_ARGS kfunc.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230303041446.3630-6-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-03 17:42:20 +0100",
          "modified_files": [
            "tools/testing/selftests/bpf/progs/cgrp_kfunc_success.c"
          ]
        },
        {
          "hash": "6fcd486b3a0a628c41f12b3a7329a18a2c74b351",
          "subject": "bpf: Refactor RCU enforcement in the verifier.",
          "message": "bpf_rcu_read_lock/unlock() are only available in clang compiled kernels. Lack\nof such key mechanism makes it impossible for sleepable bpf programs to use RCU\npointers.\n\nAllow bpf_rcu_read_lock/unlock() in GCC compiled kernels (though GCC doesn't\nsupport btf_type_tag yet) and allowlist certain field dereferences in important\ndata structures like tast_struct, cgroup, socket that are used by sleepable\nprograms either as RCU pointer or full trusted pointer (which is valid outside\nof RCU CS). Use BTF_TYPE_SAFE_RCU and BTF_TYPE_SAFE_TRUSTED macros for such\ntagging. They will be removed once GCC supports btf_type_tag.\n\nWith that refactor check_ptr_to_btf_access(). Make it strict in enforcing\nPTR_TRUSTED and PTR_UNTRUSTED while deprecating old PTR_TO_BTF_ID without\nmodifier flags. There is a chance that this strict enforcement might break\nexisting programs (especially on GCC compiled kernels), but this cleanup has to\nstart sooner than later. Note PTR_TO_CTX access still yields old deprecated\nPTR_TO_BTF_ID. Once it's converted to strict PTR_TRUSTED or PTR_UNTRUSTED the\nkfuncs and helpers will be able to default to KF_TRUSTED_ARGS. KF_RCU will\nremain as a weaker version of KF_TRUSTED_ARGS where obj refcnt could be 0.\n\nAdjust rcu_read_lock selftest to run on gcc and clang compiled kernels.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230303041446.3630-7-alexei.starovoitov@gmail.com",
          "author": "Alexei Starovoitov <ast@kernel.org>",
          "date": "2023-03-03 17:42:20 +0100",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/bpf_verifier.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/cpumask.c",
            "kernel/bpf/verifier.c",
            "tools/testing/selftests/bpf/prog_tests/cgrp_local_storage.c",
            "tools/testing/selftests/bpf/prog_tests/rcu_read_lock.c",
            "tools/testing/selftests/bpf/progs/cgrp_ls_sleepable.c",
            "tools/testing/selftests/bpf/progs/cpumask_failure.c",
            "tools/testing/selftests/bpf/progs/nested_trust_failure.c",
            "tools/testing/selftests/bpf/progs/rcu_read_lock.c",
            "tools/testing/selftests/bpf/verifier/calls.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "6c18e375310560915dc10ef41f789f2dfdf838bf",
      "merge_subject": "Merge branch 'Add support for kptrs in more BPF maps'",
      "merge_body": "Kumar Kartikeya Dwivedi says:\n\n====================\n\nThis set adds support for kptrs in percpu hashmaps, percpu LRU hashmaps,\nand local storage maps (covering sk, cgrp, task, inode).\n\nTests are expanded to test more existing maps at runtime and also test\nthe code path for the local storage maps (which is shared by all\nimplementations).\n\nA question for reviewers is what the position of the BPF runtime should\nbe on dealing with reference cycles that can be created by BPF programs\nat runtime using this additional support. For instance, one can store\nthe kptr of the task in its own task local storage, creating a cycle\nwhich prevents destruction of task local storage. Cycles can be formed\nusing arbitrarily long kptr ownership chains. Therefore, just preventing\nstorage of such kptrs in some maps is not a sufficient solution, and is\nmore likely to hurt usability.\n\nThere is precedence in existing runtimes which promise memory safety,\nlike Rust, where reference cycles and memory leaks are permitted.\nHowever, traditionally the safety guarantees of BPF have been stronger.\nThus, more discussion and thought is invited on this topic to ensure we\ncover all usage aspects.\n\nChangelog:\n----------\nv2 -> v3\nv2: https://lore.kernel.org/bpf/20230221200646.2500777-1-memxor@gmail.com/\n\n * Fix a use-after-free bug in local storage patch\n * Fix selftest for aarch64 (don't use fentry/fmod_ret)\n * Wait for RCU Tasks Trace GP along with RCU GP in selftest\n\nv1 -> v2\nv1: https://lore.kernel.org/bpf/20230219155249.1755998-1-memxor@gmail.com\n\n * Simplify selftests, fix a couple of bugs\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-03-01 10:24:33 -0800",
      "commits": [
        {
          "hash": "65334e64a493c6a0976de7ad56bf8b7a9ff04b4a",
          "subject": "bpf: Support kptrs in percpu hashmap and percpu LRU hashmap",
          "message": "Enable support for kptrs in percpu BPF hashmap and percpu BPF LRU\nhashmap by wiring up the freeing of these kptrs from percpu map\nelements.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230225154010.391965-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-03-01 10:24:33 -0800",
          "modified_files": [
            "kernel/bpf/hashtab.c",
            "kernel/bpf/syscall.c"
          ]
        },
        {
          "hash": "9db44fdd8105da00669d425acab887c668df75f6",
          "subject": "bpf: Support kptrs in local storage maps",
          "message": "Enable support for kptrs in local storage maps by wiring up the freeing\nof these kptrs from map value. Freeing of bpf_local_storage_map is only\ndelayed in case there are special fields, therefore bpf_selem_free_*\npath can also only dereference smap safely in that case. This is\nrecorded using a bool utilizing a hole in bpF_local_storage_elem. It\ncould have been tagged in the pointer value smap using the lowest bit\n(since alignment > 1), but since there was already a hole I went with\nthe simpler option. Only the map structure freeing is delayed using RCU\nbarriers, as the buckets aren't used when selem is being freed, so they\ncan be freed once all readers of the bucket lists can no longer access\nit.\n\nCc: Martin KaFai Lau <martin.lau@kernel.org>\nCc: KP Singh <kpsingh@kernel.org>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230225154010.391965-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-03-01 10:24:33 -0800",
          "modified_files": [
            "include/linux/bpf_local_storage.h",
            "kernel/bpf/bpf_local_storage.c",
            "kernel/bpf/syscall.c",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "85521e1ea4d0d7d8e62bbb0999f91e31ae421d76",
          "subject": "selftests/bpf: Add more tests for kptrs in maps",
          "message": "Firstly, ensure programs successfully load when using all of the\nsupported maps. Then, extend existing tests to test more cases at\nruntime. We are currently testing both the synchronous freeing of items\nand asynchronous destruction when map is freed, but the code needs to be\nadjusted a bit to be able to also accomodate support for percpu maps.\n\nWe now do a delete on the item (and update for array maps which has a\nsimilar effect for kptrs) to perform a synchronous free of the kptr, and\ntest destruction both for the synchronous and asynchronous deletion.\nNext time the program runs, it should observe the refcount as 1 since\nall existing references should have been released by then. By running\nthe program after both possible paths freeing kptrs, we establish that\nthey correctly release resources. Next, we augment the existing test to\nalso test the same code path shared by all local storage maps using a\ntask local storage map.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230225154010.391965-4-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
          "date": "2023-03-01 10:24:33 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/prog_tests/map_kptr.c",
            "tools/testing/selftests/bpf/progs/map_kptr.c",
            "tools/testing/selftests/bpf/progs/rcu_tasks_trace_gp.c"
          ]
        }
      ]
    },
    {
      "merge_hash": "c4b5c5bad9f07e9074c7abde3289de71c4acac48",
      "merge_subject": "Merge branch 'Add skb + xdp dynptrs'",
      "merge_body": "Joanne Koong says:\n\n====================\n\nThis patchset is the 2nd in the dynptr series. The 1st can be found here [0].\n\nThis patchset adds skb and xdp type dynptrs, which have two main benefits for\npacket parsing:\n    * allowing operations on sizes that are not statically known at\n      compile-time (eg variable-sized accesses).\n    * more ergonomic and less brittle iteration through data (eg does not need\n      manual if checking for being within bounds of data_end)\n\nWhen comparing the differences in runtime for packet parsing without dynptrs\nvs. with dynptrs, there is no noticeable difference. Patch 9 contains more\ndetails as well as examples of how to use skb and xdp dynptrs.\n\n[0] https://lore.kernel.org/bpf/20220523210712.3641569-1-joannelkoong@gmail.com/\n---\nChangelog:\n\nv12 = https://lore.kernel.org/bpf/20230226085120.3907863-1-joannelkoong@gmail.com/\nv12 -> v13:\n    * Fix missing { } for case statement\n\nv11 = https://lore.kernel.org/bpf/20230222060747.2562549-1-joannelkoong@gmail.com/\nv11 -> v12:\n    * Change constant mem size checking to use \"__szk\" kfunc annotation\n      for slices\n    * Use autoloading for success selftests\n\nv10 = https://lore.kernel.org/bpf/20230216225524.1192789-1-joannelkoong@gmail.com/\nv10 -> v11:\n    * Reject bpf_dynptr_slice_rdwr() for non-writable progs at load time\n      instead of runtime\n    * Add additional patch (__uninit kfunc annotation)\n    * Expand on documentation\n    * Add bpf_dynptr_write() calls for persisting writes in tests\n\nv9 = https://lore.kernel.org/bpf/20230127191703.3864860-1-joannelkoong@gmail.com/\nv9 -> v10:\n    * Add bpf_dynptr_slice and bpf_dynptr_slice_rdwr interface\n    * Add some more tests\n    * Split up patchset into more parts to make it easier to review\n\nv8 = https://lore.kernel.org/bpf/20230126233439.3739120-1-joannelkoong@gmail.com/\nv8 -> v9:\n    * Fix dynptr_get_type() to check non-stack dynptrs\n\nv7 = https://lore.kernel.org/bpf/20221021011510.1890852-1-joannelkoong@gmail.com/\nv7 -> v8:\n    * Change helpers to kfuncs\n    * Add 2 new patches (1/5 and 2/5)\n\nv6 = https://lore.kernel.org/bpf/20220907183129.745846-1-joannelkoong@gmail.com/\nv6 -> v7\n    * Change bpf_dynptr_data() to return read-only data slices if the skb prog\n      is read-only (Martin)\n    * Add test \"skb_invalid_write\" to test that writes to rd-only data slices\n      are rejected\n\nv5 = https://lore.kernel.org/bpf/20220831183224.3754305-1-joannelkoong@gmail.com/\nv5 -> v6\n    * Address kernel test robot errors by static inlining\n\nv4 = https://lore.kernel.org/bpf/20220822235649.2218031-1-joannelkoong@gmail.com/\nv4 -> v5\n    * Address kernel test robot errors for configs w/out CONFIG_NET set\n    * For data slices, return PTR_TO_MEM instead of PTR_TO_PACKET (Kumar)\n    * Split selftests into subtests (Andrii)\n    * Remove insn patching. Use rdonly and rdwr protos for dynptr skb\n      construction (Andrii)\n    * bpf_dynptr_data() returns NULL for rd-only dynptrs. There will be a\n      separate bpf_dynptr_data_rdonly() added later (Andrii and Kumar)\n\nv3 = https://lore.kernel.org/bpf/20220822193442.657638-1-joannelkoong@gmail.com/\nv3 -> v4\n    * Forgot to commit --amend the kernel test robot error fixups\n\nv2 = https://lore.kernel.org/bpf/20220811230501.2632393-1-joannelkoong@gmail.com/\nv2 -> v3\n    * Fix kernel test robot build test errors\n\nv1 = https://lore.kernel.org/bpf/20220726184706.954822-1-joannelkoong@gmail.com/\nv1 -> v2\n  * Return data slices to rd-only skb dynptrs (Martin)\n  * bpf_dynptr_write allows writes to frags for skb dynptrs, but always\n    invalidates associated data slices (Martin)\n  * Use switch casing instead of ifs (Andrii)\n  * Use 0xFD for experimental kind number in the selftest (Zvi)\n  * Put selftest conversions w/ dynptrs into new files (Alexei)\n  * Add new selftest \"test_cls_redirect_dynptr.c\"\n====================\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "merge_author": "Alexei Starovoitov <ast@kernel.org>",
      "merge_date": "2023-03-01 10:06:10 -0800",
      "commits": [
        {
          "hash": "2f46439346700a2b41cf0fa9432f110f42fd8821",
          "subject": "bpf: Support \"sk_buff\" and \"xdp_buff\" as valid kfunc arg types",
          "message": "The bpf mirror of the in-kernel sk_buff and xdp_buff data structures are\n__sk_buff and xdp_md. Currently, when we pass in the program ctx to a\nkfunc where the program ctx is a skb or xdp buffer, we reject the\nprogram if the in-kernel definition is sk_buff/xdp_buff instead of\n__sk_buff/xdp_md.\n\nThis change allows \"sk_buff <--> __sk_buff\" and \"xdp_buff <--> xdp_md\"\nto be recognized as valid matches. The user program may pass in their\nprogram ctx as a __sk_buff or xdp_md, and the in-kernel definition\nof the kfunc may define this arg as a sk_buff or xdp_buff.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-2-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Joanne Koong <joannelkoong@gmail.com>",
          "date": "2023-03-01 09:55:23 -0800",
          "modified_files": [
            "kernel/bpf/btf.c"
          ]
        },
        {
          "hash": "7e0dac2807e6c4ae8c56941d74971fdb0763b4f9",
          "subject": "bpf: Refactor process_dynptr_func",
          "message": "This change cleans up process_dynptr_func's flow to be more intuitive\nand updates some comments with more context.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-3-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Joanne Koong <joannelkoong@gmail.com>",
          "date": "2023-03-01 09:55:23 -0800",
          "modified_files": [
            "include/linux/bpf_verifier.h",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "1d18feb2c915c5ad0a9a61d04b8560e8efb78ce8",
          "subject": "bpf: Allow initializing dynptrs in kfuncs",
          "message": "This change allows kfuncs to take in an uninitialized dynptr as a\nparameter. Before this change, only helper functions could successfully\nuse uninitialized dynptrs. This change moves the memory access check\n(including stack state growing and slot marking) into\nprocess_dynptr_func(), which both helpers and kfuncs call into.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-4-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Joanne Koong <joannelkoong@gmail.com>",
          "date": "2023-03-01 09:55:23 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "8357b366cbb09b17c90e2cd758360a6bd2ea7507",
          "subject": "bpf: Define no-ops for externally called bpf dynptr functions",
          "message": "Some bpf dynptr functions will be called from places where\nif CONFIG_BPF_SYSCALL is not set, then the dynptr function is\nundefined. For example, when skb type dynptrs are added in the\nnext commit, dynptr functions are called from net/core/filter.c\n\nThis patch defines no-op implementations of these dynptr functions\nso that they do not break compilation by being an undefined reference.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-5-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Joanne Koong <joannelkoong@gmail.com>",
          "date": "2023-03-01 09:55:23 -0800",
          "modified_files": [
            "include/linux/bpf.h"
          ]
        },
        {
          "hash": "485ec51ef9764c0f67d35cabba0a963936b9126e",
          "subject": "bpf: Refactor verifier dynptr into get_dynptr_arg_reg",
          "message": "This commit refactors the logic for determining which register in a\nfunction is the dynptr into \"get_dynptr_arg_reg\". This will be used\nin the future when the dynptr reg for BPF_FUNC_dynptr_write will need\nto be obtained in order to support writes for skb dynptrs.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-6-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Joanne Koong <joannelkoong@gmail.com>",
          "date": "2023-03-01 09:55:23 -0800",
          "modified_files": [
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "d96d937d7c5c12237dce1f14bf0fc9900cabba09",
          "subject": "bpf: Add __uninit kfunc annotation",
          "message": "This patch adds __uninit as a kfunc annotation.\n\nThis will be useful for scenarios such as for example in dynptrs,\nindicating whether the dynptr should be checked by the verifier as an\ninitialized or an uninitialized dynptr.\n\nWithout this annotation, the alternative would be needing to hard-code\nin the verifier the specific kfunc to indicate that arg should be\ntreated as an uninitialized arg.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-7-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Joanne Koong <joannelkoong@gmail.com>",
          "date": "2023-03-01 09:55:24 -0800",
          "modified_files": [
            "Documentation/bpf/kfuncs.rst",
            "kernel/bpf/verifier.c"
          ]
        },
        {
          "hash": "b5964b968ac64c2ec2debee7518499113b27c34e",
          "subject": "bpf: Add skb dynptrs",
          "message": "Add skb dynptrs, which are dynptrs whose underlying pointer points\nto a skb. The dynptr acts on skb data. skb dynptrs have two main\nbenefits. One is that they allow operations on sizes that are not\nstatically known at compile-time (eg variable-sized accesses).\nAnother is that parsing the packet data through dynptrs (instead of\nthrough direct access of skb->data and skb->data_end) can be more\nergonomic and less brittle (eg does not need manual if checking for\nbeing within bounds of data_end).\n\nFor bpf prog types that don't support writes on skb data, the dynptr is\nread-only (bpf_dynptr_write() will return an error)\n\nFor reads and writes through the bpf_dynptr_read() and bpf_dynptr_write()\ninterfaces, reading and writing from/to data in the head as well as from/to\nnon-linear paged buffers is supported. Data slices through the\nbpf_dynptr_data API are not supported; instead bpf_dynptr_slice() and\nbpf_dynptr_slice_rdwr() (added in subsequent commit) should be used.\n\nFor examples of how skb dynptrs can be used, please see the attached\nselftests.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-8-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Joanne Koong <joannelkoong@gmail.com>",
          "date": "2023-03-01 09:55:24 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/filter.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/btf.c",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c",
            "net/core/filter.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "05421aecd4ed65da0dc17b0c3c13779ef334e9e5",
          "subject": "bpf: Add xdp dynptrs",
          "message": "Add xdp dynptrs, which are dynptrs whose underlying pointer points\nto a xdp_buff. The dynptr acts on xdp data. xdp dynptrs have two main\nbenefits. One is that they allow operations on sizes that are not\nstatically known at compile-time (eg variable-sized accesses).\nAnother is that parsing the packet data through dynptrs (instead of\nthrough direct access of xdp->data and xdp->data_end) can be more\nergonomic and less brittle (eg does not need manual if checking for\nbeing within bounds of data_end).\n\nFor reads and writes on the dynptr, this includes reading/writing\nfrom/to and across fragments. Data slices through the bpf_dynptr_data\nAPI are not supported; instead bpf_dynptr_slice() and\nbpf_dynptr_slice_rdwr() should be used.\n\nFor examples of how xdp dynptrs can be used, please see the attached\nselftests.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-9-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Joanne Koong <joannelkoong@gmail.com>",
          "date": "2023-03-01 09:55:24 -0800",
          "modified_files": [
            "include/linux/bpf.h",
            "include/linux/filter.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c",
            "net/core/filter.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "66e3a13e7c2c44d0c9dd6bb244680ca7529a8845",
          "subject": "bpf: Add bpf_dynptr_slice and bpf_dynptr_slice_rdwr",
          "message": "Two new kfuncs are added, bpf_dynptr_slice and bpf_dynptr_slice_rdwr.\nThe user must pass in a buffer to store the contents of the data slice\nif a direct pointer to the data cannot be obtained.\n\nFor skb and xdp type dynptrs, these two APIs are the only way to obtain\na data slice. However, for other types of dynptrs, there is no\ndifference between bpf_dynptr_slice(_rdwr) and bpf_dynptr_data.\n\nFor skb type dynptrs, the data is copied into the user provided buffer\nif any of the data is not in the linear portion of the skb. For xdp type\ndynptrs, the data is copied into the user provided buffer if the data is\nbetween xdp frags.\n\nIf the skb is cloned and a call to bpf_dynptr_data_rdwr is made, then\nthe skb will be uncloned (see bpf_unclone_prologue()).\n\nPlease note that any bpf_dynptr_write() automatically invalidates any prior\ndata slices of the skb dynptr. This is because the skb may be cloned or\nmay need to pull its paged buffer into the head. As such, any\nbpf_dynptr_write() will automatically have its prior data slices\ninvalidated, even if the write is to data in the skb head of an uncloned\nskb. Please note as well that any other helper calls that change the\nunderlying packet buffer (eg bpf_skb_pull_data()) invalidates any data\nslices of the skb dynptr as well, for the same reasons.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-10-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Joanne Koong <joannelkoong@gmail.com>",
          "date": "2023-03-01 09:55:24 -0800",
          "modified_files": [
            "include/linux/filter.h",
            "include/uapi/linux/bpf.h",
            "kernel/bpf/helpers.c",
            "kernel/bpf/verifier.c",
            "net/core/filter.c",
            "tools/include/uapi/linux/bpf.h"
          ]
        },
        {
          "hash": "cfa7b011894d689cccfa88a25da324fa5c34e4ed",
          "subject": "selftests/bpf: tests for using dynptrs to parse skb and xdp buffers",
          "message": "Test skb and xdp dynptr functionality in the following ways:\n\n1) progs/test_cls_redirect_dynptr.c\n   * Rewrite \"progs/test_cls_redirect.c\" test to use dynptrs to parse\n     skb data\n\n   * This is a great example of how dynptrs can be used to simplify a\n     lot of the parsing logic for non-statically known values.\n\n     When measuring the user + system time between the original version\n     vs. using dynptrs, and averaging the time for 10 runs (using\n     \"time ./test_progs -t cls_redirect\"):\n         original version: 0.092 sec\n         with dynptrs: 0.078 sec\n\n2) progs/test_xdp_dynptr.c\n   * Rewrite \"progs/test_xdp.c\" test to use dynptrs to parse xdp data\n\n     When measuring the user + system time between the original version\n     vs. using dynptrs, and averaging the time for 10 runs (using\n     \"time ./test_progs -t xdp_attach\"):\n         original version: 0.118 sec\n         with dynptrs: 0.094 sec\n\n3) progs/test_l4lb_noinline_dynptr.c\n   * Rewrite \"progs/test_l4lb_noinline.c\" test to use dynptrs to parse\n     skb data\n\n     When measuring the user + system time between the original version\n     vs. using dynptrs, and averaging the time for 10 runs (using\n     \"time ./test_progs -t l4lb_all\"):\n         original version: 0.062 sec\n         with dynptrs: 0.081 sec\n\n     For number of processed verifier instructions:\n         original version: 6268 insns\n         with dynptrs: 2588 insns\n\n4) progs/test_parse_tcp_hdr_opt_dynptr.c\n   * Add sample code for parsing tcp hdr opt lookup using dynptrs.\n     This logic is lifted from a real-world use case of packet parsing\n     in katran [0], a layer 4 load balancer. The original version\n     \"progs/test_parse_tcp_hdr_opt.c\" (not using dynptrs) is included\n     here as well, for comparison.\n\n     When measuring the user + system time between the original version\n     vs. using dynptrs, and averaging the time for 10 runs (using\n     \"time ./test_progs -t parse_tcp_hdr_opt\"):\n         original version: 0.031 sec\n         with dynptrs: 0.045 sec\n\n5) progs/dynptr_success.c\n   * Add test case \"test_skb_readonly\" for testing attempts at writes\n     on a prog type with read-only skb ctx.\n   * Add \"test_dynptr_skb_data\" for testing that bpf_dynptr_data isn't\n     supported for skb progs.\n\n6) progs/dynptr_fail.c\n   * Add test cases \"skb_invalid_data_slice{1,2,3,4}\" and\n     \"xdp_invalid_data_slice{1,2}\" for testing that helpers that modify the\n     underlying packet buffer automatically invalidate the associated\n     data slice.\n   * Add test cases \"skb_invalid_ctx\" and \"xdp_invalid_ctx\" for testing\n     that prog types that do not support bpf_dynptr_from_skb/xdp don't\n     have access to the API.\n   * Add test case \"dynptr_slice_var_len{1,2}\" for testing that\n     variable-sized len can't be passed in to bpf_dynptr_slice\n   * Add test case \"skb_invalid_slice_write\" for testing that writes to a\n     read-only data slice are rejected by the verifier.\n   * Add test case \"data_slice_out_of_bounds_skb\" for testing that\n     writes to an area outside the slice are rejected.\n   * Add test case \"invalid_slice_rdwr_rdonly\" for testing that prog\n     types that don't allow writes to packet data don't accept any calls\n     to bpf_dynptr_slice_rdwr.\n\n[0] https://github.com/facebookincubator/katran/blob/main/katran/lib/bpf/pckt_parsing.h\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230301154953.641654-11-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
          "author": "Joanne Koong <joannelkoong@gmail.com>",
          "date": "2023-03-01 10:05:19 -0800",
          "modified_files": [
            "tools/testing/selftests/bpf/DENYLIST.s390x",
            "tools/testing/selftests/bpf/bpf_kfuncs.h",
            "tools/testing/selftests/bpf/prog_tests/cls_redirect.c",
            "tools/testing/selftests/bpf/prog_tests/dynptr.c",
            "tools/testing/selftests/bpf/prog_tests/l4lb_all.c",
            "tools/testing/selftests/bpf/prog_tests/parse_tcp_hdr_opt.c",
            "tools/testing/selftests/bpf/prog_tests/xdp_attach.c",
            "tools/testing/selftests/bpf/progs/dynptr_fail.c",
            "tools/testing/selftests/bpf/progs/dynptr_success.c",
            "tools/testing/selftests/bpf/progs/test_cls_redirect_dynptr.c",
            "tools/testing/selftests/bpf/progs/test_l4lb_noinline_dynptr.c",
            "tools/testing/selftests/bpf/progs/test_parse_tcp_hdr_opt.c",
            "tools/testing/selftests/bpf/progs/test_parse_tcp_hdr_opt_dynptr.c",
            "tools/testing/selftests/bpf/progs/test_xdp_dynptr.c",
            "tools/testing/selftests/bpf/test_tcp_hdr_options.h"
          ]
        }
      ]
    }
  ]
}
