{
  "metadata": {
    "target_file": "kernel/bpf/verifier.c",
    "start_ref": "v6.3",
    "end_ref": "v6.13",
    "commit_count": 390
  },
  "commits": [
    {
      "hash": "23579010cf0a12476e96a5f1acdf78a9c5843657",
      "author": "Andrea Righi <arighi@nvidia.com>",
      "date": "2024-12-17 16:09:24 -0800",
      "message": "bpf: Fix bpf_get_smp_processor_id() on !CONFIG_SMP\n\nOn x86-64 calling bpf_get_smp_processor_id() in a kernel with CONFIG_SMP\ndisabled can trigger the following bug, as pcpu_hot is unavailable:\n\n [    8.471774] BUG: unable to handle page fault for address: 00000000936a290c\n [    8.471849] #PF: supervisor read access in kernel mode\n [    8.471881] #PF: error_code(0x0000) - not-present page\n\nFix by inlining a return 0 in the !CONFIG_SMP case.\n\nFixes: 1ae6921009e5 (\"bpf: inline bpf_get_smp_processor_id() helper\")\nSigned-off-by: Andrea Righi <arighi@nvidia.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241217195813.622568-1-arighi@nvidia.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c00d738e1673ab801e1577e4e3c780ccf88b1a5b",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2024-12-13 16:24:53 -0800",
      "message": "bpf: Revert \"bpf: Mark raw_tp arguments with PTR_MAYBE_NULL\"\n\nThis patch reverts commit\ncb4158ce8ec8 (\"bpf: Mark raw_tp arguments with PTR_MAYBE_NULL\"). The\npatch was well-intended and meant to be as a stop-gap fixing branch\nprediction when the pointer may actually be NULL at runtime. Eventually,\nit was supposed to be replaced by an automated script or compiler pass\ndetecting possibly NULL arguments and marking them accordingly.\n\nHowever, it caused two main issues observed for production programs and\nfailed to preserve backwards compatibility. First, programs relied on\nthe verifier not exploring == NULL branch when pointer is not NULL, thus\nthey started failing with a 'dereference of scalar' error.  Next,\nallowing raw_tp arguments to be modified surfaced the warning in the\nverifier that warns against reg->off when PTR_MAYBE_NULL is set.\n\nMore information, context, and discusson on both problems is available\nin [0]. Overall, this approach had several shortcomings, and the fixes\nwould further complicate the verifier's logic, and the entire masking\nscheme would have to be removed eventually anyway.\n\nHence, revert the patch in preparation of a better fix avoiding these\nissues to replace this commit.\n\n  [0]: https://lore.kernel.org/bpf/20241206161053.809580-1-memxor@gmail.com\n\nReported-by: Manu Bretelle <chantra@meta.com>\nFixes: cb4158ce8ec8 (\"bpf: Mark raw_tp arguments with PTR_MAYBE_NULL\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241213221929.3495062-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/test_tp_btf_nullable.c"
      ]
    },
    {
      "hash": "ac6542ad92759cda383ad62b4e4cbfc28136abc1",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-12-12 11:37:19 -0800",
      "message": "bpf: fix null dereference when computing changes_pkt_data of prog w/o subprogs\n\nbpf_prog_aux->func field might be NULL if program does not have\nsubprograms except for main sub-program. The fixed commit does\nbpf_prog_aux->func access unconditionally, which might lead to null\npointer dereference.\n\nThe bug could be triggered by replacing the following BPF program:\n\n    SEC(\"tc\")\n    int main_changes(struct __sk_buff *sk)\n    {\n        bpf_skb_pull_data(sk, 0);\n        return 0;\n    }\n\nWith the following BPF program:\n\n    SEC(\"freplace\")\n    long changes_pkt_data(struct __sk_buff *sk)\n    {\n        return bpf_skb_pull_data(sk, 0);\n    }\n\nbpf_prog_aux instance itself represents the main sub-program,\nuse this property to fix the bug.\n\nFixes: 81f6d0530ba0 (\"bpf: check changes_pkt_data property for extension programs\")\nReported-by: kernel test robot <lkp@intel.com>\nReported-by: Dan Carpenter <dan.carpenter@linaro.org>\nCloses: https://lore.kernel.org/r/202412111822.qGw6tOyB-lkp@intel.com/\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241212070711.427443-1-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "81f6d0530ba031b5f038a091619bf2ff29568852",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-12-10 10:24:57 -0800",
      "message": "bpf: check changes_pkt_data property for extension programs\n\nWhen processing calls to global sub-programs, verifier decides whether\nto invalidate all packet pointers in current state depending on the\nchanges_pkt_data property of the global sub-program.\n\nBecause of this, an extension program replacing a global sub-program\nmust be compatible with changes_pkt_data property of the sub-program\nbeing replaced.\n\nThis commit:\n- adds changes_pkt_data flag to struct bpf_prog_aux:\n  - this flag is set in check_cfg() for main sub-program;\n  - in jit_subprogs() for other sub-programs;\n- modifies bpf_check_attach_btf_id() to check changes_pkt_data flag;\n- moves call to check_attach_btf_id() after the call to check_cfg(),\n  because it needs changes_pkt_data flag to be set:\n\n    bpf_check:\n      ...                             ...\n    - check_attach_btf_id             resolve_pseudo_ldimm64\n      resolve_pseudo_ldimm64   -->    bpf_prog_is_offloaded\n      bpf_prog_is_offloaded           check_cfg\n      check_cfg                     + check_attach_btf_id\n      ...                             ...\n\nThe following fields are set by check_attach_btf_id():\n- env->ops\n- prog->aux->attach_btf_trace\n- prog->aux->attach_func_name\n- prog->aux->attach_func_proto\n- prog->aux->dst_trampoline\n- prog->aux->mod\n- prog->aux->saved_dst_attach_type\n- prog->aux->saved_dst_prog_type\n- prog->expected_attach_type\n\nNeither of these fields are used by resolve_pseudo_ldimm64() or\nbpf_prog_offload_verifier_prep() (for netronome and netdevsim\ndrivers), so the reordering is safe.\n\nSuggested-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-6-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "51081a3f25c742da5a659d7fc6fd77ebfdd555be",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-12-10 10:24:57 -0800",
      "message": "bpf: track changes_pkt_data property for global functions\n\nWhen processing calls to certain helpers, verifier invalidates all\npacket pointers in a current state. For example, consider the\nfollowing program:\n\n    __attribute__((__noinline__))\n    long skb_pull_data(struct __sk_buff *sk, __u32 len)\n    {\n        return bpf_skb_pull_data(sk, len);\n    }\n\n    SEC(\"tc\")\n    int test_invalidate_checks(struct __sk_buff *sk)\n    {\n        int *p = (void *)(long)sk->data;\n        if ((void *)(p + 1) > (void *)(long)sk->data_end) return TCX_DROP;\n        skb_pull_data(sk, 0);\n        *p = 42;\n        return TCX_PASS;\n    }\n\nAfter a call to bpf_skb_pull_data() the pointer 'p' can't be used\nsafely. See function filter.c:bpf_helper_changes_pkt_data() for a list\nof such helpers.\n\nAt the moment verifier invalidates packet pointers when processing\nhelper function calls, and does not traverse global sub-programs when\nprocessing calls to global sub-programs. This means that calls to\nhelpers done from global sub-programs do not invalidate pointers in\nthe caller state. E.g. the program above is unsafe, but is not\nrejected by verifier.\n\nThis commit fixes the omission by computing field\nbpf_subprog_info->changes_pkt_data for each sub-program before main\nverification pass.\nchanges_pkt_data should be set if:\n- subprogram calls helper for which bpf_helper_changes_pkt_data\n  returns true;\n- subprogram calls a global function,\n  for which bpf_subprog_info->changes_pkt_data should be set.\n\nThe verifier.c:check_cfg() pass is modified to compute this\ninformation. The commit relies on depth first instruction traversal\ndone by check_cfg() and absence of recursive function calls:\n- check_cfg() would eventually visit every call to subprogram S in a\n  state when S is fully explored;\n- when S is fully explored:\n  - every direct helper call within S is explored\n    (and thus changes_pkt_data is set if needed);\n  - every call to subprogram S1 called by S was visited with S1 fully\n    explored (and thus S inherits changes_pkt_data from S1).\n\nThe downside of such approach is that dead code elimination is not\ntaken into account: if a helper call inside global function is dead\nbecause of current configuration, verifier would conservatively assume\nthat the call occurs for the purpose of the changes_pkt_data\ncomputation.\n\nReported-by: Nick Zavaritsky <mejedi@gmail.com>\nCloses: https://lore.kernel.org/bpf/0498CA22-5779-4767-9C0C-A9515CEA711F@gmail.com/\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b238e187b4a2d3b54d80aec05a9cab6466b79dde",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-12-10 10:24:57 -0800",
      "message": "bpf: refactor bpf_helper_changes_pkt_data to use helper number\n\nUse BPF helper number instead of function pointer in\nbpf_helper_changes_pkt_data(). This would simplify usage of this\nfunction in verifier.c:check_cfg() (in a follow-up patch),\nwhere only helper number is easily available and there is no real need\nto lookup helper proto.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-3-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/filter.h",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c",
        "net/core/filter.c"
      ]
    },
    {
      "hash": "27e88bc4df1d80888fe1aaca786a7cc6e69587e2",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-12-10 10:24:57 -0800",
      "message": "bpf: add find_containing_subprog() utility function\n\nAdd a utility function, looking for a subprogram containing a given\ninstruction index, rewrite find_subprog() to use this function.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20241210041100.1898468-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b0e66977dc072906bb76555fb1a64261d7f63d0f",
      "author": "Tao Lyu <tao.lyu@epfl.ch>",
      "date": "2024-12-04 09:19:50 -0800",
      "message": "bpf: Fix narrow scalar spill onto 64-bit spilled scalar slots\n\nWhen CAP_PERFMON and CAP_SYS_ADMIN (allow_ptr_leaks) are disabled, the\nverifier aims to reject partial overwrite on an 8-byte stack slot that\ncontains a spilled pointer.\n\nHowever, in such a scenario, it rejects all partial stack overwrites as\nlong as the targeted stack slot is a spilled register, because it does\nnot check if the stack slot is a spilled pointer.\n\nIncomplete checks will result in the rejection of valid programs, which\nspill narrower scalar values onto scalar slots, as shown below.\n\n0: R1=ctx() R10=fp0\n; asm volatile ( @ repro.bpf.c:679\n0: (7a) *(u64 *)(r10 -8) = 1          ; R10=fp0 fp-8_w=1\n1: (62) *(u32 *)(r10 -8) = 1\nattempt to corrupt spilled pointer on stack\nprocessed 2 insns (limit 1000000) max_states_per_insn 0 total_states 0 peak_states 0 mark_read 0.\n\nFix this by expanding the check to not consider spilled scalar registers\nwhen rejecting the write into the stack.\n\nPrevious discussion on this patch is at link [0].\n\n  [0]: https://lore.kernel.org/bpf/20240403202409.2615469-1-tao.lyu@epfl.ch\n\nFixes: ab125ed3ec1c (\"bpf: fix check for attempt to corrupt spilled pointer\")\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Tao Lyu <tao.lyu@epfl.ch>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241204044757.1483141-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "69772f509e084ec6bca12dbcdeeeff41b0103774",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2024-12-04 09:19:50 -0800",
      "message": "bpf: Don't mark STACK_INVALID as STACK_MISC in mark_stack_slot_misc\n\nInside mark_stack_slot_misc, we should not upgrade STACK_INVALID to\nSTACK_MISC when allow_ptr_leaks is false, since invalid contents\nshouldn't be read unless the program has the relevant capabilities.\nThe relaxation only makes sense when env->allow_ptr_leaks is true.\n\nHowever, such conversion in privileged mode becomes unnecessary, as\ninvalid slots can be read without being upgraded to STACK_MISC.\n\nCurrently, the condition is inverted (i.e. checking for true instead of\nfalse), simply remove it to restore correct behavior.\n\nFixes: eaf18febd6eb (\"bpf: preserve STACK_ZERO slots on partial reg spills\")\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nReported-by: Tao Lyu <tao.lyu@epfl.ch>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241204044757.1483141-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "bd74e238ae6944b462f57ce8752440a011ba4530",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2024-12-02 18:47:41 -0800",
      "message": "bpf: Zero index arg error string for dynptr and iter\n\nAndrii spotted that process_dynptr_func's rejection of incorrect\nargument register type will print an error string where argument numbers\nare not zero-indexed, unlike elsewhere in the verifier.  Fix this by\nsubtracting 1 from regno. The same scenario exists for iterator\nmessages. Fix selftest error strings that match on the exact argument\nnumber while we're at it to ensure clean bisection.\n\nSuggested-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241203002235.3776418-1-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/dynptr_fail.c",
        "tools/testing/selftests/bpf/progs/iters_state_safety.c",
        "tools/testing/selftests/bpf/progs/iters_testmod_seq.c",
        "tools/testing/selftests/bpf/progs/test_kfunc_dynptr_param.c",
        "tools/testing/selftests/bpf/progs/verifier_bits_iter.c"
      ]
    },
    {
      "hash": "12659d28615d606b36e382f4de2dd05550d202af",
      "author": "Tao Lyu <tao.lyu@epfl.ch>",
      "date": "2024-12-02 17:47:56 -0800",
      "message": "bpf: Ensure reg is PTR_TO_STACK in process_iter_arg\n\nCurrently, KF_ARG_PTR_TO_ITER handling missed checking the reg->type and\nensuring it is PTR_TO_STACK. Instead of enforcing this in the caller of\nprocess_iter_arg, move the check into it instead so that all callers\nwill gain the check by default. This is similar to process_dynptr_func.\n\nAn existing selftest in verifier_bits_iter.c fails due to this change,\nbut it's because it was passing a NULL pointer into iter_next helper and\ngetting an error further down the checks, but probably meant to pass an\nuninitialized iterator on the stack (as is done in the subsequent test\nbelow it). We will gain coverage for non-PTR_TO_STACK arguments in later\npatches hence just change the declaration to zero-ed stack object.\n\nFixes: 06accc8779c1 (\"bpf: add support for open-coded iterator loops\")\nSuggested-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Tao Lyu <tao.lyu@epfl.ch>\n[ Kartikeya: move check into process_iter_arg, rewrite commit log ]\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241203000238.3602922-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_bits_iter.c"
      ]
    },
    {
      "hash": "96a30e469ca1d2b8cc7811b40911f8614b558241",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-11-15 10:20:47 -0800",
      "message": "bpf: use common instruction history across all states\n\nInstead of allocating and copying instruction history each time we\nenqueue child verifier state, switch to a model where we use one common\ndynamically sized array of instruction history entries across all states.\n\nThe key observation for proving this is correct is that instruction\nhistory is only relevant while state is active, which means it either is\na current state (and thus we are actively modifying instruction history\nand no other state can interfere with us) or we are checkpointed state\nwith some children still active (either enqueued or being current).\n\nIn the latter case our portion of instruction history is finalized and\nwon't change or grow, so as long as we keep it immutable until the state\nis finalized, we are good.\n\nNow, when state is finalized and is put into state hash for potentially\nfuture pruning lookups, instruction history is not used anymore. This is\nbecause instruction history is only used by precision marking logic, and\nwe never modify precision markings for finalized states.\n\nSo, instead of each state having its own small instruction history, we\nkeep a global dynamically-sized instruction history, where each state in\ncurrent DFS path from root to active state remembers its portion of\ninstruction history. Current state can append to this history, but\ncannot modify any of its parent histories.\n\nAsync callback state enqueueing, while logically detached from parent\nstate, still is part of verification backtracking tree, so has to follow\nthe same schema as normal state checkpoints.\n\nBecause the insn_hist array can be grown through realloc, states don't\nkeep pointers, they instead maintain two indices, [start, end), into\nglobal instruction history array. End is exclusive index, so\n`start == end` means there is no relevant instruction history.\n\nThis eliminates a lot of allocations and minimizes overall memory usage.\n\nFor instance, running a worst-case test from [0] (but without the\nheuristics-based fix [1]), it took 12.5 minutes until we get -ENOMEM.\nWith the changes in this patch the whole test succeeds in 10 minutes\n(very slow, so heuristics from [1] is important, of course).\n\nTo further validate correctness, veristat-based comparison was performed for\nMeta production BPF objects and BPF selftests objects. In both cases there\nwere no differences *at all* in terms of verdict or instruction and state\ncounts, providing a good confidence in the change.\n\nHaving this low-memory-overhead solution of keeping dynamic\nper-instruction history cheaply opens up some new possibilities, like\nkeeping extra information for literally every single validated\ninstruction. This will be used for simplifying precision backpropagation\nlogic in follow up patches.\n\n  [0] https://lore.kernel.org/bpf/20241029172641.1042523-2-eddyz87@gmail.com/\n  [1] https://lore.kernel.org/bpf/20241029172641.1042523-1-eddyz87@gmail.com/\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20241115001303.277272-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5bd36da1e37e7a78e8b38efd287de6e1394b7d6e",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-11-12 16:26:25 -0800",
      "message": "bpf: Support private stack for struct_ops progs\n\nFor struct_ops progs, whether a particular prog uses private stack\ndepends on prog->aux->priv_stack_requested setting before actual\ninsn-level verification for that prog. One particular implementation\nis to piggyback on struct_ops->check_member(). The next patch has\nan example for this. The struct_ops->check_member() sets\nprog->aux->priv_stack_requested to be true which enables private stack\nusage.\n\nThe struct_ops prog follows the same rule as kprobe/tracing progs after\nfunction bpf_enable_priv_stack(). For example, even a struct_ops prog\nrequests private stack, it could still use normal kernel stack if\nthe stack size is small (< 64 bytes).\n\nSimilar to tracing progs, nested same cpu same prog run will be skipped.\nA field (recursion_detected()) is added to bpf_prog_aux structure.\nIf bpf_prog->aux->recursion_detected is implemented by the struct_ops\nsubsystem and nested same cpu/prog happens, the function will be\ntriggered to report an error, collect related info, etc.\n\nAcked-by: Tejun Heo <tj@kernel.org>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241112163933.2224962-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "kernel/bpf/trampoline.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "e00931c02568dc6ac76f94b1ab471de05e6fdfe8",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-11-12 16:26:24 -0800",
      "message": "bpf: Enable private stack for eligible subprogs\n\nIf private stack is used by any subprog, set that subprog\nprog->aux->jits_use_priv_stack to be true so later jit can allocate\nprivate stack for that subprog properly.\n\nAlso set env->prog->aux->jits_use_priv_stack to be true if\nany subprog uses private stack. This is a use case for a\nsingle main prog (no subprogs) to use private stack, and\nalso a use case for later struct-ops progs where\nenv->prog->aux->jits_use_priv_stack will enable recursion\ncheck if any subprog uses private stack.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241112163912.2224007-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "a76ab5731e32d50ff5b1ae97e9dc4b23f41c23f5",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-11-12 16:26:24 -0800",
      "message": "bpf: Find eligible subprogs for private stack support\n\nPrivate stack will be allocated with percpu allocator in jit time.\nTo avoid complexity at runtime, only one copy of private stack is\navailable per cpu per prog. So runtime recursion check is necessary\nto avoid stack corruption.\n\nCurrent private stack only supports kprobe/perf_event/tp/raw_tp\nwhich has recursion check in the kernel, and prog types that use\nbpf trampoline recursion check. For trampoline related prog types,\ncurrently only tracing progs have recursion checking.\n\nTo avoid complexity, all async_cb subprogs use normal kernel stack\nincluding those subprogs used by both main prog subtree and async_cb\nsubtree. Any prog having tail call also uses kernel stack.\n\nTo avoid jit penalty with private stack support, a subprog stack\nsize threshold is set such that only if the stack size is no less\nthan the threshold, private stack is supported. The current threshold\nis 64 bytes. This avoids jit penality if the stack usage is small.\n\nA useless 'continue' is also removed from a loop in func\ncheck_max_stack_depth().\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20241112163907.2223839-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "include/linux/filter.h",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ae6e3a273f590a2b64f14a9fab3546c3a8f44ed4",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2024-11-11 08:18:55 -0800",
      "message": "bpf: Drop special callback reference handling\n\nLogic to prevent callbacks from acquiring new references for the program\n(i.e. leaving acquired references), and releasing caller references\n(i.e. those acquired in parent frames) was introduced in commit\n9d9d00ac29d0 (\"bpf: Fix reference state management for synchronous callbacks\").\n\nThis was necessary because back then, the verifier simulated each\ncallback once (that could potentially be executed N times, where N can\nbe zero). This meant that callbacks that left lingering resources or\ncleared caller resources could do it more than once, operating on\nundefined state or leaking memory.\n\nWith the fixes to callback verification in commit\nab5cfac139ab (\"bpf: verify callbacks as if they are called unknown number of times\"),\nall of this extra logic is no longer necessary. Hence, drop it as part\nof this commit.\n\nCc: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241109231430.2475236-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/prog_tests/cb_refs.c"
      ]
    },
    {
      "hash": "f6b9a69a9e56b2083aca8a925fc1a28eb698e3ed",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2024-11-11 08:18:51 -0800",
      "message": "bpf: Refactor active lock management\n\nWhen bpf_spin_lock was introduced originally, there was deliberation on\nwhether to use an array of lock IDs, but since bpf_spin_lock is limited\nto holding a single lock at any given time, we've been using a single ID\nto identify the held lock.\n\nIn preparation for introducing spin locks that can be taken multiple\ntimes, introduce support for acquiring multiple lock IDs. For this\npurpose, reuse the acquired_refs array and store both lock and pointer\nreferences. We tag the entry with REF_TYPE_PTR or REF_TYPE_LOCK to\ndisambiguate and find the relevant entry. The ptr field is used to track\nthe map_ptr or btf (for bpf_obj_new allocations) to ensure locks can be\nmatched with protected fields within the same \"allocation\", i.e.\nbpf_obj_new object or map value.\n\nThe struct active_lock is changed to an int as the state is part of the\nacquired_refs array, and we only need active_lock as a cheap way of\ndetecting lock presence.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241109231430.2475236-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d920179b3d4842a0e27cae54fdddbe5ef3977e73",
      "author": "Jiri Olsa <jolsa@kernel.org>",
      "date": "2024-11-11 08:18:03 -0800",
      "message": "bpf: Add support for uprobe multi session attach\n\nAdding support to attach BPF program for entry and return probe\nof the same function. This is common use case which at the moment\nrequires to create two uprobe multi links.\n\nAdding new BPF_TRACE_UPROBE_SESSION attach type that instructs\nkernel to attach single link program to both entry and exit probe.\n\nIt's possible to control execution of the BPF program on return\nprobe simply by returning zero or non zero from the entry BPF\nprogram execution to execute or not the BPF program on return\nprobe respectively.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-4-jolsa@kernel.org",
      "modified_files": [
        "include/uapi/linux/bpf.h",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c",
        "kernel/trace/bpf_trace.c",
        "tools/include/uapi/linux/bpf.h",
        "tools/lib/bpf/libbpf.c"
      ]
    },
    {
      "hash": "17c4b65a24938c6dd79496cce5df15f70d9c253c",
      "author": "Jiri Olsa <jolsa@kernel.org>",
      "date": "2024-11-11 08:17:57 -0800",
      "message": "bpf: Allow return values 0 and 1 for kprobe session\n\nThe kprobe session program can return only 0 or 1,\ninstruct verifier to check for that.\n\nFixes: 535a3692ba72 (\"bpf: Add support for kprobe session attach\")\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20241108134544.480660-2-jolsa@kernel.org",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "cb4158ce8ec8a5bb528cc1693356a5eb8058094d",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2024-11-04 11:37:36 -0800",
      "message": "bpf: Mark raw_tp arguments with PTR_MAYBE_NULL\n\nArguments to a raw tracepoint are tagged as trusted, which carries the\nsemantics that the pointer will be non-NULL.  However, in certain cases,\na raw tracepoint argument may end up being NULL. More context about this\nissue is available in [0].\n\nThus, there is a discrepancy between the reality, that raw_tp arguments\ncan actually be NULL, and the verifier's knowledge, that they are never\nNULL, causing explicit NULL checks to be deleted, and accesses to such\npointers potentially crashing the kernel.\n\nTo fix this, mark raw_tp arguments as PTR_MAYBE_NULL, and then special\ncase the dereference and pointer arithmetic to permit it, and allow\npassing them into helpers/kfuncs; these exceptions are made for raw_tp\nprograms only. Ensure that we don't do this when ref_obj_id > 0, as in\nthat case this is an acquired object and doesn't need such adjustment.\n\nThe reason we do mask_raw_tp_trusted_reg logic is because other will\nrecheck in places whether the register is a trusted_reg, and then\nconsider our register as untrusted when detecting the presence of the\nPTR_MAYBE_NULL flag.\n\nTo allow safe dereference, we enable PROBE_MEM marking when we see loads\ninto trusted pointers with PTR_MAYBE_NULL.\n\nWhile trusted raw_tp arguments can also be passed into helpers or kfuncs\nwhere such broken assumption may cause issues, a future patch set will\ntackle their case separately, as PTR_TO_BTF_ID (without PTR_TRUSTED) can\nalready be passed into helpers and causes similar problems. Thus, they\nare left alone for now.\n\nIt is possible that these checks also permit passing non-raw_tp args\nthat are trusted PTR_TO_BTF_ID with null marking. In such a case,\nallowing dereference when pointer is NULL expands allowed behavior, so\nwon't regress existing programs, and the case of passing these into\nhelpers is the same as above and will be dealt with later.\n\nAlso update the failure case in tp_btf_nullable selftest to capture the\nnew behavior, as the verifier will no longer cause an error when\ndirectly dereference a raw tracepoint argument marked as __nullable.\n\n  [0]: https://lore.kernel.org/bpf/ZrCZS6nisraEqehw@jlelli-thinkpadt14gen4.remote.csb\n\nReviewed-by: Jiri Olsa <jolsa@kernel.org>\nReported-by: Juri Lelli <juri.lelli@redhat.com>\nTested-by: Juri Lelli <juri.lelli@redhat.com>\nFixes: 3f00c5239344 (\"bpf: Allow trusted pointers to be passed to KF_TRUSTED_ARGS kfuncs\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241104171959.2938862-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/test_tp_btf_nullable.c"
      ]
    },
    {
      "hash": "d402755ced2ea8fc1f0513136f074002d509bfa0",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2024-11-03 16:52:06 -0800",
      "message": "bpf: Unify resource leak checks\n\nThere are similar checks for covering locks, references, RCU read\nsections and preempt_disable sections in 3 places in the verifer, i.e.\nfor tail calls, bpf_ld_[abs, ind], and exit path (for BPF_EXIT and\nbpf_throw). Unify all of these into a common check_resource_leak\nfunction to avoid code duplication.\n\nAlso update the error strings in selftests to the new ones in the same\nchange to ensure clean bisection.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241103225940.1408302-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/exceptions_fail.c",
        "tools/testing/selftests/bpf/progs/preempt_lock.c",
        "tools/testing/selftests/bpf/progs/verifier_ref_tracking.c",
        "tools/testing/selftests/bpf/progs/verifier_spin_lock.c"
      ]
    },
    {
      "hash": "46f7ed32f7a873d6675ea72e1d6317df41a55f81",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2024-11-03 16:52:06 -0800",
      "message": "bpf: Tighten tail call checks for lingering locks, RCU, preempt_disable\n\nThere are three situations when a program logically exits and transfers\ncontrol to the kernel or another program: bpf_throw, BPF_EXIT, and tail\ncalls. The former two check for any lingering locks and references, but\ntail calls currently do not. Expand the checks to check for spin locks,\nRCU read sections and preempt disabled sections.\n\nSpin locks are indirectly preventing tail calls as function calls are\ndisallowed, but the checks for preemption and RCU are more relaxed,\nhence ensure tail calls are prevented in their presence.\n\nFixes: 9bb00b2895cb (\"bpf: Add kfunc bpf_rcu_read_lock/unlock()\")\nFixes: fc7566ad0a82 (\"bpf: Introduce bpf_preempt_[disable,enable] kfuncs\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241103225940.1408302-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d0b98f6a17a5cb336121302bce0c97eb5fe32d16",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-10-29 19:43:16 -0700",
      "message": "bpf: disallow 40-bytes extra stack for bpf_fastcall patterns\n\nHou Tao reported an issue with bpf_fastcall patterns allowing extra\nstack space above MAX_BPF_STACK limit. This extra stack allowance is\nnot integrated properly with the following verifier parts:\n- backtracking logic still assumes that stack can't exceed\n  MAX_BPF_STACK;\n- bpf_verifier_env->scratched_stack_slots assumes only 64 slots are\n  available.\n\nHere is an example of an issue with precision tracking\n(note stack slot -8 tracked as precise instead of -520):\n\n    0: (b7) r1 = 42                       ; R1_w=42\n    1: (b7) r2 = 42                       ; R2_w=42\n    2: (7b) *(u64 *)(r10 -512) = r1       ; R1_w=42 R10=fp0 fp-512_w=42\n    3: (7b) *(u64 *)(r10 -520) = r2       ; R2_w=42 R10=fp0 fp-520_w=42\n    4: (85) call bpf_get_smp_processor_id#8       ; R0_w=scalar(...)\n    5: (79) r2 = *(u64 *)(r10 -520)       ; R2_w=42 R10=fp0 fp-520_w=42\n    6: (79) r1 = *(u64 *)(r10 -512)       ; R1_w=42 R10=fp0 fp-512_w=42\n    7: (bf) r3 = r10                      ; R3_w=fp0 R10=fp0\n    8: (0f) r3 += r2\n    mark_precise: frame0: last_idx 8 first_idx 0 subseq_idx -1\n    mark_precise: frame0: regs=r2 stack= before 7: (bf) r3 = r10\n    mark_precise: frame0: regs=r2 stack= before 6: (79) r1 = *(u64 *)(r10 -512)\n    mark_precise: frame0: regs=r2 stack= before 5: (79) r2 = *(u64 *)(r10 -520)\n    mark_precise: frame0: regs= stack=-8 before 4: (85) call bpf_get_smp_processor_id#8\n    mark_precise: frame0: regs= stack=-8 before 3: (7b) *(u64 *)(r10 -520) = r2\n    mark_precise: frame0: regs=r2 stack= before 2: (7b) *(u64 *)(r10 -512) = r1\n    mark_precise: frame0: regs=r2 stack= before 1: (b7) r2 = 42\n    9: R2_w=42 R3_w=fp42\n    9: (95) exit\n\nThis patch disables the additional allowance for the moment.\nAlso, two test cases are removed:\n- bpf_fastcall_max_stack_ok:\n  it fails w/o additional stack allowance;\n- bpf_fastcall_max_stack_fail:\n  this test is no longer necessary, stack size follows\n  regular rules, pattern invalidation is checked by other\n  test cases.\n\nReported-by: Hou Tao <houtao@huaweicloud.com>\nCloses: https://lore.kernel.org/bpf/20241023022752.172005-1-houtao@huaweicloud.com/\nFixes: 5b5f51bff1b6 (\"bpf: no_caller_saved_registers attribute for helper calls\")\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nTested-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20241029193911.1575719-1-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_bpf_fastcall.c"
      ]
    },
    {
      "hash": "aa30eb3260b2dea3a68d3c42a39f9a09c5e99cee",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-10-29 11:42:21 -0700",
      "message": "bpf: Force checkpoint when jmp history is too long\n\nA specifically crafted program might trick verifier into growing very\nlong jump history within a single bpf_verifier_state instance.\nVery long jump history makes mark_chain_precision() unreasonably slow,\nespecially in case if verifier processes a loop.\n\nMitigate this by forcing new state in is_state_visited() in case if\ncurrent state's jump history is too long.\n\nUse same constant as in `skip_inf_loop_check`, but multiply it by\narbitrarily chosen value 2 to account for jump history containing not\nonly information about jumps, but also information about stack access.\n\nFor an example of problematic program consider the code below,\nw/o this patch the example is processed by verifier for ~15 minutes,\nbefore failing to allocate big-enough chunk for jmp_history.\n\n    0: r7 = *(u16 *)(r1 +0);\"\n    1: r7 += 0x1ab064b9;\"\n    2: if r7 & 0x702000 goto 1b;\n    3: r7 &= 0x1ee60e;\"\n    4: r7 += r1;\"\n    5: if r7 s> 0x37d2 goto +0;\"\n    6: r0 = 0;\"\n    7: exit;\"\n\nPerf profiling shows that most of the time is spent in\nmark_chain_precision() ~95%.\n\nThe easiest way to explain why this program causes problems is to\napply the following patch:\n\n    diff --git a/include/linux/bpf.h b/include/linux/bpf.h\n    index 0c216e71cec7..4b4823961abe 100644\n    \\--- a/include/linux/bpf.h\n    \\+++ b/include/linux/bpf.h\n    \\@@ -1926,7 +1926,7 @@ struct bpf_array {\n            };\n     };\n\n    -#define BPF_COMPLEXITY_LIMIT_INSNS      1000000 /* yes. 1M insns */\n    +#define BPF_COMPLEXITY_LIMIT_INSNS      256 /* yes. 1M insns */\n     #define MAX_TAIL_CALL_CNT 33\n\n     /* Maximum number of loops for bpf_loop and bpf_iter_num.\n    diff --git a/kernel/bpf/verifier.c b/kernel/bpf/verifier.c\n    index f514247ba8ba..75e88be3bb3e 100644\n    \\--- a/kernel/bpf/verifier.c\n    \\+++ b/kernel/bpf/verifier.c\n    \\@@ -18024,8 +18024,13 @@ static int is_state_visited(struct bpf_verifier_env *env, int insn_idx)\n     skip_inf_loop_check:\n                            if (!force_new_state &&\n                                env->jmps_processed - env->prev_jmps_processed < 20 &&\n    -                           env->insn_processed - env->prev_insn_processed < 100)\n    +                           env->insn_processed - env->prev_insn_processed < 100) {\n    +                               verbose(env, \"is_state_visited: suppressing checkpoint at %d, %d jmps processed, cur->jmp_history_cnt is %d\\n\",\n    +                                       env->insn_idx,\n    +                                       env->jmps_processed - env->prev_jmps_processed,\n    +                                       cur->jmp_history_cnt);\n                                    add_new_state = false;\n    +                       }\n                            goto miss;\n                    }\n                    /* If sl->state is a part of a loop and this loop's entry is a part of\n    \\@@ -18142,6 +18147,9 @@ static int is_state_visited(struct bpf_verifier_env *env, int insn_idx)\n            if (!add_new_state)\n                    return 0;\n\n    +       verbose(env, \"is_state_visited: new checkpoint at %d, resetting env->jmps_processed\\n\",\n    +               env->insn_idx);\n    +\n            /* There were no equivalent states, remember the current one.\n             * Technically the current state is not proven to be safe yet,\n             * but it will either reach outer most bpf_exit (which means it's safe)\n\nAnd observe verification log:\n\n    ...\n    is_state_visited: new checkpoint at 5, resetting env->jmps_processed\n    5: R1=ctx() R7=ctx(...)\n    5: (65) if r7 s> 0x37d2 goto pc+0     ; R7=ctx(...)\n    6: (b7) r0 = 0                        ; R0_w=0\n    7: (95) exit\n\n    from 5 to 6: R1=ctx() R7=ctx(...) R10=fp0\n    6: R1=ctx() R7=ctx(...) R10=fp0\n    6: (b7) r0 = 0                        ; R0_w=0\n    7: (95) exit\n    is_state_visited: suppressing checkpoint at 1, 3 jmps processed, cur->jmp_history_cnt is 74\n\n    from 2 to 1: R1=ctx() R7_w=scalar(...) R10=fp0\n    1: R1=ctx() R7_w=scalar(...) R10=fp0\n    1: (07) r7 += 447767737\n    is_state_visited: suppressing checkpoint at 2, 3 jmps processed, cur->jmp_history_cnt is 75\n    2: R7_w=scalar(...)\n    2: (45) if r7 & 0x702000 goto pc-2\n    ... mark_precise 152 steps for r7 ...\n    2: R7_w=scalar(...)\n    is_state_visited: suppressing checkpoint at 1, 4 jmps processed, cur->jmp_history_cnt is 75\n    1: (07) r7 += 447767737\n    is_state_visited: suppressing checkpoint at 2, 4 jmps processed, cur->jmp_history_cnt is 76\n    2: R7_w=scalar(...)\n    2: (45) if r7 & 0x702000 goto pc-2\n    ...\n    BPF program is too large. Processed 257 insn\n\nThe log output shows that checkpoint at label (1) is never created,\nbecause it is suppressed by `skip_inf_loop_check` logic:\na. When 'if' at (2) is processed it pushes a state with insn_idx (1)\n   onto stack and proceeds to (3);\nb. At (5) checkpoint is created, and this resets\n   env->{jmps,insns}_processed.\nc. Verification proceeds and reaches `exit`;\nd. State saved at step (a) is popped from stack and is_state_visited()\n   considers if checkpoint needs to be added, but because\n   env->{jmps,insns}_processed had been just reset at step (b)\n   the `skip_inf_loop_check` logic forces `add_new_state` to false.\ne. Verifier proceeds with current state, which slowly accumulates\n   more and more entries in the jump history.\n\nThe accumulation of entries in the jump history is a problem because\nof two factors:\n- it eventually exhausts memory available for kmalloc() allocation;\n- mark_chain_precision() traverses the jump history of a state,\n  meaning that if `r7` is marked precise, verifier would iterate\n  ever growing jump history until parent state boundary is reached.\n\n(note: the log also shows a REG INVARIANTS VIOLATION warning\n       upon jset processing, but that's another bug to fix).\n\nWith this patch applied, the example above is rejected by verifier\nunder 1s of time, reaching 1M instructions limit.\n\nThe program is a simplified reproducer from syzbot report.\nPrevious discussion could be found at [1].\nThe patch does not cause any changes in verification performance,\nwhen tested on selftests from veristat.cfg and cilium programs taken\nfrom [2].\n\n[1] https://lore.kernel.org/bpf/20241009021254.2805446-1-eddyz87@gmail.com/\n[2] https://github.com/anakryiko/cilium\n\nChangelog:\n- v1 -> v2:\n  - moved patch to bpf tree;\n  - moved force_new_state variable initialization after declaration and\n    shortened the comment.\nv1: https://lore.kernel.org/bpf/20241018020307.1766906-1-eddyz87@gmail.com/\n\nFixes: 2589726d12a1 (\"bpf: introduce bounded loops\")\nReported-by: syzbot+7e46cdef14bf496a3ab4@syzkaller.appspotmail.com\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20241029172641.1042523-1-eddyz87@gmail.com\n\nCloses: https://lore.kernel.org/bpf/670429f6.050a0220.49194.0517.GAE@google.com/",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "99dde42e37497b3062516b1db7231f9dec744a00",
      "author": "Kui-Feng Lee <thinker.li@gmail.com>",
      "date": "2024-10-24 10:25:58 -0700",
      "message": "bpf: Handle BPF_UPTR in verifier\n\nThis patch adds BPF_UPTR support to the verifier. Not that only the\nmap_value will support the \"__uptr\" type tag.\n\nThis patch enforces only BPF_LDX is allowed to the value of an uptr.\nAfter BPF_LDX, it will mark the dst_reg as PTR_TO_MEM | PTR_MAYBE_NULL\nwith size deduced from the field.kptr.btf_id. This will make the\ndst_reg pointed memory to be readable and writable as scalar.\n\nThere is a redundant \"val_reg = reg_state(env, value_regno);\" statement\nin the check_map_kptr_access(). This patch takes this chance to remove\nit also.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20241023234759.860539-3-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "9806f283140ef3e4d259b7646bd8c66026bbaac5",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-10-23 22:16:45 -0700",
      "message": "bpf: fix do_misc_fixups() for bpf_get_branch_snapshot()\n\nWe need `goto next_insn;` at the end of patching instead of `continue;`.\nIt currently works by accident by making verifier re-process patched\ninstructions.\n\nReported-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nFixes: 314a53623cd4 (\"bpf: inline bpf_get_branch_snapshot() helper\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20241023161916.2896274-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "8ea607330a39184f51737c6ae706db7fdca7628e",
      "author": "Daniel Borkmann <daniel@iogearbox.net>",
      "date": "2024-10-22 15:42:56 -0700",
      "message": "bpf: Fix overloading of MEM_UNINIT's meaning\n\nLonial reported an issue in the BPF verifier where check_mem_size_reg()\nhas the following code:\n\n    if (!tnum_is_const(reg->var_off))\n        /* For unprivileged variable accesses, disable raw\n         * mode so that the program is required to\n         * initialize all the memory that the helper could\n         * just partially fill up.\n         */\n         meta = NULL;\n\nThis means that writes are not checked when the register containing the\nsize of the passed buffer has not a fixed size. Through this bug, a BPF\nprogram can write to a map which is marked as read-only, for example,\n.rodata global maps.\n\nThe problem is that MEM_UNINIT's initial meaning that \"the passed buffer\nto the BPF helper does not need to be initialized\" which was added back\nin commit 435faee1aae9 (\"bpf, verifier: add ARG_PTR_TO_RAW_STACK type\")\ngot overloaded over time with \"the passed buffer is being written to\".\n\nThe problem however is that checks such as the above which were added later\nvia 06c1c049721a (\"bpf: allow helpers access to variable memory\") set meta\nto NULL in order force the user to always initialize the passed buffer to\nthe helper. Due to the current double meaning of MEM_UNINIT, this bypasses\nverifier write checks to the memory (not boundary checks though) and only\nassumes the latter memory is read instead.\n\nFix this by reverting MEM_UNINIT back to its original meaning, and having\nMEM_WRITE as an annotation to BPF helpers in order to then trigger the\nBPF verifier checks for writing to memory.\n\nSome notes: check_arg_pair_ok() ensures that for ARG_CONST_SIZE{,_OR_ZERO}\nwe can access fn->arg_type[arg - 1] since it must contain a preceding\nARG_PTR_TO_MEM. For check_mem_reg() the meta argument can be removed\naltogether since we do check both BPF_READ and BPF_WRITE. Same for the\nequivalent check_kfunc_mem_size_reg().\n\nFixes: 7b3552d3f9f6 (\"bpf: Reject writes for PTR_TO_MAP_KEY in check_helper_mem_access\")\nFixes: 97e6d7dab1ca (\"bpf: Check PTR_TO_MEM | MEM_RDONLY in check_helper_mem_access\")\nFixes: 15baa55ff5b0 (\"bpf/verifier: allow all functions to read user provided context\")\nReported-by: Lonial Con <kongln9170@gmail.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20241021152809.33343-2-daniel@iogearbox.net\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "3878ae04e9fc24dacb77a1d32bd87e7d8108599e",
      "author": "Daniel Borkmann <daniel@iogearbox.net>",
      "date": "2024-10-17 11:06:34 -0700",
      "message": "bpf: Fix incorrect delta propagation between linked registers\n\nNathaniel reported a bug in the linked scalar delta tracking, which can lead\nto accepting a program with OOB access. The specific code is related to the\nsync_linked_regs() function and the BPF_ADD_CONST flag, which signifies a\nconstant offset between two scalar registers tracked by the same register id.\n\nThe verifier attempts to track \"similar\" scalars in order to propagate bounds\ninformation learned about one scalar to others. For instance, if r1 and r2\nare known to contain the same value, then upon encountering 'if (r1 != 0x1234)\ngoto xyz', not only does it know that r1 is equal to 0x1234 on the path where\nthat conditional jump is not taken, it also knows that r2 is.\n\nAdditionally, with env->bpf_capable set, the verifier will track scalars\nwhich should be a constant delta apart (if r1 is known to be one greater than\nr2, then if r1 is known to be equal to 0x1234, r2 must be equal to 0x1233.)\nThe code path for the latter in adjust_reg_min_max_vals() is reached when\nprocessing both 32 and 64-bit addition operations. While adjust_reg_min_max_vals()\nknows whether dst_reg was produced by a 32 or a 64-bit addition (based on the\nalu32 bool), the only information saved in dst_reg is the id of the source\nregister (reg->id, or'ed by BPF_ADD_CONST) and the value of the constant\noffset (reg->off).\n\nLater, the function sync_linked_regs() will attempt to use this information\nto propagate bounds information from one register (known_reg) to others,\nmeaning, for all R in linked_regs, it copies known_reg range (and possibly\nadjusting delta) into R for the case of R->id == known_reg->id.\n\nFor the delta adjustment, meaning, matching reg->id with BPF_ADD_CONST, the\nverifier adjusts the register as reg = known_reg; reg += delta where delta\nis computed as (s32)reg->off - (s32)known_reg->off and placed as a scalar\ninto a fake_reg to then simulate the addition of reg += fake_reg. This is\nonly correct, however, if the value in reg was created by a 64-bit addition.\nWhen reg contains the result of a 32-bit addition operation, its upper 32\nbits will always be zero. sync_linked_regs() on the other hand, may cause\nthe verifier to believe that the addition between fake_reg and reg overflows\ninto those upper bits. For example, if reg was generated by adding the\nconstant 1 to known_reg using a 32-bit alu operation, then reg->off is 1\nand known_reg->off is 0. If known_reg is known to be the constant 0xFFFFFFFF,\nsync_linked_regs() will tell the verifier that reg is equal to the constant\n0x100000000. This is incorrect as the actual value of reg will be 0, as the\n32-bit addition will wrap around.\n\nExample:\n\n  0: (b7) r0 = 0;             R0_w=0\n  1: (18) r1 = 0x80000001;    R1_w=0x80000001\n  3: (37) r1 /= 1;            R1_w=scalar()\n  4: (bf) r2 = r1;            R1_w=scalar(id=1) R2_w=scalar(id=1)\n  5: (bf) r4 = r1;            R1_w=scalar(id=1) R4_w=scalar(id=1)\n  6: (04) w2 += 2147483647;   R2_w=scalar(id=1+2147483647,smin=0,smax=umax=0xffffffff,var_off=(0x0; 0xffffffff))\n  7: (04) w4 += 0 ;           R4_w=scalar(id=1+0,smin=0,smax=umax=0xffffffff,var_off=(0x0; 0xffffffff))\n  8: (15) if r2 == 0x0 goto pc+1\n 10: R0=0 R1=0xffffffff80000001 R2=0x7fffffff R4=0xffffffff80000001 R10=fp0\n\nWhat can be seen here is that r1 is copied to r2 and r4, such that {r1,r2,r4}.id\nare all the same which later lets sync_linked_regs() to be invoked. Then, in\na next step constants are added with alu32 to r2 and r4, setting their ->off,\nas well as id |= BPF_ADD_CONST. Next, the conditional will bind r2 and\npropagate ranges to its linked registers. The verifier now believes the upper\n32 bits of r4 are r4=0xffffffff80000001, while actually r4=r1=0x80000001.\n\nOne approach for a simple fix suitable also for stable is to limit the constant\ndelta tracking to only 64-bit alu addition. If necessary at some later point,\nBPF_ADD_CONST could be split into BPF_ADD_CONST64 and BPF_ADD_CONST32 to avoid\nmixing the two under the tradeoff to further complicate sync_linked_regs().\nHowever, none of the added tests from dedf56d775c0 (\"selftests/bpf: Add tests\nfor add_const\") make this necessary at this point, meaning, BPF CI also passes\nwith just limiting tracking to 64-bit alu addition.\n\nFixes: 98d7ca374ba4 (\"bpf: Track delta between \"linked\" registers.\")\nReported-by: Nathaniel Theis <nathaniel.theis@nccgroup.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nReviewed-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20241016134913.32249-1-daniel@iogearbox.net",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "a992d7a3979120fbd7c13435d27b3da8d9ed095a",
      "author": "Namhyung Kim <namhyung@kernel.org>",
      "date": "2024-10-16 09:21:03 -0700",
      "message": "mm/bpf: Add bpf_get_kmem_cache() kfunc\n\nThe bpf_get_kmem_cache() is to get a slab cache information from a\nvirtual address like virt_to_cache().  If the address is a pointer\nto a slab object, it'd return a valid kmem_cache pointer, otherwise\nNULL is returned.\n\nIt doesn't grab a reference count of the kmem_cache so the caller is\nresponsible to manage the access.  The returned point is marked as\nPTR_UNTRUSTED.\n\nThe intended use case for now is to symbolize locks in slab objects\nfrom the lock contention tracepoints.\n\nSuggested-by: Vlastimil Babka <vbabka@suse.cz>\nAcked-by: Roman Gushchin <roman.gushchin@linux.dev> (mm/*)\nAcked-by: Vlastimil Babka <vbabka@suse.cz> #mm/slab\nSigned-off-by: Namhyung Kim <namhyung@kernel.org>\nLink: https://lore.kernel.org/r/20241010232505.1339892-3-namhyung@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "mm/slab_common.c"
      ]
    },
    {
      "hash": "ae67b9fb8c4e981e929a665dcaa070f4b05ebdb4",
      "author": "Dimitar Kanaliev <dimitar.kanaliev@siteground.com>",
      "date": "2024-10-15 11:16:24 -0700",
      "message": "bpf: Fix truncation bug in coerce_reg_to_size_sx()\n\ncoerce_reg_to_size_sx() updates the register state after a sign-extension\noperation. However, there's a bug in the assignment order of the unsigned\nmin/max values, leading to incorrect truncation:\n\n  0: (85) call bpf_get_prandom_u32#7    ; R0_w=scalar()\n  1: (57) r0 &= 1                       ; R0_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=1,var_off=(0x0; 0x1))\n  2: (07) r0 += 254                     ; R0_w=scalar(smin=umin=smin32=umin32=254,smax=umax=smax32=umax32=255,var_off=(0xfe; 0x1))\n  3: (bf) r0 = (s8)r0                   ; R0_w=scalar(smin=smin32=-2,smax=smax32=-1,umin=umin32=0xfffffffe,umax=0xffffffff,var_off=(0xfffffffffffffffe; 0x1))\n\nIn the current implementation, the unsigned 32-bit min/max values\n(u32_min_value and u32_max_value) are assigned directly from the 64-bit\nsigned min/max values (s64_min and s64_max):\n\n  reg->umin_value = reg->u32_min_value = s64_min;\n  reg->umax_value = reg->u32_max_value = s64_max;\n\nDue to the chain assigmnent, this is equivalent to:\n\n  reg->u32_min_value = s64_min;  // Unintended truncation\n  reg->umin_value = reg->u32_min_value;\n  reg->u32_max_value = s64_max;  // Unintended truncation\n  reg->umax_value = reg->u32_max_value;\n\nFixes: 1f9a1ea821ff (\"bpf: Support new sign-extension load insns\")\nReported-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nReported-by: Zac Ecob <zacecob@protonmail.com>\nSigned-off-by: Dimitar Kanaliev <dimitar.kanaliev@siteground.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nReviewed-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20241014121155.92887-2-dimitar.kanaliev@siteground.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "6cb86a0fdece87e126323ec1bb19deb16a52aedf",
      "author": "Toke H\u00f8iland-J\u00f8rgensen <toke@redhat.com>",
      "date": "2024-10-10 10:44:03 -0700",
      "message": "bpf: fix kfunc btf caching for modules\n\nThe verifier contains a cache for looking up module BTF objects when\ncalling kfuncs defined in modules. This cache uses a 'struct\nbpf_kfunc_btf_tab', which contains a sorted list of BTF objects that\nwere already seen in the current verifier run, and the BTF objects are\nlooked up by the offset stored in the relocated call instruction using\nbsearch().\n\nThe first time a given offset is seen, the module BTF is loaded from the\nfile descriptor passed in by libbpf, and stored into the cache. However,\nthere's a bug in the code storing the new entry: it stores a pointer to\nthe new cache entry, then calls sort() to keep the cache sorted for the\nnext lookup using bsearch(), and then returns the entry that was just\nstored through the stored pointer. However, because sort() modifies the\nlist of entries in place *by value*, the stored pointer may no longer\npoint to the right entry, in which case the wrong BTF object will be\nreturned.\n\nThe end result of this is an intermittent bug where, if a BPF program\ncalls two functions with the same signature in two different modules,\nthe function from the wrong module may sometimes end up being called.\nWhether this happens depends on the order of the calls in the BPF\nprogram (as that affects whether sort() reorders the array of BTF\nobjects), making it especially hard to track down. Simon, credited as\nreporter below, spent significant effort analysing and creating a\nreproducer for this issue. The reproducer is added as a selftest in a\nsubsequent patch.\n\nThe fix is straight forward: simply don't use the stored pointer after\ncalling sort(). Since we already have an on-stack pointer to the BTF\nobject itself at the point where the function return, just use that, and\npopulate it from the cache entry in the branch where the lookup\nsucceeds.\n\nFixes: 2357672c54c3 (\"bpf: Introduce BPF support for kernel module function calls\")\nReported-by: Simon Sundberg <simon.sundberg@kau.se>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Toke H\u00f8iland-J\u00f8rgensen <toke@redhat.com>\nLink: https://lore.kernel.org/r/20241010-fix-kfunc-btf-caching-for-modules-v2-1-745af6c1af98@redhat.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5bd48a3a14df4b3ee1be0757efcc0f40d4f57b35",
      "author": "Matteo Croce <teknoraver@meta.com>",
      "date": "2024-10-10 08:52:36 -0700",
      "message": "bpf: fix argument type in bpf_loop documentation\n\nThe `index` argument to bpf_loop() is threaded as an u64.\nThis lead in a subtle verifier denial where clang cloned the argument\nin another register[1].\n\n[1] https://github.com/systemd/systemd/pull/34650#issuecomment-2401092895\n\nSigned-off-by: Matteo Croce <teknoraver@meta.com>\nLink: https://lore.kernel.org/r/20241010035652.17830-1-technoboy85@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/uapi/linux/bpf.h",
        "kernel/bpf/verifier.c",
        "tools/include/uapi/linux/bpf.h"
      ]
    },
    {
      "hash": "434247637c66e1be2bc71a9987d4c3f0d8672387",
      "author": "Rik van Riel <riel@surriel.com>",
      "date": "2024-10-09 18:13:05 -0700",
      "message": "bpf: use kvzmalloc to allocate BPF verifier environment\n\nThe kzmalloc call in bpf_check can fail when memory is very fragmented,\nwhich in turn can lead to an OOM kill.\n\nUse kvzmalloc to fall back to vmalloc when memory is too fragmented to\nallocate an order 3 sized bpf verifier environment.\n\nAdmittedly this is not a very common case, and only happens on systems\nwhere memory has already been squeezed close to the limit, but this does\nnot seem like much of a hot path, and it's a simple enough fix.\n\nSigned-off-by: Rik van Riel <riel@surriel.com>\nReviewed-by: Shakeel Butt <shakeel.butt@linux.dev>\nLink: https://lore.kernel.org/r/20241008170735.16766766@imladris.surriel.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "da7d71bcb0637b7aa18934628fdb5a55f2db49a6",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-10-03 17:47:53 -0700",
      "message": "bpf: Use KF_FASTCALL to mark kfuncs supporting fastcall contract\n\nIn order to allow pahole add btf_decl_tag(\"bpf_fastcall\") for kfuncs\nsupporting bpf_fastcall, mark such functions with KF_FASTCALL in\nid_set8 objects.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240916091712.2929279-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/btf.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "e9bd9c498cb0f5843996dbe5cbce7a1836a83c70",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-10-01 17:18:52 +0200",
      "message": "bpf: sync_linked_regs() must preserve subreg_def\n\nRange propagation must not affect subreg_def marks, otherwise the\nfollowing example is rewritten by verifier incorrectly when\nBPF_F_TEST_RND_HI32 flag is set:\n\n  0: call bpf_ktime_get_ns                   call bpf_ktime_get_ns\n  1: r0 &= 0x7fffffff       after verifier   r0 &= 0x7fffffff\n  2: w1 = w0                rewrites         w1 = w0\n  3: if w0 < 10 goto +0     -------------->  r11 = 0x2f5674a6     (r)\n  4: r1 >>= 32                               r11 <<= 32           (r)\n  5: r0 = r1                                 r1 |= r11            (r)\n  6: exit;                                   if w0 < 0xa goto pc+0\n                                             r1 >>= 32\n                                             r0 = r1\n                                             exit\n\n(or zero extension of w1 at (2) is missing for architectures that\n require zero extension for upper register half).\n\nThe following happens w/o this patch:\n- r0 is marked as not a subreg at (0);\n- w1 is marked as subreg at (2);\n- w1 subreg_def is overridden at (3) by copy_register_state();\n- w1 is read at (5) but mark_insn_zext() does not mark (2)\n  for zero extension, because w1 subreg_def is not set;\n- because of BPF_F_TEST_RND_HI32 flag verifier inserts random\n  value for hi32 bits of (2) (marked (r));\n- this random value is read at (5).\n\nFixes: 75748837b7e5 (\"bpf: Propagate scalar ranges through register assignments.\")\nReported-by: Lonial Con <kongln9170@gmail.com>\nSigned-off-by: Lonial Con <kongln9170@gmail.com>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Daniel Borkmann <daniel@iogearbox.net>\nCloses: https://lore.kernel.org/bpf/7e2aa30a62d740db182c170fdd8f81c596df280d.camel@gmail.com\nLink: https://lore.kernel.org/bpf/20240924210844.1758441-1-eddyz87@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "18752d73c1898fd001569195ba4b0b8c43255f4a",
      "author": "Daniel Borkmann <daniel@iogearbox.net>",
      "date": "2024-09-13 13:17:56 -0700",
      "message": "bpf: Improve check_raw_mode_ok test for MEM_UNINIT-tagged types\n\nWhen checking malformed helper function signatures, also take other argument\ntypes into account aside from just ARG_PTR_TO_UNINIT_MEM.\n\nThis concerns (formerly) ARG_PTR_TO_{INT,LONG} given uninitialized memory can\nbe passed there, too.\n\nThe func proto sanity check goes back to commit 435faee1aae9 (\"bpf, verifier:\nadd ARG_PTR_TO_RAW_STACK type\"), and its purpose was to detect wrong func protos\nwhich had more than just one MEM_UNINIT-tagged type as arguments.\n\nThe reason more than one is currently not supported is as we mark stack slots with\nSTACK_MISC in check_helper_call() in case of raw mode based on meta.access_size to\nallow uninitialized stack memory to be passed to helpers when they just write into\nthe buffer.\n\nProbing for base type as well as MEM_UNINIT tagging ensures that other types do not\nget missed (as it used to be the case for ARG_PTR_TO_{INT,LONG}).\n\nFixes: 57c3bb725a3d (\"bpf: Introduce ARG_PTR_TO_{INT,LONG} arg types\")\nReported-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20240913191754.13290-4-daniel@iogearbox.net\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "32556ce93bc45c730829083cb60f95a2728ea48b",
      "author": "Daniel Borkmann <daniel@iogearbox.net>",
      "date": "2024-09-13 13:17:55 -0700",
      "message": "bpf: Fix helper writes to read-only maps\n\nLonial found an issue that despite user- and BPF-side frozen BPF map\n(like in case of .rodata), it was still possible to write into it from\na BPF program side through specific helpers having ARG_PTR_TO_{LONG,INT}\nas arguments.\n\nIn check_func_arg() when the argument is as mentioned, the meta->raw_mode\nis never set. Later, check_helper_mem_access(), under the case of\nPTR_TO_MAP_VALUE as register base type, it assumes BPF_READ for the\nsubsequent call to check_map_access_type() and given the BPF map is\nread-only it succeeds.\n\nThe helpers really need to be annotated as ARG_PTR_TO_{LONG,INT} | MEM_UNINIT\nwhen results are written into them as opposed to read out of them. The\nlatter indicates that it's okay to pass a pointer to uninitialized memory\nas the memory is written to anyway.\n\nHowever, ARG_PTR_TO_{LONG,INT} is a special case of ARG_PTR_TO_FIXED_SIZE_MEM\njust with additional alignment requirement. So it is better to just get\nrid of the ARG_PTR_TO_{LONG,INT} special cases altogether and reuse the\nfixed size memory types. For this, add MEM_ALIGNED to additionally ensure\nalignment given these helpers write directly into the args via *<ptr> = val.\nThe .arg*_size has been initialized reflecting the actual sizeof(*<ptr>).\n\nMEM_ALIGNED can only be used in combination with MEM_FIXED_SIZE annotated\nargument types, since in !MEM_FIXED_SIZE cases the verifier does not know\nthe buffer size a priori and therefore cannot blindly write *<ptr> = val.\n\nFixes: 57c3bb725a3d (\"bpf: Introduce ARG_PTR_TO_{INT,LONG} arg types\")\nReported-by: Lonial Con <kongln9170@gmail.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20240913191754.13290-3-daniel@iogearbox.net\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c",
        "kernel/trace/bpf_trace.c",
        "net/core/filter.c"
      ]
    },
    {
      "hash": "7dd34d7b7dcf9309fc6224caf4dd5b35bedddcb7",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-09-13 13:07:44 -0700",
      "message": "bpf: Fix a sdiv overflow issue\n\nZac Ecob reported a problem where a bpf program may cause kernel crash due\nto the following error:\n  Oops: divide error: 0000 [#1] PREEMPT SMP KASAN PTI\n\nThe failure is due to the below signed divide:\n  LLONG_MIN/-1 where LLONG_MIN equals to -9,223,372,036,854,775,808.\nLLONG_MIN/-1 is supposed to give a positive number 9,223,372,036,854,775,808,\nbut it is impossible since for 64-bit system, the maximum positive\nnumber is 9,223,372,036,854,775,807. On x86_64, LLONG_MIN/-1 will\ncause a kernel exception. On arm64, the result for LLONG_MIN/-1 is\nLLONG_MIN.\n\nFurther investigation found all the following sdiv/smod cases may trigger\nan exception when bpf program is running on x86_64 platform:\n  - LLONG_MIN/-1 for 64bit operation\n  - INT_MIN/-1 for 32bit operation\n  - LLONG_MIN%-1 for 64bit operation\n  - INT_MIN%-1 for 32bit operation\nwhere -1 can be an immediate or in a register.\n\nOn arm64, there are no exceptions:\n  - LLONG_MIN/-1 = LLONG_MIN\n  - INT_MIN/-1 = INT_MIN\n  - LLONG_MIN%-1 = 0\n  - INT_MIN%-1 = 0\nwhere -1 can be an immediate or in a register.\n\nInsn patching is needed to handle the above cases and the patched codes\nproduced results aligned with above arm64 result. The below are pseudo\ncodes to handle sdiv/smod exceptions including both divisor -1 and divisor 0\nand the divisor is stored in a register.\n\nsdiv:\n      tmp = rX\n      tmp += 1 /* [-1, 0] -> [0, 1]\n      if tmp >(unsigned) 1 goto L2\n      if tmp == 0 goto L1\n      rY = 0\n  L1:\n      rY = -rY;\n      goto L3\n  L2:\n      rY /= rX\n  L3:\n\nsmod:\n      tmp = rX\n      tmp += 1 /* [-1, 0] -> [0, 1]\n      if tmp >(unsigned) 1 goto L1\n      if tmp == 1 (is64 ? goto L2 : goto L3)\n      rY = 0;\n      goto L2\n  L1:\n      rY %= rX\n  L2:\n      goto L4  // only when !is64\n  L3:\n      wY = wY  // only when !is64\n  L4:\n\n  [1] https://lore.kernel.org/bpf/tPJLTEh7S_DxFEqAI2Ji5MBSoZVg7_G-Py2iaZpAaWtM961fFTWtsnlzwvTbzBzaUzwQAoNATXKUlt0LZOFgnDcIyKCswAnAGdUF3LBrhGQ=@protonmail.com/\n\nReported-by: Zac Ecob <zacecob@protonmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240913150326.1187788-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "8aeaed21befc90f27f4fca6dd190850d97d2e9e3",
      "author": "Philo Lu <lulie@linux.alibaba.com>",
      "date": "2024-09-11 08:56:37 -0700",
      "message": "bpf: Support __nullable argument suffix for tp_btf\n\nPointers passed to tp_btf were trusted to be valid, but some tracepoints\ndo take NULL pointer as input, such as trace_tcp_send_reset(). Then the\ninvalid memory access cannot be detected by verifier.\n\nThis patch fix it by add a suffix \"__nullable\" to the unreliable\nargument. The suffix is shown in btf, and PTR_MAYBE_NULL will be added\nto nullable arguments. Then users must check the pointer before use it.\n\nA problem here is that we use \"btf_trace_##call\" to search func_proto.\nAs it is a typedef, argument names as well as the suffix are not\nrecorded. To solve this, I use bpf_raw_event_map to find\n\"__bpf_trace##template\" from \"btf_trace_##call\", and then we can see the\nsuffix.\n\nSuggested-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Philo Lu <lulie@linux.alibaba.com>\nLink: https://lore.kernel.org/r/20240911033719.91468-2-lulie@linux.alibaba.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "bee109b7b3e50739b88252a219fa07ecd78ad628",
      "author": "Maxim Mikityanskiy <maxtram95@gmail.com>",
      "date": "2024-09-09 15:58:17 -0700",
      "message": "bpf: Fix error message on kfunc arg type mismatch\n\nWhen \"arg#%d expected pointer to ctx, but got %s\" error is printed, both\ntemplate parts actually point to the type of the argument, therefore, it\nwill also say \"but got PTR\", regardless of what was the actual register\ntype.\n\nFix the message to print the register type in the second part of the\ntemplate, change the existing test to adapt to the new format, and add a\nnew test to test the case when arg is a pointer to context, but reg is a\nscalar.\n\nFixes: 00b85860feb8 (\"bpf: Rewrite kfunc argument handling\")\nSigned-off-by: Maxim Mikityanskiy <maxim@isovalent.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/bpf/20240909133909.1315460-1-maxim@isovalent.com",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/prog_tests/kfunc_call.c",
        "tools/testing/selftests/bpf/progs/kfunc_call_fail.c",
        "tools/testing/selftests/bpf/verifier/calls.c"
      ]
    },
    {
      "hash": "1ae497c78f01855f3695b58481311ffdd429b028",
      "author": "Shung-Hsi Yu <shung-hsi.yu@suse.com>",
      "date": "2024-09-05 13:29:06 -0700",
      "message": "bpf: use type_may_be_null() helper for nullable-param check\n\nCommit 980ca8ceeae6 (\"bpf: check bpf_dummy_struct_ops program params for\ntest runs\") does bitwise AND between reg_type and PTR_MAYBE_NULL, which\nis correct, but due to type difference the compiler complains:\n\n  net/bpf/bpf_dummy_struct_ops.c:118:31: warning: bitwise operation between different enumeration types ('const enum bpf_reg_type' and 'enum bpf_type_flag') [-Wenum-enum-conversion]\n    118 |                 if (info && (info->reg_type & PTR_MAYBE_NULL))\n        |                              ~~~~~~~~~~~~~~ ^ ~~~~~~~~~~~~~~\n\nWorkaround the warning by moving the type_may_be_null() helper from\nverifier.c into bpf_verifier.h, and reuse it here to check whether param\nis nullable.\n\nFixes: 980ca8ceeae6 (\"bpf: check bpf_dummy_struct_ops program params for test runs\")\nReported-by: kernel test robot <lkp@intel.com>\nCloses: https://lore.kernel.org/oe-kbuild-all/202404241956.HEiRYwWq-lkp@intel.com/\nSigned-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240905055233.70203-1-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c",
        "net/bpf/bpf_dummy_struct_ops.c"
      ]
    },
    {
      "hash": "00750788dfc6b5167a294915e2690d8af182c496",
      "author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "date": "2024-09-04 12:45:18 -0700",
      "message": "bpf: Fix indentation issue in epilogue_idx\n\nThere is a report on new indentation issue in epilogue_idx.\nThis patch fixed it.\n\nFixes: 169c31761c8d (\"bpf: Add gen_epilogue to bpf_verifier_ops\")\nReported-by: kernel test robot <lkp@intel.com>\nCloses: https://lore.kernel.org/oe-kbuild-all/202408311622.4GzlzN33-lkp@intel.com/\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240904180847.56947-3-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "940ce73bdec5a020fec058ba947d2bf627462c53",
      "author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "date": "2024-09-04 12:45:18 -0700",
      "message": "bpf: Remove the insn_buf array stack usage from the inline_bpf_loop()\n\nThis patch removes the insn_buf array stack usage from the\ninline_bpf_loop(). Instead, the env->insn_buf is used. The\nusage in inline_bpf_loop() needs more than 16 insn, so the\nINSN_BUF_SIZE needs to be increased from 16 to 32.\nThe compiler stack size warning on the verifier is gone\nafter this change.\n\nCc: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240904180847.56947-2-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "4cc8c50c9abcb2646a7a4fcef3cea5dcb30c06cf",
      "author": "Juntong Deng <juntong.deng@outlook.com>",
      "date": "2024-08-29 18:51:26 -0700",
      "message": "bpf: Make the pointer returned by iter next method valid\n\nCurrently we cannot pass the pointer returned by iter next method as\nargument to KF_TRUSTED_ARGS or KF_RCU kfuncs, because the pointer\nreturned by iter next method is not \"valid\".\n\nThis patch sets the pointer returned by iter next method to be valid.\n\nThis is based on the fact that if the iterator is implemented correctly,\nthen the pointer returned from the iter next method should be valid.\n\nThis does not make NULL pointer valid. If the iter next method has\nKF_RET_NULL flag, then the verifier will ask the ebpf program to\ncheck NULL pointer.\n\nKF_RCU_PROTECTED iterator is a special case, the pointer returned by\niter next method should only be valid within RCU critical section,\nso it should be with MEM_RCU, not PTR_TRUSTED.\n\nAnother special case is bpf_iter_num_next, which returns a pointer with\nbase type PTR_TO_MEM. PTR_TO_MEM should not be combined with type flag\nPTR_TRUSTED (PTR_TO_MEM already means the pointer is valid).\n\nThe pointer returned by iter next method of other types of iterators\nis with PTR_TRUSTED.\n\nIn addition, this patch adds get_iter_from_state to help us get the\ncurrent iterator from the current state.\n\nSigned-off-by: Juntong Deng <juntong.deng@outlook.com>\nLink: https://lore.kernel.org/r/AM6PR03MB584869F8B448EA1C87B7CDA399962@AM6PR03MB5848.eurprd03.prod.outlook.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "169c31761c8d7f606f3ee628829c27998626c4f0",
      "author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "date": "2024-08-29 18:15:45 -0700",
      "message": "bpf: Add gen_epilogue to bpf_verifier_ops\n\nThis patch adds a .gen_epilogue to the bpf_verifier_ops. It is similar\nto the existing .gen_prologue. Instead of allowing a subsystem\nto run code at the beginning of a bpf prog, it allows the subsystem\nto run code just before the bpf prog exit.\n\nOne of the use case is to allow the upcoming bpf qdisc to ensure that\nthe skb->dev is the same as the qdisc->dev_queue->dev. The bpf qdisc\nstruct_ops implementation could either fix it up or drop the skb.\nAnother use case could be in bpf_tcp_ca.c to enforce snd_cwnd\nhas sane value (e.g. non zero).\n\nThe epilogue can do the useful thing (like checking skb->dev) if it\ncan access the bpf prog's ctx. Unlike prologue, r1 may not hold the\nctx pointer. This patch saves the r1 in the stack if the .gen_epilogue\nhas returned some instructions in the \"epilogue_buf\".\n\nThe existing .gen_prologue is done in convert_ctx_accesses().\nThe new .gen_epilogue is done in the convert_ctx_accesses() also.\nWhen it sees the (BPF_JMP | BPF_EXIT) instruction, it will be patched\nwith the earlier generated \"epilogue_buf\". The epilogue patching is\nonly done for the main prog.\n\nOnly one epilogue will be patched to the main program. When the\nbpf prog has multiple BPF_EXIT instructions, a BPF_JA is used\nto goto the earlier patched epilogue. Majority of the archs\nsupport (BPF_JMP32 | BPF_JA): x86, arm, s390, risv64, loongarch,\npowerpc and arc. This patch keeps it simple and always\nuse (BPF_JMP32 | BPF_JA). A new macro BPF_JMP32_A is added to\ngenerate the (BPF_JMP32 | BPF_JA) insn.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-4-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "include/linux/filter.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d5c47719f24438838e60bcbf97008179d6f737a8",
      "author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "date": "2024-08-29 18:15:44 -0700",
      "message": "bpf: Adjust BPF_JMP that jumps to the 1st insn of the prologue\n\nThe next patch will add a ctx ptr saving instruction\n\"(r1 = *(u64 *)(r10 -8)\" at the beginning for the main prog\nwhen there is an epilogue patch (by the .gen_epilogue() verifier\nops added in the next patch).\n\nThere is one corner case if the bpf prog has a BPF_JMP that jumps\nto the 1st instruction. It needs an adjustment such that\nthose BPF_JMP instructions won't jump to the newly added\nctx saving instruction.\nThe commit 5337ac4c9b80 (\"bpf: Fix the corner case with may_goto and jump to the 1st insn.\")\nhas the details on this case.\n\nNote that the jump back to 1st instruction is not limited to the\nctx ptr saving instruction. The same also applies to the prologue.\nA later test, pro_epilogue_goto_start.c, has a test for the prologue\nonly case.\n\nThus, this patch does one adjustment after gen_prologue and\nthe future ctx ptr saving. It is done by\nadjust_jmp_off(env->prog, 0, delta) where delta has the total\nnumber of instructions in the prologue and\nthe future ctx ptr saving instruction.\n\nThe adjust_jmp_off(env->prog, 0, delta) assumes that the\nprologue does not have a goto 1st instruction itself.\nTo accommodate the prologue might have a goto 1st insn itself,\nthis patch changes the adjust_jmp_off() to skip considering\nthe instructions between [tgt_idx, tgt_idx + delta).\n\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-3-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "6f606ffd6dd7583d8194ee3d858ba4da2eff26a3",
      "author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "date": "2024-08-29 18:15:44 -0700",
      "message": "bpf: Move insn_buf[16] to bpf_verifier_env\n\nThis patch moves the 'struct bpf_insn insn_buf[16]' stack usage\nto the bpf_verifier_env. A '#define INSN_BUF_SIZE 16' is also added\nto replace the ARRAY_SIZE(insn_buf) usages.\n\nBoth convert_ctx_accesses() and do_misc_fixup() are changed\nto use the env->insn_buf.\n\nIt is a refactoring work for adding the epilogue_buf[16] in a later patch.\n\nWith this patch, the stack size usage decreased.\n\nBefore:\n./kernel/bpf/verifier.c:22133:5: warning: stack frame size (2584)\n\nAfter:\n./kernel/bpf/verifier.c:22184:5: warning: stack frame size (2264)\n\nReviewed-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240829210833.388152-2-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "f633919d132cf1755d62278c989124c0ce23950f",
      "author": "Juntong Deng <juntong.deng@outlook.com>",
      "date": "2024-08-28 17:11:54 -0700",
      "message": "bpf: Relax KF_ACQUIRE kfuncs strict type matching constraint\n\nCurrently we cannot pass zero offset (implicit cast) or non-zero offset\npointers to KF_ACQUIRE kfuncs. This is because KF_ACQUIRE kfuncs\nrequires strict type matching, but zero offset or non-zero offset does\nnot change the type of pointer, which causes the ebpf program to be\nrejected by the verifier.\n\nThis can cause some problems, one example is that bpf_skb_peek_tail\nkfunc [0] cannot be implemented by just passing in non-zero offset\npointers. We cannot pass pointers like &sk->sk_write_queue (non-zero\noffset) or &sk->__sk_common (zero offset) to KF_ACQUIRE kfuncs.\n\nThis patch makes KF_ACQUIRE kfuncs not require strict type matching.\n\n[0]: https://lore.kernel.org/bpf/AM6PR03MB5848CA39CB4B7A4397D380B099B12@AM6PR03MB5848.eurprd03.prod.outlook.com/\n\nSigned-off-by: Juntong Deng <juntong.deng@outlook.com>\nLink: https://lore.kernel.org/r/AM6PR03MB5848FD2BD89BF0B6B5AA3B4C99952@AM6PR03MB5848.eurprd03.prod.outlook.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b0966c724584a5a9fd7fb529de19807c31f27a45",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2024-08-23 11:39:33 -0700",
      "message": "bpf: Support bpf_kptr_xchg into local kptr\n\nCurrently, users can only stash kptr into map values with bpf_kptr_xchg().\nThis patch further supports stashing kptr into local kptr by adding local\nkptr as a valid destination type.\n\nWhen stashing into local kptr, btf_record in program BTF is used instead\nof btf_record in map to search for the btf_field of the local kptr.\n\nThe local kptr specific checks in check_reg_type() only apply when the\nsource argument of bpf_kptr_xchg() is local kptr. Therefore, we make the\nscope of the check explicit as the destination now can also be local kptr.\n\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nSigned-off-by: Amery Hung <amery.hung@bytedance.com>\nLink: https://lore.kernel.org/r/20240813212424.2871455-5-amery.hung@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/uapi/linux/bpf.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d59232afb0344e33e9399f308d9b4a03876e7676",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2024-08-23 11:39:33 -0700",
      "message": "bpf: Rename ARG_PTR_TO_KPTR -> ARG_KPTR_XCHG_DEST\n\nARG_PTR_TO_KPTR is currently only used by the bpf_kptr_xchg helper.\nAlthough it limits reg types for that helper's first arg to\nPTR_TO_MAP_VALUE, any arbitrary mapval won't do: further custom\nverification logic ensures that the mapval reg being xchgd-into is\npointing to a kptr field. If this is not the case, it's not safe to xchg\ninto that reg's pointee.\n\nLet's rename the bpf_arg_type to more accurately describe the fairly\nspecific expectations that this arg type encodes.\n\nThis is a nonfunctional change.\n\nAcked-by: Martin KaFai Lau <martin.lau@kernel.org>\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nSigned-off-by: Amery Hung <amery.hung@bytedance.com>\nLink: https://lore.kernel.org/r/20240813212424.2871455-4-amery.hung@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "40609093247b586ee6f8ca8f2b30c7d583c6fd25",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-08-22 08:35:21 -0700",
      "message": "bpf: allow bpf_fastcall for bpf_cast_to_kern_ctx and bpf_rdonly_cast\n\ndo_misc_fixups() relaces bpf_cast_to_kern_ctx() and bpf_rdonly_cast()\nby a single instruction \"r0 = r1\". This follows bpf_fastcall contract.\nThis commit allows bpf_fastcall pattern rewrite for these two\nfunctions in order to use them in bpf_fastcall selftests.\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240822084112.3257995-5-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b2ee6d27e9c6be0409e96591dcee62032a8e0156",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-08-22 08:35:21 -0700",
      "message": "bpf: support bpf_fastcall patterns for kfuncs\n\nRecognize bpf_fastcall patterns around kfunc calls.\nFor example, suppose bpf_cast_to_kern_ctx() follows bpf_fastcall\ncontract (which it does), in such a case allow verifier to rewrite BPF\nprogram below:\n\n  r2 = 1;\n  *(u64 *)(r10 - 32) = r2;\n  call %[bpf_cast_to_kern_ctx];\n  r2 = *(u64 *)(r10 - 32);\n  r0 = r2;\n\nBy removing the spill/fill pair:\n\n  r2 = 1;\n  call %[bpf_cast_to_kern_ctx];\n  r0 = r2;\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240822084112.3257995-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ae010757a55b57c8b82628e8ea9b7da2269131d9",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-08-22 08:35:20 -0700",
      "message": "bpf: rename nocsr -> bpf_fastcall in verifier\n\nAttribute used by LLVM implementation of the feature had been changed\nfrom no_caller_saved_registers to bpf_fastcall (see [1]).\nThis commit replaces references to nocsr by references to bpf_fastcall\nto keep LLVM and Kernel parts in sync.\n\n[1] https://github.com/llvm/llvm-project/pull/105417\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240822084112.3257995-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "baebe9aaba1e59e34cd1fe6455bb4c3029ad3bc1",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-08-21 10:37:52 -0700",
      "message": "bpf: allow passing struct bpf_iter_<type> as kfunc arguments\n\nThere are potentially useful cases where a specific iterator type might\nneed to be passed into some kfunc. So, in addition to existing\nbpf_iter_<type>_{new,next,destroy}() kfuncs, allow to pass iterator\npointer to any kfunc.\n\nWe employ \"__iter\" naming suffix for arguments that are meant to accept\niterators. We also enforce that they accept PTR -> STRUCT btf_iter_<type>\ntype chain and point to a valid initialized on-the-stack iterator state.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240808232230.2848712-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "55f325958ccc41eaea43eb4546d4dc77c1b5ef8a",
      "author": "Al Viro <viro@zeniv.linux.org.uk>",
      "date": "2024-08-13 15:58:17 -0700",
      "message": "bpf: switch maps to CLASS(fd, ...)\n\n        Calling conventions for __bpf_map_get() would be more convenient\nif it left fpdut() on failure to callers.  Makes for simpler logics\nin the callers.\n\n\tAmong other things, the proof of memory safety no longer has to\nrely upon file->private_data never being ERR_PTR(...) for bpffs files.\nOriginal calling conventions made it impossible for the caller to tell\nwhether __bpf_map_get() has returned ERR_PTR(-EINVAL) because it has found\nthe file not be a bpf map one (in which case it would've done fdput())\nor because it found that ERR_PTR(-EINVAL) in file->private_data of a\nbpf map file (in which case fdput() would _not_ have been done).\n\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>\nReviewed-by: Christian Brauner <brauner@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/map_in_map.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c",
        "net/core/sock_map.c"
      ]
    },
    {
      "hash": "535ead44ffd08479212e31729a7118bd4e9ac699",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-08-13 15:58:14 -0700",
      "message": "bpf: factor out fetching bpf_map from FD and adding it to used_maps list\n\nFactor out the logic to extract bpf_map instances from FD embedded in\nbpf_insns, adding it to the list of used_maps (unless it's already\nthere, in which case we just reuse map's index). This simplifies the\nlogic in resolve_pseudo_ldimm64(), especially around `struct fd`\nhandling, as all that is now neatly contained in the helper and doesn't\nleak into a dozen error handling paths.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "bed2eb964c70b780fb55925892a74f26cb590b25",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-08-12 18:09:48 -0700",
      "message": "bpf: Fix a kernel verifier crash in stacksafe()\n\nDaniel Hodges reported a kernel verifier crash when playing with sched-ext.\nFurther investigation shows that the crash is due to invalid memory access\nin stacksafe(). More specifically, it is the following code:\n\n    if (exact != NOT_EXACT &&\n        old->stack[spi].slot_type[i % BPF_REG_SIZE] !=\n        cur->stack[spi].slot_type[i % BPF_REG_SIZE])\n            return false;\n\nThe 'i' iterates old->allocated_stack.\nIf cur->allocated_stack < old->allocated_stack the out-of-bound\naccess will happen.\n\nTo fix the issue add 'i >= cur->allocated_stack' check such that if\nthe condition is true, stacksafe() should fail. Otherwise,\ncur->stack[spi].slot_type[i % BPF_REG_SIZE] memory access is legal.\n\nFixes: 2793a8b015f7 (\"bpf: exact states comparison for iterator convergence checks\")\nCc: Eduard Zingerman <eddyz87@gmail.com>\nReported-by: Daniel Hodges <hodgesd@meta.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240812214847.213612-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "91b7fbf3936f5c27d1673264dc24a713290e2165",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-07-29 15:05:05 -0700",
      "message": "bpf, x86, riscv, arm: no_caller_saved_registers for bpf_get_smp_processor_id()\n\nThe function bpf_get_smp_processor_id() is processed in a different\nway, depending on the arch:\n- on x86 verifier replaces call to bpf_get_smp_processor_id() with a\n  sequence of instructions that modify only r0;\n- on riscv64 jit replaces call to bpf_get_smp_processor_id() with a\n  sequence of instructions that modify only r0;\n- on arm64 jit replaces call to bpf_get_smp_processor_id() with a\n  sequence of instructions that modify only r0 and tmp registers.\n\nThese rewrites satisfy attribute no_caller_saved_registers contract.\nAllow rewrite of no_caller_saved_registers patterns for\nbpf_get_smp_processor_id() in order to use this function as a canary\nfor no_caller_saved_registers tests.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5b5f51bff1b66cedb62b5ba74a1878341204e057",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-07-29 15:05:05 -0700",
      "message": "bpf: no_caller_saved_registers attribute for helper calls\n\nGCC and LLVM define a no_caller_saved_registers function attribute.\nThis attribute means that function scratches only some of\nthe caller saved registers defined by ABI.\nFor BPF the set of such registers could be defined as follows:\n- R0 is scratched only if function is non-void;\n- R1-R5 are scratched only if corresponding parameter type is defined\n  in the function prototype.\n\nThis commit introduces flag bpf_func_prot->allow_nocsr.\nIf this flag is set for some helper function, verifier assumes that\nit follows no_caller_saved_registers calling convention.\n\nThe contract between kernel and clang allows to simultaneously use\nsuch functions and maintain backwards compatibility with old\nkernels that don't understand no_caller_saved_registers calls\n(nocsr for short):\n\n- clang generates a simple pattern for nocsr calls, e.g.:\n\n    r1 = 1;\n    r2 = 2;\n    *(u64 *)(r10 - 8)  = r1;\n    *(u64 *)(r10 - 16) = r2;\n    call %[to_be_inlined]\n    r2 = *(u64 *)(r10 - 16);\n    r1 = *(u64 *)(r10 - 8);\n    r0 = r1;\n    r0 += r2;\n    exit;\n\n- kernel removes unnecessary spills and fills, if called function is\n  inlined by verifier or current JIT (with assumption that patch\n  inserted by verifier or JIT honors nocsr contract, e.g. does not\n  scratch r3-r5 for the example above), e.g. the code above would be\n  transformed to:\n\n    r1 = 1;\n    r2 = 2;\n    call %[to_be_inlined]\n    r0 = r1;\n    r0 += r2;\n    exit;\n\nTechnically, the transformation is split into the following phases:\n- function mark_nocsr_patterns(), called from bpf_check()\n  searches and marks potential patterns in instruction auxiliary data;\n- upon stack read or write access,\n  function check_nocsr_stack_contract() is used to verify if\n  stack offsets, presumably reserved for nocsr patterns, are used\n  only from those patterns;\n- function remove_nocsr_spills_fills(), called from bpf_check(),\n  applies the rewrite for valid patterns.\n\nSee comment in mark_nocsr_pattern_for_call() for more details.\n\nSuggested-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-3-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "45cbc7a5e004cf08528ef83633c62120cca3a5ee",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-07-29 15:05:05 -0700",
      "message": "bpf: add a get_helper_proto() utility function\n\nExtract the part of check_helper_call() as a utility function allowing\nto query 'struct bpf_func_proto' for a specific helper function id.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240722233844.1406874-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "9f5469b84577500375b311a98ffa8f4a68b5c77a",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-07-29 15:05:05 -0700",
      "message": "bpf: Get better reg range with ldsx and 32bit compare\n\nWith latest llvm19, the selftest iters/iter_arr_with_actual_elem_count\nfailed with -mcpu=v4.\n\nThe following are the details:\n  0: R1=ctx() R10=fp0\n  ; int iter_arr_with_actual_elem_count(const void *ctx) @ iters.c:1420\n  0: (b4) w7 = 0                        ; R7_w=0\n  ; int i, n = loop_data.n, sum = 0; @ iters.c:1422\n  1: (18) r1 = 0xffffc90000191478       ; R1_w=map_value(map=iters.bss,ks=4,vs=1280,off=1144)\n  3: (61) r6 = *(u32 *)(r1 +128)        ; R1_w=map_value(map=iters.bss,ks=4,vs=1280,off=1144) R6_w=scalar(smin=0,smax=umax=0xffffffff,var_off=(0x0; 0xffffffff))\n  ; if (n > ARRAY_SIZE(loop_data.data)) @ iters.c:1424\n  4: (26) if w6 > 0x20 goto pc+27       ; R6_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=32,var_off=(0x0; 0x3f))\n  5: (bf) r8 = r10                      ; R8_w=fp0 R10=fp0\n  6: (07) r8 += -8                      ; R8_w=fp-8\n  ; bpf_for(i, 0, n) { @ iters.c:1427\n  7: (bf) r1 = r8                       ; R1_w=fp-8 R8_w=fp-8\n  8: (b4) w2 = 0                        ; R2_w=0\n  9: (bc) w3 = w6                       ; R3_w=scalar(id=1,smin=smin32=0,smax=umax=smax32=umax32=32,var_off=(0x0; 0x3f)) R6_w=scalar(id=1,smin=smin32=0,smax=umax=smax32=umax32=32,var_off=(0x0; 0x3f))\n  10: (85) call bpf_iter_num_new#45179          ; R0=scalar() fp-8=iter_num(ref_id=2,state=active,depth=0) refs=2\n  11: (bf) r1 = r8                      ; R1=fp-8 R8=fp-8 refs=2\n  12: (85) call bpf_iter_num_next#45181 13: R0=rdonly_mem(id=3,ref_obj_id=2,sz=4) R6=scalar(id=1,smin=smin32=0,smax=umax=smax32=umax32=32,var_off=(0x0; 0x3f)) R7=0 R8=fp-8 R10=fp0 fp-8=iter_num(ref_id=2,state=active,depth=1) refs=2\n  ; bpf_for(i, 0, n) { @ iters.c:1427\n  13: (15) if r0 == 0x0 goto pc+2       ; R0=rdonly_mem(id=3,ref_obj_id=2,sz=4) refs=2\n  14: (81) r1 = *(s32 *)(r0 +0)         ; R0=rdonly_mem(id=3,ref_obj_id=2,sz=4) R1_w=scalar(smin=0xffffffff80000000,smax=0x7fffffff) refs=2\n  15: (ae) if w1 < w6 goto pc+4 20: R0=rdonly_mem(id=3,ref_obj_id=2,sz=4) R1=scalar(smin=0xffffffff80000000,smax=smax32=umax32=31,umax=0xffffffff0000001f,smin32=0,var_off=(0x0; 0xffffffff0000001f)) R6=scalar(id=1,smin=umin=smin32=umin32=1,smax=umax=smax32=umax32=32,var_off=(0x0; 0x3f)) R7=0 R8=fp-8 R10=fp0 fp-8=iter_num(ref_id=2,state=active,depth=1) refs=2\n  ; sum += loop_data.data[i]; @ iters.c:1429\n  20: (67) r1 <<= 2                     ; R1_w=scalar(smax=0x7ffffffc0000007c,umax=0xfffffffc0000007c,smin32=0,smax32=umax32=124,var_off=(0x0; 0xfffffffc0000007c)) refs=2\n  21: (18) r2 = 0xffffc90000191478      ; R2_w=map_value(map=iters.bss,ks=4,vs=1280,off=1144) refs=2\n  23: (0f) r2 += r1\n  math between map_value pointer and register with unbounded min value is not allowed\n\nThe source code:\n  int iter_arr_with_actual_elem_count(const void *ctx)\n  {\n        int i, n = loop_data.n, sum = 0;\n\n        if (n > ARRAY_SIZE(loop_data.data))\n                return 0;\n\n        bpf_for(i, 0, n) {\n                /* no rechecking of i against ARRAY_SIZE(loop_data.n) */\n                sum += loop_data.data[i];\n        }\n\n        return sum;\n  }\n\nThe insn #14 is a sign-extenstion load which is related to 'int i'.\nThe insn #15 did a subreg comparision. Note that smin=0xffffffff80000000 and this caused later\ninsn #23 failed verification due to unbounded min value.\n\nActually insn #15 R1 smin range can be better. Before insn #15, we have\n  R1_w=scalar(smin=0xffffffff80000000,smax=0x7fffffff)\nWith the above range, we know for R1, upper 32bit can only be 0xffffffff or 0.\nOtherwise, the value range for R1 could be beyond [smin=0xffffffff80000000,smax=0x7fffffff].\n\nAfter insn #15, for the true patch, we know smin32=0 and smax32=32. With the upper 32bit 0xffffffff,\nthen the corresponding value is [0xffffffff00000000, 0xffffffff00000020]. The range is\nobviously beyond the original range [smin=0xffffffff80000000,smax=0x7fffffff] and the\nrange is not possible. So the upper 32bit must be 0, which implies smin = smin32 and\nsmax = smax32.\n\nThis patch fixed the issue by adding additional register deduction after 32-bit compare\ninsn. If the signed 32-bit register range is non-negative then 64-bit smin is\nin range of [S32_MIN, S32_MAX], then the actual 64-bit smin/smax should be the same\nas 32-bit smin32/smax32.\n\nWith this patch, iters/iter_arr_with_actual_elem_count succeeded with better register range:\n\nfrom 15 to 20: R0=rdonly_mem(id=7,ref_obj_id=2,sz=4) R1_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=31,var_off=(0x0; 0x1f)) R6=scalar(id=1,smin=umin=smin32=umin32=1,smax=umax=smax32=umax32=32,var_off=(0x0; 0x3f)) R7=scalar(id=9,smin=0,smax=umax=0xffffffff,var_off=(0x0; 0xffffffff)) R8=scalar(id=9,smin=0,smax=umax=0xffffffff,var_off=(0x0; 0xffffffff)) R10=fp0 fp-8=iter_num(ref_id=2,state=active,depth=3) refs=2\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240723162933.2731620-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "92de36080c93296ef9005690705cba260b9bd68a",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-07-29 15:05:05 -0700",
      "message": "bpf: Fail verification for sign-extension of packet data/data_end/data_meta\n\nsyzbot reported a kernel crash due to\n  commit 1f1e864b6555 (\"bpf: Handle sign-extenstin ctx member accesses\").\nThe reason is due to sign-extension of 32-bit load for\npacket data/data_end/data_meta uapi field.\n\nThe original code looks like:\n        r2 = *(s32 *)(r1 + 76) /* load __sk_buff->data */\n        r3 = *(u32 *)(r1 + 80) /* load __sk_buff->data_end */\n        r0 = r2\n        r0 += 8\n        if r3 > r0 goto +1\n        ...\nNote that __sk_buff->data load has 32-bit sign extension.\n\nAfter verification and convert_ctx_accesses(), the final asm code looks like:\n        r2 = *(u64 *)(r1 +208)\n        r2 = (s32)r2\n        r3 = *(u64 *)(r1 +80)\n        r0 = r2\n        r0 += 8\n        if r3 > r0 goto pc+1\n        ...\nNote that 'r2 = (s32)r2' may make the kernel __sk_buff->data address invalid\nwhich may cause runtime failure.\n\nCurrently, in C code, typically we have\n        void *data = (void *)(long)skb->data;\n        void *data_end = (void *)(long)skb->data_end;\n        ...\nand it will generate\n        r2 = *(u64 *)(r1 +208)\n        r3 = *(u64 *)(r1 +80)\n        r0 = r2\n        r0 += 8\n        if r3 > r0 goto pc+1\n\nIf we allow sign-extension,\n        void *data = (void *)(long)(int)skb->data;\n        void *data_end = (void *)(long)skb->data_end;\n        ...\nthe generated code looks like\n        r2 = *(u64 *)(r1 +208)\n        r2 <<= 32\n        r2 s>>= 32\n        r3 = *(u64 *)(r1 +80)\n        r0 = r2\n        r0 += 8\n        if r3 > r0 goto pc+1\nand this will cause verification failure since \"r2 <<= 32\" is not allowed\nas \"r2\" is a packet pointer.\n\nTo fix this issue for case\n  r2 = *(s32 *)(r1 + 76) /* load __sk_buff->data */\nthis patch added additional checking in is_valid_access() callback\nfunction for packet data/data_end/data_meta access. If those accesses\nare with sign-extenstion, the verification will fail.\n\n  [1] https://lore.kernel.org/bpf/000000000000c90eee061d236d37@google.com/\n\nReported-by: syzbot+ad9ec60c8eaf69e6f99c@syzkaller.appspotmail.com\nFixes: 1f1e864b6555 (\"bpf: Handle sign-extenstin ctx member accesses\")\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240723153439.2429035-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/verifier.c",
        "net/core/filter.c"
      ]
    },
    {
      "hash": "763aa759d3b2c4f95b11855e3d37b860860107e2",
      "author": "Xu Kuohai <xukuohai@huawei.com>",
      "date": "2024-07-29 13:09:29 -0700",
      "message": "bpf: Fix compare error in function retval_range_within\n\nAfter checking lsm hook return range in verifier, the test case\n\"test_progs -t test_lsm\" failed, and the failure log says:\n\nlibbpf: prog 'test_int_hook': BPF program load failed: Invalid argument\nlibbpf: prog 'test_int_hook': -- BEGIN PROG LOAD LOG --\n0: R1=ctx() R10=fp0\n; int BPF_PROG(test_int_hook, struct vm_area_struct *vma, @ lsm.c:89\n0: (79) r0 = *(u64 *)(r1 +24)         ; R0_w=scalar(smin=smin32=-4095,smax=smax32=0) R1=ctx()\n\n[...]\n\n24: (b4) w0 = -1                      ; R0_w=0xffffffff\n; int BPF_PROG(test_int_hook, struct vm_area_struct *vma, @ lsm.c:89\n25: (95) exit\nAt program exit the register R0 has smin=4294967295 smax=4294967295 should have been in [-4095, 0]\n\nIt can be seen that instruction \"w0 = -1\" zero extended -1 to 64-bit\nregister r0, setting both smin and smax values of r0 to 4294967295.\nThis resulted in a false reject when r0 was checked with range [-4095, 0].\n\nGiven bpf lsm does not return 64-bit values, this patch fixes it by changing\nthe compare between r0 and return range from 64-bit operation to 32-bit\noperation for bpf lsm.\n\nFixes: 8fa4ecd49b81 (\"bpf: enforce exact retval range on subprog/callback exit\")\nSigned-off-by: Xu Kuohai <xukuohai@huawei.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20240719110059.797546-5-xukuohai@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5d99e198be279045e6ecefe220f5c52f8ce9bfd5",
      "author": "Xu Kuohai <xukuohai@huawei.com>",
      "date": "2024-07-29 13:09:22 -0700",
      "message": "bpf, lsm: Add check for BPF LSM return value\n\nA bpf prog returning a positive number attached to file_alloc_security\nhook makes kernel panic.\n\nThis happens because file system can not filter out the positive number\nreturned by the LSM prog using IS_ERR, and misinterprets this positive\nnumber as a file pointer.\n\nGiven that hook file_alloc_security never returned positive number\nbefore the introduction of BPF LSM, and other BPF LSM hooks may\nencounter similar issues, this patch adds LSM return value check\nin verifier, to ensure no unexpected value is returned.\n\nFixes: 520b7aa00d8c (\"bpf: lsm: Initialize the BPF LSM hooks\")\nReported-by: Xin Liu <liuxin350@huawei.com>\nSigned-off-by: Xu Kuohai <xukuohai@huawei.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240719110059.797546-3-xukuohai@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_lsm.h",
        "kernel/bpf/bpf_lsm.c",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "e42ac14180554fa23a3312d4f921dc4ea7972fb7",
      "author": "Martin KaFai Lau <martin.lau@kernel.org>",
      "date": "2024-07-29 12:54:13 -0700",
      "message": "bpf: Check unsupported ops from the bpf_struct_ops's cfi_stubs\n\nThe bpf_tcp_ca struct_ops currently uses a \"u32 unsupported_ops[]\"\narray to track which ops is not supported.\n\nAfter cfi_stubs had been added, the function pointer in cfi_stubs is\nalso NULL for the unsupported ops. Thus, the \"u32 unsupported_ops[]\"\nbecomes redundant. This observation was originally brought up in the\nbpf/cfi discussion:\nhttps://lore.kernel.org/bpf/CAADnVQJoEkdjyCEJRPASjBw1QGsKYrF33QdMGc1RZa9b88bAEA@mail.gmail.com/\n\nThe recent bpf qdisc patch (https://lore.kernel.org/bpf/20240714175130.4051012-6-amery.hung@bytedance.com/)\nalso needs to specify quite many unsupported ops. It is a good time\nto clean it up.\n\nThis patch removes the need of \"u32 unsupported_ops[]\" and tests for null-ness\nin the cfi_stubs instead.\n\nTesting the cfi_stubs is done in a new function bpf_struct_ops_supported().\nThe verifier will call bpf_struct_ops_supported() when loading the\nstruct_ops program. The \".check_member\" is removed from the bpf_tcp_ca\nin this patch. \".check_member\" could still be useful for other subsytems\nto enforce other restrictions (e.g. sched_ext checks for prog->sleepable).\n\nTo keep the same error return, ENOTSUPP is used.\n\nCc: Amery Hung <ameryhung@gmail.com>\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>\nLink: https://lore.kernel.org/r/20240722183049.2254692-2-martin.lau@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/bpf_struct_ops.c",
        "kernel/bpf/verifier.c",
        "net/ipv4/bpf_tcp_ca.c"
      ]
    },
    {
      "hash": "842edb5507a1038e009d27e69d13b94b6f085763",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-07-29 12:53:14 -0700",
      "message": "bpf: Remove mark_precise_scalar_ids()\n\nFunction mark_precise_scalar_ids() is superseded by\nbt_sync_linked_regs() and equal scalars tracking in jump history.\nmark_precise_scalar_ids() propagates precision over registers sharing\nsame ID on parent/child state boundaries, while jump history records\nallow bt_sync_linked_regs() to propagate same information with\ninstruction level granularity, which is strictly more precise.\n\nThis commit removes mark_precise_scalar_ids() and updates test cases\nin progs/verifier_scalar_ids to reflect new verifier behavior.\n\nThe tests are updated in the following manner:\n- mark_precise_scalar_ids() propagated precision regardless of\n  presence of conditional jumps, while new jump history based logic\n  only kicks in when conditional jumps are present.\n  Hence test cases are augmented with conditional jumps to still\n  trigger precision propagation.\n- As equal scalars tracking no longer relies on parent/child state\n  boundaries some test cases are no longer interesting,\n  such test cases are removed, namely:\n  - precision_same_state and precision_cross_state are superseded by\n    linked_regs_bpf_k;\n  - precision_same_state_broken_link and equal_scalars_broken_link\n    are superseded by linked_regs_broken_link.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240718202357.1746514-3-eddyz87@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_scalar_ids.c",
        "tools/testing/selftests/bpf/verifier/precise.c"
      ]
    },
    {
      "hash": "4bf79f9be434e000c8e12fe83b2f4402480f1460",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-07-29 12:53:10 -0700",
      "message": "bpf: Track equal scalars history on per-instruction level\n\nUse bpf_verifier_state->jmp_history to track which registers were\nupdated by find_equal_scalars() (renamed to collect_linked_regs())\nwhen conditional jump was verified. Use recorded information in\nbacktrack_insn() to propagate precision.\n\nE.g. for the following program:\n\n            while verifying instructions\n  1: r1 = r0              |\n  2: if r1 < 8  goto ...  | push r0,r1 as linked registers in jmp_history\n  3: if r0 > 16 goto ...  | push r0,r1 as linked registers in jmp_history\n  4: r2 = r10             |\n  5: r2 += r0             v mark_chain_precision(r0)\n\n            while doing mark_chain_precision(r0)\n  5: r2 += r0             | mark r0 precise\n  4: r2 = r10             |\n  3: if r0 > 16 goto ...  | mark r0,r1 as precise\n  2: if r1 < 8  goto ...  | mark r0,r1 as precise\n  1: r1 = r0              v\n\nTechnically, do this as follows:\n- Use 10 bits to identify each register that gains range because of\n  sync_linked_regs():\n  - 3 bits for frame number;\n  - 6 bits for register or stack slot number;\n  - 1 bit to indicate if register is spilled.\n- Use u64 as a vector of 6 such records + 4 bits for vector length.\n- Augment struct bpf_jmp_history_entry with a field 'linked_regs'\n  representing such vector.\n- When doing check_cond_jmp_op() remember up to 6 registers that\n  gain range because of sync_linked_regs() in such a vector.\n- Don't propagate range information and reset IDs for registers that\n  don't fit in 6-value vector.\n- Push a pair {instruction index, linked registers vector}\n  to bpf_verifier_state->jmp_history.\n- When doing backtrack_insn() check if any of recorded linked\n  registers is currently marked precise, if so mark all linked\n  registers as precise.\n\nThis also requires fixes for two test_verifier tests:\n- precise: test 1\n- precise: test 2\n\nBoth tests contain the following instruction sequence:\n\n19: (bf) r2 = r9                      ; R2=scalar(id=3) R9=scalar(id=3)\n20: (a5) if r2 < 0x8 goto pc+1        ; R2=scalar(id=3,umin=8)\n21: (95) exit\n22: (07) r2 += 1                      ; R2_w=scalar(id=3+1,...)\n23: (bf) r1 = r10                     ; R1_w=fp0 R10=fp0\n24: (07) r1 += -8                     ; R1_w=fp-8\n25: (b7) r3 = 0                       ; R3_w=0\n26: (85) call bpf_probe_read_kernel#113\n\nThe call to bpf_probe_read_kernel() at (26) forces r2 to be precise.\nPreviously, this forced all registers with same id to become precise\nimmediately when mark_chain_precision() is called.\nAfter this change, the precision is propagated to registers sharing\nsame id only when 'if' instruction is backtracked.\nHence verification log for both tests is changed:\nregs=r2,r9 -> regs=r2 for instructions 25..20.\n\nFixes: 904e6ddf4133 (\"bpf: Use scalar ids in mark_chain_precision()\")\nReported-by: Hao Sun <sunhao.th@gmail.com>\nSuggested-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240718202357.1746514-2-eddyz87@gmail.com\n\nCloses: https://lore.kernel.org/bpf/CAEf4BzZ0xidVCqB47XnkXcNhkPWF6_nTV7yt+_Lf0kcFEut2Mg@mail.gmail.com/",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c",
        "tools/testing/selftests/bpf/verifier/precise.c"
      ]
    },
    {
      "hash": "53dabce2652fb854eae84609ce9c37429d5d87ba",
      "author": "Vlastimil Babka <vbabka@suse.cz>",
      "date": "2024-07-17 21:05:18 -0700",
      "message": "mm, page_alloc: put should_fail_alloc_page() back behing CONFIG_FAIL_PAGE_ALLOC\n\nThis mostly reverts commit af3b854492f3 (\"mm/page_alloc.c: allow error\ninjection\").  The commit made should_fail_alloc_page() a noinline function\nthat's always called from the page allocation hotpath, even if it's empty\nbecause CONFIG_FAIL_PAGE_ALLOC is not enabled, and there is no option to\ndisable it and prevent the associated function call overhead.\n\nAs with the preceding patch \"mm, slab: put should_failslab back behind\nCONFIG_SHOULD_FAILSLAB\" and for the same reasons, put the\nshould_fail_alloc_page() back behind the config option.  When enabled, the\nALLOW_ERROR_INJECTION and BTF_ID records are preserved so it's not a\ncomplete revert.\n\nLink: https://lkml.kernel.org/r/20240711-b4-fault-injection-reverts-v1-2-9e2651945d68@suse.cz\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>\nCc: Akinobu Mita <akinobu.mita@gmail.com>\nCc: Alexei Starovoitov <ast@kernel.org>\nCc: Andrii Nakryiko <andrii@kernel.org>\nCc: Christoph Lameter <cl@linux.com>\nCc: Daniel Borkmann <daniel@iogearbox.net>\nCc: David Rientjes <rientjes@google.com>\nCc: Eduard Zingerman <eddyz87@gmail.com>\nCc: Hao Luo <haoluo@google.com>\nCc: Hyeonggon Yoo <42.hyeyoo@gmail.com>\nCc: Jiri Olsa <jolsa@kernel.org>\nCc: John Fastabend <john.fastabend@gmail.com>\nCc: KP Singh <kpsingh@kernel.org>\nCc: Martin KaFai Lau <martin.lau@linux.dev>\nCc: Mateusz Guzik <mjguzik@gmail.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Song Liu <song@kernel.org>\nCc: Stanislav Fomichev <sdf@fomichev.me>\nCc: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
      "modified_files": [
        "include/linux/fault-inject.h",
        "kernel/bpf/verifier.c",
        "mm/fail_page_alloc.c",
        "mm/page_alloc.c"
      ]
    },
    {
      "hash": "a7526fe8b94eced7d82aa00b2bcca44e39ae0769",
      "author": "Vlastimil Babka <vbabka@suse.cz>",
      "date": "2024-07-17 21:05:18 -0700",
      "message": "mm, slab: put should_failslab() back behind CONFIG_SHOULD_FAILSLAB\n\nPatch series \"revert unconditional slab and page allocator fault injection\ncalls\".\n\nThese two patches largely revert commits that added function call overhead\ninto slab and page allocation hotpaths and that cannot be currently\ndisabled even though related CONFIG_ options do exist.\n\nA much more involved solution that can keep the callsites always existing\nbut hidden behind a static key if unused, is possible [1] and can be\npursued by anyone who believes it's necessary.  Meanwhile the fact the\nshould_failslab() error injection is already not functional on kernels\nbuilt with current gcc without anyone noticing [2], and lukewarm response\nto [1] suggests the need is not there.  I believe it will be more fair to\nhave the state after this series as a baseline for possible further\noptimisation, instead of the unconditional overhead.\n\nFor example a possible compromise for anyone who's fine with an empty\nfunction call overhead but not the full CONFIG_FAILSLAB /\nCONFIG_FAIL_PAGE_ALLOC overhead is to reuse patch 1 from [1] but insert a\nstatic key check only inside should_failslab() and\nshould_fail_alloc_page() before performing the more expensive checks.\n\n[1] https://lore.kernel.org/all/20240620-fault-injection-statickeys-v2-0-e23947d3d84b@suse.cz/#t\n[2] https://github.com/bpftrace/bpftrace/issues/3258\n\n\nThis patch (of 2):\n\nThis mostly reverts commit 4f6923fbb352 (\"mm: make should_failslab always\navailable for fault injection\").  The commit made should_failslab() a\nnoinline function that's always called from the slab allocation hotpath,\neven if it's empty because CONFIG_SHOULD_FAILSLAB is not enabled, and\nthere is no option to disable that call.  This is visible in profiles and\nthe function call overhead can be noticeable especially with cpu\nmitigations.\n\nMeanwhile the bpftrace program example in the commit silently does not\nwork without CONFIG_SHOULD_FAILSLAB anyway with a recent gcc, because the\nempty function gets a .constprop clone that is actually being called\n(uselessly) from the slab hotpath, while the error injection is hooked to\nthe original function that's not being called at all [1].\n\nThus put the whole should_failslab() function back behind\nCONFIG_SHOULD_FAILSLAB.  It's not a complete revert of 4f6923fbb352 - the\nint return type that returns -ENOMEM on failure is preserved, as well\nALLOW_ERROR_INJECTION annotation.  The BTF_ID() record that was meanwhile\nadded is also guarded by CONFIG_SHOULD_FAILSLAB.\n\n[1] https://github.com/bpftrace/bpftrace/issues/3258\n\nLink: https://lkml.kernel.org/r/20240711-b4-fault-injection-reverts-v1-0-9e2651945d68@suse.cz\nLink: https://lkml.kernel.org/r/20240711-b4-fault-injection-reverts-v1-1-9e2651945d68@suse.cz\nSigned-off-by: Vlastimil Babka <vbabka@suse.cz>\nCc: Akinobu Mita <akinobu.mita@gmail.com>\nCc: Alexei Starovoitov <ast@kernel.org>\nCc: Andrii Nakryiko <andrii@kernel.org>\nCc: Christoph Lameter <cl@linux.com>\nCc: Daniel Borkmann <daniel@iogearbox.net>\nCc: David Rientjes <rientjes@google.com>\nCc: Eduard Zingerman <eddyz87@gmail.com>\nCc: Hao Luo <haoluo@google.com>\nCc: Hyeonggon Yoo <42.hyeyoo@gmail.com>\nCc: Jiri Olsa <jolsa@kernel.org>\nCc: John Fastabend <john.fastabend@gmail.com>\nCc: KP Singh <kpsingh@kernel.org>\nCc: Martin KaFai Lau <martin.lau@linux.dev>\nCc: Mateusz Guzik <mjguzik@gmail.com>\nCc: Roman Gushchin <roman.gushchin@linux.dev>\nCc: Song Liu <song@kernel.org>\nCc: Stanislav Fomichev <sdf@fomichev.me>\nCc: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
      "modified_files": [
        "include/linux/fault-inject.h",
        "kernel/bpf/verifier.c",
        "mm/failslab.c",
        "mm/slub.c"
      ]
    },
    {
      "hash": "deac5871eb0751454cb80b3ff6b69e42a6c1bab2",
      "author": "Shung-Hsi Yu <shung-hsi.yu@suse.com>",
      "date": "2024-07-12 08:54:08 -0700",
      "message": "bpf: use check_sub_overflow() to check for subtraction overflows\n\nSimilar to previous patch that drops signed_add*_overflows() and uses\n(compiler) builtin-based check_add_overflow(), do the same for\nsigned_sub*_overflows() and replace them with the generic\ncheck_sub_overflow() to make future refactoring easier and have the\nchecks implemented more efficiently.\n\nUnsigned overflow check for subtraction does not use helpers and are\nsimple enough already, so they're left untouched.\n\nAfter the change GCC 13.3.0 generates cleaner assembly on x86_64:\n\n\tif (check_sub_overflow(*dst_smin, src_reg->smax_value, dst_smin) ||\n   139bf:\tmov    0x28(%r12),%rax\n   139c4:\tmov    %edx,0x54(%r12)\n   139c9:\tsub    %r11,%rax\n   139cc:\tmov    %rax,0x28(%r12)\n   139d1:\tjo     14627 <adjust_reg_min_max_vals+0x1237>\n\t    check_sub_overflow(*dst_smax, src_reg->smin_value, dst_smax)) {\n   139d7:\tmov    0x30(%r12),%rax\n   139dc:\tsub    %r9,%rax\n   139df:\tmov    %rax,0x30(%r12)\n\tif (check_sub_overflow(*dst_smin, src_reg->smax_value, dst_smin) ||\n   139e4:\tjo     14627 <adjust_reg_min_max_vals+0x1237>\n   ...\n\t\t*dst_smin = S64_MIN;\n   14627:\tmovabs $0x8000000000000000,%rax\n   14631:\tmov    %rax,0x28(%r12)\n\t\t*dst_smax = S64_MAX;\n   14636:\tsub    $0x1,%rax\n   1463a:\tmov    %rax,0x30(%r12)\n\nBefore the change it gives:\n\n\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n   13a50:\tmov    0x28(%r12),%rdi\n   13a55:\tmov    %edx,0x54(%r12)\n\t\tdst_reg->smax_value = S64_MAX;\n   13a5a:\tmovabs $0x7fffffffffffffff,%rdx\n   13a64:\tmov    %eax,0x50(%r12)\n\t\tdst_reg->smin_value = S64_MIN;\n   13a69:\tmovabs $0x8000000000000000,%rax\n\ts64 res = (s64)((u64)a - (u64)b);\n   13a73:\tmov    %rdi,%rsi\n   13a76:\tsub    %rcx,%rsi\n\tif (b < 0)\n   13a79:\ttest   %rcx,%rcx\n   13a7c:\tjs     145ea <adjust_reg_min_max_vals+0x119a>\n\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n   13a82:\tcmp    %rsi,%rdi\n   13a85:\tjl     13ac7 <adjust_reg_min_max_vals+0x677>\n\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n   13a87:\tmov    0x30(%r12),%r8\n\ts64 res = (s64)((u64)a - (u64)b);\n   13a8c:\tmov    %r8,%rax\n   13a8f:\tsub    %r9,%rax\n\treturn res > a;\n   13a92:\tcmp    %rax,%r8\n   13a95:\tsetl   %sil\n\tif (b < 0)\n   13a99:\ttest   %r9,%r9\n   13a9c:\tjs     147d1 <adjust_reg_min_max_vals+0x1381>\n\t\tdst_reg->smax_value = S64_MAX;\n   13aa2:\tmovabs $0x7fffffffffffffff,%rdx\n\t\tdst_reg->smin_value = S64_MIN;\n   13aac:\tmovabs $0x8000000000000000,%rax\n\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n   13ab6:\ttest   %sil,%sil\n   13ab9:\tjne    13ac7 <adjust_reg_min_max_vals+0x677>\n\t\tdst_reg->smin_value -= smax_val;\n   13abb:\tmov    %rdi,%rax\n\t\tdst_reg->smax_value -= smin_val;\n   13abe:\tmov    %r8,%rdx\n\t\tdst_reg->smin_value -= smax_val;\n   13ac1:\tsub    %rcx,%rax\n\t\tdst_reg->smax_value -= smin_val;\n   13ac4:\tsub    %r9,%rdx\n   13ac7:\tmov    %rax,0x28(%r12)\n   ...\n   13ad1:\tmov    %rdx,0x30(%r12)\n   ...\n\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n   145ea:\tcmp    %rsi,%rdi\n   145ed:\tjg     13ac7 <adjust_reg_min_max_vals+0x677>\n   145f3:\tjmp    13a87 <adjust_reg_min_max_vals+0x637>\n\nSuggested-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nLink: https://lore.kernel.org/r/20240712080127.136608-4-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "28a4411076b254c67842348e3b25c2fb41a94cad",
      "author": "Shung-Hsi Yu <shung-hsi.yu@suse.com>",
      "date": "2024-07-12 08:54:08 -0700",
      "message": "bpf: use check_add_overflow() to check for addition overflows\n\nsigned_add*_overflows() was added back when there was no overflow-check\nhelper. With the introduction of such helpers in commit f0907827a8a91\n(\"compiler.h: enable builtin overflow checkers and add fallback code\"), we\ncan drop signed_add*_overflows() in kernel/bpf/verifier.c and use the\ngeneric check_add_overflow() instead.\n\nThis will make future refactoring easier, and takes advantage of\ncompiler-emitted hardware instructions that efficiently implement these\nchecks.\n\nAfter the change GCC 13.3.0 generates cleaner assembly on x86_64:\n\n\terr = adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n   13625:\tmov    0x28(%rbx),%r9  /*  r9 = src_reg->smin_value */\n   13629:\tmov    0x30(%rbx),%rcx /* rcx = src_reg->smax_value */\n   ...\n\tif (check_add_overflow(*dst_smin, src_reg->smin_value, dst_smin) ||\n   141c1:\tmov    %r9,%rax\n   141c4:\tadd    0x28(%r12),%rax\n   141c9:\tmov    %rax,0x28(%r12)\n   141ce:\tjo     146e4 <adjust_reg_min_max_vals+0x1294>\n\t    check_add_overflow(*dst_smax, src_reg->smax_value, dst_smax)) {\n   141d4:\tadd    0x30(%r12),%rcx\n   141d9:\tmov    %rcx,0x30(%r12)\n\tif (check_add_overflow(*dst_smin, src_reg->smin_value, dst_smin) ||\n   141de:\tjo     146e4 <adjust_reg_min_max_vals+0x1294>\n   ...\n\t\t*dst_smin = S64_MIN;\n   146e4:\tmovabs $0x8000000000000000,%rax\n   146ee:\tmov    %rax,0x28(%r12)\n\t\t*dst_smax = S64_MAX;\n   146f3:\tsub    $0x1,%rax\n   146f7:\tmov    %rax,0x30(%r12)\n\nBefore the change it gives:\n\n\ts64 smin_val = src_reg->smin_value;\n     675:\tmov    0x28(%rsi),%r8\n\ts64 smax_val = src_reg->smax_value;\n\tu64 umin_val = src_reg->umin_value;\n\tu64 umax_val = src_reg->umax_value;\n     679:\tmov    %rdi,%rax /* rax = dst_reg */\n\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n     67c:\tmov    0x28(%rdi),%rdi /* rdi = dst_reg->smin_value */\n\tu64 umin_val = src_reg->umin_value;\n     680:\tmov    0x38(%rsi),%rdx\n\tu64 umax_val = src_reg->umax_value;\n     684:\tmov    0x40(%rsi),%rcx\n\ts64 res = (s64)((u64)a + (u64)b);\n     688:\tlea    (%r8,%rdi,1),%r9 /* r9 = dst_reg->smin_value + src_reg->smin_value */\n\treturn res < a;\n     68c:\tcmp    %r9,%rdi\n     68f:\tsetg   %r10b /* r10b = (dst_reg->smin_value + src_reg->smin_value) > dst_reg->smin_value */\n\tif (b < 0)\n     693:\ttest   %r8,%r8\n     696:\tjs     72b <scalar_min_max_add+0xbb>\n\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n     69c:\tmovabs $0x7fffffffffffffff,%rdi\n\ts64 smax_val = src_reg->smax_value;\n     6a6:\tmov    0x30(%rsi),%r8\n\t\tdst_reg->smin_value = S64_MIN;\n     6aa:\t00 00 00 \tmovabs $0x8000000000000000,%rsi\n\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n     6b4:\ttest   %r10b,%r10b /* (dst_reg->smin_value + src_reg->smin_value) > dst_reg->smin_value ? goto 6cb */\n     6b7:\tjne    6cb <scalar_min_max_add+0x5b>\n\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n     6b9:\tmov    0x30(%rax),%r10   /* r10 = dst_reg->smax_value */\n\ts64 res = (s64)((u64)a + (u64)b);\n     6bd:\tlea    (%r10,%r8,1),%r11 /* r11 = dst_reg->smax_value + src_reg->smax_value */\n\tif (b < 0)\n     6c1:\ttest   %r8,%r8\n     6c4:\tjs     71e <scalar_min_max_add+0xae>\n\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n     6c6:\tcmp    %r11,%r10 /* (dst_reg->smax_value + src_reg->smax_value) <= dst_reg->smax_value ? goto 723 */\n     6c9:\tjle    723 <scalar_min_max_add+0xb3>\n\t} else {\n\t\tdst_reg->smin_value += smin_val;\n\t\tdst_reg->smax_value += smax_val;\n\t}\n     6cb:\tmov    %rsi,0x28(%rax)\n     ...\n     6d5:\tmov    %rdi,0x30(%rax)\n     ...\n\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n     71e:\tcmp    %r11,%r10\n     721:\tjl     6cb <scalar_min_max_add+0x5b>\n\t\tdst_reg->smin_value += smin_val;\n     723:\tmov    %r9,%rsi\n\t\tdst_reg->smax_value += smax_val;\n     726:\tmov    %r11,%rdi\n     729:\tjmp    6cb <scalar_min_max_add+0x5b>\n\t\treturn res > a;\n     72b:\tcmp    %r9,%rdi\n     72e:\tsetl   %r10b\n     732:\tjmp    69c <scalar_min_max_add+0x2c>\n     737:\tnopw   0x0(%rax,%rax,1)\n\nNote: unlike adjust_ptr_min_max_vals() and scalar*_min_max_add(), it is\nnecessary to introduce intermediate variable in adjust_jmp_off() to keep\nthe functional behavior unchanged. Without an intermediate variable\nimm/off will be altered even on overflow.\n\nSuggested-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20240712080127.136608-3-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "4a04b4f0de59dd5c621e78f15803ee0b0544eeb8",
      "author": "Shung-Hsi Yu <shung-hsi.yu@suse.com>",
      "date": "2024-07-12 08:54:07 -0700",
      "message": "bpf: fix overflow check in adjust_jmp_off()\n\nadjust_jmp_off() incorrectly used the insn->imm field for all overflow check,\nwhich is incorrect as that should only be done or the BPF_JMP32 | BPF_JA case,\nnot the general jump instruction case. Fix it by using insn->off for overflow\ncheck in the general case.\n\nFixes: 5337ac4c9b80 (\"bpf: Fix the corner case with may_goto and jump to the 1st insn.\")\nSigned-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20240712080127.136608-2-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "605c96997d89c01c11bbddb4db820ede570581c7",
      "author": "Matt Bobrowski <mattbobrowski@google.com>",
      "date": "2024-07-09 19:11:47 -0700",
      "message": "bpf: relax zero fixed offset constraint on KF_TRUSTED_ARGS/KF_RCU\n\nCurrently, BPF kfuncs which accept trusted pointer arguments\ni.e. those flagged as KF_TRUSTED_ARGS, KF_RCU, or KF_RELEASE, all\nrequire an original/unmodified trusted pointer argument to be supplied\nto them. By original/unmodified, it means that the backing register\nholding the trusted pointer argument that is to be supplied to the BPF\nkfunc must have its fixed offset set to zero, or else the BPF verifier\nwill outright reject the BPF program load. However, this zero fixed\noffset constraint that is currently enforced by the BPF verifier onto\nBPF kfuncs specifically flagged to accept KF_TRUSTED_ARGS or KF_RCU\ntrusted pointer arguments is rather unnecessary, and can limit their\nusability in practice. Specifically, it completely eliminates the\npossibility of constructing a derived trusted pointer from an original\ntrusted pointer. To put it simply, a derived pointer is a pointer\nwhich points to one of the nested member fields of the object being\npointed to by the original trusted pointer.\n\nThis patch relaxes the zero fixed offset constraint that is enforced\nupon BPF kfuncs which specifically accept KF_TRUSTED_ARGS, or KF_RCU\narguments. Although, the zero fixed offset constraint technically also\napplies to BPF kfuncs accepting KF_RELEASE arguments, relaxing this\nconstraint for such BPF kfuncs has subtle and unwanted\nside-effects. This was discovered by experimenting a little further\nwith an initial version of this patch series [0]. The primary issue\nwith relaxing the zero fixed offset constraint on BPF kfuncs accepting\nKF_RELEASE arguments is that it'd would open up the opportunity for\nBPF programs to supply both trusted pointers and derived trusted\npointers to them. For KF_RELEASE BPF kfuncs specifically, this could\nbe problematic as resources associated with the backing pointer could\nbe released by the backing BPF kfunc and cause instabilities for the\nrest of the kernel.\n\nWith this new fixed offset semantic in-place for BPF kfuncs accepting\nKF_TRUSTED_ARGS and KF_RCU arguments, we now have more flexibility\nwhen it comes to the BPF kfuncs that we're able to introduce moving\nforward.\n\nEarly discussions covering the possibility of relaxing the zero fixed\noffset constraint can be found using the link below. This will provide\nmore context on where all this has stemmed from [1].\n\nNotably, pre-existing tests have been updated such that they provide\ncoverage for the updated zero fixed offset\nfunctionality. Specifically, the nested offset test was converted from\na negative to positive test as it was already designed to assert zero\nfixed offset semantics of a KF_TRUSTED_ARGS BPF kfunc.\n\n[0] https://lore.kernel.org/bpf/ZnA9ndnXKtHOuYMe@google.com/\n[1] https://lore.kernel.org/bpf/ZhkbrM55MKQ0KeIV@google.com/\n\nSigned-off-by: Matt Bobrowski <mattbobrowski@google.com>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20240709210939.1544011-1-mattbobrowski@google.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/nested_trust_failure.c",
        "tools/testing/selftests/bpf/progs/nested_trust_success.c",
        "tools/testing/selftests/bpf/verifier/calls.c"
      ]
    },
    {
      "hash": "df34ec9db6f521118895f22795da49f2ec01f8cf",
      "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
      "date": "2024-07-02 18:31:35 +0200",
      "message": "bpf: Fix atomic probe zero-extension\n\nZero-extending results of atomic probe operations fails with:\n\n    verifier bug. zext_dst is set, but no reg is defined\n\nThe problem is that insn_def_regno() handles BPF_ATOMICs, but not\nBPF_PROBE_ATOMICs. Fix by adding the missing condition.\n\nFixes: d503a04f8bc0 (\"bpf: Add support for certain atomics in bpf_arena to x86 JIT\")\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20240701234304.14336-2-iii@linux.ibm.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ec2b9a5e11e51fea1bb04c1e7e471952e887e874",
      "author": "Matt Bobrowski <mattbobrowski@google.com>",
      "date": "2024-06-26 13:17:32 -0700",
      "message": "bpf: add missing check_func_arg_reg_off() to prevent out-of-bounds memory accesses\n\nCurrently, it's possible to pass in a modified CONST_PTR_TO_DYNPTR to\na global function as an argument. The adverse effects of this is that\nBPF helpers can continue to make use of this modified\nCONST_PTR_TO_DYNPTR from within the context of the global function,\nwhich can unintentionally result in out-of-bounds memory accesses and\ntherefore compromise overall system stability i.e.\n\n[  244.157771] BUG: KASAN: slab-out-of-bounds in bpf_dynptr_data+0x137/0x140\n[  244.161345] Read of size 8 at addr ffff88810914be68 by task test_progs/302\n[  244.167151] CPU: 0 PID: 302 Comm: test_progs Tainted: G O E 6.10.0-rc3-00131-g66b586715063 #533\n[  244.174318] Call Trace:\n[  244.175787]  <TASK>\n[  244.177356]  dump_stack_lvl+0x66/0xa0\n[  244.179531]  print_report+0xce/0x670\n[  244.182314]  ? __virt_addr_valid+0x200/0x3e0\n[  244.184908]  kasan_report+0xd7/0x110\n[  244.187408]  ? bpf_dynptr_data+0x137/0x140\n[  244.189714]  ? bpf_dynptr_data+0x137/0x140\n[  244.192020]  bpf_dynptr_data+0x137/0x140\n[  244.194264]  bpf_prog_b02a02fdd2bdc5fa_global_call_bpf_dynptr_data+0x22/0x26\n[  244.198044]  bpf_prog_b0fe7b9d7dc3abde_callback_adjust_bpf_dynptr_reg_off+0x1f/0x23\n[  244.202136]  bpf_user_ringbuf_drain+0x2c7/0x570\n[  244.204744]  ? 0xffffffffc0009e58\n[  244.206593]  ? __pfx_bpf_user_ringbuf_drain+0x10/0x10\n[  244.209795]  bpf_prog_33ab33f6a804ba2d_user_ringbuf_callback_const_ptr_to_dynptr_reg_off+0x47/0x4b\n[  244.215922]  bpf_trampoline_6442502480+0x43/0xe3\n[  244.218691]  __x64_sys_prlimit64+0x9/0xf0\n[  244.220912]  do_syscall_64+0xc1/0x1d0\n[  244.223043]  entry_SYSCALL_64_after_hwframe+0x77/0x7f\n[  244.226458] RIP: 0033:0x7ffa3eb8f059\n[  244.228582] Code: 08 89 e8 5b 5d c3 66 2e 0f 1f 84 00 00 00 00 00 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 8b 0d 8f 1d 0d 00 f7 d8 64 89 01 48\n[  244.241307] RSP: 002b:00007ffa3e9c6eb8 EFLAGS: 00000206 ORIG_RAX: 000000000000012e\n[  244.246474] RAX: ffffffffffffffda RBX: 00007ffa3e9c7cdc RCX: 00007ffa3eb8f059\n[  244.250478] RDX: 00007ffa3eb162b4 RSI: 0000000000000000 RDI: 00007ffa3e9c7fb0\n[  244.255396] RBP: 00007ffa3e9c6ed0 R08: 00007ffa3e9c76c0 R09: 0000000000000000\n[  244.260195] R10: 0000000000000000 R11: 0000000000000206 R12: ffffffffffffff80\n[  244.264201] R13: 000000000000001c R14: 00007ffc5d6b4260 R15: 00007ffa3e1c7000\n[  244.268303]  </TASK>\n\nAdd a check_func_arg_reg_off() to the path in which the BPF verifier\nverifies the arguments of global function arguments, specifically\nthose which take an argument of type ARG_PTR_TO_DYNPTR |\nMEM_RDONLY. Also, process_dynptr_func() doesn't appear to perform any\nexplicit and strict type matching on the supplied register type, so\nlet's also enforce that a register either type PTR_TO_STACK or\nCONST_PTR_TO_DYNPTR is by the caller.\n\nReported-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Matt Bobrowski <mattbobrowski@google.com>\nLink: https://lore.kernel.org/r/20240625062857.92760-1-mattbobrowski@google.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "2b2efe1937ca9f8815884bd4dcd5b32733025103",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-06-24 13:44:02 +0200",
      "message": "bpf: Fix may_goto with negative offset.\n\nZac's syzbot crafted a bpf prog that exposed two bugs in may_goto.\nThe 1st bug is the way may_goto is patched. When offset is negative\nit should be patched differently.\nThe 2nd bug is in the verifier:\nwhen current state may_goto_depth is equal to visited state may_goto_depth\nit means there is an actual infinite loop. It's not correct to prune\nexploration of the program at this point.\nNote, that this check doesn't limit the program to only one may_goto insn,\nsince 2nd and any further may_goto will increment may_goto_depth only\nin the queued state pushed for future exploration. The current state\nwill have may_goto_depth == 0 regardless of number of may_goto insns\nand the verifier has to explore the program until bpf_exit.\n\nFixes: 011832b97b31 (\"bpf: Introduce may_goto instruction\")\nReported-by: Zac Ecob <zacecob@protonmail.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nCloses: https://lore.kernel.org/bpf/CAADnVQL-15aNp04-cyHRn47Yv61NXfYyhopyZtUyxNojUZUXpA@mail.gmail.com/\nLink: https://lore.kernel.org/bpf/20240619235355.85031-1-alexei.starovoitov@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5337ac4c9b807bc46baa0713121a0afa8beacd70",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-06-21 20:18:40 +0200",
      "message": "bpf: Fix the corner case with may_goto and jump to the 1st insn.\n\nWhen the following program is processed by the verifier:\nL1: may_goto L2\n    goto L1\nL2: w0 = 0\n    exit\n\nthe may_goto insn is first converted to:\nL1: r11 = *(u64 *)(r10 -8)\n    if r11 == 0x0 goto L2\n    r11 -= 1\n    *(u64 *)(r10 -8) = r11\n    goto L1\nL2: w0 = 0\n    exit\n\nthen later as the last step the verifier inserts:\n  *(u64 *)(r10 -8) = BPF_MAX_LOOPS\nas the first insn of the program to initialize loop count.\n\nWhen the first insn happens to be a branch target of some jmp the\nbpf_patch_insn_data() logic will produce:\nL1: *(u64 *)(r10 -8) = BPF_MAX_LOOPS\n    r11 = *(u64 *)(r10 -8)\n    if r11 == 0x0 goto L2\n    r11 -= 1\n    *(u64 *)(r10 -8) = r11\n    goto L1\nL2: w0 = 0\n    exit\n\nbecause instruction patching adjusts all jmps and calls, but for this\nparticular corner case it's incorrect and the L1 label should be one\ninstruction down, like:\n    *(u64 *)(r10 -8) = BPF_MAX_LOOPS\nL1: r11 = *(u64 *)(r10 -8)\n    if r11 == 0x0 goto L2\n    r11 -= 1\n    *(u64 *)(r10 -8) = r11\n    goto L1\nL2: w0 = 0\n    exit\n\nand that's what this patch is fixing.\nAfter bpf_patch_insn_data() call adjust_jmp_off() to adjust all jmps\nthat point to newly insert BPF_ST insn to point to insn after.\n\nNote that bpf_patch_insn_data() cannot easily be changed to accommodate\nthis logic, since jumps that point before or after a sequence of patched\ninstructions have to be adjusted with the full length of the patch.\n\nConceptually it's somewhat similar to \"insert\" of instructions between other\ninstructions with weird semantics. Like \"insert\" before 1st insn would require\nadjustment of CALL insns to point to newly inserted 1st insn, but not an\nadjustment JMP insns that point to 1st, yet still adjusting JMP insns that\ncross over 1st insn (point to insn before or insn after), hence use simple\nadjust_jmp_off() logic to fix this corner case. Ideally bpf_patch_insn_data()\nwould have an auxiliary info to say where 'the start of newly inserted patch\nis', but it would be too complex for backport.\n\nFixes: 011832b97b31 (\"bpf: Introduce may_goto instruction\")\nReported-by: Zac Ecob <zacecob@protonmail.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nCloses: https://lore.kernel.org/bpf/CAADnVQJ_WWx8w4b=6Gc2EpzAjgv+6A0ridnMz2TvS2egj4r3Gw@mail.gmail.com/\nLink: https://lore.kernel.org/bpf/20240619011859.79334-1-alexei.starovoitov@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ab224b9ef7c4eaa752752455ea79bd7022209d5d",
      "author": "Rafael Passos <rafael@rcpassos.me>",
      "date": "2024-06-20 19:50:26 -0700",
      "message": "bpf: remove unused parameter in __bpf_free_used_btfs\n\nFixes a compiler warning. The __bpf_free_used_btfs function\nwas taking an extra unused struct bpf_prog_aux *aux param\n\nSigned-off-by: Rafael Passos <rafael@rcpassos.me>\nLink: https://lore.kernel.org/r/20240615022641.210320-3-rafael@rcpassos.me\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "01793ed86b5d7df1e956520b5474940743eb7ed8",
      "author": "Leon Hwang <hffilwlqm@gmail.com>",
      "date": "2024-06-20 19:48:29 -0700",
      "message": "bpf, verifier: Correct tail_call_reachable for bpf prog\n\nIt's confusing to inspect 'prog->aux->tail_call_reachable' with drgn[0],\nwhen bpf prog has tail call but 'tail_call_reachable' is false.\n\nThis patch corrects 'tail_call_reachable' when bpf prog has tail call.\n\nSigned-off-by: Leon Hwang <hffilwlqm@gmail.com>\nLink: https://lore.kernel.org/r/20240610124224.34673-2-hffilwlqm@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "44b7f7151dfc2e0947f39ed4b9bc4b0c2ccd46fc",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-06-17 10:45:46 -0700",
      "message": "bpf: Add missed var_off setting in coerce_subreg_to_size_sx()\n\nIn coerce_subreg_to_size_sx(), for the case where upper\nsign extension bits are the same for smax32 and smin32\nvalues, we missed to setup properly. This is especially\nproblematic if both smax32 and smin32's sign extension\nbits are 1.\n\nThe following is a simple example illustrating the inconsistent\nverifier states due to missed var_off:\n\n  0: (85) call bpf_get_prandom_u32#7    ; R0_w=scalar()\n  1: (bf) r3 = r0                       ; R0_w=scalar(id=1) R3_w=scalar(id=1)\n  2: (57) r3 &= 15                      ; R3_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=15,var_off=(0x0; 0xf))\n  3: (47) r3 |= 128                     ; R3_w=scalar(smin=umin=smin32=umin32=128,smax=umax=smax32=umax32=143,var_off=(0x80; 0xf))\n  4: (bc) w7 = (s8)w3\n  REG INVARIANTS VIOLATION (alu): range bounds violation u64=[0xffffff80, 0x8f] s64=[0xffffff80, 0x8f]\n    u32=[0xffffff80, 0x8f] s32=[0x80, 0xffffff8f] var_off=(0x80, 0xf)\n\nThe var_off=(0x80, 0xf) is not correct, and the correct one should\nbe var_off=(0xffffff80; 0xf) since from insn 3, we know that at\ninsn 4, the sign extension bits will be 1. This patch fixed this\nissue by setting var_off properly.\n\nFixes: 8100928c8814 (\"bpf: Support new sign-extension mov insns\")\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240615174632.3995278-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "380d5f89a4815ff88461a45de2fb6f28533df708",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-06-17 10:45:46 -0700",
      "message": "bpf: Add missed var_off setting in set_sext32_default_val()\n\nZac reported a verification failure and Alexei reproduced the issue\nwith a simple reproducer ([1]). The verification failure is due to missed\nsetting for var_off.\n\nThe following is the reproducer in [1]:\n  0: R1=ctx() R10=fp0\n  0: (71) r3 = *(u8 *)(r10 -387)        ;\n     R3_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=255,var_off=(0x0; 0xff)) R10=fp0\n  1: (bc) w7 = (s8)w3                   ;\n     R3_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=255,var_off=(0x0; 0xff))\n     R7_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=127,var_off=(0x0; 0x7f))\n  2: (36) if w7 >= 0x2533823b goto pc-3\n     mark_precise: frame0: last_idx 2 first_idx 0 subseq_idx -1\n     mark_precise: frame0: regs=r7 stack= before 1: (bc) w7 = (s8)w3\n     mark_precise: frame0: regs=r3 stack= before 0: (71) r3 = *(u8 *)(r10 -387)\n  2: R7_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=127,var_off=(0x0; 0x7f))\n  3: (b4) w0 = 0                        ; R0_w=0\n  4: (95) exit\n\nNote that after insn 1, the var_off for R7 is (0x0; 0x7f). This is not correct\nsince upper 24 bits of w7 could be 0 or 1. So correct var_off should be\n(0x0; 0xffffffff). Missing var_off setting in set_sext32_default_val() caused later\nincorrect analysis in zext_32_to_64(dst_reg) and reg_bounds_sync(dst_reg).\n\nTo fix the issue, set var_off correctly in set_sext32_default_val(). The correct\nreg state after insn 1 becomes:\n  1: (bc) w7 = (s8)w3                   ;\n     R3_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=255,var_off=(0x0; 0xff))\n     R7_w=scalar(smin=0,smax=umax=0xffffffff,smin32=-128,smax32=127,var_off=(0x0; 0xffffffff))\nand at insn 2, the verifier correctly determines either branch is possible.\n\n  [1] https://lore.kernel.org/bpf/CAADnVQLPU0Shz7dWV4bn2BgtGdxN3uFHPeobGBA72tpg5Xoykw@mail.gmail.com/\n\nFixes: 8100928c8814 (\"bpf: Support new sign-extension mov insns\")\nReported-by: Zac Ecob <zacecob@protonmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240615174626.3994813-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "98d7ca374ba4b39e7535613d40e159f09ca14da2",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-06-14 21:52:39 +0200",
      "message": "bpf: Track delta between \"linked\" registers.\n\nCompilers can generate the code\n  r1 = r2\n  r1 += 0x1\n  if r2 < 1000 goto ...\n  use knowledge of r2 range in subsequent r1 operations\n\nSo remember constant delta between r2 and r1 and update r1 after 'if' condition.\n\nUnfortunately LLVM still uses this pattern for loops with 'can_loop' construct:\nfor (i = 0; i < 1000 && can_loop; i++)\n\nThe \"undo\" pass was introduced in LLVM\nhttps://reviews.llvm.org/D121937\nto prevent this optimization, but it cannot cover all cases.\nInstead of fighting middle end optimizer in BPF backend teach the verifier\nabout this pattern.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20240613013815.953-3-alexei.starovoitov@gmail.com",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/log.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/verifier/precise.c"
      ]
    },
    {
      "hash": "a90797993afcb0eaf6bf47a062ff47eb3810a6d5",
      "author": "Vadim Fedorenko <vadfed@meta.com>",
      "date": "2024-06-13 16:33:04 -0700",
      "message": "bpf: verifier: make kfuncs args nullalble\n\nSome arguments to kfuncs might be NULL in some cases. But currently it's\nnot possible to pass NULL to any BTF structures because the check for\nthe suffix is located after all type checks. Move it to earlier place\nto allow nullable args.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Vadim Fedorenko <vadfed@meta.com>\nLink: https://lore.kernel.org/r/20240613211817.1551967-2-vadfed@meta.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b99a95bc56c52a428befbce12d9451fd7a0f3bc2",
      "author": "Maciej \u017benczykowski <maze@google.com>",
      "date": "2024-06-13 11:24:45 -0700",
      "message": "bpf: fix UML x86_64 compile failure\n\npcpu_hot (defined in arch/x86) is not available on user mode linux (ARCH=um)\n\nCc: Andrii Nakryiko <andrii@kernel.org>\nCc: John Fastabend <john.fastabend@gmail.com>\nCc: Alexei Starovoitov <ast@kernel.org>\nFixes: 1ae6921009e5 (\"bpf: inline bpf_get_smp_processor_id() helper\")\nSigned-off-by: Maciej \u017benczykowski <maze@google.com>\nLink: https://lore.kernel.org/r/20240613173146.2524647-1-maze@google.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "e73cd1cfc2177654e562b04f514be5f0f0b96da2",
      "author": "Daniel Borkmann <daniel@iogearbox.net>",
      "date": "2024-06-13 11:16:01 -0700",
      "message": "bpf: Reduce stack consumption in check_stack_write_fixed_off\n\nThe fake_reg moved into env->fake_reg given it consumes a lot of stack\nspace (120 bytes). Migrate the fake_reg in check_stack_write_fixed_off()\nas well now that we have it.\n\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/r/20240613115310.25383-2-daniel@iogearbox.net\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "92424801261d1564a0bb759da3cf3ccd69fdf5a2",
      "author": "Daniel Borkmann <daniel@iogearbox.net>",
      "date": "2024-06-13 11:16:01 -0700",
      "message": "bpf: Fix reg_set_min_max corruption of fake_reg\n\nJuan reported that after doing some changes to buzzer [0] and implementing\na new fuzzing strategy guided by coverage, they noticed the following in\none of the probes:\n\n  [...]\n  13: (79) r6 = *(u64 *)(r0 +0)         ; R0=map_value(ks=4,vs=8) R6_w=scalar()\n  14: (b7) r0 = 0                       ; R0_w=0\n  15: (b4) w0 = -1                      ; R0_w=0xffffffff\n  16: (74) w0 >>= 1                     ; R0_w=0x7fffffff\n  17: (5c) w6 &= w0                     ; R0_w=0x7fffffff R6_w=scalar(smin=smin32=0,smax=umax=umax32=0x7fffffff,var_off=(0x0; 0x7fffffff))\n  18: (44) w6 |= 2                      ; R6_w=scalar(smin=umin=smin32=umin32=2,smax=umax=umax32=0x7fffffff,var_off=(0x2; 0x7ffffffd))\n  19: (56) if w6 != 0x7ffffffd goto pc+1\n  REG INVARIANTS VIOLATION (true_reg2): range bounds violation u64=[0x7fffffff, 0x7ffffffd] s64=[0x7fffffff, 0x7ffffffd] u32=[0x7fffffff, 0x7ffffffd] s32=[0x7fffffff, 0x7ffffffd] var_off=(0x7fffffff, 0x0)\n  REG INVARIANTS VIOLATION (false_reg1): range bounds violation u64=[0x7fffffff, 0x7ffffffd] s64=[0x7fffffff, 0x7ffffffd] u32=[0x7fffffff, 0x7ffffffd] s32=[0x7fffffff, 0x7ffffffd] var_off=(0x7fffffff, 0x0)\n  REG INVARIANTS VIOLATION (false_reg2): const tnum out of sync with range bounds u64=[0x0, 0xffffffffffffffff] s64=[0x8000000000000000, 0x7fffffffffffffff] u32=[0x0, 0xffffffff] s32=[0x80000000, 0x7fffffff] var_off=(0x7fffffff, 0x0)\n  19: R6_w=0x7fffffff\n  20: (95) exit\n\n  from 19 to 21: R0=0x7fffffff R6=scalar(smin=umin=smin32=umin32=2,smax=umax=smax32=umax32=0x7ffffffe,var_off=(0x2; 0x7ffffffd)) R7=map_ptr(ks=4,vs=8) R9=ctx() R10=fp0 fp-24=map_ptr(ks=4,vs=8) fp-40=mmmmmmmm\n  21: R0=0x7fffffff R6=scalar(smin=umin=smin32=umin32=2,smax=umax=smax32=umax32=0x7ffffffe,var_off=(0x2; 0x7ffffffd)) R7=map_ptr(ks=4,vs=8) R9=ctx() R10=fp0 fp-24=map_ptr(ks=4,vs=8) fp-40=mmmmmmmm\n  21: (14) w6 -= 2147483632             ; R6_w=scalar(smin=umin=umin32=2,smax=umax=0xffffffff,smin32=0x80000012,smax32=14,var_off=(0x2; 0xfffffffd))\n  22: (76) if w6 s>= 0xe goto pc+1      ; R6_w=scalar(smin=umin=umin32=2,smax=umax=0xffffffff,smin32=0x80000012,smax32=13,var_off=(0x2; 0xfffffffd))\n  23: (95) exit\n\n  from 22 to 24: R0=0x7fffffff R6_w=14 R7=map_ptr(ks=4,vs=8) R9=ctx() R10=fp0 fp-24=map_ptr(ks=4,vs=8) fp-40=mmmmmmmm\n  24: R0=0x7fffffff R6_w=14 R7=map_ptr(ks=4,vs=8) R9=ctx() R10=fp0 fp-24=map_ptr(ks=4,vs=8) fp-40=mmmmmmmm\n  24: (14) w6 -= 14                     ; R6_w=0\n  [...]\n\nWhat can be seen here is a register invariant violation on line 19. After\nthe binary-or in line 18, the verifier knows that bit 2 is set but knows\nnothing about the rest of the content which was loaded from a map value,\nmeaning, range is [2,0x7fffffff] with var_off=(0x2; 0x7ffffffd). When in\nline 19 the verifier analyzes the branch, it splits the register states\nin reg_set_min_max() into the registers of the true branch (true_reg1,\ntrue_reg2) and the registers of the false branch (false_reg1, false_reg2).\n\nSince the test is w6 != 0x7ffffffd, the src_reg is a known constant.\nInternally, the verifier creates a \"fake\" register initialized as scalar\nto the value of 0x7ffffffd, and then passes it onto reg_set_min_max(). Now,\nfor line 19, it is mathematically impossible to take the false branch of\nthis program, yet the verifier analyzes it. It is impossible because the\nsecond bit of r6 will be set due to the prior or operation and the\nconstant in the condition has that bit unset (hex(fd) == binary(1111 1101).\n\nWhen the verifier first analyzes the false / fall-through branch, it will\ncompute an intersection between the var_off of r6 and of the constant. This\nis because the verifier creates a \"fake\" register initialized to the value\nof the constant. The intersection result later refines both registers in\nregs_refine_cond_op():\n\n  [...]\n  t = tnum_intersect(tnum_subreg(reg1->var_off), tnum_subreg(reg2->var_off));\n  reg1->var_off = tnum_with_subreg(reg1->var_off, t);\n  reg2->var_off = tnum_with_subreg(reg2->var_off, t);\n  [...]\n\nSince the verifier is analyzing the false branch of the conditional jump,\nreg1 is equal to false_reg1 and reg2 is equal to false_reg2, i.e. the reg2\nis the \"fake\" register that was meant to hold a constant value. The resulting\nvar_off of the intersection says that both registers now hold a known value\nof var_off=(0x7fffffff, 0x0) or in other words: this operation manages to\nmake the verifier think that the \"constant\" value that was passed in the\njump operation now holds a different value.\n\nNormally this would not be an issue since it should not influence the true\nbranch, however, false_reg2 and true_reg2 are pointers to the same \"fake\"\nregister. Meaning, the false branch can influence the results of the true\nbranch. In line 24, the verifier assumes R6_w=0, but the actual runtime\nvalue in this case is 1. The fix is simply not passing in the same \"fake\"\nregister location as inputs to reg_set_min_max(), but instead making a\ncopy. Moving the fake_reg into the env also reduces stack consumption by\n120 bytes. With this, the verifier successfully rejects invalid accesses\nfrom the test program.\n\n  [0] https://github.com/google/buzzer\n\nFixes: 67420501e868 (\"bpf: generalize reg_set_min_max() to handle non-const register comparisons\")\nReported-by: Juan Jos\u00e9 L\u00f3pez Jaimez <jjlopezjaimez@google.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nReviewed-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/r/20240613115310.25383-1-daniel@iogearbox.net\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "cce4c40b960673f9e020835def310f1e89d3a940",
      "author": "Daniel Xu <dxu@dxuuu.xyz>",
      "date": "2024-06-12 11:01:31 -0700",
      "message": "bpf: treewide: Align kfunc signatures to prog point-of-view\n\nPreviously, kfunc declarations in bpf_kfuncs.h (and others) used \"user\nfacing\" types for kfuncs prototypes while the actual kfunc definitions\nused \"kernel facing\" types. More specifically: bpf_dynptr vs\nbpf_dynptr_kern, __sk_buff vs sk_buff, and xdp_md vs xdp_buff.\n\nIt wasn't an issue before, as the verifier allows aliased types.\nHowever, since we are now generating kfunc prototypes in vmlinux.h (in\naddition to keeping bpf_kfuncs.h around), this conflict creates\ncompilation errors.\n\nFix this conflict by using \"user facing\" types in kfunc definitions.\nThis results in more casts, but otherwise has no additional runtime\ncost.\n\nNote, similar to 5b268d1ebcdc (\"bpf: Have bpf_rdonly_cast() take a const\npointer\"), we also make kfuncs take const arguments where appropriate in\norder to make the kfunc more permissive.\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/b58346a63a0e66bc9b7504da751b526b0b189a67.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "fs/verity/measure.c",
        "include/linux/bpf.h",
        "kernel/bpf/crypto.c",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "kernel/trace/bpf_trace.c",
        "net/core/filter.c",
        "tools/testing/selftests/bpf/progs/ip_check_defrag.c",
        "tools/testing/selftests/bpf/progs/verifier_netfilter_ctx.c"
      ]
    },
    {
      "hash": "ec209ad86324de84ef66990f0e9df0851e45e054",
      "author": "Daniel Xu <dxu@dxuuu.xyz>",
      "date": "2024-06-12 11:01:31 -0700",
      "message": "bpf: verifier: Relax caller requirements for kfunc projection type args\n\nCurrently, if a kfunc accepts a projection type as an argument (eg\nstruct __sk_buff *), the caller must exactly provide exactly the same\ntype with provable provenance.\n\nHowever in practice, kfuncs that accept projection types _must_ cast to\nthe underlying type before use b/c projection type layouts are\ncompletely made up. Thus, it is ok to relax the verifier rules around\nimplicit conversions.\n\nWe will use this functionality in the next commit when we align kfuncs\nto user-facing types.\n\nSigned-off-by: Daniel Xu <dxu@dxuuu.xyz>\nLink: https://lore.kernel.org/r/e2c025cb09ccfd4af1ec9e18284dc3cecff7514d.1718207789.git.dxu@dxuuu.xyz\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/btf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "482f7133791e894b94a57ab3251e03d4c98ea42b",
      "author": "Kui-Feng Lee <thinker.li@gmail.com>",
      "date": "2024-06-03 20:52:42 -0700",
      "message": "bpf: Remove unnecessary call to btf_field_type_size().\n\nfield->size has been initialized by bpf_parse_fields() with the value\nreturned by btf_field_type_size(). Use it instead of calling\nbtf_field_type_size() again.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240523174202.461236-3-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c95a3be45ad22ee8925d6d1ab531d5ba98216311",
      "author": "Kui-Feng Lee <thinker.li@gmail.com>",
      "date": "2024-06-03 20:52:42 -0700",
      "message": "bpf: Remove unnecessary checks on the offset of btf_field.\n\nreg_find_field_offset() always return a btf_field with a matching offset\nvalue. Checking the offset of the returned btf_field is unnecessary.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240523174202.461236-2-thinker.li@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "aeb8fe0283d4d3b0f27a87c5f5c938e7324f7d8f",
      "author": "Jiri Olsa <jolsa@kernel.org>",
      "date": "2024-05-31 14:54:48 -0700",
      "message": "bpf: Fix bpf_session_cookie BTF_ID in special_kfunc_set list\n\nThe bpf_session_cookie is unavailable for !CONFIG_FPROBE as reported\nby Sebastian [1].\n\nTo fix that we remove CONFIG_FPROBE ifdef for session kfuncs, which\nis fine, because there's filter for session programs.\n\nThen based on bpf_trace.o dependency:\n  obj-$(CONFIG_BPF_EVENTS) += bpf_trace.o\n\nwe add bpf_session_cookie BTF_ID in special_kfunc_set list dependency\non CONFIG_BPF_EVENTS.\n\n[1] https://lore.kernel.org/bpf/20240531071557.MvfIqkn7@linutronix.de/T/#m71c6d5ec71db2967288cb79acedc15cc5dbfeec5\nReported-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>\nTested-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>\nSuggested-by: Alexei Starovoitov <ast@kernel.org>\nFixes: 5c919acef8514 (\"bpf: Add support for kprobe session cookie\")\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nLink: https://lore.kernel.org/r/20240531194500.2967187-1-jolsa@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "kernel/trace/bpf_trace.c"
      ]
    },
    {
      "hash": "98e948fb60d41447fd8d2d0c3b8637fc6b6dc26d",
      "author": "Jakub Sitnicki <jakub@cloudflare.com>",
      "date": "2024-05-27 19:33:40 +0200",
      "message": "bpf: Allow delete from sockmap/sockhash only if update is allowed\n\nWe have seen an influx of syzkaller reports where a BPF program attached to\na tracepoint triggers a locking rule violation by performing a map_delete\non a sockmap/sockhash.\n\nWe don't intend to support this artificial use scenario. Extend the\nexisting verifier allowed-program-type check for updating sockmap/sockhash\nto also cover deleting from a map.\n\nFrom now on only BPF programs which were previously allowed to update\nsockmap/sockhash can delete from these map types.\n\nFixes: ff9105993240 (\"bpf, sockmap: Prevent lock inversion deadlock in map delete elem\")\nReported-by: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>\nReported-by: syzbot+ec941d6e24f633a59172@syzkaller.appspotmail.com\nSigned-off-by: Jakub Sitnicki <jakub@cloudflare.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nTested-by: syzbot+ec941d6e24f633a59172@syzkaller.appspotmail.com\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nCloses: https://syzkaller.appspot.com/bug?extid=ec941d6e24f633a59172\nLink: https://lore.kernel.org/bpf/20240527-sockmap-verify-deletes-v1-1-944b372f2101@cloudflare.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "2ddec2c80b4402c293c7e6e0881cecaaf77e8cec",
      "author": "Puranjay Mohan <puranjay@kernel.org>",
      "date": "2024-05-12 16:54:34 -0700",
      "message": "riscv, bpf: inline bpf_get_smp_processor_id()\n\nInline the calls to bpf_get_smp_processor_id() in the riscv bpf jit.\n\nRISCV saves the pointer to the CPU's task_struct in the TP (thread\npointer) register. This makes it trivial to get the CPU's processor id.\nAs thread_info is the first member of task_struct, we can read the\nprocessor id from TP + offsetof(struct thread_info, cpu).\n\n          RISCV64 JIT output for `call bpf_get_smp_processor_id`\n\t  ======================================================\n\n                Before                           After\n               --------                         -------\n\n         auipc   t1,0x848c                  ld    a5,32(tp)\n         jalr    604(t1)\n         mv      a5,a0\n\nBenchmark using [1] on Qemu.\n\n./benchs/run_bench_trigger.sh glob-arr-inc arr-inc hash-inc\n\n+---------------+------------------+------------------+--------------+\n|      Name     |     Before       |       After      |   % change   |\n|---------------+------------------+------------------+--------------|\n| glob-arr-inc  | 1.077 \u00b1 0.006M/s | 1.336 \u00b1 0.010M/s |   + 24.04%   |\n| arr-inc       | 1.078 \u00b1 0.002M/s | 1.332 \u00b1 0.015M/s |   + 23.56%   |\n| hash-inc      | 0.494 \u00b1 0.004M/s | 0.653 \u00b1 0.001M/s |   + 32.18%   |\n+---------------+------------------+------------------+--------------+\n\nNOTE: This benchmark includes changes from this patch and the previous\n      patch that implemented the per-cpu insn.\n\n[1] https://github.com/anakryiko/linux/commit/8dec900975ef\n\nSigned-off-by: Puranjay Mohan <puranjay@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Bj\u00f6rn T\u00f6pel <bjorn@kernel.org>\nLink: https://lore.kernel.org/r/20240502151854.9810-3-puranjay@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "arch/riscv/net/bpf_jit_comp64.c",
        "include/linux/filter.h",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "41d047a871062f1a4d1871a1908d380c14e75428",
      "author": "Cupertino Miranda <cupertino.miranda@oracle.com>",
      "date": "2024-05-06 17:09:12 -0700",
      "message": "bpf/verifier: relax MUL range computation check\n\nMUL instruction required that src_reg would be a known value (i.e.\nsrc_reg would be a const value). The condition in this case can be\nrelaxed, since the range computation algorithm used in current code\nalready supports a proper range computation for any valid range value on\nits operands.\n\nSigned-off-by: Cupertino Miranda <cupertino.miranda@oracle.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nCc: Yonghong Song <yonghong.song@linux.dev>\nCc: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nCc: David Faust <david.faust@oracle.com>\nCc: Jose Marchesi <jose.marchesi@oracle.com>\nCc: Elena Zannoni <elena.zannoni@oracle.com>\nLink: https://lore.kernel.org/r/20240506141849.185293-6-cupertino.miranda@oracle.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "138cc42c05d11fd5ee82ee1606d2c9823373a926",
      "author": "Cupertino Miranda <cupertino.miranda@oracle.com>",
      "date": "2024-05-06 17:09:11 -0700",
      "message": "bpf/verifier: improve XOR and OR range computation\n\nRange for XOR and OR operators would not be attempted unless src_reg\nwould resolve to a single value, i.e. a known constant value.\nThis condition is unnecessary, and the following XOR/OR operator\nhandling could compute a possible better range.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\n\nSigned-off-by: Cupertino Miranda <cupertino.miranda@oracle.com\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nCc: Yonghong Song <yonghong.song@linux.dev>\nCc: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nCc: David Faust <david.faust@oracle.com>\nCc: Jose Marchesi <jose.marchesi@oracle.com>\nCc: Elena Zannoni <elena.zannoni@oracle.com>\nCc: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nLink: https://lore.kernel.org/r/20240506141849.185293-4-cupertino.miranda@oracle.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "0922c78f592c60e5a8fe6ab968479def124d4ff3",
      "author": "Cupertino Miranda <cupertino.miranda@oracle.com>",
      "date": "2024-05-06 17:09:11 -0700",
      "message": "bpf/verifier: refactor checks for range computation\n\nSplit range computation checks in its own function, isolating pessimitic\nrange set for dst_reg and failing return to a single point.\n\nSigned-off-by: Cupertino Miranda <cupertino.miranda@oracle.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nCc: Yonghong Song <yonghong.song@linux.dev>\nCc: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nCc: David Faust <david.faust@oracle.com>\nCc: Jose Marchesi <jose.marchesi@oracle.com>\nCc: Elena Zannoni <elena.zannoni@oracle.com>\nCc: Andrii Nakryiko <andrii.nakryiko@gmail.com>\n\nbpf/verifier: improve code after range computation recent changes.\nLink: https://lore.kernel.org/r/20240506141849.185293-3-cupertino.miranda@oracle.com\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d786957ebd3fb4cfd9147dbcccd1e8f3871b45ce",
      "author": "Cupertino Miranda <cupertino.miranda@oracle.com>",
      "date": "2024-05-06 17:09:11 -0700",
      "message": "bpf/verifier: replace calls to mark_reg_unknown.\n\nIn order to further simplify the code in adjust_scalar_min_max_vals all\nthe calls to mark_reg_unknown are replaced by __mark_reg_unknown.\n\nstatic void mark_reg_unknown(struct bpf_verifier_env *env,\n  \t\t\t     struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\t... mark all regs not init ...\n\t\treturn;\n    }\n\t__mark_reg_unknown(env, regs + regno);\n}\n\nThe 'regno >= MAX_BPF_REG' does not apply to\nadjust_scalar_min_max_vals(), because it is only called from the\nfollowing stack:\n  - check_alu_op\n    - adjust_reg_min_max_vals\n      - adjust_scalar_min_max_vals\n\nThe check_alu_op() does check_reg_arg() which verifies that both src and\ndst register numbers are within bounds.\n\nSigned-off-by: Cupertino Miranda <cupertino.miranda@oracle.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nCc: Yonghong Song <yonghong.song@linux.dev>\nCc: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nCc: David Faust <david.faust@oracle.com>\nCc: Jose Marchesi <jose.marchesi@oracle.com>\nCc: Elena Zannoni <elena.zannoni@oracle.com>\nCc: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nLink: https://lore.kernel.org/r/20240506141849.185293-2-cupertino.miranda@oracle.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5c919acef85147886eb2abf86fb147f94680a8b0",
      "author": "Jiri Olsa <jolsa@kernel.org>",
      "date": "2024-04-30 09:45:53 -0700",
      "message": "bpf: Add support for kprobe session cookie\n\nAdding support for cookie within the session of kprobe multi\nentry and return program.\n\nThe session cookie is u64 value and can be retrieved be new\nkfunc bpf_session_cookie, which returns pointer to the cookie\nvalue. The bpf program can use the pointer to store (on entry)\nand load (on return) the value.\n\nThe cookie value is implemented via fprobe feature that allows\nto share values between entry and return ftrace fprobe callbacks.\n\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240430112830.1184228-4-jolsa@kernel.org",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "kernel/trace/bpf_trace.c"
      ]
    },
    {
      "hash": "0db63c0b86e981a1e97d2596d64ceceba1a5470e",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-04-29 14:16:41 -0700",
      "message": "bpf: Fix verifier assumptions about socket->sk\n\nThe verifier assumes that 'sk' field in 'struct socket' is valid\nand non-NULL when 'socket' pointer itself is trusted and non-NULL.\nThat may not be the case when socket was just created and\npassed to LSM socket_accept hook.\nFix this verifier assumption and adjust tests.\n\nReported-by: Liam Wisehart <liamwisehart@meta.com>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nFixes: 6fcd486b3a0a (\"bpf: Refactor RCU enforcement in the verifier.\")\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/r/20240427002544.68803-1-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/bench_local_storage_create.c",
        "tools/testing/selftests/bpf/progs/local_storage.c",
        "tools/testing/selftests/bpf/progs/lsm_cgroup.c"
      ]
    },
    {
      "hash": "66e13b615a0ce76b785d780ecc9776ba71983629",
      "author": "Puranjay Mohan <puranjay12@gmail.com>",
      "date": "2024-04-26 09:45:18 -0700",
      "message": "bpf: verifier: prevent userspace memory access\n\nWith BPF_PROBE_MEM, BPF allows de-referencing an untrusted pointer. To\nthwart invalid memory accesses, the JITs add an exception table entry\nfor all such accesses. But in case the src_reg + offset is a userspace\naddress, the BPF program might read that memory if the user has\nmapped it.\n\nMake the verifier add guard instructions around such memory accesses and\nskip the load if the address falls into the userspace region.\n\nThe JITs need to implement bpf_arch_uaddress_limit() to define where\nthe userspace addresses end for that architecture or TASK_SIZE is taken\nas default.\n\nThe implementation is as follows:\n\nREG_AX =  SRC_REG\nif(offset)\n\tREG_AX += offset;\nREG_AX >>= 32;\nif (REG_AX <= (uaddress_limit >> 32))\n\tDST_REG = 0;\nelse\n\tDST_REG = *(size *)(SRC_REG + offset);\n\nComparing just the upper 32 bits of the load address with the upper\n32 bits of uaddress_limit implies that the values are being aligned down\nto a 4GB boundary before comparison.\n\nThe above means that all loads with address <= uaddress_limit + 4GB are\nskipped. This is acceptable because there is a large hole (much larger\nthan 4GB) between userspace and kernel space memory, therefore a\ncorrectly functioning BPF program should not access this 4GB memory\nabove the userspace.\n\nLet's analyze what this patch does to the following fentry program\ndereferencing an untrusted pointer:\n\n  SEC(\"fentry/tcp_v4_connect\")\n  int BPF_PROG(fentry_tcp_v4_connect, struct sock *sk)\n  {\n                *(volatile long *)sk;\n                return 0;\n  }\n\n    BPF Program before              |           BPF Program after\n    ------------------              |           -----------------\n\n  0: (79) r1 = *(u64 *)(r1 +0)          0: (79) r1 = *(u64 *)(r1 +0)\n  -----------------------------------------------------------------------\n  1: (79) r1 = *(u64 *)(r1 +0) --\\      1: (bf) r11 = r1\n  ----------------------------\\   \\     2: (77) r11 >>= 32\n  2: (b7) r0 = 0               \\   \\    3: (b5) if r11 <= 0x8000 goto pc+2\n  3: (95) exit                  \\   \\-> 4: (79) r1 = *(u64 *)(r1 +0)\n                                 \\      5: (05) goto pc+1\n                                  \\     6: (b7) r1 = 0\n                                   \\--------------------------------------\n                                        7: (b7) r0 = 0\n                                        8: (95) exit\n\nAs you can see from above, in the best case (off=0), 5 extra instructions\nare emitted.\n\nNow, we analyze the same program after it has gone through the JITs of\nARM64 and RISC-V architectures. We follow the single load instruction\nthat has the untrusted pointer and see what instrumentation has been\nadded around it.\n\n                                x86-64 JIT\n                                ==========\n     JIT's Instrumentation\n          (upstream)\n     ---------------------\n\n   0:   nopl   0x0(%rax,%rax,1)\n   5:   xchg   %ax,%ax\n   7:   push   %rbp\n   8:   mov    %rsp,%rbp\n   b:   mov    0x0(%rdi),%rdi\n  ---------------------------------\n   f:   movabs $0x800000000000,%r11\n  19:   cmp    %r11,%rdi\n  1c:   jb     0x000000000000002a\n  1e:   mov    %rdi,%r11\n  21:   add    $0x0,%r11\n  28:   jae    0x000000000000002e\n  2a:   xor    %edi,%edi\n  2c:   jmp    0x0000000000000032\n  2e:   mov    0x0(%rdi),%rdi\n  ---------------------------------\n  32:   xor    %eax,%eax\n  34:   leave\n  35:   ret\n\nThe x86-64 JIT already emits some instructions to protect against user\nmemory access. This patch doesn't make any changes for the x86-64 JIT.\n\n                                  ARM64 JIT\n                                  =========\n\n        No Intrumentation                       Verifier's Instrumentation\n           (upstream)                                  (This patch)\n        -----------------                       --------------------------\n\n   0:   add     x9, x30, #0x0                0:   add     x9, x30, #0x0\n   4:   nop                                  4:   nop\n   8:   paciasp                              8:   paciasp\n   c:   stp     x29, x30, [sp, #-16]!        c:   stp     x29, x30, [sp, #-16]!\n  10:   mov     x29, sp                     10:   mov     x29, sp\n  14:   stp     x19, x20, [sp, #-16]!       14:   stp     x19, x20, [sp, #-16]!\n  18:   stp     x21, x22, [sp, #-16]!       18:   stp     x21, x22, [sp, #-16]!\n  1c:   stp     x25, x26, [sp, #-16]!       1c:   stp     x25, x26, [sp, #-16]!\n  20:   stp     x27, x28, [sp, #-16]!       20:   stp     x27, x28, [sp, #-16]!\n  24:   mov     x25, sp                     24:   mov     x25, sp\n  28:   mov     x26, #0x0                   28:   mov     x26, #0x0\n  2c:   sub     x27, x25, #0x0              2c:   sub     x27, x25, #0x0\n  30:   sub     sp, sp, #0x0                30:   sub     sp, sp, #0x0\n  34:   ldr     x0, [x0]                    34:   ldr     x0, [x0]\n--------------------------------------------------------------------------------\n  38:   ldr     x0, [x0] ----------\\        38:   add     x9, x0, #0x0\n-----------------------------------\\\\       3c:   lsr     x9, x9, #32\n  3c:   mov     x7, #0x0            \\\\      40:   cmp     x9, #0x10, lsl #12\n  40:   mov     sp, sp               \\\\     44:   b.ls    0x0000000000000050\n  44:   ldp     x27, x28, [sp], #16   \\\\--> 48:   ldr     x0, [x0]\n  48:   ldp     x25, x26, [sp], #16    \\    4c:   b       0x0000000000000054\n  4c:   ldp     x21, x22, [sp], #16     \\   50:   mov     x0, #0x0\n  50:   ldp     x19, x20, [sp], #16      \\---------------------------------------\n  54:   ldp     x29, x30, [sp], #16         54:   mov     x7, #0x0\n  58:   add     x0, x7, #0x0                58:   mov     sp, sp\n  5c:   autiasp                             5c:   ldp     x27, x28, [sp], #16\n  60:   ret                                 60:   ldp     x25, x26, [sp], #16\n  64:   nop                                 64:   ldp     x21, x22, [sp], #16\n  68:   ldr     x10, 0x0000000000000070     68:   ldp     x19, x20, [sp], #16\n  6c:   br      x10                         6c:   ldp     x29, x30, [sp], #16\n                                            70:   add     x0, x7, #0x0\n                                            74:   autiasp\n                                            78:   ret\n                                            7c:   nop\n                                            80:   ldr     x10, 0x0000000000000088\n                                            84:   br      x10\n\nThere are 6 extra instructions added in ARM64 in the best case. This will\nbecome 7 in the worst case (off != 0).\n\n                           RISC-V JIT (RISCV_ISA_C Disabled)\n                           ==========\n\n        No Intrumentation           Verifier's Instrumentation\n           (upstream)                      (This patch)\n        -----------------           --------------------------\n\n   0:   nop                            0:   nop\n   4:   nop                            4:   nop\n   8:   li      a6, 33                 8:   li      a6, 33\n   c:   addi    sp, sp, -16            c:   addi    sp, sp, -16\n  10:   sd      s0, 8(sp)             10:   sd      s0, 8(sp)\n  14:   addi    s0, sp, 16            14:   addi    s0, sp, 16\n  18:   ld      a0, 0(a0)             18:   ld      a0, 0(a0)\n---------------------------------------------------------------\n  1c:   ld      a0, 0(a0) --\\         1c:   mv      t0, a0\n--------------------------\\  \\        20:   srli    t0, t0, 32\n  20:   li      a5, 0      \\  \\       24:   lui     t1, 4096\n  24:   ld      s0, 8(sp)   \\  \\      28:   sext.w  t1, t1\n  28:   addi    sp, sp, 16   \\  \\     2c:   bgeu    t1, t0, 12\n  2c:   sext.w  a0, a5        \\  \\--> 30:   ld      a0, 0(a0)\n  30:   ret                    \\      34:   j       8\n                                \\     38:   li      a0, 0\n                                 \\------------------------------\n                                      3c:   li      a5, 0\n                                      40:   ld      s0, 8(sp)\n                                      44:   addi    sp, sp, 16\n                                      48:   sext.w  a0, a5\n                                      4c:   ret\n\nThere are 7 extra instructions added in RISC-V.\n\nFixes: 800834285361 (\"bpf, arm64: Add BPF exception tables\")\nReported-by: Breno Leitao <leitao@debian.org>\nSuggested-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: Ilya Leoshkevich <iii@linux.ibm.com>\nSigned-off-by: Puranjay Mohan <puranjay12@gmail.com>\nLink: https://lore.kernel.org/r/20240424100210.11982-2-puranjay@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "arch/x86/net/bpf_jit_comp.c",
        "include/linux/filter.h",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "3e1c6f35409f9e447bf37f64840f5b65576bfb78",
      "author": "Vadim Fedorenko <vadfed@meta.com>",
      "date": "2024-04-24 16:01:10 -0700",
      "message": "bpf: make common crypto API for TC/XDP programs\n\nAdd crypto API support to BPF to be able to decrypt or encrypt packets\nin TC/XDP BPF programs. Special care should be taken for initialization\npart of crypto algo because crypto alloc) doesn't work with preemtion\ndisabled, it can be run only in sleepable BPF program. Also async crypto\nis not supported because of the very same issue - TC/XDP BPF programs\nare not sleepable.\n\nSigned-off-by: Vadim Fedorenko <vadfed@meta.com>\nLink: https://lore.kernel.org/r/20240422225024.2847039-2-vadfed@meta.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_crypto.h",
        "kernel/bpf/Makefile",
        "kernel/bpf/crypto.c",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "fc7566ad0a826cdc8886c5dbbb39ce72a0dc6333",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2024-04-24 09:47:49 -0700",
      "message": "bpf: Introduce bpf_preempt_[disable,enable] kfuncs\n\nIntroduce two new BPF kfuncs, bpf_preempt_disable and\nbpf_preempt_enable. These kfuncs allow disabling preemption in BPF\nprograms. Nesting is allowed, since the intended use cases includes\nbuilding native BPF spin locks without kernel helper involvement. Apart\nfrom that, this can be used to per-CPU data structures for cases where\nprograms (or userspace) may preempt one or the other. Currently, while\nper-CPU access is stable, whether it will be consistent is not\nguaranteed, as only migration is disabled for BPF programs.\n\nGlobal functions are disallowed from being called, but support for them\nwill be added as a follow up not just preempt kfuncs, but rcu_read_lock\nkfuncs as well. Static subprog calls are permitted. Sleepable helpers\nand kfuncs are disallowed in non-preemptible regions.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20240424031315.2757363-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "81f1d7a583fa1fa14f0c4e6140d34b5e3d08d227",
      "author": "Benjamin Tissoires <bentiss@kernel.org>",
      "date": "2024-04-23 19:46:57 -0700",
      "message": "bpf: wq: add bpf_wq_set_callback_impl\n\nTo support sleepable async callbacks, we need to tell push_async_cb()\nwhether the cb is sleepable or not.\n\nThe verifier now detects that we are in bpf_wq_set_callback_impl and\ncan allow a sleepable callback to happen.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-13-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d940c9b94d7e6d9cff288623e3e8bf5fdea98b79",
      "author": "Benjamin Tissoires <bentiss@kernel.org>",
      "date": "2024-04-23 18:31:25 -0700",
      "message": "bpf: add support for KF_ARG_PTR_TO_WORKQUEUE\n\nIntroduce support for KF_ARG_PTR_TO_WORKQUEUE. The kfuncs will use bpf_wq\nas argument and that will be recognized as workqueue argument by verifier.\nbpf_wq_kern casting can happen inside kfunc, but using bpf_wq in\nargument makes life easier for users who work with non-kern type in BPF\nprogs.\n\nDuplicate process_timer_func into process_wq_func.\nmeta argument is only needed to ensure bpf_wq_init's workqueue and map\narguments are coming from the same map (map_uid logic is necessary for\ncorrect inner-map handling), so also amend check_kfunc_args() to\nmatch what helpers functions check is doing.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-8-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ad2c03e691be3268eefc75ff1d892db3f0e79f62",
      "author": "Benjamin Tissoires <bentiss@kernel.org>",
      "date": "2024-04-23 18:31:24 -0700",
      "message": "bpf: verifier: bail out if the argument is not a map\n\nWhen a kfunc is declared with a KF_ARG_PTR_TO_MAP, we should have\nreg->map_ptr set to a non NULL value, otherwise, that means that the\nunderlying type is not a map.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-7-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d56b63cf0c0f71e1b2e04dd8220b408f049e67ff",
      "author": "Benjamin Tissoires <bentiss@kernel.org>",
      "date": "2024-04-23 18:31:24 -0700",
      "message": "bpf: add support for bpf_wq user type\n\nMostly a copy/paste from the bpf_timer API, without the initialization\nand free, as they will be done in a separate patch.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240420-bpf_wq-v2-5-6c986a5a741f@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "a7de265cb2d849f8986a197499ad58dca0a4f209",
      "author": "Rafael Passos <rafael@rcpassos.me>",
      "date": "2024-04-22 17:48:08 +0200",
      "message": "bpf: Fix typos in comments\n\nFound the following typos in comments, and fixed them:\n\ns/unpriviledged/unprivileged/\ns/reponsible/responsible/\ns/possiblities/possibilities/\ns/Divison/Division/\ns/precsion/precision/\ns/havea/have a/\ns/reponsible/responsible/\ns/responsibile/responsible/\ns/tigher/tighter/\ns/respecitve/respective/\n\nSigned-off-by: Rafael Passos <rafael@rcpassos.me>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/6af7deb4-bb24-49e8-b3f1-8dd410597337@smtp-relay.sendinblue.com",
      "modified_files": [
        "kernel/bpf/bpf_local_storage.c",
        "kernel/bpf/core.c",
        "kernel/bpf/hashtab.c",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "e1a7545981e2086feaa40dcb7b0db8de0bdada19",
      "author": "Rafael Passos <rafael@rcpassos.me>",
      "date": "2024-04-22 17:12:05 +0200",
      "message": "bpf: Fix typo in function save_aux_ptr_type\n\nI found this typo in the save_aux_ptr_type function.\ns/allow_trust_missmatch/allow_trust_mismatch/\nI did not find this anywhere else in the codebase.\n\nSigned-off-by: Rafael Passos <rafael@rcpassos.me>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/fbe1d636-8172-4698-9a5a-5a3444b55322@smtp-relay.sendinblue.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1f586614f3ffa80fdf2116b2a1bebcdb5969cef8",
      "author": "Harishankar Vishwanathan <harishankar.vishwanathan@gmail.com>",
      "date": "2024-04-16 17:55:27 +0200",
      "message": "bpf: Harden and/or/xor value tracking in verifier\n\nThis patch addresses a latent unsoundness issue in the\nscalar(32)_min_max_and/or/xor functions. While it is not a bugfix,\nit ensures that the functions produce sound outputs for all inputs.\n\nThe issue occurs in these functions when setting signed bounds. The\nfollowing example illustrates the issue for scalar_min_max_and(),\nbut it applies to the other functions.\n\nIn scalar_min_max_and() the following clause is executed when ANDing\npositive numbers:\n\n  /* ANDing two positives gives a positive, so safe to\n   * cast result into s64.\n   */\n  dst_reg->smin_value = dst_reg->umin_value;\n  dst_reg->smax_value = dst_reg->umax_value;\n\nHowever, if umin_value and umax_value of dst_reg cross the sign boundary\n(i.e., if (s64)dst_reg->umin_value > (s64)dst_reg->umax_value), then we\nwill end up with smin_value > smax_value, which is unsound.\n\nPrevious works [1, 2] have discovered and reported this issue. Our tool\nAgni [2, 3] consideres it a false positive. This is because, during the\nverification of the abstract operator scalar_min_max_and(), Agni restricts\nits inputs to those passing through reg_bounds_sync(). This mimics\nreal-world verifier behavior, as reg_bounds_sync() is invariably executed\nat the tail of every abstract operator. Therefore, such behavior is\nunlikely in an actual verifier execution.\n\nHowever, it is still unsound for an abstract operator to set signed bounds\nsuch that smin_value > smax_value. This patch fixes it, making the abstract\noperator sound for all (well-formed) inputs.\n\nIt is worth noting that while the previous code updated the signed bounds\n(using the output unsigned bounds) only when the *input signed* bounds\nwere positive, the new code updates them whenever the *output unsigned*\nbounds do not cross the sign boundary.\n\nAn alternative approach to fix this latent unsoundness would be to\nunconditionally set the signed bounds to unbounded [S64_MIN, S64_MAX], and\nlet reg_bounds_sync() refine the signed bounds using the unsigned bounds\nand the tnum. We found that our approach produces more precise (tighter)\nbounds.\n\nFor example, consider these inputs to BPF_AND:\n\n  /* dst_reg */\n  var_off.value: 8608032320201083347\n  var_off.mask: 615339716653692460\n  smin_value: 8070450532247928832\n  smax_value: 8070450532247928832\n  umin_value: 13206380674380886586\n  umax_value: 13206380674380886586\n  s32_min_value: -2110561598\n  s32_max_value: -133438816\n  u32_min_value: 4135055354\n  u32_max_value: 4135055354\n\n  /* src_reg */\n  var_off.value: 8584102546103074815\n  var_off.mask: 9862641527606476800\n  smin_value: 2920655011908158522\n  smax_value: 7495731535348625717\n  umin_value: 7001104867969363969\n  umax_value: 8584102543730304042\n  s32_min_value: -2097116671\n  s32_max_value: 71704632\n  u32_min_value: 1047457619\n  u32_max_value: 4268683090\n\nAfter going through tnum_and() -> scalar32_min_max_and() ->\nscalar_min_max_and() -> reg_bounds_sync(), our patch produces the following\nbounds for s32:\n\n  s32_min_value: -1263875629\n  s32_max_value: -159911942\n\nWhereas, setting the signed bounds to unbounded in scalar_min_max_and()\nproduces:\n\n  s32_min_value: -1263875629\n  s32_max_value: -1\n\nAs observed, our patch produces a tighter s32 bound. We also confirmed\nusing Agni and SMT verification that our patch always produces signed\nbounds that are equal to or more precise than setting the signed bounds to\nunbounded in scalar_min_max_and().\n\n  [1] https://sanjit-bhat.github.io/assets/pdf/ebpf-verifier-range-analysis22.pdf\n  [2] https://link.springer.com/chapter/10.1007/978-3-031-37709-9_12\n  [3] https://github.com/bpfverif/agni\n\nCo-developed-by: Matan Shachnai <m.shachnai@rutgers.edu>\nSigned-off-by: Matan Shachnai <m.shachnai@rutgers.edu>\nCo-developed-by: Srinivas Narayana <srinivas.narayana@rutgers.edu>\nSigned-off-by: Srinivas Narayana <srinivas.narayana@rutgers.edu>\nCo-developed-by: Santosh Nagarakatte <santosh.nagarakatte@rutgers.edu>\nSigned-off-by: Santosh Nagarakatte <santosh.nagarakatte@rutgers.edu>\nSigned-off-by: Harishankar Vishwanathan <harishankar.vishwanathan@gmail.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20240402212039.51815-1-harishankar.vishwanathan@gmail.com\nLink: https://lore.kernel.org/bpf/20240416115303.331688-1-harishankar.vishwanathan@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "37eacb9f6e89fb399a79e952bc9c78eb3e16290e",
      "author": "Anton Protopopov <aspsk@isovalent.com>",
      "date": "2024-04-12 18:37:20 +0200",
      "message": "bpf: Fix a verifier verbose message\n\nLong ago a map file descriptor in a pseudo ldimm64 instruction could\nonly be present as an immediate value insn[0].imm, and thus this value\nwas used in a verbose verifier message printed when the file descriptor\nwasn't valid. Since addition of BPF_PSEUDO_MAP_IDX_VALUE/BPF_PSEUDO_MAP_IDX\nthe insn[0].imm field can also contain an index pointing to the file\ndescriptor in the attr.fd_array array. However, if the file descriptor\nis invalid, the verifier still prints the verbose message containing\nvalue of insn[0].imm. Patch the verifier message to always print the\nactual file descriptor value.\n\nFixes: 387544bfa291 (\"bpf: Introduce fd_idx\")\nSigned-off-by: Anton Protopopov <aspsk@isovalent.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20240412141100.3562942-1-aspsk@isovalent.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d503a04f8bc0c75dc9db9452d8cc79d748afb752",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-04-09 10:24:26 -0700",
      "message": "bpf: Add support for certain atomics in bpf_arena to x86 JIT\n\nSupport atomics in bpf_arena that can be JITed as a single x86 instruction.\nInstructions that are JITed as loops are not supported at the moment,\nsince they require more complex extable and loop logic.\n\nJITs can choose to do smarter things with bpf_jit_supports_insn().\nLike arm64 may decide to support all bpf atomics instructions\nwhen emit_lse_atomic is available and none in ll_sc mode.\n\nbpf_jit_supports_percpu_insn(), bpf_jit_supports_ptr_xchg() and\nother such callbacks can be replaced with bpf_jit_supports_insn()\nin the future.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240405231134.17274-1-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "arch/x86/net/bpf_jit_comp.c",
        "include/linux/filter.h",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "9d482da9e17a4ddd5563428f74302a36b2610306",
      "author": "Philo Lu <lulie@linux.alibaba.com>",
      "date": "2024-04-05 10:31:17 -0700",
      "message": "bpf: allow invoking bpf_for_each_map_elem with different maps\n\nTaking different maps within a single bpf_for_each_map_elem call is not\nallowed before, because from the second map,\nbpf_insn_aux_data->map_ptr_state will be marked as *poison*. In fact\nboth map_ptr and state are needed to support this use case: map_ptr is\nused by set_map_elem_callback_state() while poison state is needed to\ndetermine whether to use direct call.\n\nSigned-off-by: Philo Lu <lulie@linux.alibaba.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240405025536.18113-3-lulie@linux.alibaba.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "0a525621b7e5b49202b19d8f75382c6778fdd0c1",
      "author": "Philo Lu <lulie@linux.alibaba.com>",
      "date": "2024-04-05 10:31:17 -0700",
      "message": "bpf: store both map ptr and state in bpf_insn_aux_data\n\nCurrently, bpf_insn_aux_data->map_ptr_state is used to store either\nmap_ptr or its poison state (i.e., BPF_MAP_PTR_POISON). Thus\nBPF_MAP_PTR_POISON must be checked before reading map_ptr. In certain\ncases, we may need valid map_ptr even in case of poison state.\nThis will be explained in next patch with bpf_for_each_map_elem()\nhelper.\n\nThis patch changes map_ptr_state into a new struct including both map\npointer and its state (poison/unpriv). It's in the same union with\nstruct bpf_loop_inline_state, so there is no extra memory overhead.\nBesides, macros BPF_MAP_PTR_UNPRIV/BPF_MAP_PTR_POISON/BPF_MAP_PTR are no\nlonger needed.\n\nThis patch does not change any existing functionality.\n\nSigned-off-by: Philo Lu <lulie@linux.alibaba.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240405025536.18113-2-lulie@linux.alibaba.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "58babe27180c8d4cb54d831589cf801bd9268876",
      "author": "Arnd Bergmann <arnd@arndb.de>",
      "date": "2024-04-05 08:39:15 -0700",
      "message": "bpf: fix perf_snapshot_branch_stack link failure\n\nThe newly added code to handle bpf_get_branch_snapshot fails to link when\nCONFIG_PERF_EVENTS is disabled:\n\naarch64-linux-ld: kernel/bpf/verifier.o: in function `do_misc_fixups':\nverifier.c:(.text+0x1090c): undefined reference to `__SCK__perf_snapshot_branch_stack'\n\nAdd a build-time check for that Kconfig symbol around the code to\nremove the link time dependency.\n\nFixes: 314a53623cd4 (\"bpf: inline bpf_get_branch_snapshot() helper\")\nSigned-off-by: Arnd Bergmann <arnd@arndb.de>\nLink: https://lore.kernel.org/r/20240405142637.577046-1-arnd@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1f2a74b41ea8b902687eb97c4e7e3f558801865b",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-04-04 18:31:08 -0700",
      "message": "bpf: prevent r10 register from being marked as precise\n\nr10 is a special register that is not under BPF program's control and is\nalways effectively precise. The rest of precision logic assumes that\nonly r0-r9 SCALAR registers are marked as precise, so prevent r10 from\nbeing marked precise.\n\nThis can happen due to signed cast instruction allowing to do something\nlike `r0 = (s8)r10;`, which later, if r0 needs to be precise, would lead\nto an attempt to mark r10 as precise.\n\nPrevent this with an extra check during instruction backtracking.\n\nFixes: 8100928c8814 (\"bpf: Support new sign-extension mov insns\")\nReported-by: syzbot+148110ee7cf72f39f33e@syzkaller.appspotmail.com\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240404214536.3551295-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "314a53623cd4e62e1b88126e5ed2bc87073d90ee",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-04-04 13:08:01 -0700",
      "message": "bpf: inline bpf_get_branch_snapshot() helper\n\nInline bpf_get_branch_snapshot() helper using architecture-agnostic\ninline BPF code which calls directly into underlying callback of\nperf_snapshot_branch_stack static call. This callback is set early\nduring kernel initialization and is never updated or reset, so it's ok\nto fetch actual implementation using static_call_query() and call\ndirectly into it.\n\nThis change eliminates a full function call and saves one LBR entry\nin PERF_SAMPLE_BRANCH_ANY LBR mode.\n\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240404002640.1774210-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "af682b767a41772499f8e54ca7d7e1deb3395f44",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-04-04 16:13:26 +0200",
      "message": "bpf: Optimize emit_mov_imm64().\n\nTurned out that bpf prog callback addresses, bpf prog addresses\nused in bpf_trampoline, and in other cases the 64-bit address\ncan be represented as sign extended 32-bit value.\n\nAccording to https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82339\n\"Skylake has 0.64c throughput for mov r64, imm64, vs. 0.25 for mov r32, imm32.\"\nSo use shorter encoding and faster instruction when possible.\n\nSpecial care is needed in jit_subprogs(), since bpf_pseudo_func()\ninstruction cannot change its size during the last step of JIT.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/CAADnVQKFfpY-QZBrOU2CG8v2du8Lgyb7MNVmOZVK_yTyOdNbBA@mail.gmail.com\nLink: https://lore.kernel.org/bpf/20240401233800.42737-1-alexei.starovoitov@gmail.com",
      "modified_files": [
        "arch/x86/net/bpf_jit_comp.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1ae6921009e5d72787e07ccc04754514ccf6bc99",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-04-03 10:29:56 -0700",
      "message": "bpf: inline bpf_get_smp_processor_id() helper\n\nIf BPF JIT supports per-CPU MOV instruction, inline bpf_get_smp_processor_id()\nto eliminate unnecessary function calls.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/r/20240402021307.1012571-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "9dc182c58b5f5d4ac125ac85ad553f7142aa08d4",
      "author": "Anton Protopopov <aspsk@isovalent.com>",
      "date": "2024-04-02 16:12:00 +0200",
      "message": "bpf: Add a verbose message if map limit is reached\n\nWhen more than 64 maps are used by a program and its subprograms the\nverifier returns -E2BIG. Add a verbose message which highlights the\nsource of the error and also print the actual limit.\n\nSigned-off-by: Anton Protopopov <aspsk@isovalent.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240402073347.195920-1-aspsk@isovalent.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "6dae957c8eef6eae5b386462767de97303235d5c",
      "author": "Anton Protopopov <aspsk@isovalent.com>",
      "date": "2024-03-29 09:19:55 -0700",
      "message": "bpf: fix possible file descriptor leaks in verifier\n\nThe resolve_pseudo_ldimm64() function might have leaked file\ndescriptors when BPF_MAP_TYPE_ARENA was used in a program (some\nerror paths missed a corresponding fdput). Add missing fdputs.\n\nv2:\n  remove unrelated changes from the fix\n\nFixes: 6082b6c328b5 (\"bpf: Recognize addr_space_cast instruction in the verifier.\")\nSigned-off-by: Anton Protopopov <aspsk@isovalent.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20240329071106.67968-1-aspsk@isovalent.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "786bf0e7e2ec90b349a7bab6e97947982ab31f2c",
      "author": "Mykyta Yatsenko <yatsenko@meta.com>",
      "date": "2024-03-28 18:30:53 -0700",
      "message": "bpf: improve error message for unsupported helper\n\nBPF verifier emits \"unknown func\" message when given BPF program type\ndoes not support BPF helper. This message may be confusing for users, as\nimportant context that helper is unknown only to current program type is\nnot provided.\n\nThis patch changes message to \"program of this type cannot use helper \"\nand aligns dependent code in libbpf and tests. Any suggestions on\nimproving/changing this message are welcome.\n\nSigned-off-by: Mykyta Yatsenko <yatsenko@meta.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Quentin Monnet <qmo@kernel.org>\nLink: https://lore.kernel.org/r/20240325152210.377548-1-yatsenko@meta.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/bpf/bpftool/feature.c",
        "tools/lib/bpf/libbpf_probes.c",
        "tools/testing/selftests/bpf/prog_tests/bpf_tcp_ca.c",
        "tools/testing/selftests/bpf/progs/verifier_helper_restricted.c"
      ]
    },
    {
      "hash": "ecc6a2101840177e57c925c102d2d29f260d37c8",
      "author": "Andrei Matei <andreimatei1@gmail.com>",
      "date": "2024-03-27 09:56:36 -0700",
      "message": "bpf: Protect against int overflow for stack access size\n\nThis patch re-introduces protection against the size of access to stack\nmemory being negative; the access size can appear negative as a result\nof overflowing its signed int representation. This should not actually\nhappen, as there are other protections along the way, but we should\nprotect against it anyway. One code path was missing such protections\n(fixed in the previous patch in the series), causing out-of-bounds array\naccesses in check_stack_range_initialized(). This patch causes the\nverification of a program with such a non-sensical access size to fail.\n\nThis check used to exist in a more indirect way, but was inadvertendly\nremoved in a833a17aeac7.\n\nFixes: a833a17aeac7 (\"bpf: Fix verification of indirect var-off stack access\")\nReported-by: syzbot+33f4297b5f927648741a@syzkaller.appspotmail.com\nReported-by: syzbot+aafd0513053a1cbf52ef@syzkaller.appspotmail.com\nCloses: https://lore.kernel.org/bpf/CAADnVQLORV5PT0iTAhRER+iLBTkByCYNBYyvBSgjN1T31K+gOw@mail.gmail.com/\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nLink: https://lore.kernel.org/r/20240327024245.318299-3-andreimatei1@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "122fdbd2a030a95128737fc77e47df15a8f170c3",
      "author": "Puranjay Mohan <puranjay12@gmail.com>",
      "date": "2024-03-22 20:44:09 -0700",
      "message": "bpf: verifier: reject addr_space_cast insn without arena\n\nThe verifier allows using the addr_space_cast instruction in a program\nthat doesn't have an associated arena. This was caught in the form an\ninvalid memory access in do_misc_fixups() when while converting\naddr_space_cast to a normal 32-bit mov, env->prog->aux->arena was\ndereferenced to check for BPF_F_NO_USER_CONV flag.\n\nReject programs that include the addr_space_cast instruction but don't\nhave an associated arena.\n\nroot@rv-tester:~# ./reproducer\n Unable to handle kernel access to user memory without uaccess routines at virtual address 0000000000000030\n Oops [#1]\n [<ffffffff8017eeaa>] do_misc_fixups+0x43c/0x1168\n [<ffffffff801936d6>] bpf_check+0xda8/0x22b6\n [<ffffffff80174b32>] bpf_prog_load+0x486/0x8dc\n [<ffffffff80176566>] __sys_bpf+0xbd8/0x214e\n [<ffffffff80177d14>] __riscv_sys_bpf+0x22/0x2a\n [<ffffffff80d2493a>] do_trap_ecall_u+0x102/0x17c\n [<ffffffff80d3048c>] ret_from_exception+0x0/0x64\n\nFixes: 6082b6c328b5 (\"bpf: Recognize addr_space_cast instruction in the verifier.\")\nReported-by: xingwei lee <xrivendell7@gmail.com>\nReported-by: yue sun <samsun1006219@gmail.com>\nCloses: https://lore.kernel.org/bpf/CABOYnLz09O1+2gGVJuCxd_24a-7UueXzV-Ff+Fr+h5EKFDiYCQ@mail.gmail.com/\nSigned-off-by: Puranjay Mohan <puranjay12@gmail.com>\nLink: https://lore.kernel.org/r/20240322153518.11555-1-puranjay12@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "f7f5d1808b1b66935a24dd796dd1a0612ca9c147",
      "author": "Puranjay Mohan <puranjay12@gmail.com>",
      "date": "2024-03-22 20:36:36 -0700",
      "message": "bpf: verifier: fix addr_space_cast from as(1) to as(0)\n\nThe verifier currently converts addr_space_cast from as(1) to as(0) that\nis: BPF_ALU64 | BPF_MOV | BPF_X with off=1 and imm=1\nto\nBPF_ALU | BPF_MOV | BPF_X with imm=1 (32-bit mov)\n\nBecause of this imm=1, the JITs that have bpf_jit_needs_zext() == true,\ninterpret the converted instruction as BPF_ZEXT_REG(DST) which is a\nspecial form of mov32, used for doing explicit zero extension on dst.\nThese JITs will just zero extend the dst reg and will not move the src to\ndst before the zext.\n\nFix do_misc_fixups() to set imm=0 when converting addr_space_cast to a\nnormal mov32.\n\nThe JITs that have bpf_jit_needs_zext() == true rely on the verifier to\nemit zext instructions. Mark dst_reg as subreg when doing cast from\nas(1) to as(0) so the verifier emits a zext instruction after the mov.\n\nFixes: 6082b6c328b5 (\"bpf: Recognize addr_space_cast instruction in the verifier.\")\nSigned-off-by: Puranjay Mohan <puranjay12@gmail.com>\nLink: https://lore.kernel.org/r/20240321153939.113996-1-puranjay12@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "4c2a26fc80bcb851dc630590f2eec157991eccbf",
      "author": "Harishankar Vishwanathan <harishankar.vishwanathan@gmail.com>",
      "date": "2024-03-21 11:56:26 -0700",
      "message": "bpf-next: Avoid goto in regs_refine_cond_op()\n\nIn case of GE/GT/SGE/JST instructions, regs_refine_cond_op()\nreuses the logic that does analysis of LE/LT/SLE/SLT instructions.\nThis commit avoids the use of a goto to perform the reuse.\n\nSigned-off-by: Harishankar Vishwanathan <harishankar.vishwanathan@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240321002955.808604-1-harishankar.vishwanathan@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "7d2cc63eca0c993c99d18893214abf8f85d566d8",
      "author": "Christophe Leroy <christophe.leroy@csgroup.eu>",
      "date": "2024-03-14 19:28:52 -0700",
      "message": "bpf: Take return from set_memory_ro() into account with bpf_prog_lock_ro()\n\nset_memory_ro() can fail, leaving memory unprotected.\n\nCheck its return and take it into account as an error.\n\nLink: https://github.com/KSPP/linux/issues/7\nSigned-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nCc: linux-hardening@vger.kernel.org <linux-hardening@vger.kernel.org>\nReviewed-by: Kees Cook <keescook@chromium.org>\nMessage-ID: <286def78955e04382b227cb3e4b6ba272a7442e3.1709850515.git.christophe.leroy@csgroup.eu>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/filter.h",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "4d8926a0407cff0c864b759b59104f4fb6f8efab",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-03-14 19:28:16 -0700",
      "message": "bpf: preserve sleepable bit in subprog info\n\nCopy over main program's sleepable bit into subprog's info. This might\nbe important for, e.g., freplace cases.\n\nSuggested-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nMessage-ID: <20240314000127.3881569-1-andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "44d79142ede8162fd67bf8ca4ddbda1fbcfa94f1",
      "author": "Puranjay Mohan <puranjay12@gmail.com>",
      "date": "2024-03-14 12:04:45 -0700",
      "message": "bpf: Temporarily disable atomic operations in BPF arena\n\nCurrently, the x86 JIT handling PROBE_MEM32 tagged accesses is not\nequipped to handle atomic accesses into PTR_TO_ARENA, as no PROBE_MEM32\ntagging is performed and no handling is enabled for them.\n\nThis will lead to unsafety as the offset into arena will dereferenced\ndirectly without turning it into a base + offset access into the arena\nregion.\n\nSince the changes to the x86 JIT will be fairly involved, for now,\ntemporarily disallow use of PTR_TO_ARENA as the destination operand for\natomics until support is added to the JIT backend.\n\nFixes: 2fe99eb0ccf2 (\"bpf: Add x86-64 JIT support for PROBE_MEM32 pseudo instructions.\")\nReported-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Puranjay Mohan <puranjay12@gmail.com>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nMessage-ID: <20240314174931.98702-1-puranjay12@gmail.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "66c8473135c62f478301a0e5b3012f203562dfa6",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-03-11 16:41:25 -0700",
      "message": "bpf: move sleepable flag from bpf_prog_aux to bpf_prog\n\nprog->aux->sleepable is checked very frequently as part of (some) BPF\nprogram run hot paths. So this extra aux indirection seems wasteful and\non busy systems might cause unnecessary memory cache misses.\n\nLet's move sleepable flag into prog itself to eliminate unnecessary\npointer dereference.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nMessage-ID: <20240309004739.2961431-1-andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/bpf_iter.c",
        "kernel/bpf/core.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/trampoline.c",
        "kernel/bpf/verifier.c",
        "kernel/events/core.c",
        "kernel/trace/bpf_trace.c",
        "net/bpf/bpf_dummy_struct_ops.c"
      ]
    },
    {
      "hash": "2edc3de6fb650924a87fffebebc3b7572cbf6e38",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-03-11 15:37:24 -0700",
      "message": "bpf: Recognize btf_decl_tag(\"arg: Arena\") as PTR_TO_ARENA.\n\nIn global bpf functions recognize btf_decl_tag(\"arg:arena\") as PTR_TO_ARENA.\n\nNote, when the verifier sees:\n\n__weak void foo(struct bar *p)\n\nit recognizes 'p' as PTR_TO_MEM and 'struct bar' has to be a struct with scalars.\nHence the only way to use arena pointers in global functions is to tag them with \"arg:arena\".\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-7-alexei.starovoitov@gmail.com",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "6082b6c328b5486da2b356eae94b8b83c98b5565",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-03-11 15:37:24 -0700",
      "message": "bpf: Recognize addr_space_cast instruction in the verifier.\n\nrY = addr_space_cast(rX, 0, 1) tells the verifier that rY->type = PTR_TO_ARENA.\nAny further operations on PTR_TO_ARENA register have to be in 32-bit domain.\n\nThe verifier will mark load/store through PTR_TO_ARENA with PROBE_MEM32.\nJIT will generate them as kern_vm_start + 32bit_addr memory accesses.\n\nrY = addr_space_cast(rX, 1, 0) tells the verifier that rY->type = unknown scalar.\nIf arena->map_flags has BPF_F_NO_USER_CONV set then convert cast_user to mov32 as well.\nOtherwise JIT will convert it to:\n  rY = (u32)rX;\n  if (rY)\n     rY |= arena->user_vm_start & ~(u64)~0U;\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-6-alexei.starovoitov@gmail.com",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "kernel/bpf/log.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "317460317a02a1af512697e6e964298dedd8a163",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-03-11 15:37:23 -0700",
      "message": "bpf: Introduce bpf_arena.\n\nIntroduce bpf_arena, which is a sparse shared memory region between the bpf\nprogram and user space.\n\nUse cases:\n1. User space mmap-s bpf_arena and uses it as a traditional mmap-ed\n   anonymous region, like memcached or any key/value storage. The bpf\n   program implements an in-kernel accelerator. XDP prog can search for\n   a key in bpf_arena and return a value without going to user space.\n2. The bpf program builds arbitrary data structures in bpf_arena (hash\n   tables, rb-trees, sparse arrays), while user space consumes it.\n3. bpf_arena is a \"heap\" of memory from the bpf program's point of view.\n   The user space may mmap it, but bpf program will not convert pointers\n   to user base at run-time to improve bpf program speed.\n\nInitially, the kernel vm_area and user vma are not populated. User space\ncan fault in pages within the range. While servicing a page fault,\nbpf_arena logic will insert a new page into the kernel and user vmas. The\nbpf program can allocate pages from that region via\nbpf_arena_alloc_pages(). This kernel function will insert pages into the\nkernel vm_area. The subsequent fault-in from user space will populate that\npage into the user vma. The BPF_F_SEGV_ON_FAULT flag at arena creation time\ncan be used to prevent fault-in from user space. In such a case, if a page\nis not allocated by the bpf program and not present in the kernel vm_area,\nthe user process will segfault. This is useful for use cases 2 and 3 above.\n\nbpf_arena_alloc_pages() is similar to user space mmap(). It allocates pages\neither at a specific address within the arena or allocates a range with the\nmaple tree. bpf_arena_free_pages() is analogous to munmap(), which frees\npages and removes the range from the kernel vm_area and from user process\nvmas.\n\nbpf_arena can be used as a bpf program \"heap\" of up to 4GB. The speed of\nbpf program is more important than ease of sharing with user space. This is\nuse case 3. In such a case, the BPF_F_NO_USER_CONV flag is recommended.\nIt will tell the verifier to treat the rX = bpf_arena_cast_user(rY)\ninstruction as a 32-bit move wX = wY, which will improve bpf prog\nperformance. Otherwise, bpf_arena_cast_user is translated by JIT to\nconditionally add the upper 32 bits of user vm_start (if the pointer is not\nNULL) to arena pointers before they are stored into memory. This way, user\nspace sees them as valid 64-bit pointers.\n\nDiff https://github.com/llvm/llvm-project/pull/84410 enables LLVM BPF\nbackend generate the bpf_addr_space_cast() instruction to cast pointers\nbetween address_space(1) which is reserved for bpf_arena pointers and\ndefault address space zero. All arena pointers in a bpf program written in\nC language are tagged as __attribute__((address_space(1))). Hence, clang\nprovides helpful diagnostics when pointers cross address space. Libbpf and\nthe kernel support only address_space == 1. All other address space\nidentifiers are reserved.\n\nrX = bpf_addr_space_cast(rY, /* dst_as */ 1, /* src_as */ 0) tells the\nverifier that rX->type = PTR_TO_ARENA. Any further operations on\nPTR_TO_ARENA register have to be in the 32-bit domain. The verifier will\nmark load/store through PTR_TO_ARENA with PROBE_MEM32. JIT will generate\nthem as kern_vm_start + 32bit_addr memory accesses. The behavior is similar\nto copy_from_kernel_nofault() except that no address checks are necessary.\nThe address is guaranteed to be in the 4GB range. If the page is not\npresent, the destination register is zeroed on read, and the operation is\nignored on write.\n\nrX = bpf_addr_space_cast(rY, 0, 1) tells the verifier that rX->type =\nunknown scalar. If arena->map_flags has BPF_F_NO_USER_CONV set, then the\nverifier converts such cast instructions to mov32. Otherwise, JIT will emit\nnative code equivalent to:\nrX = (u32)rY;\nif (rY)\n  rX |= clear_lo32_bits(arena->user_vm_start); /* replace hi32 bits in rX */\n\nAfter such conversion, the pointer becomes a valid user pointer within\nbpf_arena range. The user process can access data structures created in\nbpf_arena without any additional computations. For example, a linked list\nbuilt by a bpf program can be walked natively by user space.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nReviewed-by: Barret Rhoden <brho@google.com>\nLink: https://lore.kernel.org/bpf/20240308010812.89848-2-alexei.starovoitov@gmail.com",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_types.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/Makefile",
        "kernel/bpf/arena.c",
        "kernel/bpf/core.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c",
        "tools/include/uapi/linux/bpf.h"
      ]
    },
    {
      "hash": "8d94f1357c00d7706c1f3d0bb568e054cef6aea1",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-03-07 14:58:48 -0800",
      "message": "bpf: Recognize '__map' suffix in kfunc arguments\n\nRecognize 'void *p__map' kfunc argument as 'struct bpf_map *p__map'.\nIt allows kfunc to have 'void *' argument for maps, since bpf progs\nwill call them as:\nstruct {\n        __uint(type, BPF_MAP_TYPE_ARENA);\n\t...\n} arena SEC(\".maps\");\n\nbpf_kfunc_with_map(... &arena ...);\n\nUnderneath libbpf will load CONST_PTR_TO_MAP into the register via ld_imm64\ninsn. If kfunc was defined with 'struct bpf_map *' it would pass the\nverifier as well, but bpf prog would need to type cast the argument\n(void *)&arena, which is not clean.\n\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/r/20240307031228.42896-3-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "88d1d4a7eebea2836859246d91fe9d141789dfc3",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-03-07 14:58:48 -0800",
      "message": "bpf: Allow kfuncs return 'void *'\n\nRecognize return of 'void *' from kfunc as returning unknown scalar.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/r/20240307031228.42896-2-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "4f81c16f50baf6d5d8bfa6eef3250dcfa22cbc08",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-03-06 15:18:00 -0800",
      "message": "bpf: Recognize that two registers are safe when their ranges match\n\nWhen open code iterators, bpf_loop or may_goto are used the following two\nstates are equivalent and safe to prune the search:\n\ncur state: fp-8_w=scalar(id=3,smin=umin=smin32=umin32=2,smax=umax=smax32=umax32=11,var_off=(0x0; 0xf))\nold state: fp-8_rw=scalar(id=2,smin=umin=smin32=umin32=1,smax=umax=smax32=umax32=11,var_off=(0x0; 0xf))\n\nIn other words \"exact\" state match should ignore liveness and precision\nmarks, since open coded iterator logic didn't complete their propagation,\nreg_old->type == NOT_INIT && reg_cur->type != NOT_INIT is also not safe to\nprune while looping, but range_within logic that applies to scalars,\nptr_to_mem, map_value, pkt_ptr is safe to rely on.\n\nAvoid doing such comparison when regular infinite loop detection logic is\nused, otherwise bounded loop logic will declare such \"infinite loop\" as\nfalse positive. Such example is in progs/verifier_loops1.c\nnot_an_inifinite_loop().\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nTested-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240306031929.42666-3-alexei.starovoitov@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "011832b97b311bb9e3c27945bc0d1089a14209c9",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2024-03-06 15:17:31 -0800",
      "message": "bpf: Introduce may_goto instruction\n\nIntroduce may_goto instruction that from the verifier pov is similar to\nopen coded iterators bpf_for()/bpf_repeat() and bpf_loop() helper, but it\ndoesn't iterate any objects.\nIn assembly 'may_goto' is a nop most of the time until bpf runtime has to\nterminate the program for whatever reason. In the current implementation\nmay_goto has a hidden counter, but other mechanisms can be used.\nFor programs written in C the later patch introduces 'cond_break' macro\nthat combines 'may_goto' with 'break' statement and has similar semantics:\ncond_break is a nop until bpf runtime has to break out of this loop.\nIt can be used in any normal \"for\" or \"while\" loop, like\n\n  for (i = zero; i < cnt; cond_break, i++) {\n\nThe verifier recognizes that may_goto is used in the program, reserves\nadditional 8 bytes of stack, initializes them in subprog prologue, and\nreplaces may_goto instruction with:\naux_reg = *(u64 *)(fp - 40)\nif aux_reg == 0 goto pc+off\naux_reg -= 1\n*(u64 *)(fp - 40) = aux_reg\n\nmay_goto instruction can be used by LLVM to implement __builtin_memcpy,\n__builtin_strcmp.\n\nmay_goto is not a full substitute for bpf_for() macro.\nbpf_for() doesn't have induction variable that verifiers sees,\nso 'i' in bpf_for(i, 0, 100) is seen as imprecise and bounded.\n\nBut when the code is written as:\nfor (i = 0; i < 100; cond_break, i++)\nthe verifier see 'i' as precise constant zero,\nhence cond_break (aka may_goto) doesn't help to converge the loop.\nA static or global variable can be used as a workaround:\nstatic int zero = 0;\nfor (i = zero; i < 100; cond_break, i++) // works!\n\nmay_goto works well with arena pointers that don't need to be bounds\nchecked on access. Load/store from arena returns imprecise unbounded\nscalar and loops with may_goto pass the verifier.\n\nReserve new opcode BPF_JMP | BPF_JCOND for may_goto insn.\nJCOND stands for conditional pseudo jump.\nSince goto_or_nop insn was proposed, it may use the same opcode.\nmay_goto vs goto_or_nop can be distinguished by src_reg:\ncode = BPF_JMP | BPF_JCOND\nsrc_reg = 0 - may_goto\nsrc_reg = 1 - goto_or_nop\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nTested-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20240306031929.42666-2-alexei.starovoitov@gmail.com",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/core.c",
        "kernel/bpf/disasm.c",
        "kernel/bpf/verifier.c",
        "tools/include/uapi/linux/bpf.h"
      ]
    },
    {
      "hash": "e9a8e5a587ca55fec6c58e4881742705d45bee54",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-03-05 16:15:56 -0800",
      "message": "bpf: check bpf_func_state->callback_depth when pruning states\n\nWhen comparing current and cached states verifier should consider\nbpf_func_state->callback_depth. Current state cannot be pruned against\ncached state, when current states has more iterations left compared to\ncached state. Current state has more iterations left when it's\ncallback_depth is smaller.\n\nBelow is an example illustrating this bug, minimized from mailing list\ndiscussion [0] (assume that BPF_F_TEST_STATE_FREQ is set).\nThe example is not a safe program: if loop_cb point (1) is followed by\nloop_cb point (2), then division by zero is possible at point (4).\n\n    struct ctx {\n    \t__u64 a;\n    \t__u64 b;\n    \t__u64 c;\n    };\n\n    static void loop_cb(int i, struct ctx *ctx)\n    {\n    \t/* assume that generated code is \"fallthrough-first\":\n    \t * if ... == 1 goto\n    \t * if ... == 2 goto\n    \t * <default>\n    \t */\n    \tswitch (bpf_get_prandom_u32()) {\n    \tcase 1:  /* 1 */ ctx->a = 42; return 0; break;\n    \tcase 2:  /* 2 */ ctx->b = 42; return 0; break;\n    \tdefault: /* 3 */ ctx->c = 42; return 0; break;\n    \t}\n    }\n\n    SEC(\"tc\")\n    __failure\n    __flag(BPF_F_TEST_STATE_FREQ)\n    int test(struct __sk_buff *skb)\n    {\n    \tstruct ctx ctx = { 7, 7, 7 };\n\n    \tbpf_loop(2, loop_cb, &ctx, 0);              /* 0 */\n    \t/* assume generated checks are in-order: .a first */\n    \tif (ctx.a == 42 && ctx.b == 42 && ctx.c == 7)\n    \t\tasm volatile(\"r0 /= 0;\":::\"r0\");    /* 4 */\n    \treturn 0;\n    }\n\nPrior to this commit verifier built the following checkpoint tree for\nthis example:\n\n .------------------------------------- Checkpoint / State name\n |    .-------------------------------- Code point number\n |    |   .---------------------------- Stack state {ctx.a,ctx.b,ctx.c}\n |    |   |        .------------------- Callback depth in frame #0\n v    v   v        v\n   - (0) {7P,7P,7},depth=0\n     - (3) {7P,7P,7},depth=1\n       - (0) {7P,7P,42},depth=1\n         - (3) {7P,7,42},depth=2\n           - (0) {7P,7,42},depth=2      loop terminates because of depth limit\n             - (4) {7P,7,42},depth=0    predicted false, ctx.a marked precise\n             - (6) exit\n(a)      - (2) {7P,7,42},depth=2\n           - (0) {7P,42,42},depth=2     loop terminates because of depth limit\n             - (4) {7P,42,42},depth=0   predicted false, ctx.a marked precise\n             - (6) exit\n(b)      - (1) {7P,7P,42},depth=2\n           - (0) {42P,7P,42},depth=2    loop terminates because of depth limit\n             - (4) {42P,7P,42},depth=0  predicted false, ctx.{a,b} marked precise\n             - (6) exit\n     - (2) {7P,7,7},depth=1             considered safe, pruned using checkpoint (a)\n(c)  - (1) {7P,7P,7},depth=1            considered safe, pruned using checkpoint (b)\n\nHere checkpoint (b) has callback_depth of 2, meaning that it would\nnever reach state {42,42,7}.\nWhile checkpoint (c) has callback_depth of 1, and thus\ncould yet explore the state {42,42,7} if not pruned prematurely.\nThis commit makes forbids such premature pruning,\nallowing verifier to explore states sub-tree starting at (c):\n\n(c)  - (1) {7,7,7P},depth=1\n       - (0) {42P,7,7P},depth=1\n         ...\n         - (2) {42,7,7},depth=2\n           - (0) {42,42,7},depth=2      loop terminates because of depth limit\n             - (4) {42,42,7},depth=0    predicted true, ctx.{a,b,c} marked precise\n               - (5) division by zero\n\n[0] https://lore.kernel.org/bpf/9b251840-7cb8-4d17-bd23-1fc8071d8eef@linux.dev/\n\nFixes: bb124da69c47 (\"bpf: keep track of max number of bpf_loop callback iterations\")\nSuggested-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240222154121.6991-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "2ab256e93249f5ac1da665861aa0f03fb4208d9c",
      "author": "Benjamin Tissoires <bentiss@kernel.org>",
      "date": "2024-02-22 17:48:53 -0800",
      "message": "bpf: add is_async_callback_calling_insn() helper\n\nCurrently we have a special case for BPF_FUNC_timer_set_callback,\nlet's introduce a helper we can extend for the kfunc that will come in\na later patch\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240221-hid-bpf-sleepable-v3-3-1fb378ca6301@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "dfe6625df48ec54c6dc9b86d361f26962d09de88",
      "author": "Benjamin Tissoires <bentiss@kernel.org>",
      "date": "2024-02-22 17:47:15 -0800",
      "message": "bpf: introduce in_sleepable() helper\n\nNo code change, but it'll allow to have only one place to change\neverything when we add in_sleepable in cur_state.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240221-hid-bpf-sleepable-v3-2-1fb378ca6301@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "55bad79e33aeb670317290158a4b2ff71cdc8380",
      "author": "Benjamin Tissoires <bentiss@kernel.org>",
      "date": "2024-02-22 17:42:23 -0800",
      "message": "bpf: allow more maps in sleepable bpf programs\n\nThese 2 maps types are required for HID-BPF when a user wants to do\nIO with a device from a sleepable tracing point.\n\nAllowing BPF_MAP_TYPE_QUEUE (and therefore BPF_MAP_TYPE_STACK) allows\nfor a BPF program to prepare from an IRQ the list of HID commands to send\nback to the device and then these commands can be retrieved from the\nsleepable trace point.\n\nSigned-off-by: Benjamin Tissoires <bentiss@kernel.org>\nLink: https://lore.kernel.org/r/20240221-hid-bpf-sleepable-v3-1-1fb378ca6301@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "682158ab532a5bd24399fec25b65fec561f0f6e9",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-02-15 13:45:27 -0800",
      "message": "bpf: Fix test verif_scale_strobemeta_subprogs failure due to llvm19\n\nWith latest llvm19, I hit the following selftest failures with\n\n  $ ./test_progs -j\n  libbpf: prog 'on_event': BPF program load failed: Permission denied\n  libbpf: prog 'on_event': -- BEGIN PROG LOAD LOG --\n  combined stack size of 4 calls is 544. Too large\n  verification time 1344153 usec\n  stack depth 24+440+0+32\n  processed 51008 insns (limit 1000000) max_states_per_insn 19 total_states 1467 peak_states 303 mark_read 146\n  -- END PROG LOAD LOG --\n  libbpf: prog 'on_event': failed to load: -13\n  libbpf: failed to load object 'strobemeta_subprogs.bpf.o'\n  scale_test:FAIL:expect_success unexpected error: -13 (errno 13)\n  #498     verif_scale_strobemeta_subprogs:FAIL\n\nThe verifier complains too big of the combined stack size (544 bytes) which\nexceeds the maximum stack limit 512. This is a regression from llvm19 ([1]).\n\nIn the above error log, the original stack depth is 24+440+0+32.\nTo satisfy interpreter's need, in verifier the stack depth is adjusted to\n32+448+32+32=544 which exceeds 512, hence the error. The same adjusted\nstack size is also used for jit case.\n\nBut the jitted codes could use smaller stack size.\n\n  $ egrep -r stack_depth | grep round_up\n  arm64/net/bpf_jit_comp.c:       ctx->stack_size = round_up(prog->aux->stack_depth, 16);\n  loongarch/net/bpf_jit.c:        bpf_stack_adjust = round_up(ctx->prog->aux->stack_depth, 16);\n  powerpc/net/bpf_jit_comp.c:     cgctx.stack_size = round_up(fp->aux->stack_depth, 16);\n  riscv/net/bpf_jit_comp32.c:             round_up(ctx->prog->aux->stack_depth, STACK_ALIGN);\n  riscv/net/bpf_jit_comp64.c:     bpf_stack_adjust = round_up(ctx->prog->aux->stack_depth, 16);\n  s390/net/bpf_jit_comp.c:        u32 stack_depth = round_up(fp->aux->stack_depth, 8);\n  sparc/net/bpf_jit_comp_64.c:            stack_needed += round_up(stack_depth, 16);\n  x86/net/bpf_jit_comp.c:         EMIT3_off32(0x48, 0x81, 0xEC, round_up(stack_depth, 8));\n  x86/net/bpf_jit_comp.c: int tcc_off = -4 - round_up(stack_depth, 8);\n  x86/net/bpf_jit_comp.c:                     round_up(stack_depth, 8));\n  x86/net/bpf_jit_comp.c: int tcc_off = -4 - round_up(stack_depth, 8);\n  x86/net/bpf_jit_comp.c:         EMIT3_off32(0x48, 0x81, 0xC4, round_up(stack_depth, 8));\n\nIn the above, STACK_ALIGN in riscv/net/bpf_jit_comp32.c is defined as 16.\nSo stack is aligned in either 8 or 16, x86/s390 having 8-byte stack alignment and\nthe rest having 16-byte alignment.\n\nThis patch calculates total stack depth based on 16-byte alignment if jit is requested.\nFor the above failing case, the new stack size will be 32+448+0+32=512 and no verification\nfailure. llvm19 regression will be discussed separately in llvm upstream.\n\nThe verifier change caused three test failures as these tests compared messages\nwith stack size. More specifically,\n  - test_global_funcs/global_func1: fail with interpreter mode and success with jit mode.\n    Adjusted stack sizes so both jit and interpreter modes will fail.\n  - async_stack_depth/{pseudo_call_check, async_call_root_check}: since jit and interpreter\n    will calculate different stack sizes, the failure msg is adjusted to omit those\n    specific stack size numbers.\n\n  [1] https://lore.kernel.org/bpf/32bde0f0-1881-46c9-931a-673be566c61d@linux.dev/\n\nSuggested-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20240214232951.4113094-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/async_stack_depth.c",
        "tools/testing/selftests/bpf/progs/test_global_func1.c"
      ]
    },
    {
      "hash": "fb5b86cfd4ef21ea18966718f6bf6c8f1b9df12e",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-02-13 18:46:46 -0800",
      "message": "bpf: simplify btf_get_prog_ctx_type() into btf_is_prog_ctx_type()\n\nReturn result of btf_get_prog_ctx_type() is never used and callers only\ncheck NULL vs non-NULL case to determine if given type matches expected\nPTR_TO_CTX type. So rename function to `btf_is_prog_ctx_type()` and\nreturn a simple true/false. We'll use this simpler interface to handle\nkprobe program type's special typedef case in the next patch.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240212233221.2575350-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/btf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1611603537a4b88cec7993f32b70c03113801a46",
      "author": "Kui-Feng Lee <thinker.li@gmail.com>",
      "date": "2024-02-13 15:16:44 -0800",
      "message": "bpf: Create argument information for nullable arguments.\n\nCollect argument information from the type information of stub functions to\nmark arguments of BPF struct_ops programs with PTR_MAYBE_NULL if they are\nnullable.  A nullable argument is annotated by suffixing \"__nullable\" at\nthe argument name of stub function.\n\nFor nullable arguments, this patch sets a struct bpf_ctx_arg_aux to label\ntheir reg_type with PTR_TO_BTF_ID | PTR_TRUSTED | PTR_MAYBE_NULL. This\nmakes the verifier to check programs and ensure that they properly check\nthe pointer. The programs should check if the pointer is null before\naccessing the pointed memory.\n\nThe implementer of a struct_ops type should annotate the arguments that can\nbe null. The implementer should define a stub function (empty) as a\nplaceholder for each defined operator. The name of a stub function should\nbe in the pattern \"<st_op_type>__<operator name>\". For example, for\ntest_maybe_null of struct bpf_testmod_ops, it's stub function name should\nbe \"bpf_testmod_ops__test_maybe_null\". You mark an argument nullable by\nsuffixing the argument name with \"__nullable\" at the stub function.\n\nSince we already has stub functions for kCFI, we just reuse these stub\nfunctions with the naming convention mentioned earlier. These stub\nfunctions with the naming convention is only required if there are nullable\narguments to annotate. For functions having not nullable arguments, stub\nfunctions are not necessary for the purpose of this patch.\n\nThis patch will prepare a list of struct bpf_ctx_arg_aux, aka arg_info, for\neach member field of a struct_ops type.  \"arg_info\" will be assigned to\n\"prog->aux->ctx_arg_info\" of BPF struct_ops programs in\ncheck_struct_ops_btf_id() so that it can be used by btf_ctx_access() later\nto set reg_type properly for the verifier.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240209023750.1153905-4-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/btf.h",
        "kernel/bpf/bpf_struct_ops.c",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "6115a0aeef01aef152ad7738393aad11422bfb82",
      "author": "Kui-Feng Lee <thinker.li@gmail.com>",
      "date": "2024-02-13 15:16:44 -0800",
      "message": "bpf: Move __kfunc_param_match_suffix() to btf.c.\n\nMove __kfunc_param_match_suffix() to btf.c and rename it as\nbtf_param_match_suffix(). It can be reused by bpf_struct_ops later.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240209023750.1153905-3-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "include/linux/btf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "11f522256e9043b0fcd2f994278645d3e201d20c",
      "author": "Hari Bathini <hbathini@linux.ibm.com>",
      "date": "2024-02-13 11:13:39 -0800",
      "message": "bpf: Fix warning for bpf_cpumask in verifier\n\nCompiling with CONFIG_BPF_SYSCALL & !CONFIG_BPF_JIT throws the below\nwarning:\n\n  \"WARN: resolve_btfids: unresolved symbol bpf_cpumask\"\n\nFix it by adding the appropriate #ifdef.\n\nSigned-off-by: Hari Bathini <hbathini@linux.ibm.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20240208100115.602172-1-hbathini@linux.ibm.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "6fceea0fa59f6786a2847a4cae409117624e8b58",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2024-02-05 20:00:14 -0800",
      "message": "bpf: Transfer RCU lock state between subprog calls\n\nAllow transferring an imbalanced RCU lock state between subprog calls\nduring verification. This allows patterns where a subprog call returns\nwith an RCU lock held, or a subprog call releases an RCU lock held by\nthe caller. Currently, the verifier would end up complaining if the RCU\nlock is not released when processing an exit from a subprog, which is\nnon-ideal if its execution is supposed to be enclosed in an RCU read\nsection of the caller.\n\nInstead, simply only check whether we are processing exit for frame#0\nand do not complain on an active RCU lock otherwise. We only need to\nupdate the check when processing BPF_EXIT insn, as copy_verifier_state\nis already set up to do the right thing.\n\nSuggested-by: David Vernet <void@manifault.com>\nTested-by: Yafang Shao <laoar.shao@gmail.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20240205055646.1112186-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "a44b1334aadd82203f661adb9adb41e53ad0e8d1",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2024-02-05 19:58:47 -0800",
      "message": "bpf: Allow calling static subprogs while holding a bpf_spin_lock\n\nCurrently, calling any helpers, kfuncs, or subprogs except the graph\ndata structure (lists, rbtrees) API kfuncs while holding a bpf_spin_lock\nis not allowed. One of the original motivations of this decision was to\nforce the BPF programmer's hand into keeping the bpf_spin_lock critical\nsection small, and to ensure the execution time of the program does not\nincrease due to lock waiting times. In addition to this, some of the\nhelpers and kfuncs may be unsafe to call while holding a bpf_spin_lock.\n\nHowever, when it comes to subprog calls, atleast for static subprogs,\nthe verifier is able to explore their instructions during verification.\nTherefore, it is similar in effect to having the same code inlined into\nthe critical section. Hence, not allowing static subprog calls in the\nbpf_spin_lock critical section is mostly an annoyance that needs to be\nworked around, without providing any tangible benefit.\n\nUnlike static subprog calls, global subprog calls are not safe to permit\nwithin the critical section, as the verifier does not explore them\nduring verification, therefore whether the same lock will be taken\nagain, or unlocked, cannot be ascertained.\n\nTherefore, allow calling static subprogs within a bpf_spin_lock critical\nsection, and only reject it in case the subprog linkage is global.\n\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: David Vernet <void@manifault.com>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20240204222349.938118-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_spin_lock.c"
      ]
    },
    {
      "hash": "8f13c34087d3eb64329529b8517e5a6251653176",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-02-02 18:08:58 -0800",
      "message": "bpf: handle trusted PTR_TO_BTF_ID_OR_NULL in argument check logic\n\nAdd PTR_TRUSTED | PTR_MAYBE_NULL modifiers for PTR_TO_BTF_ID to\ncheck_reg_type() to support passing trusted nullable PTR_TO_BTF_ID\nregisters into global functions accepting `__arg_trusted __arg_nullable`\narguments. This hasn't been caught earlier because tests were either\npassing known non-NULL PTR_TO_BTF_ID registers or known NULL (SCALAR)\nregisters.\n\nWhen utilizing this functionality in complicated real-world BPF\napplication that passes around PTR_TO_BTF_ID_OR_NULL, it became apparent\nthat verifier rejects valid case because check_reg_type() doesn't handle\nthis case explicitly. Existing check_reg_type() logic is already\nanticipating this combination, so we just need to explicitly list this\ncombo in the switch statement.\n\nFixes: e2b3c4ff5d18 (\"bpf: add __arg_trusted global func arg tag\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240202190529.2374377-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "6efbde200bf3cf2dbf6e7181893fed13a79c789b",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-02-02 13:22:14 -0800",
      "message": "bpf: Handle scalar spill vs all MISC in stacksafe()\n\nWhen check_stack_read_fixed_off() reads value from an spi\nall stack slots of which are set to STACK_{MISC,INVALID},\nthe destination register is set to unbound SCALAR_VALUE.\n\nExploit this fact by allowing stacksafe() to use a fake\nunbound scalar register to compare 'mmmm mmmm' stack value\nin old state vs spilled 64-bit scalar in current state\nand vice versa.\n\nVeristat results after this patch show some gains:\n\n./veristat -C -e file,prog,states -f 'states_pct>10'  not-opt after\nFile                     Program                States   (DIFF)\n-----------------------  ---------------------  ---------------\nbpf_overlay.o            tail_rev_nodeport_lb4    -45 (-15.85%)\nbpf_xdp.o                tail_lb_ipv4            -541 (-19.57%)\npyperf100.bpf.o          on_event                -680 (-10.42%)\npyperf180.bpf.o          on_event               -2164 (-19.62%)\npyperf600.bpf.o          on_event               -9799 (-24.84%)\nstrobemeta.bpf.o         on_event               -9157 (-65.28%)\nxdp_synproxy_kern.bpf.o  syncookie_tc             -54 (-19.29%)\nxdp_synproxy_kern.bpf.o  syncookie_xdp            -74 (-24.50%)\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240127175237.526726-6-maxtram95@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c1e6148cb4f83cec841db1f066e8db4a86c1f118",
      "author": "Maxim Mikityanskiy <maxim@isovalent.com>",
      "date": "2024-02-02 13:22:14 -0800",
      "message": "bpf: Preserve boundaries and track scalars on narrowing fill\n\nWhen the width of a fill is smaller than the width of the preceding\nspill, the information about scalar boundaries can still be preserved,\nas long as it's coerced to the right width (done by coerce_reg_to_size).\nEven further, if the actual value fits into the fill width, the ID can\nbe preserved as well for further tracking of equal scalars.\n\nImplement the above improvements, which makes narrowing fills behave the\nsame as narrowing spills and MOVs between registers.\n\nTwo tests are adjusted to accommodate for endianness differences and to\ntake into account that it's now allowed to do a narrowing fill from the\nleast significant bits.\n\nreg_bounds_sync is added to coerce_reg_to_size to correctly adjust\numin/umax boundaries after the var_off truncation, for example, a 64-bit\nvalue 0xXXXXXXXX00000000, when read as a 32-bit, gets umin = 0, umax =\n0xFFFFFFFF, var_off = (0x0; 0xffffffff00000000), which needs to be\nsynced down to umax = 0, otherwise reg_bounds_sanity_check doesn't pass.\n\nSigned-off-by: Maxim Mikityanskiy <maxim@isovalent.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20240127175237.526726-4-maxtram95@gmail.com",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
      ]
    },
    {
      "hash": "e67ddd9b1cff7872d43ead73a1403c4e532003d9",
      "author": "Maxim Mikityanskiy <maxim@isovalent.com>",
      "date": "2024-02-02 13:22:14 -0800",
      "message": "bpf: Track spilled unbounded scalars\n\nSupport the pattern where an unbounded scalar is spilled to the stack,\nthen boundary checks are performed on the src register, after which the\nstack frame slot is refilled into a register.\n\nBefore this commit, the verifier didn't treat the src register and the\nstack slot as related if the src register was an unbounded scalar. The\nregister state wasn't copied, the id wasn't preserved, and the stack\nslot was marked as STACK_MISC. Subsequent boundary checks on the src\nregister wouldn't result in updating the boundaries of the spilled\nvariable on the stack.\n\nAfter this commit, the verifier will preserve the bond between src and\ndst even if src is unbounded, which permits to do boundary checks on src\nand refill dst later, still remembering its boundaries. Such a pattern\nis sometimes generated by clang when compiling complex long functions.\n\nOne test is adjusted to reflect that now unbounded scalars are tracked.\n\nSigned-off-by: Maxim Mikityanskiy <maxim@isovalent.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20240127175237.526726-2-maxtram95@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
      ]
    },
    {
      "hash": "e2b3c4ff5d183da6d1863c2321413406a2752e7a",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-01-30 09:41:50 -0800",
      "message": "bpf: add __arg_trusted global func arg tag\n\nAdd support for passing PTR_TO_BTF_ID registers to global subprogs.\nCurrently only PTR_TRUSTED flavor of PTR_TO_BTF_ID is supported.\nNon-NULL semantics is assumed, so caller will be forced to prove\nPTR_TO_BTF_ID can't be NULL.\n\nNote, we disallow global subprogs to destroy passed in PTR_TO_BTF_ID\narguments, even the trusted one. We achieve that by not setting\nref_obj_id when validating subprog code. This basically enforces (in\nRust terms) borrowing semantics vs move semantics. Borrowing semantics\nseems to be a better fit for isolated global subprog validation\napproach.\n\nImplementation-wise, we utilize existing logic for matching\nuser-provided BTF type to kernel-side BTF type, used by BPF CO-RE logic\nand following same matching rules. We enforce a unique match for types.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240130000648.2144827-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "e6be8cd5d3cf54ccd0ae66027d6f4697b15f4c3e",
      "author": "Kui-Feng Lee <thinker.li@gmail.com>",
      "date": "2024-01-25 20:49:59 -0800",
      "message": "bpf: Fix error checks against bpf_get_btf_vmlinux().\n\nIn bpf_struct_ops_map_alloc, it needs to check for NULL in the returned\npointer of bpf_get_btf_vmlinux() when CONFIG_DEBUG_INFO_BTF is not set.\nENOTSUPP is used to preserve the same behavior before the\nstruct_ops kmod support.\n\nIn the function check_struct_ops_btf_id(), instead of redoing the\nbpf_get_btf_vmlinux() that has already been done in syscall.c, the fix\nhere is to check for prog->aux->attach_btf_id.\nBPF_PROG_TYPE_STRUCT_OPS must require attach_btf_id and syscall.c\nguarantees a valid attach_btf as long as attach_btf_id is set.\nWhen attach_btf_id is not set, this patch returns -ENOTSUPP\nbecause it is what the selftest in test_libbpf_probe_prog_types()\nand libbpf_probes.c are expecting for feature probing purpose.\n\nChanges from v1:\n\n - Remove an unnecessary NULL check in check_struct_ops_btf_id()\n\nReported-by: syzbot+88f0aafe5f950d7489d7@syzkaller.appspotmail.com\nCloses: https://lore.kernel.org/bpf/00000000000040d68a060fc8db8c@google.com/\nReported-by: syzbot+1336f3d4b10bcda75b89@syzkaller.appspotmail.com\nCloses: https://lore.kernel.org/bpf/00000000000026353b060fc21c07@google.com/\nFixes: fcc2c1fb0651 (\"bpf: pass attached BTF to the bpf_struct_ops subsystem\")\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240126023113.1379504-1-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "kernel/bpf/bpf_struct_ops.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d79a3549754725bb90e58104417449edddf3da3d",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-01-24 16:21:01 -0800",
      "message": "bpf: Consistently use BPF token throughout BPF verifier logic\n\nRemove remaining direct queries to perfmon_capable() and bpf_capable()\nin BPF verifier logic and instead use BPF token (if available) to make\ndecisions about privileges.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/20240124022127.2379740-9-andrii@kernel.org",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/filter.h",
        "kernel/bpf/arraymap.c",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c",
        "net/core/filter.c"
      ]
    },
    {
      "hash": "e3f87fdfed7b770dd7066b02262b12747881e76d",
      "author": "Kui-Feng Lee <thinker.li@gmail.com>",
      "date": "2024-01-23 16:37:44 -0800",
      "message": "bpf: hold module refcnt in bpf_struct_ops map creation and prog verification.\n\nTo ensure that a module remains accessible whenever a struct_ops object of\na struct_ops type provided by the module is still in use.\n\nstruct bpf_struct_ops_map doesn't hold a refcnt to btf anymore since a\nmodule will hold a refcnt to it's btf already. But, struct_ops programs are\ndifferent. They hold their associated btf, not the module since they need\nonly btf to assure their types (signatures).\n\nHowever, verifier holds the refcnt of the associated module of a struct_ops\ntype temporarily when verify a struct_ops prog. Verifier needs the help\nfrom the verifier operators (struct bpf_verifier_ops) provided by the owner\nmodule to verify data access of a prog, provide information, and generate\ncode.\n\nThis patch also add a count of links (links_cnt) to bpf_struct_ops_map. It\navoids bpf_struct_ops_map_put_progs() from accessing btf after calling\nmodule_put() in bpf_struct_ops_map_free().\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-10-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "kernel/bpf/bpf_struct_ops.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "fcc2c1fb0651477c8ed78a3a293c175ccd70697a",
      "author": "Kui-Feng Lee <thinker.li@gmail.com>",
      "date": "2024-01-23 16:37:44 -0800",
      "message": "bpf: pass attached BTF to the bpf_struct_ops subsystem\n\nPass the fd of a btf from the userspace to the bpf() syscall, and then\nconvert the fd into a btf. The btf is generated from the module that\ndefines the target BPF struct_ops type.\n\nIn order to inform the kernel about the module that defines the target\nstruct_ops type, the userspace program needs to provide a btf fd for the\nrespective module's btf. This btf contains essential information on the\ntypes defined within the module, including the target struct_ops type.\n\nA btf fd must be provided to the kernel for struct_ops maps and for the bpf\nprograms attached to those maps.\n\nIn the case of the bpf programs, the attach_btf_obj_fd parameter is passed\nas part of the bpf_attr and is converted into a btf. This btf is then\nstored in the prog->aux->attach_btf field. Here, it just let the verifier\naccess attach_btf directly.\n\nIn the case of struct_ops maps, a btf fd is passed as value_type_btf_obj_fd\nof bpf_attr. The bpf_struct_ops_map_alloc() function converts the fd to a\nbtf and stores it as st_map->btf. A flag BPF_F_VTYPE_BTF_OBJ_FD is added\nfor map_flags to indicate that the value of value_type_btf_obj_fd is set.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-9-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "include/uapi/linux/bpf.h",
        "kernel/bpf/bpf_struct_ops.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c",
        "tools/include/uapi/linux/bpf.h"
      ]
    },
    {
      "hash": "689423db3bda2244c24db8a64de4cdb37be1de41",
      "author": "Kui-Feng Lee <thinker.li@gmail.com>",
      "date": "2024-01-23 16:37:44 -0800",
      "message": "bpf: lookup struct_ops types from a given module BTF.\n\nThis is a preparation for searching for struct_ops types from a specified\nmodule. BTF is always btf_vmlinux now. This patch passes a pointer of BTF\nto bpf_struct_ops_find_value() and bpf_struct_ops_find(). Once the new\nregistration API of struct_ops types is used, other BTFs besides\nbtf_vmlinux can also be passed to them.\n\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-8-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/bpf_struct_ops.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "4c5763ed996a61b51d721d0968d0df957826ea49",
      "author": "Kui-Feng Lee <thinker.li@gmail.com>",
      "date": "2024-01-23 16:37:44 -0800",
      "message": "bpf, net: introduce bpf_struct_ops_desc.\n\nMove some of members of bpf_struct_ops to bpf_struct_ops_desc.  type_id is\nunavailabe in bpf_struct_ops anymore. Modules should get it from the btf\nreceived by kmod's init function.\n\nCc: netdev@vger.kernel.org\nSigned-off-by: Kui-Feng Lee <thinker.li@gmail.com>\nLink: https://lore.kernel.org/r/20240119225005.668602-4-thinker.li@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/bpf_struct_ops.c",
        "kernel/bpf/verifier.c",
        "net/bpf/bpf_dummy_struct_ops.c",
        "net/ipv4/bpf_tcp_ca.c"
      ]
    },
    {
      "hash": "2ce793ebe207328b1210bb53effd702740987148",
      "author": "Hao Sun <sunhao.th@gmail.com>",
      "date": "2024-01-23 15:01:39 -0800",
      "message": "bpf: Refactor ptr alu checking rules to allow alu explicitly\n\nCurrent checking rules are structured to disallow alu on particular ptr\ntypes explicitly, so default cases are allowed implicitly. This may lead\nto newly added ptr types being allowed unexpectedly. So restruture it to\nallow alu explicitly. The tradeoff is mainly a bit more cases added in\nthe switch. The following table from Eduard summarizes the rules:\n\n        | Pointer type        | Arithmetics allowed |\n        |---------------------+---------------------|\n        | PTR_TO_CTX          | yes                 |\n        | CONST_PTR_TO_MAP    | conditionally       |\n        | PTR_TO_MAP_VALUE    | yes                 |\n        | PTR_TO_MAP_KEY      | yes                 |\n        | PTR_TO_STACK        | yes                 |\n        | PTR_TO_PACKET_META  | yes                 |\n        | PTR_TO_PACKET       | yes                 |\n        | PTR_TO_PACKET_END   | no                  |\n        | PTR_TO_FLOW_KEYS    | conditionally       |\n        | PTR_TO_SOCKET       | no                  |\n        | PTR_TO_SOCK_COMMON  | no                  |\n        | PTR_TO_TCP_SOCK     | no                  |\n        | PTR_TO_TP_BUFFER    | yes                 |\n        | PTR_TO_XDP_SOCK     | no                  |\n        | PTR_TO_BTF_ID       | yes                 |\n        | PTR_TO_MEM          | yes                 |\n        | PTR_TO_BUF          | yes                 |\n        | PTR_TO_FUNC         | yes                 |\n        | CONST_PTR_TO_DYNPTR | yes                 |\n\nThe refactored rules are equivalent to the original one. Note that\nPTR_TO_FUNC and CONST_PTR_TO_DYNPTR are not reject here because: (1)\ncheck_mem_access() rejects load/store on those ptrs, and those ptrs\nwith offset passing to calls are rejected check_func_arg_reg_off();\n(2) someone may rely on the verifier not rejecting programs earily.\n\nSigned-off-by: Hao Sun <sunhao.th@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240117094012.36798-1-sunhao.th@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "9a4c57f52b5e0de3a6b1f40c5b656730ce33ee01",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-01-23 14:40:23 -0800",
      "message": "bpf: Track aligned st store as imprecise spilled registers\n\nWith patch set [1], precision backtracing supports register spill/fill\nto/from the stack. The patch [2] allows initial imprecise register spill\nwith content 0. This is a common case for cpuv3 and lower for\ninitializing the stack variables with pattern\n  r1 = 0\n  *(u64 *)(r10 - 8) = r1\nand the [2] has demonstrated good verification improvement.\n\nFor cpuv4, the initialization could be\n  *(u64 *)(r10 - 8) = 0\nThe current verifier marks the r10-8 contents with STACK_ZERO.\nSimilar to [2], let us permit the above insn to behave like\nimprecise register spill which can reduce number of verified states.\nThe change is in function check_stack_write_fixed_off().\n\nBefore this patch, spilled zero will be marked as STACK_ZERO\nwhich can provide precise values. In check_stack_write_var_off(),\nSTACK_ZERO will be maintained if writing a const zero\nso later it can provide precise values if needed.\n\nThe above handling of '*(u64 *)(r10 - 8) = 0' as a spill\nwill have issues in check_stack_write_var_off() as the spill\nwill be converted to STACK_MISC and the precise value 0\nis lost. To fix this issue, if the spill slots with const\nzero and the BPF_ST write also with const zero, the spill slots\nare preserved, which can later provide precise values\nif needed. Without the change in check_stack_write_var_off(),\nthe test_verifier subtest 'BPF_ST_MEM stack imm zero, variable offset'\nwill fail.\n\nI checked cpuv3 and cpuv4 with and without this patch with veristat.\nThere is no state change for cpuv3 since '*(u64 *)(r10 - 8) = 0'\nis only generated with cpuv4.\n\nFor cpuv4:\n$ ../veristat -C old.cpuv4.csv new.cpuv4.csv -e file,prog,insns,states -f 'insns_diff!=0'\nFile                                        Program              Insns (A)  Insns (B)  Insns    (DIFF)  States (A)  States (B)  States (DIFF)\n------------------------------------------  -------------------  ---------  ---------  ---------------  ----------  ----------  -------------\nlocal_storage_bench.bpf.linked3.o           get_local                  228        168    -60 (-26.32%)          17          14   -3 (-17.65%)\npyperf600_bpf_loop.bpf.linked3.o            on_event                  6066       4889  -1177 (-19.40%)         403         321  -82 (-20.35%)\ntest_cls_redirect.bpf.linked3.o             cls_redirect             35483      35387     -96 (-0.27%)        2179        2177    -2 (-0.09%)\ntest_l4lb_noinline.bpf.linked3.o            balancer_ingress          4494       4522     +28 (+0.62%)         217         219    +2 (+0.92%)\ntest_l4lb_noinline_dynptr.bpf.linked3.o     balancer_ingress          1432       1455     +23 (+1.61%)          92          94    +2 (+2.17%)\ntest_xdp_noinline.bpf.linked3.o             balancer_ingress_v6       3462       3458      -4 (-0.12%)         216         216    +0 (+0.00%)\nverifier_iterating_callbacks.bpf.linked3.o  widening                    52         41    -11 (-21.15%)           4           3   -1 (-25.00%)\nxdp_synproxy_kern.bpf.linked3.o             syncookie_tc             12412      11719    -693 (-5.58%)         345         330   -15 (-4.35%)\nxdp_synproxy_kern.bpf.linked3.o             syncookie_xdp            12478      11794    -684 (-5.48%)         346         331   -15 (-4.34%)\n\ntest_l4lb_noinline and test_l4lb_noinline_dynptr has minor regression, but\npyperf600_bpf_loop and local_storage_bench gets pretty good improvement.\n\n  [1] https://lore.kernel.org/all/20231205184248.1502704-1-andrii@kernel.org/\n  [2] https://lore.kernel.org/all/20231205184248.1502704-9-andrii@kernel.org/\n\nCc: Kuniyuki Iwashima <kuniyu@amazon.com>\nCc: Martin KaFai Lau <kafai@fb.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nTested-by: Kuniyuki Iwashima <kuniyu@amazon.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20240110051348.2737007-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
      ]
    },
    {
      "hash": "8ecfc371d829bfed75e0ef2cab45b2290b982f64",
      "author": "Maxim Mikityanskiy <maxim@isovalent.com>",
      "date": "2024-01-23 14:40:23 -0800",
      "message": "bpf: Assign ID to scalars on spill\n\nCurrently, when a scalar bounded register is spilled to the stack, its\nID is preserved, but only if was already assigned, i.e. if this register\nwas MOVed before.\n\nAssign an ID on spill if none is set, so that equal scalars could be\ntracked if a register is spilled to the stack and filled into another\nregister.\n\nOne test is adjusted to reflect the change in register IDs.\n\nSigned-off-by: Maxim Mikityanskiy <maxim@isovalent.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240108205209.838365-9-maxtram95@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_direct_packet_access.c",
        "tools/testing/selftests/bpf/verifier/precise.c"
      ]
    },
    {
      "hash": "87e51ac6cb19c5d33d70d4cae9e26d2a3a5fcba0",
      "author": "Maxim Mikityanskiy <maxim@isovalent.com>",
      "date": "2024-01-23 14:40:23 -0800",
      "message": "bpf: Add the get_reg_width function\n\nPut calculation of the register value width into a dedicated function.\nThis function will also be used in a following commit.\n\nSigned-off-by: Maxim Mikityanskiy <maxim@isovalent.com>\nLink: https://lore.kernel.org/r/20240108205209.838365-8-maxtram95@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "8e0e074aafb8fec227363ed905ddd2ac7e4575e4",
      "author": "Maxim Mikityanskiy <maxim@isovalent.com>",
      "date": "2024-01-23 14:40:22 -0800",
      "message": "bpf: Add the assign_scalar_id_before_mov function\n\nExtract the common code that generates a register ID for src_reg before\nMOV if needed into a new function. This function will also be used in\na following commit.\n\nSigned-off-by: Maxim Mikityanskiy <maxim@isovalent.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240108205209.838365-7-maxtram95@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d5b892fd607abec2a1e49b6a2afc278c329a0ee2",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2024-01-23 14:40:22 -0800",
      "message": "bpf: make infinite loop detection in is_state_visited() exact\n\nCurrent infinite loops detection mechanism is speculative:\n- first, states_maybe_looping() check is done which simply does memcmp\n  for R1-R10 in current frame;\n- second, states_equal(..., exact=false) is called. With exact=false\n  states_equal() would compare scalars for equality only if in old\n  state scalar has precision mark.\n\nSuch logic might be problematic if compiler makes some unlucky stack\nspill/fill decisions. An artificial example of a false positive looks\nas follows:\n\n        r0 = ... unknown scalar ...\n        r0 &= 0xff;\n        *(u64 *)(r10 - 8) = r0;\n        r0 = 0;\n    loop:\n        r0 = *(u64 *)(r10 - 8);\n        if r0 > 10 goto exit_;\n        r0 += 1;\n        *(u64 *)(r10 - 8) = r0;\n        r0 = 0;\n        goto loop;\n\nThis commit updates call to states_equal to use exact=true, forcing\nall scalar comparisons to be exact.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240108205209.838365-3-maxtram95@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "54c11ec4935a61af32bb03fc52e7172c97bd7203",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2024-01-23 14:40:21 -0800",
      "message": "bpf: prepare btf_prepare_func_args() for multiple tags per argument\n\nAdd btf_arg_tag flags enum to be able to record multiple tags per\nargument. Also streamline pointer argument processing some more.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20240105000909.2818934-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "7c05e7f3e74e7e550534d524e04d7e6f78d6fa24",
      "author": "Hou Tao <houtao1@huawei.com>",
      "date": "2024-01-23 14:40:21 -0800",
      "message": "bpf: Support inlining bpf_kptr_xchg() helper\n\nThe motivation of inlining bpf_kptr_xchg() comes from the performance\nprofiling of bpf memory allocator benchmark. The benchmark uses\nbpf_kptr_xchg() to stash the allocated objects and to pop the stashed\nobjects for free. After inling bpf_kptr_xchg(), the performance for\nobject free on 8-CPUs VM increases about 2%~10%. The inline also has\ndownside: both the kasan and kcsan checks on the pointer will be\nunavailable.\n\nbpf_kptr_xchg() can be inlined by converting the calling of\nbpf_kptr_xchg() into an atomic_xchg() instruction. But the conversion\ndepends on two conditions:\n1) JIT backend supports atomic_xchg() on pointer-sized word\n2) For the specific arch, the implementation of xchg is the same as\n   atomic_xchg() on pointer-sized words.\n\nIt seems most 64-bit JIT backends satisfies these two conditions. But\nas a precaution, defining a weak function bpf_jit_supports_ptr_xchg()\nto state whether such conversion is safe and only supporting inline for\n64-bit host.\n\nFor x86-64, it supports BPF_XCHG atomic operation and both xchg() and\natomic_xchg() use arch_xchg() to implement the exchange, so enabling the\ninline of bpf_kptr_xchg() on x86-64 first.\n\nReviewed-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20240105104819.3916743-2-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "arch/x86/net/bpf_jit_comp.c",
        "include/linux/filter.h",
        "kernel/bpf/core.c",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "22c7fa171a02d310e3a3f6ed46a698ca8a0060ed",
      "author": "Hao Sun <sunhao.th@gmail.com>",
      "date": "2024-01-16 17:12:29 +0100",
      "message": "bpf: Reject variable offset alu on PTR_TO_FLOW_KEYS\n\nFor PTR_TO_FLOW_KEYS, check_flow_keys_access() only uses fixed off\nfor validation. However, variable offset ptr alu is not prohibited\nfor this ptr kind. So the variable offset is not checked.\n\nThe following prog is accepted:\n\n  func#0 @0\n  0: R1=ctx() R10=fp0\n  0: (bf) r6 = r1                       ; R1=ctx() R6_w=ctx()\n  1: (79) r7 = *(u64 *)(r6 +144)        ; R6_w=ctx() R7_w=flow_keys()\n  2: (b7) r8 = 1024                     ; R8_w=1024\n  3: (37) r8 /= 1                       ; R8_w=scalar()\n  4: (57) r8 &= 1024                    ; R8_w=scalar(smin=smin32=0,\n  smax=umax=smax32=umax32=1024,var_off=(0x0; 0x400))\n  5: (0f) r7 += r8\n  mark_precise: frame0: last_idx 5 first_idx 0 subseq_idx -1\n  mark_precise: frame0: regs=r8 stack= before 4: (57) r8 &= 1024\n  mark_precise: frame0: regs=r8 stack= before 3: (37) r8 /= 1\n  mark_precise: frame0: regs=r8 stack= before 2: (b7) r8 = 1024\n  6: R7_w=flow_keys(smin=smin32=0,smax=umax=smax32=umax32=1024,var_off\n  =(0x0; 0x400)) R8_w=scalar(smin=smin32=0,smax=umax=smax32=umax32=1024,\n  var_off=(0x0; 0x400))\n  6: (79) r0 = *(u64 *)(r7 +0)          ; R0_w=scalar()\n  7: (95) exit\n\nThis prog loads flow_keys to r7, and adds the variable offset r8\nto r7, and finally causes out-of-bounds access:\n\n  BUG: unable to handle page fault for address: ffffc90014c80038\n  [...]\n  Call Trace:\n   <TASK>\n   bpf_dispatcher_nop_func include/linux/bpf.h:1231 [inline]\n   __bpf_prog_run include/linux/filter.h:651 [inline]\n   bpf_prog_run include/linux/filter.h:658 [inline]\n   bpf_prog_run_pin_on_cpu include/linux/filter.h:675 [inline]\n   bpf_flow_dissect+0x15f/0x350 net/core/flow_dissector.c:991\n   bpf_prog_test_run_flow_dissector+0x39d/0x620 net/bpf/test_run.c:1359\n   bpf_prog_test_run kernel/bpf/syscall.c:4107 [inline]\n   __sys_bpf+0xf8f/0x4560 kernel/bpf/syscall.c:5475\n   __do_sys_bpf kernel/bpf/syscall.c:5561 [inline]\n   __se_sys_bpf kernel/bpf/syscall.c:5559 [inline]\n   __x64_sys_bpf+0x73/0xb0 kernel/bpf/syscall.c:5559\n   do_syscall_x64 arch/x86/entry/common.c:52 [inline]\n   do_syscall_64+0x3f/0x110 arch/x86/entry/common.c:83\n   entry_SYSCALL_64_after_hwframe+0x63/0x6b\n\nFix this by rejecting ptr alu with variable offset on flow_keys.\nApplying the patch rejects the program with \"R7 pointer arithmetic\non flow_keys prohibited\".\n\nFixes: d58e468b1112 (\"flow_dissector: implements flow dissector BPF hook\")\nSigned-off-by: Hao Sun <sunhao.th@gmail.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/bpf/20240115082028.9992-1-sunhao.th@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "19bfcdf9498aa968ea293417fbbc39e523527ca8",
      "author": "Dmitrii Dolgov <9erthalion6@gmail.com>",
      "date": "2024-01-04 20:31:34 -0800",
      "message": "bpf: Relax tracing prog recursive attach rules\n\nCurrently, it's not allowed to attach an fentry/fexit prog to another\none fentry/fexit. At the same time it's not uncommon to see a tracing\nprogram with lots of logic in use, and the attachment limitation\nprevents usage of fentry/fexit for performance analysis (e.g. with\n\"bpftool prog profile\" command) in this case. An example could be\nfalcosecurity libs project that uses tp_btf tracing programs.\n\nFollowing the corresponding discussion [1], the reason for that is to\navoid tracing progs call cycles without introducing more complex\nsolutions. But currently it seems impossible to load and attach tracing\nprograms in a way that will form such a cycle. The limitation is coming\nfrom the fact that attach_prog_fd is specified at the prog load (thus\nmaking it impossible to attach to a program loaded after it in this\nway), as well as tracing progs not implementing link_detach.\n\nReplace \"no same type\" requirement with verification that no more than\none level of attachment nesting is allowed. In this way only one\nfentry/fexit program could be attached to another fentry/fexit to cover\nprofiling use case, and still no cycle could be formed. To implement,\nadd a new field into bpf_prog_aux to track nested attachment for tracing\nprograms.\n\n[1]: https://lore.kernel.org/bpf/20191108064039.2041889-16-ast@kernel.org/\n\nAcked-by: Jiri Olsa <olsajiri@gmail.com>\nAcked-by: Song Liu <song@kernel.org>\nSigned-off-by: Dmitrii Dolgov <9erthalion6@gmail.com>\nLink: https://lore.kernel.org/r/20240103190559.14750-2-9erthalion6@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5c1a37653260ed5d9c8b26fb7fe7b99629612982",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-01-03 21:08:26 -0800",
      "message": "bpf: Limit up to 512 bytes for bpf_global_percpu_ma allocation\n\nFor percpu data structure allocation with bpf_global_percpu_ma,\nthe maximum data size is 4K. But for a system with large\nnumber of cpus, bigger data size (e.g., 2K, 4K) might consume\na lot of memory. For example, the percpu memory consumption\nwith unit size 2K and 1024 cpus will be 2K * 1K * 1k = 2GB\nmemory.\n\nWe should discourage such usage. Let us limit the maximum data\nsize to be 512 for bpf_global_percpu_ma allocation.\n\nAcked-by: Hou Tao <houtao1@huawei.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20231222031801.1290841-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c39aa3b289e9c10d0d246cd919b06809f13b72b8",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2024-01-03 21:08:25 -0800",
      "message": "bpf: Allow per unit prefill for non-fix-size percpu memory allocator\n\nCommit 41a5db8d8161 (\"Add support for non-fix-size percpu mem allocation\")\nadded support for non-fix-size percpu memory allocation.\nSuch allocation will allocate percpu memory for all buckets on all\ncpus and the memory consumption is in the order to quadratic.\nFor example, let us say, 4 cpus, unit size 16 bytes, so each\ncpu has 16 * 4 = 64 bytes, with 4 cpus, total will be 64 * 4 = 256 bytes.\nThen let us say, 8 cpus with the same unit size, each cpu\nhas 16 * 8 = 128 bytes, with 8 cpus, total will be 128 * 8 = 1024 bytes.\nSo if the number of cpus doubles, the number of memory consumption\nwill be 4 times. So for a system with large number of cpus, the\nmemory consumption goes up quickly with quadratic order.\nFor example, for 4KB percpu allocation, 128 cpus. The total memory\nconsumption will 4KB * 128 * 128 = 64MB. Things will become\nworse if the number of cpus is bigger (e.g., 512, 1024, etc.)\n\nIn Commit 41a5db8d8161, the non-fix-size percpu memory allocation is\ndone in boot time, so for system with large number of cpus, the initial\npercpu memory consumption is very visible. For example, for 128 cpu\nsystem, the total percpu memory allocation will be at least\n(16 + 32 + 64 + 96 + 128 + 196 + 256 + 512 + 1024 + 2048 + 4096)\n  * 128 * 128 = ~138MB.\nwhich is pretty big. It will be even bigger for larger number of cpus.\n\nNote that the current prefill also allocates 4 entries if the unit size\nis less than 256. So on top of 138MB memory consumption, this will\nadd more consumption with\n3 * (16 + 32 + 64 + 96 + 128 + 196 + 256) * 128 * 128 = ~38MB.\nNext patch will try to reduce this memory consumption.\n\nLater on, Commit 1fda5bb66ad8 (\"bpf: Do not allocate percpu memory\nat init stage\") moved the non-fix-size percpu memory allocation\nto bpf verificaiton stage. Once a particular bpf_percpu_obj_new()\nis called by bpf program, the memory allocator will try to fill in\nthe cache with all sizes, causing the same amount of percpu memory\nconsumption as in the boot stage.\n\nTo reduce the initial percpu memory consumption for non-fix-size\npercpu memory allocation, instead of filling the cache with all\nsupported allocation sizes, this patch intends to fill the cache\nonly for the requested size. As typically users will not use large\npercpu data structure, this can save memory significantly.\nFor example, the allocation size is 64 bytes with 128 cpus.\nThen total percpu memory amount will be 64 * 128 * 128 = 1MB,\nmuch less than previous 138MB.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231222031745.1289082-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_mem_alloc.h",
        "kernel/bpf/memalloc.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "8a021e7fa10576eeb3938328f39bbf98fe7d4715",
      "author": "Andrei Matei <andreimatei1@gmail.com>",
      "date": "2024-01-03 10:37:56 -0800",
      "message": "bpf: Simplify checking size of helper accesses\n\nThis patch simplifies the verification of size arguments associated to\npointer arguments to helpers and kfuncs. Many helpers take a pointer\nargument followed by the size of the memory access performed to be\nperformed through that pointer. Before this patch, the handling of the\nsize argument in check_mem_size_reg() was confusing and wasteful: if the\nsize register's lower bound was 0, then the verification was done twice:\nonce considering the size of the access to be the lower-bound of the\nrespective argument, and once considering the upper bound (even if the\ntwo are the same). The upper bound checking is a super-set of the\nlower-bound checking(*), except: the only point of the lower-bound check\nis to handle the case where zero-sized-accesses are explicitly not\nallowed and the lower-bound is zero. This static condition is now\nchecked explicitly, replacing a much more complex, expensive and\nconfusing verification call to check_helper_mem_access().\n\nError messages change in this patch. Before, messages about illegal\nzero-size accesses depended on the type of the pointer and on other\nconditions, and sometimes the message was plain wrong: in some tests\nthat changed you'll see that the old message was something like \"R1 min\nvalue is outside of the allowed memory range\", where R1 is the pointer\nregister; the error was wrongly claiming that the pointer was bad\ninstead of the size being bad. Other times the information that the size\ncame for a register with a possible range of values was wrong, and the\nerror presented the size as a fixed zero. Now the errors refer to the\nright register. However, the old error messages did contain useful\ninformation about the pointer register which is now lost; recovering\nthis information was deemed not important enough.\n\n(*) Besides standing to reason that the checks for a bigger size access\nare a super-set of the checks for a smaller size access, I have also\nmechanically verified this by reading the code for all types of\npointers. I could convince myself that it's true for all but\nPTR_TO_BTF_ID (check_ptr_to_btf_access). There, simply looking\nline-by-line does not immediately prove what we want. If anyone has any\nqualms, let me know.\n\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20231221232225.568730-2-andreimatei1@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_helper_value_access.c",
        "tools/testing/selftests/bpf/progs/verifier_raw_stack.c"
      ]
    },
    {
      "hash": "5abde62465222edd3080b70099bd809f166d5d7d",
      "author": "Simon Horman <horms@kernel.org>",
      "date": "2023-12-21 22:40:25 +0100",
      "message": "bpf: Avoid unnecessary use of comma operator in verifier\n\nAlthough it does not seem to have any untoward side-effects, the use\nof ';' to separate to assignments seems more appropriate than ','.\n\nFlagged by clang-17 -Wcomma\n\nNo functional change intended. Compile tested only.\n\nSigned-off-by: Simon Horman <horms@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nReviewed-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/bpf/20231221-bpf-verifier-comma-v1-1-cde2530912e9@kernel.org",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "a64bfe618665ea9c722f922cba8c6e3234eac5ac",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-19 18:06:46 -0800",
      "message": "bpf: add support for passing dynptr pointer to global subprog\n\nAdd ability to pass a pointer to dynptr into global functions.\nThis allows to have global subprogs that accept and work with generic\ndynptrs that are created by caller. Dynptr argument is detected based on\nthe name of a struct type, if it's \"bpf_dynptr\", it's assumed to be\na proper dynptr pointer. Both actual struct and forward struct\ndeclaration types are supported.\n\nThis is conceptually exactly the same semantics as\nbpf_user_ringbuf_drain()'s use of dynptr to pass a variable-sized\npointer to ringbuf record. So we heavily rely on CONST_PTR_TO_DYNPTR\nbits of already existing logic in the verifier.\n\nDuring global subprog validation, we mark such CONST_PTR_TO_DYNPTR as\nhaving LOCAL type, as that's the most unassuming type of dynptr and it\ndoesn't have any special helpers that can try to free or acquire extra\nreferences (unlike skb, xdp, or ringbuf dynptr). So that seems like a safe\n\"choice\" to make from correctness standpoint. It's still possible to\npass any type of dynptr to such subprog, though, because generic dynptr\nhelpers, like getting data/slice pointers, read/write memory copying\nroutines, dynptr adjustment and getter routines all work correctly with\nany type of dynptr.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "94e1c70a34523b5e1529e4ec508316acc6a26a2b",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-19 18:06:46 -0800",
      "message": "bpf: support 'arg:xxx' btf_decl_tag-based hints for global subprog args\n\nAdd support for annotating global BPF subprog arguments to provide more\ninformation about expected semantics of the argument. Currently,\nverifier relies purely on argument's BTF type information, and supports\nthree general use cases: scalar, pointer-to-context, and\npointer-to-fixed-size-memory.\n\nScalar and pointer-to-fixed-mem work well in practice and are quite\nnatural to use. But pointer-to-context is a bit problematic, as typical\nBPF users don't realize that they need to use a special type name to\nsignal to verifier that argument is not just some pointer, but actually\na PTR_TO_CTX. Further, even if users do know which type to use, it is\nlimiting in situations where the same BPF program logic is used across\nfew different program types. Common case is kprobes, tracepoints, and\nperf_event programs having a helper to send some data over BPF perf\nbuffer. bpf_perf_event_output() requires `ctx` argument, and so it's\nquite cumbersome to share such global subprog across few BPF programs of\ndifferent types, necessitating extra static subprog that is context\ntype-agnostic.\n\nLong story short, there is a need to go beyond types and allow users to\nadd hints to global subprog arguments to define expectations.\n\nThis patch adds such support for two initial special tags:\n  - pointer to context;\n  - non-null qualifier for generic pointer arguments.\n\nAll of the above came up in practice already and seem generally useful\nadditions. Non-null qualifier is an often requested feature, which\ncurrently has to be worked around by having unnecessary NULL checks\ninside subprogs even if we know that arguments are never NULL. Pointer\nto context was discussed earlier.\n\nAs for implementation, we utilize btf_decl_tag attribute and set up an\n\"arg:xxx\" convention to specify argument hint. As such:\n  - btf_decl_tag(\"arg:ctx\") is a PTR_TO_CTX hint;\n  - btf_decl_tag(\"arg:nonnull\") marks pointer argument as not allowed to\n    be NULL, making NULL check inside global subprog unnecessary.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "f18c3d88deedf0defc3e4800341cc7bcaaabcdf9",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-19 18:06:46 -0800",
      "message": "bpf: reuse subprog argument parsing logic for subprog call checks\n\nRemove duplicated BTF parsing logic when it comes to subprog call check.\nInstead, use (potentially cached) results of btf_prepare_func_args() to\nabstract away expectations of each subprog argument in generic terms\n(e.g., \"this is pointer to context\", or \"this is a pointer to memory of\nsize X\"), and then use those simple high-level argument type\nexpectations to validate actual register states to check if they match\nexpectations.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/test_global_func5.c"
      ]
    },
    {
      "hash": "c5a7244759b1eeacc59d0426fb73859afa942d0d",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-19 18:06:46 -0800",
      "message": "bpf: move subprog call logic back to verifier.c\n\nSubprog call logic in btf_check_subprog_call() currently has both a lot\nof BTF parsing logic (which is, presumably, what justified putting it\ninto btf.c), but also a bunch of register state checks, some of each\nutilize deep verifier logic helpers, necessarily exported from\nverifier.c: check_ptr_off_reg(), check_func_arg_reg_off(),\nand check_mem_reg().\n\nGoing forward, btf_check_subprog_call() will have a minimum of\nBTF-related logic, but will get more internal verifier logic related to\nregister state manipulation. So move it into verifier.c to minimize\namount of verifier-specific logic exposed to btf.c.\n\nWe do this move before refactoring btf_check_func_arg_match() to\npreserve as much history post-refactoring as possible.\n\nNo functional changes.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "e26080d0da87f20222ca6712b65f95a856fadee0",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-19 18:06:46 -0800",
      "message": "bpf: prepare btf_prepare_func_args() for handling static subprogs\n\nGeneralize btf_prepare_func_args() to support both global and static\nsubprogs. We are going to utilize this property in the next patch,\nreusing btf_prepare_func_args() for subprog call logic instead of\nreparsing BTF information in a completely separate implementation.\n\nbtf_prepare_func_args() now detects whether subprog is global or static\nmakes slight logic adjustments for static func cases, like not failing\nfatally (-EFAULT) for conditions that are allowable for static subprogs.\n\nSomewhat subtle (but major!) difference is the handling of pointer arguments.\nBoth global and static functions need to handle special context\narguments (which are pointers to predefined type names), but static\nsubprogs give up on any other pointers, falling back to marking subprog\nas \"unreliable\", disabling the use of BTF type information altogether.\n\nFor global functions, though, we are assuming that such pointers to\nunrecognized types are just pointers to fixed-sized memory region (or\nerror out if size cannot be established, like for `void *` pointers).\n\nThis patch accommodates these small differences and sets up a stage for\nrefactoring in the next patch, eliminating a separate BTF-based parsing\nlogic in btf_check_func_arg_match().\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5eccd2db42d77e3570619c32d39e39bf486607cf",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-19 18:06:46 -0800",
      "message": "bpf: reuse btf_prepare_func_args() check for main program BTF validation\n\nInstead of btf_check_subprog_arg_match(), use btf_prepare_func_args()\nlogic to validate \"trustworthiness\" of main BPF program's BTF information,\nif it is present.\n\nWe ignored results of original BTF check anyway, often times producing\nconfusing and ominously-sounding \"reg type unsupported for arg#0\nfunction\" message, which has no apparent effect on program correctness\nand verification process.\n\nAll the -EFAULT returning sanity checks are already performed in\ncheck_btf_info_early(), so there is zero reason to have this duplication\nof logic between btf_check_subprog_call() and btf_check_subprog_arg_match().\nDropping btf_check_subprog_arg_match() simplifies\nbtf_check_func_arg_match() further removing `bool processing_call` flag.\n\nOne subtle bit that was done by btf_check_subprog_arg_match() was\npotentially marking main program's BTF as unreliable. We do this\nexplicitly now with a dedicated simple check, preserving the original\nbehavior, but now based on well factored btf_prepare_func_args() logic.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/prog_tests/log_fixup.c",
        "tools/testing/selftests/bpf/progs/cgrp_kfunc_failure.c",
        "tools/testing/selftests/bpf/progs/task_kfunc_failure.c"
      ]
    },
    {
      "hash": "4ba1d0f23414135e4f426dae4cb5cdc2ce246f89",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-19 18:06:46 -0800",
      "message": "bpf: abstract away global subprog arg preparation logic from reg state setup\n\nbtf_prepare_func_args() is used to understand expectations and\nrestrictions on global subprog arguments. But current implementation is\nhard to extend, as it intermixes BTF-based func prototype parsing and\ninterpretation logic with setting up register state at subprog entry.\n\nWorse still, those registers are not completely set up inside\nbtf_prepare_func_args(), requiring some more logic later in\ndo_check_common(). Like calling mark_reg_unknown() and similar\ninitialization operations.\n\nThis intermixing of BTF interpretation and register state setup is\nproblematic. First, it causes duplication of BTF parsing logic for global\nsubprog verification (to set up initial state of global subprog) and\nglobal subprog call sites analysis (when we need to check that whatever\nis being passed into global subprog matches expectations), performed in\nbtf_check_subprog_call().\n\nGiven we want to extend global func argument with tags later, this\nduplication is problematic. So refactor btf_prepare_func_args() to do\nonly BTF-based func proto and args parsing, returning high-level\nargument \"expectations\" only, with no regard to specifics of register\nstate. I.e., if it's a context argument, instead of setting register\nstate to PTR_TO_CTX, we return ARG_PTR_TO_CTX enum for that argument as\n\"an argument specification\" for further processing inside\ndo_check_common(). Similarly for SCALAR arguments, PTR_TO_MEM, etc.\n\nThis allows to reuse btf_prepare_func_args() in following patches at\nglobal subprog call site analysis time. It also keeps register setup\ncode consistently in one place, do_check_common().\n\nBesides all this, we cache this argument specs information inside\nenv->subprog_info, eliminating the need to redo these potentially\nexpensive BTF traversals, especially if BPF program's BTF is big and/or\nthere are lots of global subprog calls.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231215011334.2307144-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d028f87517d6775dccff4ddbca2740826f9e53f1",
      "author": "Menglong Dong <menglong8.dong@gmail.com>",
      "date": "2023-12-19 17:18:55 -0800",
      "message": "bpf: make the verifier tracks the \"not equal\" for regs\n\nWe can derive some new information for BPF_JNE in regs_refine_cond_op().\nTake following code for example:\n\n  /* The type of \"a\" is u32 */\n  if (a > 0 && a < 100) {\n    /* the range of the register for a is [0, 99], not [1, 99],\n     * and will cause the following error:\n     *\n     *   invalid zero-sized read\n     *\n     * as a can be 0.\n     */\n    bpf_skb_store_bytes(skb, xx, xx, a, 0);\n  }\n\nIn the code above, \"a > 0\" will be compiled to \"jmp xxx if a == 0\". In the\nTRUE branch, the dst_reg will be marked as known to 0. However, in the\nfallthrough(FALSE) branch, the dst_reg will not be handled, which makes\nthe [min, max] for a is [0, 99], not [1, 99].\n\nFor BPF_JNE, we can reduce the range of the dst reg if the src reg is a\nconst and is exactly the edge of the dst reg.\n\nSigned-off-by: Menglong Dong <menglong8.dong@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231219134800.1550388-2-menglong8.dong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d17aff807f845cf93926c28705216639c7279110",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-19 08:23:03 -0800",
      "message": "Revert BPF token-related functionality\n\nThis patch includes the following revert (one  conflicting BPF FS\npatch and three token patch sets, represented by merge commits):\n  - revert 0f5d5454c723 \"Merge branch 'bpf-fs-mount-options-parsing-follow-ups'\";\n  - revert 750e785796bb \"bpf: Support uid and gid when mounting bpffs\";\n  - revert 733763285acf \"Merge branch 'bpf-token-support-in-libbpf-s-bpf-object'\";\n  - revert c35919dcce28 \"Merge branch 'bpf-token-and-bpf-fs-based-delegation'\".\n\nLink: https://lore.kernel.org/bpf/CAHk-=wg7JuFYwGy=GOMbRCtOL+jwSQsdUaBsRWkDVYbxipbM5A@mail.gmail.com\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>",
      "modified_files": [
        "drivers/media/rc/bpf-lirc.c",
        "include/linux/bpf.h",
        "include/linux/filter.h",
        "include/linux/lsm_hook_defs.h",
        "include/linux/security.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/Makefile",
        "kernel/bpf/arraymap.c",
        "kernel/bpf/bpf_lsm.c",
        "kernel/bpf/cgroup.c",
        "kernel/bpf/core.c",
        "kernel/bpf/helpers.c",
        "kernel/bpf/inode.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/token.c",
        "kernel/bpf/verifier.c",
        "kernel/trace/bpf_trace.c",
        "net/core/filter.c",
        "net/ipv4/bpf_tcp_ca.c",
        "net/netfilter/nf_bpf_link.c",
        "security/security.c",
        "security/selinux/hooks.c",
        "tools/include/uapi/linux/bpf.h",
        "tools/lib/bpf/Build",
        "tools/lib/bpf/bpf.c",
        "tools/lib/bpf/bpf.h",
        "tools/lib/bpf/btf.c",
        "tools/lib/bpf/elf.c",
        "tools/lib/bpf/features.c",
        "tools/lib/bpf/libbpf.c",
        "tools/lib/bpf/libbpf.h",
        "tools/lib/bpf/libbpf.map",
        "tools/lib/bpf/libbpf_internal.h",
        "tools/lib/bpf/libbpf_probes.c",
        "tools/lib/bpf/str_error.h",
        "tools/testing/selftests/bpf/prog_tests/libbpf_probes.c",
        "tools/testing/selftests/bpf/prog_tests/libbpf_str.c",
        "tools/testing/selftests/bpf/prog_tests/token.c",
        "tools/testing/selftests/bpf/progs/priv_map.c",
        "tools/testing/selftests/bpf/progs/priv_prog.c"
      ]
    },
    {
      "hash": "8e432e6197cef6250dfd6fdffd41c06613c874ca",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-18 23:54:21 +0100",
      "message": "bpf: Ensure precise is reset to false in __mark_reg_const_zero()\n\nIt is safe to always start with imprecise SCALAR_VALUE register.\nPreviously __mark_reg_const_zero() relied on caller to reset precise\nmark, but it's very error prone and we already missed it in a few\nplaces. So instead make __mark_reg_const_zero() reset precision always,\nas it's a safe default for SCALAR_VALUE. Explanation is basically the\nsame as for why we are resetting (or rather not setting) precision in\ncurrent state. If necessary, precision propagation will set it to\nprecise correctly.\n\nAs such, also remove a big comment about forward precision propagation\nin mark_reg_stack_read() and avoid unnecessarily setting precision to\ntrue after reading from STACK_ZERO stack. Again, precision propagation\nwill correctly handle this, if that SCALAR_VALUE register will ever be\nneeded to be precise.\n\nReported-by: Maxim Mikityanskiy <maxtram95@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: Maxim Mikityanskiy <maxtram95@gmail.com>\nAcked-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20231218173601.53047-1-andrii@kernel.org",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_spill_fill.c"
      ]
    },
    {
      "hash": "745e0311306507ddbe1727ac798c8f956812b810",
      "author": "Andrei Matei <andreimatei1@gmail.com>",
      "date": "2023-12-12 15:35:32 -0800",
      "message": "bpf: Comment on check_mem_size_reg\n\nThis patch adds a comment to check_mem_size_reg -- a function whose\nmeaning is not very transparent. The function implicitly deals with two\nregisters connected by convention, which is not obvious.\n\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20231210225149.67639-1-andreimatei1@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "56c26d5ad86dfe48a76855a91b523ab4f372c003",
      "author": "Yang Li <yang.lee@linux.alibaba.com>",
      "date": "2023-12-12 09:52:07 -0800",
      "message": "bpf: Remove unused backtrack_state helper functions\n\nThe function are defined in the verifier.c file, but not called\nelsewhere, so delete the unused function.\n\nkernel/bpf/verifier.c:3448:20: warning: unused function 'bt_set_slot'\nkernel/bpf/verifier.c:3453:20: warning: unused function 'bt_clear_slot'\nkernel/bpf/verifier.c:3488:20: warning: unused function 'bt_is_slot_set'\n\nReported-by: Abaci Robot <abaci@linux.alibaba.com>\nSigned-off-by: Yang Li <yang.lee@linux.alibaba.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231212005436.103829-1-yang.lee@linux.alibaba.com\n\nCloses: https://bugzilla.openanolis.cn/show_bug.cgi?id=7714",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1a1ad782dcbbacd9e8d4e2e7ff1bf14d1db80727",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-11 19:23:32 -0800",
      "message": "bpf: tidy up exception callback management a bit\n\nUse the fact that we are passing subprog index around and have\na corresponding struct bpf_subprog_info in bpf_verifier_env for each\nsubprogram. We don't need to separately pass around a flag whether\nsubprog is exception callback or not, each relevant verifier function\ncan determine this using provided subprog index if we maintain\nbpf_subprog_info properly.\n\nAlso move out exception callback-specific logic from\nbtf_prepare_func_args(), keeping it generic. We can enforce all these\nrestriction right before exception callback verification pass. We add\nout parameter, arg_cnt, for now, but this will be unnecessary with\nsubsequent refactoring and will be removed.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231204233931.49758-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "482d548d40b0af9af730e4869903d4433e44f014",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-09 18:43:20 -0800",
      "message": "bpf: handle fake register spill to stack with BPF_ST_MEM instruction\n\nWhen verifier validates BPF_ST_MEM instruction that stores known\nconstant to stack (e.g., *(u64 *)(r10 - 8) = 123), it effectively spills\na fake register with a constant (but initially imprecise) value to\na stack slot. Because read-side logic treats it as a proper register\nfill from stack slot, we need to mark such stack slot initialization as\nINSN_F_STACK_ACCESS instruction to stop precision backtracking from\nmissing it.\n\nFixes: 41f6f64e6999 (\"bpf: support non-r10 register spill/fill to/from stack in precision tracking\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231209010958.66758-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "2929bfac006d8f8e22b307d04e0d71bcb84db698",
      "author": "Andrei Matei <andreimatei1@gmail.com>",
      "date": "2023-12-08 14:19:00 -0800",
      "message": "bpf: Minor cleanup around stack bounds\n\nPush the rounding up of stack offsets into the function responsible for\ngrowing the stack, rather than relying on all the callers to do it.\nUncertainty about whether the callers did it or not tripped up people in\na previous review.\n\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231208032519.260451-4-andreimatei1@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "6b4a64bafd107e521c01eec3453ce94a3fb38529",
      "author": "Andrei Matei <andreimatei1@gmail.com>",
      "date": "2023-12-08 14:19:00 -0800",
      "message": "bpf: Fix accesses to uninit stack slots\n\nPrivileged programs are supposed to be able to read uninitialized stack\nmemory (ever since 6715df8d5) but, before this patch, these accesses\nwere permitted inconsistently. In particular, accesses were permitted\nabove state->allocated_stack, but not below it. In other words, if the\nstack was already \"large enough\", the access was permitted, but\notherwise the access was rejected instead of being allowed to \"grow the\nstack\". This undesired rejection was happening in two places:\n- in check_stack_slot_within_bounds()\n- in check_stack_range_initialized()\nThis patch arranges for these accesses to be permitted. A bunch of tests\nthat were relying on the old rejection had to change; all of them were\nchanged to add also run unprivileged, in which case the old behavior\npersists. One tests couldn't be updated - global_func16 - because it\ncan't run unprivileged for other reasons.\n\nThis patch also fixes the tracking of the stack size for variable-offset\nreads. This second fix is bundled in the same commit as the first one\nbecause they're inter-related. Before this patch, writes to the stack\nusing registers containing a variable offset (as opposed to registers\nwith fixed, known values) were not properly contributing to the\nfunction's needed stack size. As a result, it was possible for a program\nto verify, but then to attempt to read out-of-bounds data at runtime\nbecause a too small stack had been allocated for it.\n\nEach function tracks the size of the stack it needs in\nbpf_subprog_info.stack_depth, which is maintained by\nupdate_stack_depth(). For regular memory accesses, check_mem_access()\nwas calling update_state_depth() but it was passing in only the fixed\npart of the offset register, ignoring the variable offset. This was\nincorrect; the minimum possible value of that register should be used\ninstead.\n\nThis tracking is now fixed by centralizing the tracking of stack size in\ngrow_stack_state(), and by lifting the calls to grow_stack_state() to\ncheck_stack_access_within_bounds() as suggested by Andrii. The code is\nnow simpler and more convincingly tracks the correct maximum stack size.\ncheck_stack_range_initialized() can now rely on enough stack having been\nallocated for the access; this helps with the fix for the first issue.\n\nA few tests were changed to also check the stack depth computation. The\none that fails without this patch is verifier_var_off:stack_write_priv_vs_unpriv.\n\nFixes: 01f810ace9ed3 (\"bpf: Allow variable-offset stack access\")\nReported-by: Hao Sun <sunhao.th@gmail.com>\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20231208032519.260451-3-andreimatei1@gmail.com\n\nCloses: https://lore.kernel.org/bpf/CABWLsev9g8UP_c3a=1qbuZUi20tGoUXoU07FPf-5FLvhOKOY+Q@mail.gmail.com/",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/iters.c",
        "tools/testing/selftests/bpf/progs/test_global_func16.c",
        "tools/testing/selftests/bpf/progs/verifier_basic_stack.c",
        "tools/testing/selftests/bpf/progs/verifier_int_ptr.c",
        "tools/testing/selftests/bpf/progs/verifier_raw_stack.c",
        "tools/testing/selftests/bpf/progs/verifier_var_off.c",
        "tools/testing/selftests/bpf/verifier/atomic_cmpxchg.c",
        "tools/testing/selftests/bpf/verifier/calls.c"
      ]
    },
    {
      "hash": "1d38a9ee81570c4bd61f557832dead4d6f816760",
      "author": "Andrei Matei <andreimatei1@gmail.com>",
      "date": "2023-12-07 13:58:10 -0800",
      "message": "bpf: Guard stack limits against 32bit overflow\n\nThis patch promotes the arithmetic around checking stack bounds to be\ndone in the 64-bit domain, instead of the current 32bit. The arithmetic\nimplies adding together a 64-bit register with a int offset. The\nregister was checked to be below 1<<29 when it was variable, but not\nwhen it was fixed. The offset either comes from an instruction (in which\ncase it is 16 bit), from another register (in which case the caller\nchecked it to be below 1<<29 [1]), or from the size of an argument to a\nkfunc (in which case it can be a u32 [2]). Between the register being\ninconsistently checked to be below 1<<29, and the offset being up to an\nu32, it appears that we were open to overflowing the `int`s which were\ncurrently used for arithmetic.\n\n[1] https://github.com/torvalds/linux/blob/815fb87b753055df2d9e50f6cd80eb10235fe3e9/kernel/bpf/verifier.c#L7494-L7498\n[2] https://github.com/torvalds/linux/blob/815fb87b753055df2d9e50f6cd80eb10235fe3e9/kernel/bpf/verifier.c#L11904\n\nReported-by: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20231207041150.229139-4-andreimatei1@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "a833a17aeac73b33f79433d7cee68d5cafd71e4f",
      "author": "Andrei Matei <andreimatei1@gmail.com>",
      "date": "2023-12-07 13:57:53 -0800",
      "message": "bpf: Fix verification of indirect var-off stack access\n\nThis patch fixes a bug around the verification of possibly-zero-sized\nstack accesses. When the access was done through a var-offset stack\npointer, check_stack_access_within_bounds was incorrectly computing the\nmaximum-offset of a zero-sized read to be the same as the register's min\noffset. Instead, we have to take in account the register's maximum\npossible value. The patch also simplifies how the max offset is checked;\nthe check is now simpler than for min offset.\n\nThe bug was allowing accesses to erroneously pass the\ncheck_stack_access_within_bounds() checks, only to later crash in\ncheck_stack_range_initialized() when all the possibly-affected stack\nslots are iterated (this time with a correct max offset).\ncheck_stack_range_initialized() is relying on\ncheck_stack_access_within_bounds() for its accesses to the\nstack-tracking vector to be within bounds; in the case of zero-sized\naccesses, we were essentially only verifying that the lowest possible\nslot was within bounds. We would crash when the max-offset of the stack\npointer was >= 0 (which shouldn't pass verification, and hopefully is\nnot something anyone's code attempts to do in practice).\n\nThanks Hao for reporting!\n\nFixes: 01f810ace9ed3 (\"bpf: Allow variable-offset stack access\")\nReported-by: Hao Sun <sunhao.th@gmail.com>\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20231207041150.229139-2-andreimatei1@gmail.com\n\nCloses: https://lore.kernel.org/bpf/CACkBjsZGEUaRCHsmaX=h-efVogsRfK1FPxmkgb0Os_frnHiNdw@mail.gmail.com/",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "8062fb12de99b2da33754c6a3be1bfc30d9a35f4",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-06 10:02:59 -0800",
      "message": "bpf: consistently use BPF token throughout BPF verifier logic\n\nRemove remaining direct queries to perfmon_capable() and bpf_capable()\nin BPF verifier logic and instead use BPF token (if available) to make\ndecisions about privileges.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231130185229.2688956-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/filter.h",
        "kernel/bpf/arraymap.c",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c",
        "net/core/filter.c"
      ]
    },
    {
      "hash": "18a433b62061e3d787bfc3e670fa711fecbd7cb4",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-05 13:40:21 -0800",
      "message": "bpf: track aligned STACK_ZERO cases as imprecise spilled registers\n\nNow that precision backtracing is supporting register spill/fill to/from\nstack, there is another oportunity to be exploited here: minimizing\nprecise STACK_ZERO cases. With a simple code change we can rely on\ninitially imprecise register spill tracking for cases when register\nspilled to stack was a known zero.\n\nThis is a very common case for initializing on the stack variables,\nincluding rather large structures. Often times zero has no special\nmeaning for the subsequent BPF program logic and is often overwritten\nwith non-zero values soon afterwards. But due to STACK_ZERO vs\nSTACK_MISC tracking, such initial zero initialization actually causes\nduplication of verifier states as STACK_ZERO is clearly different than\nSTACK_MISC or spilled SCALAR_VALUE register.\n\nThe effect of this (now) trivial change is huge, as can be seen below.\nThese are differences between BPF selftests, Cilium, and Meta-internal\nBPF object files relative to previous patch in this series. You can see\nimprovements ranging from single-digit percentage improvement for\ninstructions and states, all the way to 50-60% reduction for some of\nMeta-internal host agent programs, and even some Cilium programs.\n\nFor Meta-internal ones I left only the differences for largest BPF\nobject files by states/instructions, as there were too many differences\nin the overall output. All the differences were improvements, reducting\nnumber of states and thus instructions validated.\n\nNote, Meta-internal BPF object file names are not printed below.\nMany copies of balancer_ingress are actually many different\nconfigurations of Katran, so they are different BPF programs, which\nexplains state reduction going from -16% all the way to 31%, depending\non BPF program logic complexity.\n\nI also tooked a closer look at a few small-ish BPF programs to validate\nthe behavior. Let's take bpf_iter_netrlink.bpf.o (first row below).\nWhile it's just 8 vs 5 states, verifier log is still pretty long to\ninclude it here. But the reduction in states is due to the following\npiece of C code:\n\n        unsigned long ino;\n\n\t...\n\n        sk = s->sk_socket;\n        if (!sk) {\n                ino = 0;\n        } else {\n                inode = SOCK_INODE(sk);\n                bpf_probe_read_kernel(&ino, sizeof(ino), &inode->i_ino);\n        }\n        BPF_SEQ_PRINTF(seq, \"%-8u %-8lu\\n\", s->sk_drops.counter, ino);\n\treturn 0;\n\nYou can see that in some situations `ino` is zero-initialized, while in\nothers it's unknown value filled out by bpf_probe_read_kernel(). Before\nthis change code after if/else branches have to be validated twice. Once\nwith (precise) ino == 0, due to eager STACK_ZERO logic, and then again\nfor when ino is just STACK_MISC. But BPF_SEQ_PRINTF() doesn't care about\nprecise value of ino, so with the change in this patch verifier is able\nto prune states from after one of the branches, reducing number of total\nstates (and instructions) required for successful validation.\n\nSimilar principle applies to bigger real-world applications, just at\na much larger scale.\n\nSELFTESTS\n=========\nFile                                     Program                  Insns (A)  Insns (B)  Insns    (DIFF)  States (A)  States (B)  States (DIFF)\n---------------------------------------  -----------------------  ---------  ---------  ---------------  ----------  ----------  -------------\nbpf_iter_netlink.bpf.linked3.o           dump_netlink                   148        104    -44 (-29.73%)           8           5   -3 (-37.50%)\nbpf_iter_unix.bpf.linked3.o              dump_unix                     8474       8404     -70 (-0.83%)         151         147    -4 (-2.65%)\nbpf_loop.bpf.linked3.o                   stack_check                    560        324   -236 (-42.14%)          42          24  -18 (-42.86%)\nlocal_storage_bench.bpf.linked3.o        get_local                      120         77    -43 (-35.83%)           9           6   -3 (-33.33%)\nloop6.bpf.linked3.o                      trace_virtqueue_add_sgs      10167       9868    -299 (-2.94%)         226         206   -20 (-8.85%)\npyperf600_bpf_loop.bpf.linked3.o         on_event                      4872       3423  -1449 (-29.74%)         322         229  -93 (-28.88%)\nstrobemeta.bpf.linked3.o                 on_event                    180697     176036   -4661 (-2.58%)        4780        4734   -46 (-0.96%)\ntest_cls_redirect.bpf.linked3.o          cls_redirect                 65594      65401    -193 (-0.29%)        4230        4212   -18 (-0.43%)\ntest_global_func_args.bpf.linked3.o      test_cls                       145        136      -9 (-6.21%)          10           9   -1 (-10.00%)\ntest_l4lb.bpf.linked3.o                  balancer_ingress              4760       2612  -2148 (-45.13%)         113         102   -11 (-9.73%)\ntest_l4lb_noinline.bpf.linked3.o         balancer_ingress              4845       4877     +32 (+0.66%)         219         221    +2 (+0.91%)\ntest_l4lb_noinline_dynptr.bpf.linked3.o  balancer_ingress              2072       2087     +15 (+0.72%)          97          98    +1 (+1.03%)\ntest_seg6_loop.bpf.linked3.o             __add_egr_x                  12440       9975  -2465 (-19.82%)         364         353   -11 (-3.02%)\ntest_tcp_hdr_options.bpf.linked3.o       estab                         2558       2572     +14 (+0.55%)         179         180    +1 (+0.56%)\ntest_xdp_dynptr.bpf.linked3.o            _xdp_tx_iptunnel               645        596     -49 (-7.60%)          26          24    -2 (-7.69%)\ntest_xdp_noinline.bpf.linked3.o          balancer_ingress_v6           3520       3516      -4 (-0.11%)         216         216    +0 (+0.00%)\nxdp_synproxy_kern.bpf.linked3.o          syncookie_tc                 82661      81241   -1420 (-1.72%)        5073        5155   +82 (+1.62%)\nxdp_synproxy_kern.bpf.linked3.o          syncookie_xdp                84964      82297   -2667 (-3.14%)        5130        5157   +27 (+0.53%)\n\nMETA-INTERNAL\n=============\nProgram                                 Insns (A)  Insns (B)  Insns      (DIFF)  States (A)  States (B)  States   (DIFF)\n--------------------------------------  ---------  ---------  -----------------  ----------  ----------  ---------------\nbalancer_ingress                            27925      23608    -4317 (-15.46%)        1488        1482      -6 (-0.40%)\nbalancer_ingress                            31824      27546    -4278 (-13.44%)        1658        1652      -6 (-0.36%)\nbalancer_ingress                            32213      27935    -4278 (-13.28%)        1689        1683      -6 (-0.36%)\nbalancer_ingress                            32213      27935    -4278 (-13.28%)        1689        1683      -6 (-0.36%)\nbalancer_ingress                            31824      27546    -4278 (-13.44%)        1658        1652      -6 (-0.36%)\nbalancer_ingress                            38647      29562    -9085 (-23.51%)        2069        1835   -234 (-11.31%)\nbalancer_ingress                            38647      29562    -9085 (-23.51%)        2069        1835   -234 (-11.31%)\nbalancer_ingress                            40339      30792    -9547 (-23.67%)        2193        1934   -259 (-11.81%)\nbalancer_ingress                            37321      29055    -8266 (-22.15%)        1972        1795    -177 (-8.98%)\nbalancer_ingress                            38176      29753    -8423 (-22.06%)        2008        1831    -177 (-8.81%)\nbalancer_ingress                            29193      20910    -8283 (-28.37%)        1599        1422   -177 (-11.07%)\nbalancer_ingress                            30013      21452    -8561 (-28.52%)        1645        1447   -198 (-12.04%)\nbalancer_ingress                            28691      24290    -4401 (-15.34%)        1545        1531     -14 (-0.91%)\nbalancer_ingress                            34223      28965    -5258 (-15.36%)        1984        1875    -109 (-5.49%)\nbalancer_ingress                            35481      26158    -9323 (-26.28%)        2095        1806   -289 (-13.79%)\nbalancer_ingress                            35481      26158    -9323 (-26.28%)        2095        1806   -289 (-13.79%)\nbalancer_ingress                            35868      26455    -9413 (-26.24%)        2140        1827   -313 (-14.63%)\nbalancer_ingress                            35868      26455    -9413 (-26.24%)        2140        1827   -313 (-14.63%)\nbalancer_ingress                            35481      26158    -9323 (-26.28%)        2095        1806   -289 (-13.79%)\nbalancer_ingress                            35481      26158    -9323 (-26.28%)        2095        1806   -289 (-13.79%)\nbalancer_ingress                            34844      29485    -5359 (-15.38%)        2036        1918    -118 (-5.80%)\nfbflow_egress                                3256       2652     -604 (-18.55%)         218         192    -26 (-11.93%)\nfbflow_ingress                               1026        944       -82 (-7.99%)          70          63     -7 (-10.00%)\nsslwall_tc_egress                            8424       7360    -1064 (-12.63%)         498         458     -40 (-8.03%)\nsyar_accept_protect                         15040       9539    -5501 (-36.58%)         364         220   -144 (-39.56%)\nsyar_connect_tcp_v6                         15036       9535    -5501 (-36.59%)         360         216   -144 (-40.00%)\nsyar_connect_udp_v4                         15039       9538    -5501 (-36.58%)         361         217   -144 (-39.89%)\nsyar_connect_connect4_protect4              24805      15833    -8972 (-36.17%)         756         480   -276 (-36.51%)\nsyar_lsm_file_open                         167772     151813    -15959 (-9.51%)        1836        1667    -169 (-9.20%)\nsyar_namespace_create_new                   14805       9304    -5501 (-37.16%)         353         209   -144 (-40.79%)\nsyar_python3_detect                         17531      12030    -5501 (-31.38%)         391         247   -144 (-36.83%)\nsyar_ssh_post_fork                          16412      10911    -5501 (-33.52%)         405         261   -144 (-35.56%)\nsyar_enter_execve                           14728       9227    -5501 (-37.35%)         345         201   -144 (-41.74%)\nsyar_enter_execveat                         14728       9227    -5501 (-37.35%)         345         201   -144 (-41.74%)\nsyar_exit_execve                            16622      11121    -5501 (-33.09%)         376         232   -144 (-38.30%)\nsyar_exit_execveat                          16622      11121    -5501 (-33.09%)         376         232   -144 (-38.30%)\nsyar_syscalls_kill                          15288       9787    -5501 (-35.98%)         398         254   -144 (-36.18%)\nsyar_task_enter_pivot_root                  14898       9397    -5501 (-36.92%)         357         213   -144 (-40.34%)\nsyar_syscalls_setreuid                      16678      11177    -5501 (-32.98%)         429         285   -144 (-33.57%)\nsyar_syscalls_setuid                        16678      11177    -5501 (-32.98%)         429         285   -144 (-33.57%)\nsyar_syscalls_process_vm_readv              14959       9458    -5501 (-36.77%)         364         220   -144 (-39.56%)\nsyar_syscalls_process_vm_writev             15757      10256    -5501 (-34.91%)         390         246   -144 (-36.92%)\ndo_uprobe                                   15519      10018    -5501 (-35.45%)         373         229   -144 (-38.61%)\nedgewall                                   179715      55783  -123932 (-68.96%)       12607        3999  -8608 (-68.28%)\nbictcp_state                                 7570       4131    -3439 (-45.43%)         496         269   -227 (-45.77%)\ncubictcp_state                               7570       4131    -3439 (-45.43%)         496         269   -227 (-45.77%)\ntcp_rate_skb_delivered                        447        272     -175 (-39.15%)          29          18    -11 (-37.93%)\nkprobe__bbr_set_state                        4566       2615    -1951 (-42.73%)         209         124    -85 (-40.67%)\nkprobe__bictcp_state                         4566       2615    -1951 (-42.73%)         209         124    -85 (-40.67%)\ninet_sock_set_state                          1501       1337     -164 (-10.93%)          93          85      -8 (-8.60%)\ntcp_retransmit_skb                           1145        981     -164 (-14.32%)          67          59     -8 (-11.94%)\ntcp_retransmit_synack                        1183        951     -232 (-19.61%)          67          55    -12 (-17.91%)\nbpf_tcptuner                                 1459       1187     -272 (-18.64%)          99          80    -19 (-19.19%)\ntw_egress                                     801        776       -25 (-3.12%)          69          66      -3 (-4.35%)\ntw_ingress                                    795        770       -25 (-3.14%)          69          66      -3 (-4.35%)\nttls_tc_ingress                             19025      19383      +358 (+1.88%)         470         465      -5 (-1.06%)\nttls_nat_egress                               490        299     -191 (-38.98%)          33          20    -13 (-39.39%)\nttls_nat_ingress                              448        285     -163 (-36.38%)          32          21    -11 (-34.38%)\ntw_twfw_egress                             511127     212071  -299056 (-58.51%)       16733        8504  -8229 (-49.18%)\ntw_twfw_ingress                            500095     212069  -288026 (-57.59%)       16223        8504  -7719 (-47.58%)\ntw_twfw_tc_eg                              511113     212064  -299049 (-58.51%)       16732        8504  -8228 (-49.18%)\ntw_twfw_tc_in                              500095     212069  -288026 (-57.59%)       16223        8504  -7719 (-47.58%)\ntw_twfw_egress                              12632      12435      -197 (-1.56%)         276         260     -16 (-5.80%)\ntw_twfw_ingress                             12631      12454      -177 (-1.40%)         278         261     -17 (-6.12%)\ntw_twfw_tc_eg                               12595      12435      -160 (-1.27%)         274         259     -15 (-5.47%)\ntw_twfw_tc_in                               12631      12454      -177 (-1.40%)         278         261     -17 (-6.12%)\ntw_xdp_dump                                   266        209      -57 (-21.43%)           9           8     -1 (-11.11%)\n\nCILIUM\n=========\nFile           Program                           Insns (A)  Insns (B)  Insns     (DIFF)  States (A)  States (B)  States  (DIFF)\n-------------  --------------------------------  ---------  ---------  ----------------  ----------  ----------  --------------\nbpf_host.o     cil_to_netdev                          6047       4578   -1469 (-24.29%)         362         249  -113 (-31.22%)\nbpf_host.o     handle_lxc_traffic                     2227       1585    -642 (-28.83%)         156         103   -53 (-33.97%)\nbpf_host.o     tail_handle_ipv4_from_netdev           2244       1458    -786 (-35.03%)         163         106   -57 (-34.97%)\nbpf_host.o     tail_handle_nat_fwd_ipv4              21022      10479  -10543 (-50.15%)        1289         670  -619 (-48.02%)\nbpf_host.o     tail_handle_nat_fwd_ipv6              15433      11375   -4058 (-26.29%)         905         643  -262 (-28.95%)\nbpf_host.o     tail_ipv4_host_policy_ingress          2219       1367    -852 (-38.40%)         161          96   -65 (-40.37%)\nbpf_host.o     tail_nodeport_nat_egress_ipv4         22460      19862   -2598 (-11.57%)        1469        1293  -176 (-11.98%)\nbpf_host.o     tail_nodeport_nat_ingress_ipv4         5526       3534   -1992 (-36.05%)         366         243  -123 (-33.61%)\nbpf_host.o     tail_nodeport_nat_ingress_ipv6         5132       4256    -876 (-17.07%)         241         219    -22 (-9.13%)\nbpf_host.o     tail_nodeport_nat_ipv6_egress          3702       3542     -160 (-4.32%)         215         205    -10 (-4.65%)\nbpf_lxc.o      tail_handle_nat_fwd_ipv4              21022      10479  -10543 (-50.15%)        1289         670  -619 (-48.02%)\nbpf_lxc.o      tail_handle_nat_fwd_ipv6              15433      11375   -4058 (-26.29%)         905         643  -262 (-28.95%)\nbpf_lxc.o      tail_ipv4_ct_egress                    5073       3374   -1699 (-33.49%)         262         172   -90 (-34.35%)\nbpf_lxc.o      tail_ipv4_ct_ingress                   5093       3385   -1708 (-33.54%)         262         172   -90 (-34.35%)\nbpf_lxc.o      tail_ipv4_ct_ingress_policy_only       5093       3385   -1708 (-33.54%)         262         172   -90 (-34.35%)\nbpf_lxc.o      tail_ipv6_ct_egress                    4593       3878    -715 (-15.57%)         194         151   -43 (-22.16%)\nbpf_lxc.o      tail_ipv6_ct_ingress                   4606       3891    -715 (-15.52%)         194         151   -43 (-22.16%)\nbpf_lxc.o      tail_ipv6_ct_ingress_policy_only       4606       3891    -715 (-15.52%)         194         151   -43 (-22.16%)\nbpf_lxc.o      tail_nodeport_nat_ingress_ipv4         5526       3534   -1992 (-36.05%)         366         243  -123 (-33.61%)\nbpf_lxc.o      tail_nodeport_nat_ingress_ipv6         5132       4256    -876 (-17.07%)         241         219    -22 (-9.13%)\nbpf_overlay.o  tail_handle_nat_fwd_ipv4              20524      10114  -10410 (-50.72%)        1271         638  -633 (-49.80%)\nbpf_overlay.o  tail_nodeport_nat_egress_ipv4         22718      19490   -3228 (-14.21%)        1475        1275  -200 (-13.56%)\nbpf_overlay.o  tail_nodeport_nat_ingress_ipv4         5526       3534   -1992 (-36.05%)         366         243  -123 (-33.61%)\nbpf_overlay.o  tail_nodeport_nat_ingress_ipv6         5132       4256    -876 (-17.07%)         241         219    -22 (-9.13%)\nbpf_overlay.o  tail_nodeport_nat_ipv6_egress          3638       3548      -90 (-2.47%)         209         203     -6 (-2.87%)\nbpf_overlay.o  tail_rev_nodeport_lb4                  4368       3820    -548 (-12.55%)         248         215   -33 (-13.31%)\nbpf_overlay.o  tail_rev_nodeport_lb6                  2867       2428    -439 (-15.31%)         167         140   -27 (-16.17%)\nbpf_sock.o     cil_sock6_connect                      1718       1703      -15 (-0.87%)         100          99     -1 (-1.00%)\nbpf_xdp.o      tail_handle_nat_fwd_ipv4              12917      12443     -474 (-3.67%)         875         849    -26 (-2.97%)\nbpf_xdp.o      tail_handle_nat_fwd_ipv6              13515      13264     -251 (-1.86%)         715         702    -13 (-1.82%)\nbpf_xdp.o      tail_lb_ipv4                          39492      36367    -3125 (-7.91%)        2430        2251   -179 (-7.37%)\nbpf_xdp.o      tail_lb_ipv6                          80441      78058    -2383 (-2.96%)        3647        3523   -124 (-3.40%)\nbpf_xdp.o      tail_nodeport_ipv6_dsr                 1038        901    -137 (-13.20%)          61          55     -6 (-9.84%)\nbpf_xdp.o      tail_nodeport_nat_egress_ipv4         13027      12096     -931 (-7.15%)         868         809    -59 (-6.80%)\nbpf_xdp.o      tail_nodeport_nat_ingress_ipv4         7617       5900   -1717 (-22.54%)         522         413  -109 (-20.88%)\nbpf_xdp.o      tail_nodeport_nat_ingress_ipv6         7575       7395     -180 (-2.38%)         383         374     -9 (-2.35%)\nbpf_xdp.o      tail_rev_nodeport_lb4                  6808       6739      -69 (-1.01%)         403         396     -7 (-1.74%)\nbpf_xdp.o      tail_rev_nodeport_lb6                 16173      15847     -326 (-2.02%)        1010         990    -20 (-1.98%)\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "e322f0bcb8d371f4606eaf141c7f967e1a79bcb7",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-05 13:40:21 -0800",
      "message": "bpf: preserve constant zero when doing partial register restore\n\nSimilar to special handling of STACK_ZERO, when reading 1/2/4 bytes from\nstack from slot that has register spilled into it and that register has\na constant value zero, preserve that zero and mark spilled register as\nprecise for that. This makes spilled const zero register and STACK_ZERO\ncases equivalent in their behavior.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "eaf18febd6ebc381aeb61543705148b3e28c7c47",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-05 13:40:20 -0800",
      "message": "bpf: preserve STACK_ZERO slots on partial reg spills\n\nInstead of always forcing STACK_ZERO slots to STACK_MISC, preserve it in\nsituations where this is possible. E.g., when spilling register as\n1/2/4-byte subslots on the stack, all the remaining bytes in the stack\nslot do not automatically become unknown. If we knew they contained\nzeroes, we can preserve those STACK_ZERO markers.\n\nAdd a helper mark_stack_slot_misc(), similar to scrub_spilled_slot(),\nbut that doesn't overwrite either STACK_INVALID nor STACK_ZERO. Note\nthat we need to take into account possibility of being in unprivileged\nmode, in which case STACK_INVALID is forced to STACK_MISC for correctness,\nas treating STACK_INVALID as equivalent STACK_MISC is only enabled in\nprivileged mode.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ab125ed3ec1c10ccc36bc98c7a4256ad114a3dae",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-05 13:40:20 -0800",
      "message": "bpf: fix check for attempt to corrupt spilled pointer\n\nWhen register is spilled onto a stack as a 1/2/4-byte register, we set\nslot_type[BPF_REG_SIZE - 1] (plus potentially few more below it,\ndepending on actual spill size). So to check if some stack slot has\nspilled register we need to consult slot_type[7], not slot_type[0].\n\nTo avoid the need to remember and double-check this in the future, just\nuse is_spilled_reg() helper.\n\nFixes: 27113c59b6d0 (\"bpf: Check the other end of slot_type for STACK_SPILL\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "41f6f64e6999a837048b1bd13a2f8742964eca6b",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-05 13:40:20 -0800",
      "message": "bpf: support non-r10 register spill/fill to/from stack in precision tracking\n\nUse instruction (jump) history to record instructions that performed\nregister spill/fill to/from stack, regardless if this was done through\nread-only r10 register, or any other register after copying r10 into it\n*and* potentially adjusting offset.\n\nTo make this work reliably, we push extra per-instruction flags into\ninstruction history, encoding stack slot index (spi) and stack frame\nnumber in extra 10 bit flags we take away from prev_idx in instruction\nhistory. We don't touch idx field for maximum performance, as it's\nchecked most frequently during backtracking.\n\nThis change removes basically the last remaining practical limitation of\nprecision backtracking logic in BPF verifier. It fixes known\ndeficiencies, but also opens up new opportunities to reduce number of\nverified states, explored in the subsequent patches.\n\nThere are only three differences in selftests' BPF object files\naccording to veristat, all in the positive direction (less states).\n\nFile                                    Program        Insns (A)  Insns (B)  Insns  (DIFF)  States (A)  States (B)  States (DIFF)\n--------------------------------------  -------------  ---------  ---------  -------------  ----------  ----------  -------------\ntest_cls_redirect_dynptr.bpf.linked3.o  cls_redirect        2987       2864  -123 (-4.12%)         240         231    -9 (-3.75%)\nxdp_synproxy_kern.bpf.linked3.o         syncookie_tc       82848      82661  -187 (-0.23%)        5107        5073   -34 (-0.67%)\nxdp_synproxy_kern.bpf.linked3.o         syncookie_xdp      85116      84964  -152 (-0.18%)        5162        5130   -32 (-0.62%)\n\nNote, I avoided renaming jmp_history to more generic insn_hist to\nminimize number of lines changed and potential merge conflicts between\nbpf and bpf-next trees.\n\nNotice also cur_hist_entry pointer reset to NULL at the beginning of\ninstruction verification loop. This pointer avoids the problem of\nrelying on last jump history entry's insn_idx to determine whether we\nalready have entry for current instruction or not. It can happen that we\nadded jump history entry because current instruction is_jmp_point(), but\nalso we need to add instruction flags for stack access. In this case, we\ndon't want to entries, so we need to reuse last added entry, if it is\npresent.\n\nRelying on insn_idx comparison has the same ambiguity problem as the one\nthat was fixed recently in [0], so we avoid that.\n\n  [0] https://patchwork.kernel.org/project/netdevbpf/patch/20231110002638.4168352-3-andrii@kernel.org/\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nReported-by: Tao Lyu <tao.lyu@epfl.ch>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231205184248.1502704-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c",
        "tools/testing/selftests/bpf/verifier/precise.c"
      ]
    },
    {
      "hash": "af66bfd3c8538ed21cf72af18426fc4a408665cf",
      "author": "Hou Tao <houtao1@huawei.com>",
      "date": "2023-12-04 17:50:26 -0800",
      "message": "bpf: Optimize the free of inner map\n\nWhen removing the inner map from the outer map, the inner map will be\nfreed after one RCU grace period and one RCU tasks trace grace\nperiod, so it is certain that the bpf program, which may access the\ninner map, has exited before the inner map is freed.\n\nHowever there is no need to wait for one RCU tasks trace grace period if\nthe outer map is only accessed by non-sleepable program. So adding\nsleepable_refcnt in bpf_map and increasing sleepable_refcnt when adding\nthe outer map into env->used_maps for sleepable program. Although the\nmax number of bpf program is INT_MAX - 1, the number of bpf programs\nwhich are being loaded may be greater than INT_MAX, so using atomic64_t\ninstead of atomic_t for sleepable_refcnt. When removing the inner map\nfrom the outer map, using sleepable_refcnt to decide whether or not a\nRCU tasks trace grace period is needed before freeing the inner map.\n\nSigned-off-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231204140425.1480317-6-houtao@huaweicloud.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/core.c",
        "kernel/bpf/map_in_map.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5bd90cdc65ef9ef5e13c9ff23620079db5c608a0",
      "author": "Andrei Matei <andreimatei1@gmail.com>",
      "date": "2023-12-04 15:57:27 +0100",
      "message": "bpf: Minor logging improvement\n\nOne place where we were logging a register was only logging the variable\npart, not also the fixed part.\n\nSigned-off-by: Andrei Matei <andreimatei1@gmail.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20231204011248.2040084-1-andreimatei1@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "eabe518de533a4291996020977054a7a7b78c7d3",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-02 11:36:51 -0800",
      "message": "bpf: enforce precision of R0 on program/async callback return\n\nGiven we enforce a valid range for program and async callback return\nvalue, we must mark R0 as precise to avoid incorrect state pruning.\n\nFixes: b5dc0163d8fd (\"bpf: precise scalar_value tracking\")\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "0ef24c8dfae24a4b8aa2e92eac20faecdc5502e5",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-02 11:36:50 -0800",
      "message": "bpf: unify async callback and program retval checks\n\nUse common logic to verify program return values and async callback\nreturn values. This allows to avoid duplication of any extra steps\nnecessary, like precision marking, which will be added in the next\npatch.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c871d0e00f0e8c207ce8ff89025e35cc49a8a3c3",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-02 11:36:50 -0800",
      "message": "bpf: enforce precise retval range on program exit\n\nSimilarly to subprog/callback logic, enforce return value of BPF program\nusing more precise smin/smax range.\n\nWe need to adjust a bunch of tests due to a changed format of an error\nmessage.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/exceptions_assert.c",
        "tools/testing/selftests/bpf/progs/exceptions_fail.c",
        "tools/testing/selftests/bpf/progs/test_global_func15.c",
        "tools/testing/selftests/bpf/progs/timer_failure.c",
        "tools/testing/selftests/bpf/progs/user_ringbuf_fail.c",
        "tools/testing/selftests/bpf/progs/verifier_cgroup_inv_retcode.c",
        "tools/testing/selftests/bpf/progs/verifier_netfilter_retcode.c",
        "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c"
      ]
    },
    {
      "hash": "8fa4ecd49b81ccd9d1d87f1c8b2260e218644878",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-02 11:36:50 -0800",
      "message": "bpf: enforce exact retval range on subprog/callback exit\n\nInstead of relying on potentially imprecise tnum representation of\nexpected return value range for callbacks and subprogs, validate that\nsmin/smax range satisfy exact expected range of return values.\n\nE.g., if callback would need to return [0, 2] range, tnum can't\nrepresent this precisely and instead will allow [0, 3] range. By\nchecking smin/smax range, we can make sure that subprog/callback indeed\nreturns only valid [0, 2] range.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "0acd03a5bd188b0c501d285d938439618bd855c4",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-02 11:36:50 -0800",
      "message": "bpf: enforce precision of R0 on callback return\n\nGiven verifier checks actual value, r0 has to be precise, so we need to\npropagate precision properly. r0 also has to be marked as read,\notherwise subsequent state comparisons will ignore such register as\nunimportant and precision won't really help here.\n\nFixes: 69c087ba6225 (\"bpf: Add bpf_for_each_map_elem() helper\")\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5fad52bee30414270104525e3a0266327a6e9d11",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-12-02 11:36:50 -0800",
      "message": "bpf: provide correct register name for exception callback retval check\n\nbpf_throw() is checking R1, so let's report R1 in the log.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231202175705.885270-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/exceptions_assert.c",
        "tools/testing/selftests/bpf/progs/exceptions_fail.c"
      ]
    },
    {
      "hash": "2afae08c9dcb8ac648414277cec70c2fe6a34d9e",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-24 10:40:06 +0100",
      "message": "bpf: Validate global subprogs lazily\n\nSlightly change BPF verifier logic around eagerness and order of global\nsubprog validation. Instead of going over every global subprog eagerly\nand validating it before main (entry) BPF program is verified, turn it\naround. Validate main program first, mark subprogs that were called from\nmain program for later verification, but otherwise assume it is valid.\nAfterwards, go over marked global subprogs and validate those,\npotentially marking some more global functions as being called. Continue\nthis process until all (transitively) callable global subprogs are\nvalidated. It's a BFS traversal at its heart and will always converge.\n\nThis is an important change because it allows to feature-gate some\nsubprograms that might not be verifiable on some older kernel, depending\non supported set of features.\n\nE.g., at some point, global functions were allowed to accept a pointer\nto memory, which size is identified by user-provided type.\nUnfortunately, older kernels don't support this feature. With BPF CO-RE\napproach, the natural way would be to still compile BPF object file once\nand guard calls to this global subprog with some CO-RE check or using\n.rodata variables. That's what people do to guard usage of new helpers\nor kfuncs, and any other new BPF-side feature that might be missing on\nold kernels.\n\nThat's currently impossible to do with global subprogs, unfortunately,\nbecause they are eagerly and unconditionally validated. This patch set\naims to change this, so that in the future when global funcs gain new\nfeatures, those can be guarded using BPF CO-RE techniques in the same\nfashion as any other new kernel feature.\n\nTwo selftests had to be adjusted in sync with these changes.\n\ntest_global_func12 relied on eager global subprog validation failing\nbefore main program failure is detected (unknown return value). Fix by\nmaking sure that main program is always valid.\n\nverifier_subprog_precision's parent_stack_slot_precise subtest relied on\nverifier checkpointing heuristic to do a checkpoint at instruction #5,\nbut that's no longer true because we don't have enough jumps validated\nbefore reaching insn #5 due to global subprogs being validated later.\n\nOther than that, no changes, as one would expect.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20231124035937.403208-3-andrii@kernel.org",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/test_global_func12.c",
        "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c"
      ]
    },
    {
      "hash": "491dd8edecbc5027ee317f3f1e7e9800fb66d88f",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-24 10:40:06 +0100",
      "message": "bpf: Emit global subprog name in verifier logs\n\nWe have the name, instead of emitting just func#N to identify global\nsubprog, augment verifier log messages with actual function name to make\nit more user-friendly.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20231124035937.403208-2-andrii@kernel.org",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "bb124da69c47dd98d69361ec13244ece50bec63e",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-11-20 18:36:40 -0800",
      "message": "bpf: keep track of max number of bpf_loop callback iterations\n\nIn some cases verifier can't infer convergence of the bpf_loop()\niteration. E.g. for the following program:\n\n    static int cb(__u32 idx, struct num_context* ctx)\n    {\n        ctx->i++;\n        return 0;\n    }\n\n    SEC(\"?raw_tp\")\n    int prog(void *_)\n    {\n        struct num_context ctx = { .i = 0 };\n        __u8 choice_arr[2] = { 0, 1 };\n\n        bpf_loop(2, cb, &ctx, 0);\n        return choice_arr[ctx.i];\n    }\n\nEach 'cb' simulation would eventually return to 'prog' and reach\n'return choice_arr[ctx.i]' statement. At which point ctx.i would be\nmarked precise, thus forcing verifier to track multitude of separate\nstates with {.i=0}, {.i=1}, ... at bpf_loop() callback entry.\n\nThis commit allows \"brute force\" handling for such cases by limiting\nnumber of callback body simulations using 'umax' value of the first\nbpf_loop() parameter.\n\nFor this, extend bpf_func_state with 'callback_depth' field.\nIncrement this field when callback visiting state is pushed to states\ntraversal stack. For frame #N it's 'callback_depth' field counts how\nmany times callback with frame depth N+1 had been executed.\nUse bpf_func_state specifically to allow independent tracking of\ncallback depths when multiple nested bpf_loop() calls are present.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-11-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c"
      ]
    },
    {
      "hash": "cafe2c21508a38cdb3ed22708842e957b2572c3e",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-11-20 18:36:40 -0800",
      "message": "bpf: widening for callback iterators\n\nCallbacks are similar to open coded iterators, so add imprecise\nwidening logic for callback body processing. This makes callback based\nloops behave identically to open coded iterators, e.g. allowing to\nverify programs like below:\n\n  struct ctx { u32 i; };\n  int cb(u32 idx, struct ctx* ctx)\n  {\n          ++ctx->i;\n          return 0;\n  }\n  ...\n  struct ctx ctx = { .i = 0 };\n  bpf_loop(100, cb, &ctx, 0);\n  ...\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-9-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ab5cfac139ab8576fb54630d4cca23c3e690ee90",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-11-20 18:35:44 -0800",
      "message": "bpf: verify callbacks as if they are called unknown number of times\n\nPrior to this patch callbacks were handled as regular function calls,\nexecution of callback body was modeled exactly once.\nThis patch updates callbacks handling logic as follows:\n- introduces a function push_callback_call() that schedules callback\n  body verification in env->head stack;\n- updates prepare_func_exit() to reschedule callback body verification\n  upon BPF_EXIT;\n- as calls to bpf_*_iter_next(), calls to callback invoking functions\n  are marked as checkpoints;\n- is_state_visited() is updated to stop callback based iteration when\n  some identical parent state is found.\n\nPaths with callback function invoked zero times are now verified first,\nwhich leads to necessity to modify some selftests:\n- the following negative tests required adding release/unlock/drop\n  calls to avoid previously masked unrelated error reports:\n  - cb_refs.c:underflow_prog\n  - exceptions_fail.c:reject_rbtree_add_throw\n  - exceptions_fail.c:reject_with_cp_reference\n- the following precision tracking selftests needed change in expected\n  log trace:\n  - verifier_subprog_precision.c:callback_result_precise\n    (note: r0 precision is no longer propagated inside callback and\n           I think this is a correct behavior)\n  - verifier_subprog_precision.c:parent_callee_saved_reg_precise_with_callback\n  - verifier_subprog_precision.c:parent_stack_slot_precise_with_callback\n\nReported-by: Andrew Werner <awerner32@gmail.com>\nCloses: https://lore.kernel.org/bpf/CA+vRuzPChFNXmouzGG+wsy=6eMcfr1mFG0F3g7rbg-sedGKW3w@mail.gmail.com/\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-7-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/cb_refs.c",
        "tools/testing/selftests/bpf/progs/exceptions_fail.c",
        "tools/testing/selftests/bpf/progs/verifier_subprog_precision.c"
      ]
    },
    {
      "hash": "58124a98cb8eda69d248d7f1de954c8b2767c945",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-11-20 18:33:35 -0800",
      "message": "bpf: extract setup_func_entry() utility function\n\nMove code for simulated stack frame creation to a separate utility\nfunction. This function would be used in the follow-up change for\ncallbacks handling.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-6-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "683b96f9606ab7308ffb23c46ab43cecdef8a241",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-11-20 18:33:35 -0800",
      "message": "bpf: extract __check_reg_arg() utility function\n\nSplit check_reg_arg() into two utility functions:\n- check_reg_arg() operating on registers from current verifier state;\n- __check_reg_arg() operating on a specific set of registers passed as\n  a parameter;\n\nThe __check_reg_arg() function would be used by a follow-up change for\ncallbacks handling.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231121020701.26440-5-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "42feb6620accded89cad5f455665e21281813d79",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-18 11:39:59 -0800",
      "message": "bpf: move verifier state printing code to kernel/bpf/log.c\n\nMove a good chunk of code from verifier.c to log.c: verifier state\nverbose printing logic. This is an important and very much\nlogging/debugging oriented code. It fits the overlall log.c's focus on\nverifier logging, and moving it allows to keep growing it without\nunnecessarily adding to verifier.c code that otherwise contains a core\nverification logic.\n\nThere are not many shared dependencies between this code and the rest of\nverifier.c code, except a few single-line helpers for various register\ntype checks and a bit of state \"scratching\" helpers. We move all such\ntrivial helpers into include/bpf/bpf_verifier.h as static inlines.\n\nNo functional changes in this patch.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231118034623.3320920-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/log.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "db840d389bad60ce6f3aadc1079da13e7e993a16",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-18 11:39:58 -0800",
      "message": "bpf: move verbose_linfo() into kernel/bpf/log.c\n\nverifier.c is huge. Let's try to move out parts that are logging-related\ninto log.c, as we previously did with bpf_log() and other related stuff.\nThis patch moves line info verbose output routines: it's pretty\nself-contained and isolated code, so there is no problem with this.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231118034623.3320920-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/log.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ff8867af01daa7ea770bebf5f91199b7434b74e5",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-17 10:30:02 -0800",
      "message": "bpf: rename BPF_F_TEST_SANITY_STRICT to BPF_F_TEST_REG_INVARIANTS\n\nRename verifier internal flag BPF_F_TEST_SANITY_STRICT to more neutral\nBPF_F_TEST_REG_INVARIANTS. This is a follow up to [0].\n\nA few selftests and veristat need to be adjusted in the same patch as\nwell.\n\n  [0] https://patchwork.kernel.org/project/netdevbpf/patch/20231112010609.848406-5-andrii@kernel.org/\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231117171404.225508-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c",
        "tools/include/uapi/linux/bpf.h",
        "tools/testing/selftests/bpf/prog_tests/bpf_verif_scale.c",
        "tools/testing/selftests/bpf/prog_tests/reg_bounds.c",
        "tools/testing/selftests/bpf/progs/verifier_bounds.c",
        "tools/testing/selftests/bpf/test_loader.c",
        "tools/testing/selftests/bpf/test_sock_addr.c",
        "tools/testing/selftests/bpf/test_verifier.c",
        "tools/testing/selftests/bpf/testing_helpers.c",
        "tools/testing/selftests/bpf/veristat.c"
      ]
    },
    {
      "hash": "cf5fe3c71c5a34ac0108afc550407c672d0a032d",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-15 12:03:42 -0800",
      "message": "bpf: make __reg{32,64}_deduce_bounds logic more robust\n\nThis change doesn't seem to have any effect on selftests and production\nBPF object files, but we preemptively try to make it more robust.\n\nFirst, \"learn sign from signed bounds\" comment is misleading, as we are\nlearning not just sign, but also values.\n\nSecond, we simplify the check for determining whether entire range is\npositive or negative similarly to other checks added earlier, using\nappropriate u32/u64 cast and single comparisons. As explain in comments\nin __reg64_deduce_bounds(), the checks are equivalent.\n\nLast but not least, smin/smax and s32_min/s32_max reassignment based on\nmin/max of both umin/umax and smin/smax (and 32-bit equivalents) is hard\nto explain and justify. We are updating unsigned bounds from signed\nbounds, why would we update signed bounds at the same time? This might\nbe correct, but it's far from obvious why and the code or comments don't\ntry to justify this. Given we've added a separate deduction of signed\nbounds from unsigned bounds earlier, this seems at least redundant, if\nnot just wrong.\n\nIn short, we remove doubtful pieces, and streamline the rest to follow\nthe logic and approach of the rest of reg_bounds_sync() checks.\n\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231112010609.848406-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "3cf98cf594ea923b8b1e0385b580d3d8aae68c06",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-15 12:03:42 -0800",
      "message": "bpf: remove redundant s{32,64} -> u{32,64} deduction logic\n\nEquivalent checks were recently added in more succinct and, arguably,\nsafer form in:\n  - f188765f23a5 (\"bpf: derive smin32/smax32 from umin32/umax32 bounds\");\n  - 2e74aef782d3 (\"bpf: derive smin/smax from umin/max bounds\").\n\nThe checks we are removing in this patch set do similar checks to detect\nif entire u32/u64 range has signed bit set or not set, but does it with\ntwo separate checks.\n\nFurther, we forcefully overwrite either smin or smax (and 32-bit equvalents)\nwithout applying normal min/max intersection logic. It's not clear why\nthat would be correct in all cases and seems to work by accident. This\nlogic is also \"gated\" by previous signed -> unsigned derivation, which\nreturns early.\n\nAll this is quite confusing and seems error-prone, while we already have\nat least equivalent checks happening earlier. So remove this duplicate\nand error-prone logic to simplify things a bit.\n\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231112010609.848406-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5f99f312bd3bedb3b266b0d26376a8c500cdc97f",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-15 12:03:42 -0800",
      "message": "bpf: add register bounds sanity checks and sanitization\n\nAdd simple sanity checks that validate well-formed ranges (min <= max)\nacross u64, s64, u32, and s32 ranges. Also for cases when the value is\nconstant (either 64-bit or 32-bit), we validate that ranges and tnums\nare in agreement.\n\nThese bounds checks are performed at the end of BPF_ALU/BPF_ALU64\noperations, on conditional jumps, and for LDX instructions (where subreg\nzero/sign extension is probably the most important to check). This\ncovers most of the interesting cases.\n\nAlso, we validate the sanity of the return register when manually\nadjusting it for some special helpers.\n\nBy default, sanity violation will trigger a warning in verifier log and\nresetting register bounds to \"unbounded\" ones. But to aid development\nand debugging, BPF_F_TEST_SANITY_STRICT flag is added, which will\ntrigger hard failure of verification with -EFAULT on register bounds\nviolations. This allows selftests to catch such issues. veristat will\nalso gain a CLI option to enable this behavior.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231112010609.848406-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c",
        "tools/include/uapi/linux/bpf.h"
      ]
    },
    {
      "hash": "be41a203bb9e0159099e189e510388fe61962eb8",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-15 12:03:42 -0800",
      "message": "bpf: enhance BPF_JEQ/BPF_JNE is_branch_taken logic\n\nUse 32-bit subranges to prune some 64-bit BPF_JEQ/BPF_JNE conditions\nthat otherwise would be \"inconclusive\" (i.e., is_branch_taken() would\nreturn -1). This can happen, for example, when registers are initialized\nas 64-bit u64/s64, then compared for inequality as 32-bit subregisters,\nand then followed by 64-bit equality/inequality check. That 32-bit\ninequality can establish some pattern for lower 32 bits of a register\n(e.g., s< 0 condition determines whether the bit #31 is zero or not),\nwhile overall 64-bit value could be anything (according to a value range\nrepresentation).\n\nThis is not a fancy quirky special case, but actually a handling that's\nnecessary to prevent correctness issue with BPF verifier's range\ntracking: set_range_min_max() assumes that register ranges are\nnon-overlapping, and if that condition is not guaranteed by\nis_branch_taken() we can end up with invalid ranges, where min > max.\n\n  [0] https://lore.kernel.org/bpf/CACkBjsY2q1_fUohD7hRmKGqv1MV=eP2f6XK8kjkYNw7BaiF8iQ@mail.gmail.com/\n\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231112010609.848406-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "96381879a370425a30b810906946f64c0726450e",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-15 12:03:41 -0800",
      "message": "bpf: generalize is_scalar_branch_taken() logic\n\nGeneralize is_branch_taken logic for SCALAR_VALUE register to handle\ncases when both registers are not constants. Previously supported\n<range> vs <scalar> cases are a natural subset of more generic <range>\nvs <range> set of cases.\n\nGeneralized logic relies on straightforward segment intersection checks.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231112010609.848406-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "67420501e8681ae18f9f0ea0a69cd2f432100e70",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-15 12:03:41 -0800",
      "message": "bpf: generalize reg_set_min_max() to handle non-const register comparisons\n\nGeneralize bounds adjustment logic of reg_set_min_max() to handle not\njust register vs constant case, but in general any register vs any\nregister cases. For most of the operations it's trivial extension based\non range vs range comparison logic, we just need to properly pick\nmin/max of a range to compare against min/max of the other range.\n\nFor BPF_JSET we keep the original capabilities, just make sure JSET is\nintegrated in the common framework. This is manifested in the\ninternal-only BPF_JSET + BPF_X \"opcode\" to allow for simpler and more\nuniform rev_opcode() handling. See the code for details. This allows to\nreuse the same code exactly both for TRUE and FALSE branches without\nexplicitly handling both conditions with custom code.\n\nNote also that now we don't need a special handling of BPF_JEQ/BPF_JNE\ncase none of the registers are constants. This is now just a normal\ngeneric case handled by reg_set_min_max().\n\nTo make tnum handling cleaner, tnum_with_subreg() helper is added, as\nthat's a common operator when dealing with 32-bit subregister bounds.\nThis keeps the overall logic much less noisy when it comes to tnums.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231112010609.848406-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/tnum.h",
        "kernel/bpf/tnum.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1fda5bb66ad8fb24ecb3858e61a13a6548428898",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-11-15 07:51:06 -0800",
      "message": "bpf: Do not allocate percpu memory at init stage\n\nKirill Shutemov reported significant percpu memory consumption increase after\nbooting in 288-cpu VM ([1]) due to commit 41a5db8d8161 (\"bpf: Add support for\nnon-fix-size percpu mem allocation\"). The percpu memory consumption is\nincreased from 111MB to 969MB. The number is from /proc/meminfo.\n\nI tried to reproduce the issue with my local VM which at most supports upto\n255 cpus. With 252 cpus, without the above commit, the percpu memory\nconsumption immediately after boot is 57MB while with the above commit the\npercpu memory consumption is 231MB.\n\nThis is not good since so far percpu memory from bpf memory allocator is not\nwidely used yet. Let us change pre-allocation in init stage to on-demand\nallocation when verifier detects there is a need of percpu memory for bpf\nprogram. With this change, percpu memory consumption after boot can be reduced\nsignicantly.\n\n  [1] https://lore.kernel.org/lkml/20231109154934.4saimljtqx625l3v@box.shutemov.name/\n\nFixes: 41a5db8d8161 (\"bpf: Add support for non-fix-size percpu mem allocation\")\nReported-and-tested-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: Hou Tao <houtao1@huawei.com>\nLink: https://lore.kernel.org/r/20231111013928.948838-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "10e14e9652bf9e8104151bfd9200433083deae3d",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 22:57:24 -0800",
      "message": "bpf: fix control-flow graph checking in privileged mode\n\nWhen BPF program is verified in privileged mode, BPF verifier allows\nbounded loops. This means that from CFG point of view there are\ndefinitely some back-edges. Original commit adjusted check_cfg() logic\nto not detect back-edges in control flow graph if they are resulting\nfrom conditional jumps, which the idea that subsequent full BPF\nverification process will determine whether such loops are bounded or\nnot, and either accept or reject the BPF program. At least that's my\nreading of the intent.\n\nUnfortunately, the implementation of this idea doesn't work correctly in\nall possible situations. Conditional jump might not result in immediate\nback-edge, but just a few unconditional instructions later we can arrive\nat back-edge. In such situations check_cfg() would reject BPF program\neven in privileged mode, despite it might be bounded loop. Next patch\nadds one simple program demonstrating such scenario.\n\nTo keep things simple, instead of trying to detect back edges in\nprivileged mode, just assume every back edge is valid and let subsequent\nBPF verification prove or reject bounded loops.\n\nNote a few test changes. For unknown reason, we have a few tests that\nare specified to detect a back-edge in a privileged mode, but looking at\ntheir code it seems like the right outcome is passing check_cfg() and\nletting subsequent verification to make a decision about bounded or not\nbounded looping.\n\nBounded recursion case is also interesting. The example should pass, as\nrecursion is limited to just a few levels and so we never reach maximum\nnumber of nested frames and never exhaust maximum stack depth. But the\nway that max stack depth logic works today it falsely detects this as\nexceeding max nested frame count. This patch series doesn't attempt to\nfix this orthogonal problem, so we just adjust expected verifier failure.\n\nSuggested-by: Alexei Starovoitov <ast@kernel.org>\nFixes: 2589726d12a1 (\"bpf: introduce bounded loops\")\nReported-by: Hao Sun <sunhao.th@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231110061412.2995786-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_loops1.c",
        "tools/testing/selftests/bpf/verifier/calls.c"
      ]
    },
    {
      "hash": "4bb7ea946a370707315ab774432963ce47291946",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 20:11:20 -0800",
      "message": "bpf: fix precision backtracking instruction iteration\n\nFix an edge case in __mark_chain_precision() which prematurely stops\nbacktracking instructions in a state if it happens that state's first\nand last instruction indexes are the same. This situations doesn't\nnecessarily mean that there were no instructions simulated in a state,\nbut rather that we starting from the instruction, jumped around a bit,\nand then ended up at the same instruction before checkpointing or\nmarking precision.\n\nTo distinguish between these two possible situations, we need to consult\njump history. If it's empty or contain a single record \"bridging\" parent\nstate and first instruction of processed state, then we indeed\nbacktracked all instructions in this state. But if history is not empty,\nwe are definitely not done yet.\n\nMove this logic inside get_prev_insn_idx() to contain it more nicely.\nUse -ENOENT return code to denote \"we are out of instructions\"\nsituation.\n\nThis bug was exposed by verifier_loop1.c's bounded_recursion subtest, once\nthe next fix in this patch set is applied.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nFixes: b5dc0163d8fd (\"bpf: precise scalar_value tracking\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231110002638.4168352-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "3feb263bb516ee7e1da0acd22b15afbb9a7daa19",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 20:11:20 -0800",
      "message": "bpf: handle ldimm64 properly in check_cfg()\n\nldimm64 instructions are 16-byte long, and so have to be handled\nappropriately in check_cfg(), just like the rest of BPF verifier does.\n\nThis has implications in three places:\n  - when determining next instruction for non-jump instructions;\n  - when determining next instruction for callback address ldimm64\n    instructions (in visit_func_call_insn());\n  - when checking for unreachable instructions, where second half of\n    ldimm64 is expected to be unreachable;\n\nWe take this also as an opportunity to report jump into the middle of\nldimm64. And adjust few test_verifier tests accordingly.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nReported-by: Hao Sun <sunhao.th@gmail.com>\nFixes: 475fb78fbf48 (\"bpf: verifier (add branch/goto checks)\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231110002638.4168352-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/verifier/ld_imm64.c"
      ]
    },
    {
      "hash": "1b12171533a9bb23cf6fba7262b479028b65e1e8",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-11-09 19:07:51 -0800",
      "message": "bpf: Mark direct ld of stashed bpf_{rb,list}_node as non-owning ref\n\nThis patch enables the following pattern:\n\n  /* mapval contains a __kptr pointing to refcounted local kptr */\n  mapval = bpf_map_lookup_elem(&map, &idx);\n  if (!mapval || !mapval->some_kptr) { /* omitted */ }\n\n  p = bpf_refcount_acquire(&mapval->some_kptr);\n\nCurrently this doesn't work because bpf_refcount_acquire expects an\nowning or non-owning ref. The verifier defines non-owning ref as a type:\n\n  PTR_TO_BTF_ID | MEM_ALLOC | NON_OWN_REF\n\nwhile mapval->some_kptr is PTR_TO_BTF_ID | PTR_UNTRUSTED. It's possible\nto do the refcount_acquire by first bpf_kptr_xchg'ing mapval->some_kptr\ninto a temp kptr, refcount_acquiring that, and xchg'ing back into\nmapval, but this is unwieldy and shouldn't be necessary.\n\nThis patch modifies btf_ld_kptr_type such that user-allocated types are\nmarked MEM_ALLOC and if those types have a bpf_{rb,list}_node they're\nmarked NON_OWN_REF as well. Additionally, due to changes to\nbpf_obj_drop_impl earlier in this series, rcu_protected_object now\nreturns true for all user-allocated types, resulting in\nmapval->some_kptr being marked MEM_RCU.\n\nAfter this patch's changes, mapval->some_kptr is now:\n\n  PTR_TO_BTF_ID | MEM_ALLOC | NON_OWN_REF | MEM_RCU\n\nwhich results in it passing the non-owning ref test, and the motivating\nexample passing verification.\n\nFuture work will likely get rid of special non-owning ref lifetime logic\nin the verifier, at which point we'll be able to delete the NON_OWN_REF\nflag entirely.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20231107085639.3016113-6-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "82ce364c6087e31ff9837380a4641a856284064c",
      "author": "Shung-Hsi Yu <shung-hsi.yu@suse.com>",
      "date": "2023-11-09 19:07:51 -0800",
      "message": "bpf: replace register_is_const() with is_reg_const()\n\nThe addition of is_reg_const() in commit 171de12646d2 (\"bpf: generalize\nis_branch_taken to handle all conditional jumps in one place\") has made the\nregister_is_const() redundant. Give the former has more feature, plus the\nfact the latter is only used in one place, replace register_is_const() with\nis_reg_const(), and remove the definition of register_is_const.\n\nThis requires moving the definition of is_reg_const() further up. And since\nthe comment of reg_const_value() reference is_reg_const(), move it up as\nwell.\n\nSigned-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231108140043.12282-1-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "045edee19d591e59ed53772bf6dfc9b1ed9577eb",
      "author": "Song Liu <song@kernel.org>",
      "date": "2023-11-09 19:07:38 -0800",
      "message": "bpf: Introduce KF_ARG_PTR_TO_CONST_STR\n\nSimilar to ARG_PTR_TO_CONST_STR for BPF helpers, KF_ARG_PTR_TO_CONST_STR\nspecifies kfunc args that point to const strings. Annotation \"__str\" is\nused to specify kfunc arg of type KF_ARG_PTR_TO_CONST_STR. Also, add\ndocumentation for the \"__str\" annotation.\n\nbpf_get_file_xattr() will be the first kfunc that uses this type.\n\nSigned-off-by: Song Liu <song@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Vadim Fedorenko <vadim.fedorenko@linux.dev>\nLink: https://lore.kernel.org/bpf/20231107045725.2278852-4-song@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "Documentation/bpf/kfuncs.rst",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "0b51940729150e807fc4b7767164e6bb6cf4f7dd",
      "author": "Song Liu <song@kernel.org>",
      "date": "2023-11-09 19:07:38 -0800",
      "message": "bpf: Factor out helper check_reg_const_str()\n\nARG_PTR_TO_CONST_STR is used to specify constant string args for BPF\nhelpers. The logic that verifies a reg is ARG_PTR_TO_CONST_STR is\nimplemented in check_func_arg().\n\nAs we introduce kfuncs with constant string args, it is necessary to\ndo the same check for kfuncs (in check_kfunc_args). Factor out the logic\nfor ARG_PTR_TO_CONST_STR to a new check_reg_const_str() so that it can be\nreused.\n\ncheck_func_arg() ensures check_reg_const_str() is only called with reg of\ntype PTR_TO_MAP_VALUE. Add a redundent type check in check_reg_const_str()\nto avoid misuse in the future. Other than this redundent check, there is\nno change in behavior.\n\nSigned-off-by: Song Liu <song@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Vadim Fedorenko <vadim.fedorenko@linux.dev>\nLink: https://lore.kernel.org/bpf/20231107045725.2278852-3-song@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "4621202adc5bc0d1006af37fe8b9aca131387d3c",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:40 -0800",
      "message": "bpf: generalize reg_set_min_max() to handle two sets of two registers\n\nChange reg_set_min_max() to take FALSE/TRUE sets of two registers each,\ninstead of assuming that we are always comparing to a constant. For now\nwe still assume that right-hand side registers are constants (and make\nsure that's the case by swapping src/dst regs, if necessary), but\nsubsequent patches will remove this limitation.\n\nreg_set_min_max() is now called unconditionally for any register\ncomparison, so that might include pointer vs pointer. This makes it\nconsistent with is_branch_taken() generality. But we currently only\nsupport adjustments based on SCALAR vs SCALAR comparisons, so\nreg_set_min_max() has to guard itself againts pointers.\n\nTaking two by two registers allows to further unify and simplify\ncheck_cond_jmp_op() logic. We utilize fake register for BPF_K\nconditional jump case, just like with is_branch_taken() part.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-18-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "811476e9cc578cb6c776627ac069dc45a8431791",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:40 -0800",
      "message": "bpf: prepare reg_set_min_max for second set of registers\n\nSimilarly to is_branch_taken()-related refactorings, start preparing\nreg_set_min_max() to handle more generic case of two non-const\nregisters. Start with renaming arguments to accommodate later addition\nof second register as an input argument.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-17-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "4d345887d2e5a1915600cb5d37b16c4088c6ee1c",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:40 -0800",
      "message": "bpf: unify 32-bit and 64-bit is_branch_taken logic\n\nCombine 32-bit and 64-bit is_branch_taken logic for SCALAR_VALUE\nregisters. It makes it easier to see parallels between two domains\n(32-bit and 64-bit), and makes subsequent refactoring more\nstraightforward.\n\nNo functional changes.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-16-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b74c2a842bba941945279027083fcee1e9aaa73f",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:40 -0800",
      "message": "bpf: generalize is_branch_taken to handle all conditional jumps in one place\n\nMake is_branch_taken() a single entry point for branch pruning decision\nmaking, handling both pointer vs pointer, pointer vs scalar, and scalar\nvs scalar cases in one place. This also nicely cleans up check_cond_jmp_op().\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-15-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c697289efe4ef38bc5c62f119cb74433f784b826",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:39 -0800",
      "message": "bpf: move is_branch_taken() down\n\nMove is_branch_taken() slightly down. In subsequent patched we'll need\nboth flip_opcode() and is_pkt_ptr_branch_taken() for is_branch_taken(),\nbut instead of sprinkling forward declarations around, it makes more\nsense to move is_branch_taken() lower below is_pkt_ptr_branch_taken(),\nand also keep it closer to very tightly related reg_set_min_max(), as\nthey are two critical parts of the same SCALAR range tracking logic.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-14-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c31534267c180f7ed00288d239a501b554885300",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:39 -0800",
      "message": "bpf: generalize is_branch_taken() to work with two registers\n\nWhile still assuming that second register is a constant, generalize\nis_branch_taken-related code to accept two registers instead of register\nplus explicit constant value. This also, as a side effect, allows to\nsimplify check_cond_jmp_op() by unifying BPF_K case with BPF_X case, for\nwhich we use a fake register to represent BPF_K's imm constant as\na register.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231102033759.2541186-13-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c2a3ab094683ddc154879a1364fc7cb0228f96a6",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:39 -0800",
      "message": "bpf: rename is_branch_taken reg arguments to prepare for the second one\n\nJust taking mundane refactoring bits out into a separate patch. No\nfunctional changes.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231102033759.2541186-12-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "9e314f5d8682e1fe6ac214fb34580a238b6fd3c4",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:39 -0800",
      "message": "bpf: drop knowledge-losing __reg_combine_{32,64}_into_{64,32} logic\n\nWhen performing 32-bit conditional operation operating on lower 32 bits\nof a full 64-bit register, register full value isn't changed. We just\npotentially gain new knowledge about that register's lower 32 bits.\n\nUnfortunately, __reg_combine_{32,64}_into_{64,32} logic that\nreg_set_min_max() performs as a last step, can lose information in some\ncases due to __mark_reg64_unbounded() and __reg_assign_32_into_64().\nThat's bad and completely unnecessary. Especially __reg_assign_32_into_64()\nlooks completely out of place here, because we are not performing\nzero-extending subregister assignment during conditional jump.\n\nSo this patch replaced __reg_combine_* with just a normal\nreg_bounds_sync() which will do a proper job of deriving u64/s64 bounds\nfrom u32/s32, and vice versa (among all other combinations).\n\n__reg_combine_64_into_32() is also used in one more place,\ncoerce_reg_to_size(), while handling 1- and 2-byte register loads.\nLooking into this, it seems like besides marking subregister as\nunbounded before performing reg_bounds_sync(), we were also performing\ndeduction of smin32/smax32 and umin32/umax32 bounds from respective\nsmin/smax and umin/umax bounds. It's now redundant as reg_bounds_sync()\nperforms all the same logic more generically (e.g., without unnecessary\nassumption that upper 32 bits of full register should be zero).\n\nLong story short, we remove __reg_combine_64_into_32() completely, and\ncoerce_reg_to_size() now only does resetting subreg to unbounded and then\nperforming reg_bounds_sync() to recover as much information as possible\nfrom 64-bit umin/umax and smin/smax bounds, set explicitly in\ncoerce_reg_to_size() earlier.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231102033759.2541186-10-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d7f00873817129e62f8c70891cb13c8eafe9feef",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:39 -0800",
      "message": "bpf: try harder to deduce register bounds from different numeric domains\n\nThere are cases (caught by subsequent reg_bounds tests in selftests/bpf)\nwhere performing one round of __reg_deduce_bounds() doesn't propagate\nall the information from, say, s32 to u32 bounds and than from newly\nlearned u32 bounds back to u64 and s64. So perform __reg_deduce_bounds()\ntwice to make sure such derivations are propagated fully after\nreg_bounds_sync().\n\nOne such example is test `(s64)[0xffffffff00000001; 0] (u64)<\n0xffffffff00000000` from selftest patch from this patch set. It demonstrates an\nintricate dance of u64 -> s64 -> u64 -> u32 bounds adjustments, which requires\ntwo rounds of __reg_deduce_bounds(). Here are corresponding refinement log from\nselftest, showing evolution of knowledge.\n\nREFINING (FALSE R1) (u64)SRC=[0xffffffff00000000; U64_MAX] (u64)DST_OLD=[0; U64_MAX] (u64)DST_NEW=[0xffffffff00000000; U64_MAX]\nREFINING (FALSE R1) (u64)SRC=[0xffffffff00000000; U64_MAX] (s64)DST_OLD=[0xffffffff00000001; 0] (s64)DST_NEW=[0xffffffff00000001; -1]\nREFINING (FALSE R1) (s64)SRC=[0xffffffff00000001; -1] (u64)DST_OLD=[0xffffffff00000000; U64_MAX] (u64)DST_NEW=[0xffffffff00000001; U64_MAX]\nREFINING (FALSE R1) (u64)SRC=[0xffffffff00000001; U64_MAX] (u32)DST_OLD=[0; U32_MAX] (u32)DST_NEW=[1; U32_MAX]\n\nR1 initially has smin/smax set to [0xffffffff00000001; -1], while umin/umax is\nunknown. After (u64)< comparison, in FALSE branch we gain knowledge that\numin/umax is [0xffffffff00000000; U64_MAX]. That causes smin/smax to learn that\nzero can't happen and upper bound is -1. Then smin/smax is adjusted from\numin/umax improving lower bound from 0xffffffff00000000 to 0xffffffff00000001.\nAnd then eventually umin32/umax32 bounds are drived from umin/umax and become\n[1; U32_MAX].\n\nSelftest in the last patch is actually implementing a multi-round fixed-point\nconvergence logic, but so far all the tests are handled by two rounds of\nreg_bounds_sync() on the verifier state, so we keep it simple for now.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c51d5ad6543cc36334ef1fcd762d0df767a0bf7e",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:39 -0800",
      "message": "bpf: improve deduction of 64-bit bounds from 32-bit bounds\n\nAdd a few interesting cases in which we can tighten 64-bit bounds based\non newly learnt information about 32-bit bounds. E.g., when full u64/s64\nregisters are used in BPF program, and then eventually compared as\nu32/s32. The latter comparison doesn't change the value of full\nregister, but it does impose new restrictions on possible lower 32 bits\nof such full registers. And we can use that to derive additional full\nregister bounds information.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231102033759.2541186-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "6593f2e6741f03b49bffc9d55ddd4c1c47853c39",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:39 -0800",
      "message": "bpf: add special smin32/smax32 derivation from 64-bit bounds\n\nAdd a special case where we can derive valid s32 bounds from umin/umax\nor smin/smax by stitching together negative s32 subrange and\nnon-negative s32 subrange. That requires upper 32 bits to form a [N, N+1]\nrange in u32 domain (taking into account wrap around, so 0xffffffff\nto 0x00000000 is a valid [N, N+1] range in this sense). See code comment\nfor concrete examples.\n\nEduard Zingerman also provided an alternative explanation ([0]) for more\nmathematically inclined readers:\n\nSuppose:\n. there are numbers a, b, c\n. 2**31 <= b < 2**32\n. 0 <= c < 2**31\n. umin = 2**32 * a + b\n. umax = 2**32 * (a + 1) + c\n\nThe number of values in the range represented by [umin; umax] is:\n. N = umax - umin + 1 = 2**32 + c - b + 1\n. min(N) = 2**32 + 0 - (2**32-1) + 1 = 2, with b = 2**32-1, c = 0\n. max(N) = 2**32 + (2**31 - 1) - 2**31 + 1 = 2**32, with b = 2**31, c = 2**31-1\n\nHence [(s32)b; (s32)c] forms a valid range.\n\n  [0] https://lore.kernel.org/bpf/d7af631802f0cfae20df77fe70068702d24bbd31.camel@gmail.com/\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c1efab6468fd5ef541d47d81dbb62cca27f8db3b",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:39 -0800",
      "message": "bpf: derive subreg bounds from full bounds when upper 32 bits are constant\n\nComments in code try to explain the idea behind why this is correct.\nPlease check the code and comments.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d540517990a9d105bf0312760665964916ac044f",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:39 -0800",
      "message": "bpf: derive smin32/smax32 from umin32/umax32 bounds\n\nAll the logic that applies to u64 vs s64, equally applies for u32 vs s32\nrelationships (just taken in a smaller 32-bit numeric space). So do the\nsame deduction of smin32/smax32 from umin32/umax32, if we can.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "93f7378734b595fb61e89b802002fb7e3a1267d2",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-11-09 18:58:39 -0800",
      "message": "bpf: derive smin/smax from umin/max bounds\n\nAdd smin/smax derivation from appropriate umin/umax values. Previously the\nlogic was surprisingly asymmetric, trying to derive umin/umax from smin/smax\n(if possible), but not trying to do the same in the other direction. A simple\naddition to __reg64_deduce_bounds() fixes this.\n\nAdded also generic comment about u64/s64 ranges and their relationship.\nHopefully that helps readers to understand all the bounds deductions\na bit better.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231102033759.2541186-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "291d044fd51f8484066300ee42afecf8c8db7b3a",
      "author": "Shung-Hsi Yu <shung-hsi.yu@suse.com>",
      "date": "2023-11-01 22:54:27 -0700",
      "message": "bpf: Fix precision tracking for BPF_ALU | BPF_TO_BE | BPF_END\n\nBPF_END and BPF_NEG has a different specification for the source bit in\nthe opcode compared to other ALU/ALU64 instructions, and is either\nreserved or use to specify the byte swap endianness. In both cases the\nsource bit does not encode source operand location, and src_reg is a\nreserved field.\n\nbacktrack_insn() currently does not differentiate BPF_END and BPF_NEG\nfrom other ALU/ALU64 instructions, which leads to r0 being incorrectly\nmarked as precise when processing BPF_ALU | BPF_TO_BE | BPF_END\ninstructions. This commit teaches backtrack_insn() to correctly mark\nprecision for such case.\n\nWhile precise tracking of BPF_NEG and other BPF_END instructions are\ncorrect and does not need fixing, this commit opt to process all BPF_NEG\nand BPF_END instructions within the same if-clause to better align with\ncurrent convention used in the verifier (e.g. check_alu_op).\n\nFixes: b5dc0163d8fd (\"bpf: precise scalar_value tracking\")\nCc: stable@vger.kernel.org\nReported-by: Mohamed Mahmoud <mmahmoud@redhat.com>\nCloses: https://lore.kernel.org/r/87jzrrwptf.fsf@toke.dk\nTested-by: Toke H\u00f8iland-J\u00f8rgensen <toke@redhat.com>\nTested-by: Tao Lyu <tao.lyu@epfl.ch>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/r/20231102053913.12004-2-shung-hsi.yu@suse.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "3091b667498b0a212e760e1033e5f9b8c33a948f",
      "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
      "date": "2023-11-01 22:49:20 -0700",
      "message": "bpf: Relax allowlist for css_task iter\n\nThe newly added open-coded css_task iter would try to hold the global\ncss_set_lock in bpf_iter_css_task_new, so the bpf side has to be careful in\nwhere it allows to use this iter. The mainly concern is dead locking on\ncss_set_lock. check_css_task_iter_allowlist() in verifier enforced css_task\ncan only be used in bpf_lsm hooks and sleepable bpf_iter.\n\nThis patch relax the allowlist for css_task iter. Any lsm and any iter\n(even non-sleepable) and any sleepable are safe since they would not hold\nthe css_set_lock before entering BPF progs context.\n\nThis patch also fixes the misused BPF_TRACE_ITER in\ncheck_css_task_iter_allowlist which compared bpf_prog_type with\nbpf_attach_type.\n\nFixes: 9c66dc94b62ae (\"bpf: Introduce css_task open-coded iterator kfuncs\")\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20231031050438.93297-2-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/iters_task_failure.c"
      ]
    },
    {
      "hash": "811c363645b33e6e22658634329e95f383dfc705",
      "author": "Hao Sun <sunhao.th@gmail.com>",
      "date": "2023-11-01 22:30:27 -0700",
      "message": "bpf: Fix check_stack_write_fixed_off() to correctly spill imm\n\nIn check_stack_write_fixed_off(), imm value is cast to u32 before being\nspilled to the stack. Therefore, the sign information is lost, and the\nrange information is incorrect when load from the stack again.\n\nFor the following prog:\n0: r2 = r10\n1: *(u64*)(r2 -40) = -44\n2: r0 = *(u64*)(r2 - 40)\n3: if r0 s<= 0xa goto +2\n4: r0 = 1\n5: exit\n6: r0  = 0\n7: exit\n\nThe verifier gives:\nfunc#0 @0\n0: R1=ctx(off=0,imm=0) R10=fp0\n0: (bf) r2 = r10                      ; R2_w=fp0 R10=fp0\n1: (7a) *(u64 *)(r2 -40) = -44        ; R2_w=fp0 fp-40_w=4294967252\n2: (79) r0 = *(u64 *)(r2 -40)         ; R0_w=4294967252 R2_w=fp0\nfp-40_w=4294967252\n3: (c5) if r0 s< 0xa goto pc+2\nmark_precise: frame0: last_idx 3 first_idx 0 subseq_idx -1\nmark_precise: frame0: regs=r0 stack= before 2: (79) r0 = *(u64 *)(r2 -40)\n3: R0_w=4294967252\n4: (b7) r0 = 1                        ; R0_w=1\n5: (95) exit\nverification time 7971 usec\nstack depth 40\nprocessed 6 insns (limit 1000000) max_states_per_insn 0 total_states 0\npeak_states 0 mark_read 0\n\nSo remove the incorrect cast, since imm field is declared as s32, and\n__mark_reg_known() takes u64, so imm would be correctly sign extended\nby compiler.\n\nFixes: ecdf985d7615 (\"bpf: track immediate values written to stack by BPF_ST instruction\")\nCc: stable@vger.kernel.org\nSigned-off-by: Hao Sun <sunhao.th@gmail.com>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231101-fix-check-stack-write-v3-1-f05c2b1473d5@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "05670f81d1287c40ec861186e4c4e3401013e7fb",
      "author": "Matthieu Baerts <matttbe@kernel.org>",
      "date": "2023-11-01 22:28:25 -0700",
      "message": "bpf: fix compilation error without CGROUPS\n\nOur MPTCP CI complained [1] -- and KBuild too -- that it was no longer\npossible to build the kernel without CONFIG_CGROUPS:\n\n  kernel/bpf/task_iter.c: In function 'bpf_iter_css_task_new':\n  kernel/bpf/task_iter.c:919:14: error: 'CSS_TASK_ITER_PROCS' undeclared (first use in this function)\n    919 |         case CSS_TASK_ITER_PROCS | CSS_TASK_ITER_THREADED:\n        |              ^~~~~~~~~~~~~~~~~~~\n  kernel/bpf/task_iter.c:919:14: note: each undeclared identifier is reported only once for each function it appears in\n  kernel/bpf/task_iter.c:919:36: error: 'CSS_TASK_ITER_THREADED' undeclared (first use in this function)\n    919 |         case CSS_TASK_ITER_PROCS | CSS_TASK_ITER_THREADED:\n        |                                    ^~~~~~~~~~~~~~~~~~~~~~\n  kernel/bpf/task_iter.c:927:60: error: invalid application of 'sizeof' to incomplete type 'struct css_task_iter'\n    927 |         kit->css_it = bpf_mem_alloc(&bpf_global_ma, sizeof(struct css_task_iter));\n        |                                                            ^~~~~~\n  kernel/bpf/task_iter.c:930:9: error: implicit declaration of function 'css_task_iter_start'; did you mean 'task_seq_start'? [-Werror=implicit-function-declaration]\n    930 |         css_task_iter_start(css, flags, kit->css_it);\n        |         ^~~~~~~~~~~~~~~~~~~\n        |         task_seq_start\n  kernel/bpf/task_iter.c: In function 'bpf_iter_css_task_next':\n  kernel/bpf/task_iter.c:940:16: error: implicit declaration of function 'css_task_iter_next'; did you mean 'class_dev_iter_next'? [-Werror=implicit-function-declaration]\n    940 |         return css_task_iter_next(kit->css_it);\n        |                ^~~~~~~~~~~~~~~~~~\n        |                class_dev_iter_next\n  kernel/bpf/task_iter.c:940:16: error: returning 'int' from a function with return type 'struct task_struct *' makes pointer from integer without a cast [-Werror=int-conversion]\n    940 |         return css_task_iter_next(kit->css_it);\n        |                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  kernel/bpf/task_iter.c: In function 'bpf_iter_css_task_destroy':\n  kernel/bpf/task_iter.c:949:9: error: implicit declaration of function 'css_task_iter_end' [-Werror=implicit-function-declaration]\n    949 |         css_task_iter_end(kit->css_it);\n        |         ^~~~~~~~~~~~~~~~~\n\nThis patch simply surrounds with a #ifdef the new code requiring CGroups\nsupport. It seems enough for the compiler and this is similar to\nbpf_iter_css_{new,next,destroy}() functions where no other #ifdef have\nbeen added in kernel/bpf/helpers.c and in the selftests.\n\nFixes: 9c66dc94b62a (\"bpf: Introduce css_task open-coded iterator kfuncs\")\nLink: https://github.com/multipath-tcp/mptcp_net-next/actions/runs/6665206927\nReported-by: kernel test robot <lkp@intel.com>\nCloses: https://lore.kernel.org/oe-kbuild-all/202310260528.aHWgVFqq-lkp@intel.com/\nSigned-off-by: Matthieu Baerts <matttbe@kernel.org>\n[ added missing ifdefs for BTF_ID cgroup definitions ]\nSigned-off-by: Jiri Olsa <jolsa@kernel.org>\nLink: https://lore.kernel.org/r/20231101181601.1493271-1-jolsa@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/helpers.c",
        "kernel/bpf/task_iter.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "42d31dd601fa43b9afdf069d1ba410b2306a4c76",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-10-24 14:45:51 +0200",
      "message": "bpf: Improve JEQ/JNE branch taken logic\n\nWhen determining if an if/else branch will always or never be taken, use\nsigned range knowledge in addition to currently used unsigned range knowledge.\nIf either signed or unsigned range suggests that condition is always/never\ntaken, return corresponding branch_taken verdict.\n\nCurrent use of unsigned range for this seems arbitrary and unnecessarily\nincomplete. It is possible for *signed* operations to be performed on\nregister, which could \"invalidate\" unsigned range for that register. In such\ncase branch_taken will be artificially useless, even if we can still tell\nthat some constant is outside of register value range based on its signed\nbounds.\n\nveristat-based validation shows zero differences across selftests, Cilium,\nand Meta-internal BPF object files.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Shung-Hsi Yu <shung-hsi.yu@suse.com>\nLink: https://lore.kernel.org/bpf/20231022205743.72352-2-andrii@kernel.org",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b4d8239534fddc036abe4a0fdbf474d9894d4641",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-10-23 21:49:32 -0700",
      "message": "bpf: print full verifier states on infinite loop detection\n\nAdditional logging in is_state_visited(): if infinite loop is detected\nprint full verifier state for both current and equivalent states.\n\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-8-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "2a0992829ea3864939d917a5c7b48be6629c6217",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-10-23 21:49:32 -0700",
      "message": "bpf: correct loop detection for iterators convergence\n\nIt turns out that .branches > 0 in is_state_visited() is not a\nsufficient condition to identify if two verifier states form a loop\nwhen iterators convergence is computed. This commit adds logic to\ndistinguish situations like below:\n\n (I)            initial       (II)            initial\n                  |                             |\n                  V                             V\n     .---------> hdr                           ..\n     |            |                             |\n     |            V                             V\n     |    .------...                    .------..\n     |    |       |                     |       |\n     |    V       V                     V       V\n     |   ...     ...               .-> hdr     ..\n     |    |       |                |    |       |\n     |    V       V                |    V       V\n     |   succ <- cur               |   succ <- cur\n     |    |                        |    |\n     |    V                        |    V\n     |   ...                       |   ...\n     |    |                        |    |\n     '----'                        '----'\n\nFor both (I) and (II) successor 'succ' of the current state 'cur' was\npreviously explored and has branches count at 0. However, loop entry\n'hdr' corresponding to 'succ' might be a part of current DFS path.\nIf that is the case 'succ' and 'cur' are members of the same loop\nand have to be compared exactly.\n\nCo-developed-by: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nCo-developed-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nReviewed-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-6-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "2793a8b015f7f1caadb9bce9c63dc659f7522676",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-10-23 21:49:31 -0700",
      "message": "bpf: exact states comparison for iterator convergence checks\n\nConvergence for open coded iterators is computed in is_state_visited()\nby examining states with branches count > 1 and using states_equal().\nstates_equal() computes sub-state relation using read and precision marks.\nRead and precision marks are propagated from children states,\nthus are not guaranteed to be complete inside a loop when branches\ncount > 1. This could be demonstrated using the following unsafe program:\n\n     1. r7 = -16\n     2. r6 = bpf_get_prandom_u32()\n     3. while (bpf_iter_num_next(&fp[-8])) {\n     4.   if (r6 != 42) {\n     5.     r7 = -32\n     6.     r6 = bpf_get_prandom_u32()\n     7.     continue\n     8.   }\n     9.   r0 = r10\n    10.   r0 += r7\n    11.   r8 = *(u64 *)(r0 + 0)\n    12.   r6 = bpf_get_prandom_u32()\n    13. }\n\nHere verifier would first visit path 1-3, create a checkpoint at 3\nwith r7=-16, continue to 4-7,3 with r7=-32.\n\nBecause instructions at 9-12 had not been visitied yet existing\ncheckpoint at 3 does not have read or precision mark for r7.\nThus states_equal() would return true and verifier would discard\ncurrent state, thus unsafe memory access at 11 would not be caught.\n\nThis commit fixes this loophole by introducing exact state comparisons\nfor iterator convergence logic:\n- registers are compared using regs_exact() regardless of read or\n  precision marks;\n- stack slots have to have identical type.\n\nUnfortunately, this is too strict even for simple programs like below:\n\n    i = 0;\n    while(iter_next(&it))\n      i++;\n\nAt each iteration step i++ would produce a new distinct state and\neventually instruction processing limit would be reached.\n\nTo avoid such behavior speculatively forget (widen) range for\nimprecise scalar registers, if those registers were not precise at the\nend of the previous iteration and do not match exactly.\n\nThis a conservative heuristic that allows to verify wide range of\nprograms, however it precludes verification of programs that conjure\nan imprecise value on the first loop iteration and use it as precise\non the second.\n\nTest case iter_task_vma_for_each() presents one of such cases:\n\n        unsigned int seen = 0;\n        ...\n        bpf_for_each(task_vma, vma, task, 0) {\n                if (seen >= 1000)\n                        break;\n                ...\n                seen++;\n        }\n\nHere clang generates the following code:\n\n<LBB0_4>:\n      24:       r8 = r6                          ; stash current value of\n                ... body ...                       'seen'\n      29:       r1 = r10\n      30:       r1 += -0x8\n      31:       call bpf_iter_task_vma_next\n      32:       r6 += 0x1                        ; seen++;\n      33:       if r0 == 0x0 goto +0x2 <LBB0_6>  ; exit on next() == NULL\n      34:       r7 += 0x10\n      35:       if r8 < 0x3e7 goto -0xc <LBB0_4> ; loop on seen < 1000\n\n<LBB0_6>:\n      ... exit ...\n\nNote that counter in r6 is copied to r8 and then incremented,\nconditional jump is done using r8. Because of this precision mark for\nr6 lags one state behind of precision mark on r8 and widening logic\nkicks in.\n\nAdding barrier_var(seen) after conditional is sufficient to force\nclang use the same register for both counting and conditional jump.\n\nThis issue was discussed in the thread [1] which was started by\nAndrew Werner <awerner32@gmail.com> demonstrating a similar bug\nin callback functions handling. The callbacks would be addressed\nin a followup patch.\n\n[1] https://lore.kernel.org/bpf/97a90da09404c65c8e810cf83c94ac703705dc0e.camel@gmail.com/\n\nCo-developed-by: Andrii Nakryiko <andrii.nakryiko@gmail.com>\nCo-developed-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-4-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/iters_task_vma.c"
      ]
    },
    {
      "hash": "4c97259abc9bc8df7712f76f58ce385581876857",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-10-23 21:49:31 -0700",
      "message": "bpf: extract same_callsites() as utility function\n\nExtract same_callsites() from clean_live_states() as a utility function.\nThis function would be used by the next patch in the set.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-3-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "3c4e420cb6536026ddd50eaaff5f30e4f144200d",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-10-23 21:49:31 -0700",
      "message": "bpf: move explored_state() closer to the beginning of verifier.c\n\nSubsequent patches would make use of explored_state() function.\nMove it up to avoid adding unnecessary prototype.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20231024000917.12153-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "cb3ecf7915a1d7ce5304402f4d8616d9fa5193f7",
      "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
      "date": "2023-10-19 17:02:46 -0700",
      "message": "bpf: Let bpf_iter_task_new accept null task ptr\n\nWhen using task_iter to iterate all threads of a specific task, we enforce\nthat the user must pass a valid task pointer to ensure safety. However,\nwhen iterating all threads/process in the system, BPF verifier still\nrequire a valid ptr instead of \"nullable\" pointer, even though it's\npointless, which is a kind of surprising from usability standpoint. It\nwould be nice if we could let that kfunc accept a explicit null pointer\nwhen we are using BPF_TASK_ITER_ALL_{PROCS, THREADS} and a valid pointer\nwhen using BPF_TASK_ITER_THREAD.\n\nGiven a trival kfunc:\n\t__bpf_kfunc void FN(struct TYPE_A *obj);\n\nBPF Prog would reject a nullptr for obj. The error info is:\n\"arg#x pointer type xx xx must point to scalar, or struct with scalar\"\nreported by get_kfunc_ptr_arg_type(). The reg->type is SCALAR_VALUE and\nthe btf type of ref_t is not scalar or scalar_struct which leads to the\nrejection of get_kfunc_ptr_arg_type.\n\nThis patch add \"__nullable\" annotation:\n\t__bpf_kfunc void FN(struct TYPE_A *obj__nullable);\nHere __nullable indicates obj can be optional, user can pass a explicit\nnullptr or a normal TYPE_A pointer. In get_kfunc_ptr_arg_type(), we will\ndetect whether the current arg is optional and register is null, If so,\nreturn a new kfunc_ptr_arg_type KF_ARG_PTR_TO_NULL and skip to the next\narg in check_kfunc_args().\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231018061746.111364-7-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/task_iter.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "dfab99df147b0d364f0c199f832ff2aedfb2265a",
      "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
      "date": "2023-10-19 17:02:46 -0700",
      "message": "bpf: teach the verifier to enforce css_iter and task_iter in RCU CS\n\ncss_iter and task_iter should be used in rcu section. Specifically, in\nsleepable progs explicit bpf_rcu_read_lock() is needed before use these\niters. In normal bpf progs that have implicit rcu_read_lock(), it's OK to\nuse them directly.\n\nThis patch adds a new a KF flag KF_RCU_PROTECTED for bpf_iter_task_new and\nbpf_iter_css_new. It means the kfunc should be used in RCU CS. We check\nwhether we are in rcu cs before we want to invoke this kfunc. If the rcu\nprotection is guaranteed, we would let st->type = PTR_TO_STACK | MEM_RCU.\nOnce user do rcu_unlock during the iteration, state MEM_RCU of regs would\nbe cleared. is_iter_reg_valid_init() will reject if reg->type is UNTRUSTED.\n\nIt is worth noting that currently, bpf_rcu_read_unlock does not\nclear the state of the STACK_ITER reg, since bpf_for_each_spilled_reg\nonly considers STACK_SPILL. This patch also let bpf_for_each_spilled_reg\nsearch STACK_ITER.\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20231018061746.111364-6-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "include/linux/btf.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "9c66dc94b62aef23300f05f63404afb8990920b4",
      "author": "Chuyi Zhou <zhouchuyi@bytedance.com>",
      "date": "2023-10-19 17:02:46 -0700",
      "message": "bpf: Introduce css_task open-coded iterator kfuncs\n\nThis patch adds kfuncs bpf_iter_css_task_{new,next,destroy} which allow\ncreation and manipulation of struct bpf_iter_css_task in open-coded\niterator style. These kfuncs actually wrapps css_task_iter_{start,next,\nend}. BPF programs can use these kfuncs through bpf_for_each macro for\niteration of all tasks under a css.\n\ncss_task_iter_*() would try to get the global spin-lock *css_set_lock*, so\nthe bpf side has to be careful in where it allows to use this iter.\nCurrently we only allow it in bpf_lsm and bpf iter-s.\n\nSigned-off-by: Chuyi Zhou <zhouchuyi@bytedance.com>\nAcked-by: Tejun Heo <tj@kernel.org>\nLink: https://lore.kernel.org/r/20231018061746.111364-3-zhouchuyi@bytedance.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/helpers.c",
        "kernel/bpf/task_iter.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/bpf_experimental.h"
      ]
    },
    {
      "hash": "1a8a315f008a58f54fecb012b928aa6a494435b3",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-10-16 13:49:18 +0200",
      "message": "bpf: Ensure proper register state printing for cond jumps\n\nVerifier emits relevant register state involved in any given instruction\nnext to it after `;` to the right, if possible. Or, worst case, on the\nseparate line repeating instruction index.\n\nE.g., a nice and simple case would be:\n\n  2: (d5) if r0 s<= 0x0 goto pc+1       ; R0_w=0\n\nBut if there is some intervening extra output (e.g., precision\nbacktracking log) involved, we are supposed to see the state after the\nprecision backtrack log:\n\n  4: (75) if r0 s>= 0x0 goto pc+1\n  mark_precise: frame0: last_idx 4 first_idx 0 subseq_idx -1\n  mark_precise: frame0: regs=r0 stack= before 2: (d5) if r0 s<= 0x0 goto pc+1\n  mark_precise: frame0: regs=r0 stack= before 1: (b7) r0 = 0\n  6: R0_w=0\n\nFirst off, note that in `6: R0_w=0` instruction index corresponds to the\nnext instruction, not to the conditional jump instruction itself, which\nis wrong and we'll get to that.\n\nBut besides that, the above is a happy case that does work today. Yet,\nif it so happens that precision backtracking had to traverse some of the\nparent states, this `6: R0_w=0` state output would be missing.\n\nThis is due to a quirk of print_verifier_state() routine, which performs\nmark_verifier_state_clean(env) at the end. This marks all registers as\n\"non-scratched\", which means that subsequent logic to print *relevant*\nregisters (that is, \"scratched ones\") fails and doesn't see anything\nrelevant to print and skips the output altogether.\n\nprint_verifier_state() is used both to print instruction context, but\nalso to print an **entire** verifier state indiscriminately, e.g.,\nduring precision backtracking (and in a few other situations, like\nduring entering or exiting subprogram).  Which means if we have to print\nentire parent state before getting to printing instruction context\nstate, instruction context is marked as clean and is omitted.\n\nLong story short, this is definitely not intentional. So we fix this\nbehavior in this patch by teaching print_verifier_state() to clear\nscratch state only if it was used to print instruction state, not the\nparent/callback state. This is determined by print_all option, so if\nit's not set, we don't clear scratch state. This fixes missing\ninstruction state for these cases.\n\nAs for the mismatched instruction index, we fix that by making sure we\ncall print_insn_state() early inside check_cond_jmp_op() before we\nadjusted insn_idx based on jump branch taken logic. And with that we get\ndesired correct information:\n\n  9: (16) if w4 == 0x1 goto pc+9\n  mark_precise: frame0: last_idx 9 first_idx 9 subseq_idx -1\n  mark_precise: frame0: parent state regs=r4 stack=: R2_w=1944 R4_rw=P1 R10=fp0\n  mark_precise: frame0: last_idx 8 first_idx 0 subseq_idx 9\n  mark_precise: frame0: regs=r4 stack= before 8: (66) if w4 s> 0x3 goto pc+5\n  mark_precise: frame0: regs=r4 stack= before 7: (b7) r4 = 1\n  9: R4=1\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231011223728.3188086-6-andrii@kernel.org",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "72f8a1de4a7ecb23393a920dface58d5a96f42d8",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-10-16 13:49:18 +0200",
      "message": "bpf: Disambiguate SCALAR register state output in verifier logs\n\nCurrently the way that verifier prints SCALAR_VALUE register state (and\nPTR_TO_PACKET, which can have var_off and ranges info as well) is very\nambiguous.\n\nIn the name of brevity we are trying to eliminate \"unnecessary\" output\nof umin/umax, smin/smax, u32_min/u32_max, and s32_min/s32_max values, if\npossible. Current rules are that if any of those have their default\nvalue (which for mins is the minimal value of its respective types: 0,\nS32_MIN, or S64_MIN, while for maxs it's U32_MAX, S32_MAX, S64_MAX, or\nU64_MAX) *OR* if there is another min/max value that as matching value.\nE.g., if smin=100 and umin=100, we'll emit only umin=10, omitting smin\naltogether. This approach has a few problems, being both ambiguous and\nsort-of incorrect in some cases.\n\nAmbiguity is due to missing value could be either default value or value\nof umin/umax or smin/smax. This is especially confusing when we mix\nsigned and unsigned ranges. Quite often, umin=0 and smin=0, and so we'll\nhave only `umin=0` leaving anyone reading verifier log to guess whether\nsmin is actually 0 or it's actually -9223372036854775808 (S64_MIN). And\noften times it's important to know, especially when debugging tricky\nissues.\n\n\"Sort-of incorrectness\" comes from mixing negative and positive values.\nE.g., if umin is some large positive number, it can be equal to smin\nwhich is, interpreted as signed value, is actually some negative value.\nCurrently, that smin will be omitted and only umin will be emitted with\na large positive value, giving an impression that smin is also positive.\n\nAnyway, ambiguity is the biggest issue making it impossible to have an\nexact understanding of register state, preventing any sort of automated\ntesting of verifier state based on verifier log. This patch is\nattempting to rectify the situation by removing ambiguity, while\nminimizing the verboseness of register state output.\n\nThe rules are straightforward:\n  - if some of the values are missing, then it definitely has a default\n  value. I.e., `umin=0` means that umin is zero, but smin is actually\n  S64_MIN;\n  - all the various boundaries that happen to have the same value are\n  emitted in one equality separated sequence. E.g., if umin and smin are\n  both 100, we'll emit `smin=umin=100`, making this explicit;\n  - we do not mix negative and positive values together, and even if\n  they happen to have the same bit-level value, they will be emitted\n  separately with proper sign. I.e., if both umax and smax happen to be\n  0xffffffffffffffff, we'll emit them both separately as\n  `smax=-1,umax=18446744073709551615`;\n  - in the name of a bit more uniformity and consistency,\n  {u32,s32}_{min,max} are renamed to {s,u}{min,max}32, which seems to\n  improve readability.\n\nThe above means that in case of all 4 ranges being, say, [50, 100] range,\nwe'd previously see hugely ambiguous:\n\n    R1=scalar(umin=50,umax=100)\n\nNow, we'll be more explicit:\n\n    R1=scalar(smin=umin=smin32=umin32=50,smax=umax=smax32=umax32=100)\n\nThis is slightly more verbose, but distinct from the case when we don't\nknow anything about signed boundaries and 32-bit boundaries, which under\nnew rules will match the old case:\n\n    R1=scalar(umin=50,umax=100)\n\nAlso, in the name of simplicity of implementation and consistency, order\nfor {s,u}32_{min,max} are emitted *before* var_off. Previously they were\nemitted afterwards, for unclear reasons.\n\nThis patch also includes a few fixes to selftests that expect exact\nregister state to accommodate slight changes to verifier format. You can\nsee that the changes are pretty minimal in common cases.\n\nNote, the special case when SCALAR_VALUE register is a known constant\nisn't changed, we'll emit constant value once, interpreted as signed\nvalue.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/bpf/20231011223728.3188086-5-andrii@kernel.org",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/exceptions_assert.c",
        "tools/testing/selftests/bpf/progs/verifier_ldsx.c"
      ]
    },
    {
      "hash": "859051dd165ec6cc915f0f2114699021144fd249",
      "author": "Daan De Meyer <daan.j.demeyer@gmail.com>",
      "date": "2023-10-11 17:27:47 -0700",
      "message": "bpf: Implement cgroup sockaddr hooks for unix sockets\n\nThese hooks allows intercepting connect(), getsockname(),\ngetpeername(), sendmsg() and recvmsg() for unix sockets. The unix\nsocket hooks get write access to the address length because the\naddress length is not fixed when dealing with unix sockets and\nneeds to be modified when a unix socket address is modified by\nthe hook. Because abstract socket unix addresses start with a\nNUL byte, we cannot recalculate the socket address in kernelspace\nafter running the hook by calculating the length of the unix socket\npath using strlen().\n\nThese hooks can be used when users want to multiplex syscall to a\nsingle unix socket to multiple different processes behind the scenes\nby redirecting the connect() and other syscalls to process specific\nsockets.\n\nWe do not implement support for intercepting bind() because when\nusing bind() with unix sockets with a pathname address, this creates\nan inode in the filesystem which must be cleaned up. If we rewrite\nthe address, the user might try to clean up the wrong file, leaking\nthe socket in the filesystem where it is never cleaned up. Until we\nfigure out a solution for this (and a use case for intercepting bind()),\nwe opt to not allow rewriting the sockaddr in bind() calls.\n\nWe also implement recvmsg() support for connected streams so that\nafter a connect() that is modified by a sockaddr hook, any corresponding\nrecmvsg() on the connected socket can also be modified to make the\nconnected program think it is connected to the \"intended\" remote.\n\nReviewed-by: Kuniyuki Iwashima <kuniyu@amazon.com>\nSigned-off-by: Daan De Meyer <daan.j.demeyer@gmail.com>\nLink: https://lore.kernel.org/r/20231011185113.140426-5-daan.j.demeyer@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "include/linux/bpf-cgroup-defs.h",
        "include/linux/bpf-cgroup.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/cgroup.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c",
        "net/core/filter.c",
        "net/unix/af_unix.c",
        "tools/include/uapi/linux/bpf.h"
      ]
    },
    {
      "hash": "829955981c557c7fc7416581c4cd68a8a0c28620",
      "author": "David Vernet <void@manifault.com>",
      "date": "2023-10-09 23:10:58 +0200",
      "message": "bpf: Fix verifier log for async callback return values\n\nThe verifier, as part of check_return_code(), verifies that async\ncallbacks such as from e.g. timers, will return 0. It does this by\ncorrectly checking that R0->var_off is in tnum_const(0), which\neffectively checks that it's in a range of 0. If this condition fails,\nhowever, it prints an error message which says that the value should\nhave been in (0x0; 0x1). This results in possibly confusing output such\nas the following in which an async callback returns 1:\n\n  At async callback the register R0 has value (0x1; 0x0) should have been in (0x0; 0x1)\n\nThe fix is easy -- we should just pass the tnum_const(0) as the correct\nrange to verbose_invalid_scalar(), which will then print the following:\n\n  At async callback the register R0 has value (0x1; 0x0) should have been in (0x0; 0x0)\n\nFixes: bfc6bb74e4f1 (\"bpf: Implement verifier support for validation of async callbacks.\")\nSigned-off-by: David Vernet <void@manifault.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20231009161414.235829-1-void@manifault.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "577c06af8188d1f6919ef7b62fc1b78fb1b86eb7",
      "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
      "date": "2023-09-21 14:21:59 -0700",
      "message": "bpf: Disable zero-extension for BPF_MEMSX\n\nOn the architectures that use bpf_jit_needs_zext(), e.g., s390x, the\nverifier incorrectly inserts a zero-extension after BPF_MEMSX, leading\nto miscompilations like the one below:\n\n      24:       89 1a ff fe 00 00 00 00 \"r1 = *(s16 *)(r10 - 2);\"       # zext_dst set\n   0x3ff7fdb910e:       lgh     %r2,-2(%r13,%r0)                        # load halfword\n   0x3ff7fdb9114:       llgfr   %r2,%r2                                 # wrong!\n      25:       65 10 00 03 00 00 7f ff if r1 s> 32767 goto +3 <l0_1>   # check_cond_jmp_op()\n\nDisable such zero-extensions. The JITs need to insert sign-extension\nthemselves, if necessary.\n\nSuggested-by: Puranjay Mohan <puranjay12@gmail.com>\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nReviewed-by: Puranjay Mohan <puranjay12@gmail.com>\nLink: https://lore.kernel.org/r/20230919101336.2223655-2-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "81335f90e8a88b81932df011105c46e708744f44",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-09-20 03:26:25 -0700",
      "message": "bpf: unconditionally reset backtrack_state masks on global func exit\n\nIn mark_chain_precision() logic, when we reach the entry to a global\nfunc, it is expected that R1-R5 might be still requested to be marked\nprecise. This would correspond to some integer input arguments being\ntracked as precise. This is all expected and handled as a special case.\n\nWhat's not expected is that we'll leave backtrack_state structure with\nsome register bits set. This is because for subsequent precision\npropagations backtrack_state is reused without clearing masks, as all\ncode paths are carefully written in a way to leave empty backtrack_state\nwith zeroed out masks, for speed.\n\nThe fix is trivial, we always clear register bit in the register mask, and\nthen, optionally, set reg->precise if register is SCALAR_VALUE type.\n\nReported-by: Chris Mason <clm@meta.com>\nFixes: be2ef8161572 (\"bpf: allow precision tracking for programs with subprogs\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230918210110.2241458-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "aec42f36237b09e42eac39f6c74305aec02b4694",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-09-19 02:26:47 -0700",
      "message": "bpf: Remove unused variables.\n\nRemove unused prev_offset, min_size, krec_size variables.\n\nReported-by: kernel test robot <lkp@intel.com>\nCloses: https://lore.kernel.org/oe-kbuild-all/202309190634.fL17FWoT-lkp@intel.com/\nFixes: aaa619ebccb2 (\"bpf: Refactor check_btf_func and split into two phases\")\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "06d686f771ddc27a8554cd8f5b22e071040dc90e",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-09-16 09:36:43 -0700",
      "message": "bpf: Fix kfunc callback register type handling\n\nThe kfunc code to handle KF_ARG_PTR_TO_CALLBACK does not check the reg\ntype before using reg->subprogno. This can accidently permit invalid\npointers from being passed into callback helpers (e.g. silently from\ndifferent paths). Likewise, reg->subprogno from the per-register type\nunion may not be meaningful either. We need to reject any other type\nexcept PTR_TO_FUNC.\n\nAcked-by: Dave Marchevsky <davemarchevsky@fb.com>\nFixes: 5d92ddc3de1b (\"bpf: Add callback validation to kfunc verifier logic\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-14-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "fd548e1a46185000191a89cae4be560e076ed6c7",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-09-16 09:36:32 -0700",
      "message": "bpf: Disallow fentry/fexit/freplace for exception callbacks\n\nDuring testing, it was discovered that extensions to exception callbacks\nhad no checks, upon running a testcase, the kernel ended up running off\nthe end of a program having final call as bpf_throw, and hitting int3\ninstructions.\n\nThe reason is that while the default exception callback would have reset\nthe stack frame to return back to the main program's caller, the\nreplacing extension program will simply return back to bpf_throw, which\nwill instead return back to the program and the program will continue\nexecution, now in an undefined state where anything could happen.\n\nThe way to support extensions to an exception callback would be to mark\nthe BPF_PROG_TYPE_EXT main subprog as an exception_cb, and prevent it\nfrom calling bpf_throw. This would make the JIT produce a prologue that\nrestores saved registers and reset the stack frame. But let's not do\nthat until there is a concrete use case for this, and simply disallow\nthis for now.\n\nSimilar issues will exist for fentry and fexit cases, where trampoline\nsaves data on the stack when invoking exception callback, which however\nwill then end up resetting the stack frame, and on return, the fexit\nprogram will never will invoked as the return address points to the main\nprogram's caller in the kernel. Instead of additional complexity and\nback and forth between the two stacks to enable such a use case, simply\nforbid it.\n\nOne key point here to note is that currently X86_TAIL_CALL_OFFSET didn't\nrequire any modifications, even though we emit instructions before the\ncorresponding endbr64 instruction. This is because we ensure that a main\nsubprog never serves as an exception callback, and therefore the\nexception callback (which will be a global subprog) can never serve as\nthe tail call target, eliminating any discrepancies. However, once we\nsupport a BPF_PROG_TYPE_EXT to also act as an exception callback, it\nwill end up requiring change to the tail call offset to account for the\nextra instructions. For simplicitly, tail calls could be disabled for\nsuch targets.\n\nNoting the above, it appears better to wait for a concrete use case\nbefore choosing to permit extension programs to replace exception\ncallbacks.\n\nAs a precaution, we disable fentry and fexit for exception callbacks as\nwell.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-13-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "a923819fb2c5be029a69c0ca53239865c9bc05dd",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-09-16 09:34:21 -0700",
      "message": "bpf: Treat first argument as return value for bpf_throw\n\nIn case of the default exception callback, change the behavior of\nbpf_throw, where the passed cookie value is no longer ignored, but\nis instead the return value of the default exception callback. As\nsuch, we need to place restrictions on the value being passed into\nbpf_throw in such a case, only allowing those permitted by the\ncheck_return_code function.\n\nThus, bpf_throw can now control the return value of the program from\neach call site without having the user install a custom exception\ncallback just to override the return value when an exception is thrown.\n\nWe also modify the hidden subprog instructions to now move BPF_REG_1 to\nBPF_REG_0, so as to set the return value before exit in the default\ncallback.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-9-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b62bf8a5e9110922f58f6ea8fe747e1759f49e61",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-09-16 09:34:21 -0700",
      "message": "bpf: Perform CFG walk for exception callback\n\nSince exception callbacks are not referenced using bpf_pseudo_func and\nbpf_pseudo_call instructions, check_cfg traversal will never explore\ninstructions of the exception callback. Even after adding the subprog,\nthe program will then fail with a 'unreachable insn' error.\n\nWe thus need to begin walking from the start of the exception callback\nagain in check_cfg after a complete CFG traversal finishes, so as to\nexplore the CFG rooted at the exception callback.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-8-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b9ae0c9dd0aca79bffc17be51c2dc148d1f72708",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-09-16 09:34:21 -0700",
      "message": "bpf: Add support for custom exception callbacks\n\nBy default, the subprog generated by the verifier to handle a thrown\nexception hardcodes a return value of 0. To allow user-defined logic\nand modification of the return value when an exception is thrown,\nintroduce the 'exception_callback:' declaration tag, which marks a\ncallback as the default exception handler for the program.\n\nThe format of the declaration tag is 'exception_callback:<value>', where\n<value> is the name of the exception callback. Each main program can be\ntagged using this BTF declaratiion tag to associate it with an exception\ncallback. In case the tag is absent, the default callback is used.\n\nAs such, the exception callback cannot be modified at runtime, only set\nduring verification.\n\nAllowing modification of the callback for the current program execution\nat runtime leads to issues when the programs begin to nest, as any\nper-CPU state maintaing this information will have to be saved and\nrestored. We don't want it to stay in bpf_prog_aux as this takes a\nglobal effect for all programs. An alternative solution is spilling\nthe callback pointer at a known location on the program stack on entry,\nand then passing this location to bpf_throw as a parameter.\n\nHowever, since exceptions are geared more towards a use case where they\nare ideally never invoked, optimizing for this use case and adding to\nthe complexity has diminishing returns.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-7-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/bpf_experimental.h"
      ]
    },
    {
      "hash": "aaa619ebccb2b78b3c6d2c0cd72d206ee8fc0025",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-09-16 09:34:21 -0700",
      "message": "bpf: Refactor check_btf_func and split into two phases\n\nThis patch splits the check_btf_info's check_btf_func check into two\nseparate phases.  The first phase sets up the BTF and prepares\nfunc_info, but does not perform any validation of required invariants\nfor subprogs just yet. This is left to the second phase, which happens\nwhere check_btf_info executes currently, and performs the line_info and\nCO-RE relocation.\n\nThe reason to perform this split is to obtain the userspace supplied\nfunc_info information before we perform the add_subprog call, where we\nwould now require finding and adding subprogs that may not have a\nbpf_pseudo_call or bpf_pseudo_func instruction in the program.\n\nWe require this as we want to enable userspace to supply exception\ncallbacks that can override the default hidden subprogram generated by\nthe verifier (which performs a hardcoded action). In such a case, the\nexception callback may never be referenced in an instruction, but will\nstill be suitably annotated (by way of BTF declaration tags). For\nfinding this exception callback, we would require the program's BTF\ninformation, and the supplied func_info information which maps BTF type\nIDs to subprograms.\n\nSince the exception callback won't actually be referenced through\ninstructions, later checks in check_cfg and do_check_subprogs will not\nverify the subprog. This means that add_subprog needs to add them in the\nadd_subprog_and_kfunc phase before we move forward, which is why the BTF\nand func_info are required at that point.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-6-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "f18b03fabaa9b7c80e80b72a621f481f0d706ae0",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-09-16 09:34:21 -0700",
      "message": "bpf: Implement BPF exceptions\n\nThis patch implements BPF exceptions, and introduces a bpf_throw kfunc\nto allow programs to throw exceptions during their execution at runtime.\nA bpf_throw invocation is treated as an immediate termination of the\nprogram, returning back to its caller within the kernel, unwinding all\nstack frames.\n\nThis allows the program to simplify its implementation, by testing for\nruntime conditions which the verifier has no visibility into, and assert\nthat they are true. In case they are not, the program can simply throw\nan exception from the other branch.\n\nBPF exceptions are explicitly *NOT* an unlikely slowpath error handling\nprimitive, and this objective has guided design choices of the\nimplementation of the them within the kernel (with the bulk of the cost\nfor unwinding the stack offloaded to the bpf_throw kfunc).\n\nThe implementation of this mechanism requires use of add_hidden_subprog\nmechanism introduced in the previous patch, which generates a couple of\ninstructions to move R1 to R0 and exit. The JIT then rewrites the\nprologue of this subprog to take the stack pointer and frame pointer as\ninputs and reset the stack frame, popping all callee-saved registers\nsaved by the main subprog. The bpf_throw function then walks the stack\nat runtime, and invokes this exception subprog with the stack and frame\npointers as parameters.\n\nReviewers must take note that currently the main program is made to save\nall callee-saved registers on x86_64 during entry into the program. This\nis because we must do an equivalent of a lightweight context switch when\nunwinding the stack, therefore we need the callee-saved registers of the\ncaller of the BPF program to be able to return with a sane state.\n\nNote that we have to additionally handle r12, even though it is not used\nby the program, because when throwing the exception the program makes an\nentry into the kernel which could clobber r12 after saving it on the\nstack. To be able to preserve the value we received on program entry, we\npush r12 and restore it from the generated subprogram when unwinding the\nstack.\n\nFor now, bpf_throw invocation fails when lingering resources or locks\nexist in that path of the program. In a future followup, bpf_throw will\nbe extended to perform frame-by-frame unwinding to release lingering\nresources for each stack frame, removing this limitation.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-5-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "arch/x86/net/bpf_jit_comp.c",
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "include/linux/filter.h",
        "kernel/bpf/core.c",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/bpf_experimental.h"
      ]
    },
    {
      "hash": "335d1c5b545284d75ef96ee42e461eacefe865bb",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-09-16 09:34:21 -0700",
      "message": "bpf: Implement support for adding hidden subprogs\n\nIntroduce support in the verifier for generating a subprogram and\ninclude it as part of a BPF program dynamically after the do_check phase\nis complete. The first user will be the next patch which generates\ndefault exception callbacks if none are set for the program. The phase\nof invocation will be do_misc_fixups. Note that this is an internal\nverifier function, and should be used with instruction blocks which\nuphold the invariants stated in check_subprogs.\n\nSince these subprogs are always appended to the end of the instruction\nsequence of the program, it becomes relatively inexpensive to do the\nrelated adjustments to the subprog_info of the program. Only the fake\nexit subprogram is shifted forward, making room for our new subprog.\n\nThis is useful to insert a new subprogram, get it JITed, and obtain its\nfunction pointer. The next patch will use this functionality to insert a\ndefault exception callback which will be invoked after unwinding the\nstack.\n\nNote that these added subprograms are invisible to userspace, and never\nreported in BPF_OBJ_GET_INFO_BY_ID etc. For now, only a single\nsubprogram is supported, but more can be easily supported in the future.\n\nTo this end, two function counts are introduced now, the existing\nfunc_cnt, and real_func_cnt, the latter including hidden programs. This\nallows us to conver the JIT code to use the real_func_cnt for management\nof resources while syscall path continues working with existing\nfunc_cnt.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230912233214.1518551-4-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "kernel/bpf/core.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "2b5dcb31a19a2e0acd869b12c9db9b2d696ef544",
      "author": "Leon Hwang <hffilwlqm@gmail.com>",
      "date": "2023-09-12 13:06:12 -0700",
      "message": "bpf, x64: Fix tailcall infinite loop\n\nFrom commit ebf7d1f508a73871 (\"bpf, x64: rework pro/epilogue and tailcall\nhandling in JIT\"), the tailcall on x64 works better than before.\n\nFrom commit e411901c0b775a3a (\"bpf: allow for tailcalls in BPF subprograms\nfor x64 JIT\"), tailcall is able to run in BPF subprograms on x64.\n\nFrom commit 5b92a28aae4dd0f8 (\"bpf: Support attaching tracing BPF program\nto other BPF programs\"), BPF program is able to trace other BPF programs.\n\nHow about combining them all together?\n\n1. FENTRY/FEXIT on a BPF subprogram.\n2. A tailcall runs in the BPF subprogram.\n3. The tailcall calls the subprogram's caller.\n\nAs a result, a tailcall infinite loop comes up. And the loop would halt\nthe machine.\n\nAs we know, in tail call context, the tail_call_cnt propagates by stack\nand rax register between BPF subprograms. So do in trampolines.\n\nFixes: ebf7d1f508a7 (\"bpf, x64: rework pro/epilogue and tailcall handling in JIT\")\nFixes: e411901c0b77 (\"bpf: allow for tailcalls in BPF subprograms for x64 JIT\")\nReviewed-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>\nSigned-off-by: Leon Hwang <hffilwlqm@gmail.com>\nLink: https://lore.kernel.org/r/20230912150442.2009-3-hffilwlqm@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "arch/x86/net/bpf_jit_comp.c",
        "include/linux/bpf.h",
        "kernel/bpf/trampoline.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5b221ecb3a9e48013d7b4ad7960af3adba23d1d1",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-09-08 08:42:18 -0700",
      "message": "bpf: Mark OBJ_RELEASE argument as MEM_RCU when possible\n\nIn previous selftests/bpf patch, we have\n  p = bpf_percpu_obj_new(struct val_t);\n  if (!p)\n          goto out;\n\n  p1 = bpf_kptr_xchg(&e->pc, p);\n  if (p1) {\n          /* race condition */\n          bpf_percpu_obj_drop(p1);\n  }\n\n  p = e->pc;\n  if (!p)\n          goto out;\n\nAfter bpf_kptr_xchg(), we need to re-read e->pc into 'p'.\nThis is due to that the second argument of bpf_kptr_xchg() is marked\nOBJ_RELEASE and it will be marked as invalid after the call.\nSo after bpf_kptr_xchg(), 'p' is an unknown scalar,\nand the bpf program needs to reread from the map value.\n\nThis patch checks if the 'p' has type MEM_ALLOC and MEM_PERCPU,\nand if 'p' is RCU protected. If this is the case, 'p' can be marked\nas MEM_RCU. MEM_ALLOC needs to be removed since 'p' is not\nan owning reference any more. Such a change makes re-read\nfrom the map value unnecessary.\n\nNote that re-reading 'e->pc' after bpf_kptr_xchg() might get\na different value from 'p' if immediately before 'p = e->pc',\nanother cpu may do another bpf_kptr_xchg() and swap in another value\ninto 'e->pc'. If this is the case, then 'p = e->pc' may\nget either 'p' or another value, and race condition already exists.\nSo removing direct re-reading seems fine too.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152816.2000760-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "01cc55af93884f1ff5a883426e1924378dfcc62a",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-09-08 08:42:17 -0700",
      "message": "bpf: Add bpf_this_cpu_ptr/bpf_per_cpu_ptr support for allocated percpu obj\n\nThe bpf helpers bpf_this_cpu_ptr() and bpf_per_cpu_ptr() are re-purposed\nfor allocated percpu objects. For an allocated percpu obj,\nthe reg type is 'PTR_TO_BTF_ID | MEM_PERCPU | MEM_RCU'.\n\nThe return type for these two re-purposed helpera is\n'PTR_TO_MEM | MEM_RCU | MEM_ALLOC'.\nThe MEM_ALLOC allows that the per-cpu data can be read and written.\n\nSince the memory allocator bpf_mem_alloc() returns\na ptr to a percpu ptr for percpu data, the first argument\nof bpf_this_cpu_ptr() and bpf_per_cpu_ptr() is patched\nwith a dereference before passing to the helper func.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152749.1997202-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "36d8bdf75a93190e5669b9d1d95994e13e15ba1d",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-09-08 08:42:17 -0700",
      "message": "bpf: Add alloc/xchg/direct_access support for local percpu kptr\n\nAdd two new kfunc's, bpf_percpu_obj_new_impl() and\nbpf_percpu_obj_drop_impl(), to allocate a percpu obj.\nTwo functions are very similar to bpf_obj_new_impl()\nand bpf_obj_drop_impl(). The major difference is related\nto percpu handling.\n\n    bpf_rcu_read_lock()\n    struct val_t __percpu_kptr *v = map_val->percpu_data;\n    ...\n    bpf_rcu_read_unlock()\n\nFor a percpu data map_val like above 'v', the reg->type\nis set as\n\tPTR_TO_BTF_ID | MEM_PERCPU | MEM_RCU\nif inside rcu critical section.\n\nMEM_RCU marking here is similar to NON_OWN_REF as 'v'\nis not a owning reference. But NON_OWN_REF is\ntrusted and typically inside the spinlock while\nMEM_RCU is under rcu read lock. RCU is preferred here\nsince percpu data structures mean potential concurrent\naccess into its contents.\n\nAlso, bpf_percpu_obj_new_impl() is restricted such that\nno pointers or special fields are allowed. Therefore,\nthe bpf_list_head and bpf_rb_root will not be supported\nin this patch set to avoid potential memory leak issue\ndue to racing between bpf_obj_free_fields() and another\nbpf_kptr_xchg() moving an allocated object to\nbpf_list_head and bpf_rb_root.\n\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230827152744.1996739-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5861d1e8dbc4e1a03ebffb96ac041026cdd34c07",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-08-25 09:23:17 -0700",
      "message": "bpf: Allow bpf_spin_{lock,unlock} in sleepable progs\n\nCommit 9e7a4d9831e8 (\"bpf: Allow LSM programs to use bpf spin locks\")\ndisabled bpf_spin_lock usage in sleepable progs, stating:\n\n Sleepable LSM programs can be preempted which means that allowng spin\n locks will need more work (disabling preemption and the verifier\n ensuring that no sleepable helpers are called when a spin lock is\n held).\n\nThis patch disables preemption before grabbing bpf_spin_lock. The second\nrequirement above \"no sleepable helpers are called when a spin lock is\nheld\" is implicitly enforced by current verifier logic due to helper\ncalls in spin_lock CS being disabled except for a few exceptions, none\nof which sleep.\n\nDue to above preemption changes, bpf_spin_lock CS can also be considered\na RCU CS, so verifier's in_rcu_cs check is modified to account for this.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230821193311.3290257-7-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "0816b8c6bf7fc87cec4273dc199e8f0764b9e7b1",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-08-25 09:23:16 -0700",
      "message": "bpf: Consider non-owning refs to refcounted nodes RCU protected\n\nAn earlier patch in the series ensures that the underlying memory of\nnodes with bpf_refcount - which can have multiple owners - is not reused\nuntil RCU grace period has elapsed. This prevents\nuse-after-free with non-owning references that may point to\nrecently-freed memory. While RCU read lock is held, it's safe to\ndereference such a non-owning ref, as by definition RCU GP couldn't have\nelapsed and therefore underlying memory couldn't have been reused.\n\nFrom the perspective of verifier \"trustedness\" non-owning refs to\nrefcounted nodes are now trusted only in RCU CS and therefore should no\nlonger pass is_trusted_reg, but rather is_rcu_reg. Let's mark them\nMEM_RCU in order to reflect this new state.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230821193311.3290257-6-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ba2464c86f182c6fdb69fe2f77a3d04c19a72357",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-08-25 09:23:16 -0700",
      "message": "bpf: Reenable bpf_refcount_acquire\n\nNow that all reported issues are fixed, bpf_refcount_acquire can be\nturned back on. Also reenable all bpf_refcount-related tests which were\ndisabled.\n\nThis a revert of:\n * commit f3514a5d6740 (\"selftests/bpf: Disable newly-added 'owner' field test until refcount re-enabled\")\n * commit 7deca5eae833 (\"bpf: Disable bpf_refcount_acquire kfunc calls until race conditions are fixed\")\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230821193311.3290257-5-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/prog_tests/refcounted_kptr.c"
      ]
    },
    {
      "hash": "f0d991a070750ada4f4397304b580ed6f68d3187",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-08-25 09:23:16 -0700",
      "message": "bpf: Ensure kptr_struct_meta is non-NULL for collection insert and refcount_acquire\n\nIt's straightforward to prove that kptr_struct_meta must be non-NULL for\nany valid call to these kfuncs:\n\n  * btf_parse_struct_metas in btf.c creates a btf_struct_meta for any\n    struct in user BTF with a special field (e.g. bpf_refcount,\n    {rb,list}_node). These are stored in that BTF's struct_meta_tab.\n\n  * __process_kf_arg_ptr_to_graph_node in verifier.c ensures that nodes\n    have {rb,list}_node field and that it's at the correct offset.\n    Similarly, check_kfunc_args ensures bpf_refcount field existence for\n    node param to bpf_refcount_acquire.\n\n  * So a btf_struct_meta must have been created for the struct type of\n    node param to these kfuncs\n\n  * That BTF and its struct_meta_tab are guaranteed to still be around.\n    Any arbitrary {rb,list} node the BPF program interacts with either:\n    came from bpf_obj_new or a collection removal kfunc in the same\n    program, in which case the BTF is associated with the program and\n    still around; or came from bpf_kptr_xchg, in which case the BTF was\n    associated with the map and is still around\n\nInstead of silently continuing with NULL struct_meta, which caused\nconfusing bugs such as those addressed by commit 2140a6e3422d (\"bpf: Set\nkptr_struct_meta for node param to list and rbtree insert funcs\"), let's\nerror out. Then, at runtime, we can confidently say that the\nimplementations of these kfuncs were given a non-NULL kptr_struct_meta,\nmeaning that special-field-specific functionality like\nbpf_obj_free_fields and the bpf_obj_drop change introduced later in this\nseries are guaranteed to execute.\n\nThis patch doesn't change functionality, just makes it easier to reason\nabout existing functionality.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nAcked-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230821193311.3290257-2-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d75e30dddf73449bc2d10bb8e2f1a2c446bc67a2",
      "author": "Yafang Shao <laoar.shao@gmail.com>",
      "date": "2023-08-23 09:37:29 -0700",
      "message": "bpf: Fix issue in verifying allow_ptr_leaks\n\nAfter we converted the capabilities of our networking-bpf program from\ncap_sys_admin to cap_net_admin+cap_bpf, our networking-bpf program\nfailed to start. Because it failed the bpf verifier, and the error log\nis \"R3 pointer comparison prohibited\".\n\nA simple reproducer as follows,\n\nSEC(\"cls-ingress\")\nint ingress(struct __sk_buff *skb)\n{\n\tstruct iphdr *iph = (void *)(long)skb->data + sizeof(struct ethhdr);\n\n\tif ((long)(iph + 1) > (long)skb->data_end)\n\t\treturn TC_ACT_STOLEN;\n\treturn TC_ACT_OK;\n}\n\nPer discussion with Yonghong and Alexei [1], comparison of two packet\npointers is not a pointer leak. This patch fixes it.\n\nOur local kernel is 6.1.y and we expect this fix to be backported to\n6.1.y, so stable is CCed.\n\n[1]. https://lore.kernel.org/bpf/CAADnVQ+Nmspr7Si+pxWn8zkE7hX-7s93ugwC+94aXSy4uQ9vBg@mail.gmail.com/\n\nSuggested-by: Yonghong Song <yonghong.song@linux.dev>\nSuggested-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>\nSigned-off-by: Yafang Shao <laoar.shao@gmail.com>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nCc: stable@vger.kernel.org\nLink: https://lore.kernel.org/r/20230823020703.3790-2-laoar.shao@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "6785b2edf48c6b1c3ea61fe3b0d2e02b8fbf90c0",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-08-22 12:52:48 -0700",
      "message": "bpf: Fix check_func_arg_reg_off bug for graph root/node\n\nThe commit being fixed introduced a hunk into check_func_arg_reg_off\nthat bypasses reg->off == 0 enforcement when offset points to a graph\nnode or root. This might possibly be done for treating bpf_rbtree_remove\nand others as KF_RELEASE and then later check correct reg->off in helper\nargument checks.\n\nBut this is not the case, those helpers are already not KF_RELEASE and\npermit non-zero reg->off and verify it later to match the subobject in\nBTF type.\n\nHowever, this logic leads to bpf_obj_drop permitting free of register\narguments with non-zero offset when they point to a graph root or node\nwithin them, which is not ok.\n\nFor instance:\n\nstruct foo {\n\tint i;\n\tint j;\n\tstruct bpf_rb_node node;\n};\n\nstruct foo *f = bpf_obj_new(typeof(*f));\nif (!f) ...\nbpf_obj_drop(f); // OK\nbpf_obj_drop(&f->i); // still ok from verifier PoV\nbpf_obj_drop(&f->node); // Not OK, but permitted right now\n\nFix this by dropping the whole part of code altogether.\n\nFixes: 6a3cd3318ff6 (\"bpf: Migrate release_on_unlock logic to non-owning ref semantics\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230822175140.1317749-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ab6c637ad0276e42f8acabcbc64932a6d346dab3",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-08-22 09:43:55 -0700",
      "message": "bpf: Fix a bpf_kptr_xchg() issue with local kptr\n\nWhen reviewing local percpu kptr support, Alexei discovered a bug\nwherea bpf_kptr_xchg() may succeed even if the map value kptr type and\nlocally allocated obj type do not match ([1]). Missed struct btf_id\ncomparison is the reason for the bug. This patch added such struct btf_id\ncomparison and will flag verification failure if types do not match.\n\n  [1] https://lore.kernel.org/bpf/20230819002907.io3iphmnuk43xblu@macbook-pro-8.dhcp.thefacebook.com/#t\n\nReported-by: Alexei Starovoitov <ast@kernel.org>\nFixes: 738c96d5e2e3 (\"bpf: Allow local kptrs to be exchanged via bpf_kptr_xchg\")\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230822050053.2886960-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "db2baf82b098aa10ac16f34e44732ec450fb11c7",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-08-07 16:23:35 -0700",
      "message": "bpf: Fix an incorrect verification success with movsx insn\n\nsyzbot reports a verifier bug which triggers a runtime panic.\nThe test bpf program is:\n   0: (62) *(u32 *)(r10 -8) = 553656332\n   1: (bf) r1 = (s16)r10\n   2: (07) r1 += -8\n   3: (b7) r2 = 3\n   4: (bd) if r2 <= r1 goto pc+0\n   5: (85) call bpf_trace_printk#-138320\n   6: (b7) r0 = 0\n   7: (95) exit\n\nAt insn 1, the current implementation keeps 'r1' as a frame pointer,\nwhich caused later bpf_trace_printk helper call crash since frame\npointer address is not valid any more. Note that at insn 4,\nthe 'pointer vs. scalar' comparison is allowed for privileged\nprog run.\n\nTo fix the problem with above insn 1, the fix in the patch adopts\nsimilar pattern to existing 'R1 = (u32) R2' handling. For unprivileged\nprog run, verification will fail with 'R<num> sign-extension part of pointer'.\nFor privileged prog run, the dst_reg 'r1' will be marked as\nan unknown scalar, so later 'bpf_trace_pointk' helper will complain\nsince it expected certain pointers.\n\nReported-by: syzbot+d61b595e9205573133b3@syzkaller.appspotmail.com\nFixes: 8100928c8814 (\"bpf: Support new sign-extension mov insns\")\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20230807175721.671696-1-yonghong.song@linux.dev\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "680ee0456a5712309db9ec2692e908ea1d6b1644",
      "author": "Jakub Kicinski <kuba@kernel.org>",
      "date": "2023-08-03 08:38:07 -0700",
      "message": "net: invert the netdevice.h vs xdp.h dependency\n\nxdp.h is far more specific and is included in only 67 other\nfiles vs netdevice.h's 1538 include sites.\nMake xdp.h include netdevice.h, instead of the other way around.\nThis decreases the incremental allmodconfig builds size when\nxdp.h is touched from 5947 to 662 objects.\n\nMove bpf_prog_run_xdp() to xdp.h, seems appropriate and filter.h\nis a mega-header in its own right so it's nice to avoid xdp.h\ngetting included there as well.\n\nThe only unfortunate part is that the typedef for xdp_features_t\nhas to move to netdevice.h, since its embedded in struct netdevice.\n\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>\nAcked-by: Jesper Dangaard Brouer <hawk@kernel.org>\nLink: https://lore.kernel.org/r/20230803010230.1755386-4-kuba@kernel.org\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "include/linux/filter.h",
        "include/linux/netdevice.h",
        "include/net/busy_poll.h",
        "include/net/xdp.h",
        "include/trace/events/xdp.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/offload.c",
        "kernel/bpf/verifier.c",
        "net/netfilter/nf_conntrack_bpf.c"
      ]
    },
    {
      "hash": "09fedc731874123e0f6e5e5e3572db0c60378c2a",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-07-28 08:54:04 -0700",
      "message": "bpf: Fix compilation warning with -Wparentheses\n\nThe kernel test robot reported compilation warnings when -Wparentheses is\nadded to KBUILD_CFLAGS with gcc compiler. The following is the error message:\n\n  .../bpf-next/kernel/bpf/verifier.c: In function \u2018coerce_reg_to_size_sx\u2019:\n  .../bpf-next/kernel/bpf/verifier.c:5901:14:\n    error: suggest parentheses around comparison in operand of \u2018==\u2019 [-Werror=parentheses]\n    if (s64_max >= 0 == s64_min >= 0) {\n        ~~~~~~~~^~~~\n  .../bpf-next/kernel/bpf/verifier.c: In function \u2018coerce_subreg_to_size_sx\u2019:\n  .../bpf-next/kernel/bpf/verifier.c:5965:14:\n    error: suggest parentheses around comparison in operand of \u2018==\u2019 [-Werror=parentheses]\n    if (s32_min >= 0 == s32_max >= 0) {\n        ~~~~~~~~^~~~\n\nTo fix the issue, add proper parentheses for the above '>=' condition\nto silence the warning/error.\n\nI tried a few clang compilers like clang16 and clang18 and they do not emit\nsuch warnings with -Wparentheses.\n\nReported-by: kernel test robot <lkp@intel.com>\nCloses: https://lore.kernel.org/oe-kbuild-all/202307281133.wi0c4SqG-lkp@intel.com/\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nLink: https://lore.kernel.org/r/20230728055740.2284534-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "4cd58e9af8b9d9fff6b7145e742abbfcda0af4af",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-07-27 18:52:33 -0700",
      "message": "bpf: Support new 32bit offset jmp instruction\n\nAdd interpreter/jit/verifier support for 32bit offset jmp instruction.\nIf a conditional jmp instruction needs more than 16bit offset,\nit can be simulated with a conditional jmp + a 32bit jmp insn.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011231.3716103-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "arch/x86/net/bpf_jit_comp.c",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ec0e2da95f72d4a46050a4d994e4fe471474fd80",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-07-27 18:52:33 -0700",
      "message": "bpf: Support new signed div/mod instructions.\n\nAdd interpreter/jit support for new signed div/mod insns.\nThe new signed div/mod instructions are encoded with\nunsigned div/mod instructions plus insn->off == 1.\nAlso add basic verifier support to ensure new insns get\naccepted.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011219.3714605-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "arch/x86/net/bpf_jit_comp.c",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "0845c3db7bf5c4ceb7100bcd8fd594d9ccf3c29a",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-07-27 18:52:33 -0700",
      "message": "bpf: Support new unconditional bswap instruction\n\nThe existing 'be' and 'le' insns will do conditional bswap\ndepends on host endianness. This patch implements\nunconditional bswap insns.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011213.3712808-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "arch/x86/net/bpf_jit_comp.c",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1f1e864b65554e33fe74e3377e58b12f4302f2eb",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-07-27 18:52:33 -0700",
      "message": "bpf: Handle sign-extenstin ctx member accesses\n\nCurrently, if user accesses a ctx member with signed types,\nthe compiler will generate an unsigned load followed by\nnecessary left and right shifts.\n\nWith the introduction of sign-extension load, compiler may\njust emit a ldsx insn instead. Let us do a final movsx sign\nextension to the final unsigned ctx load result to\nsatisfy original sign extension requirement.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011207.3712528-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "8100928c881482a73ed8bd499d602bab0fe55608",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-07-27 18:52:33 -0700",
      "message": "bpf: Support new sign-extension mov insns\n\nAdd interpreter/jit support for new sign-extension mov insns.\nThe original 'MOV' insn is extended to support reg-to-reg\nsigned version for both ALU and ALU64 operations. For ALU mode,\nthe insn->off value of 8 or 16 indicates sign-extension\nfrom 8- or 16-bit value to 32-bit value. For ALU64 mode,\nthe insn->off value of 8/16/32 indicates sign-extension\nfrom 8-, 16- or 32-bit value to 64-bit value.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011202.3712300-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "arch/x86/net/bpf_jit_comp.c",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1f9a1ea821ff25353a0e80d971e7958cd55b47a3",
      "author": "Yonghong Song <yonghong.song@linux.dev>",
      "date": "2023-07-27 18:52:33 -0700",
      "message": "bpf: Support new sign-extension load insns\n\nAdd interpreter/jit support for new sign-extension load insns\nwhich adds a new mode (BPF_MEMSX).\nAlso add verifier support to recognize these insns and to\ndo proper verification with new insns. In verifier, besides\nto deduce proper bounds for the dst_reg, probed memory access\nis also properly handled.\n\nAcked-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Yonghong Song <yonghong.song@linux.dev>\nLink: https://lore.kernel.org/r/20230728011156.3711870-1-yonghong.song@linux.dev\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "arch/x86/net/bpf_jit_comp.c",
        "include/linux/filter.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c",
        "tools/include/uapi/linux/bpf.h"
      ]
    },
    {
      "hash": "5ba190c29cf92f157bd63c9909c7050d6dc43df7",
      "author": "Anton Protopopov <aspsk@isovalent.com>",
      "date": "2023-07-19 09:48:52 -0700",
      "message": "bpf: consider CONST_PTR_TO_MAP as trusted pointer to struct bpf_map\n\nAdd the BTF id of struct bpf_map to the reg2btf_ids array. This makes the\nvalues of the CONST_PTR_TO_MAP type to be considered as trusted by kfuncs.\nThis, in turn, allows users to execute trusted kfuncs which accept `struct\nbpf_map *` arguments from non-tracing programs.\n\nWhile exporting the btf_bpf_map_id variable, save some bytes by defining\nit as BTF_ID_LIST_GLOBAL_SINGLE (which is u32[1]) and not as BTF_ID_LIST\n(which is u32[64]).\n\nSigned-off-by: Anton Protopopov <aspsk@isovalent.com>\nLink: https://lore.kernel.org/r/20230719092952.41202-3-aspsk@isovalent.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/btf_ids.h",
        "kernel/bpf/map_iter.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "831deb2976de4458adae4daee56aa6f740ed4acc",
      "author": "Anton Protopopov <aspsk@isovalent.com>",
      "date": "2023-07-19 09:48:52 -0700",
      "message": "bpf: consider types listed in reg2btf_ids as trusted\n\nThe reg2btf_ids array contains a list of types for which we can (and need)\nto find a corresponding static BTF id. All the types in the list can be\nconsidered as trusted for purposes of kfuncs.\n\nSigned-off-by: Anton Protopopov <aspsk@isovalent.com>\nLink: https://lore.kernel.org/r/20230719092952.41202-2-aspsk@isovalent.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b5e9ad522c4ccd32d322877515cff8d47ed731b9",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-07-18 15:21:09 -0700",
      "message": "bpf: Repeat check_max_stack_depth for async callbacks\n\nWhile the check_max_stack_depth function explores call chains emanating\nfrom the main prog, which is typically enough to cover all possible call\nchains, it doesn't explore those rooted at async callbacks unless the\nasync callback will have been directly called, since unlike non-async\ncallbacks it skips their instruction exploration as they don't\ncontribute to stack depth.\n\nIt could be the case that the async callback leads to a callchain which\nexceeds the stack depth, but this is never reachable while only\nexploring the entry point from main subprog. Hence, repeat the check for\nthe main subprog *and* all async callbacks marked by the symbolic\nexecution pass of the verifier, as execution of the program may begin at\nany of them.\n\nConsider functions with following stack depths:\nmain: 256\nasync: 256\nfoo: 256\n\nmain:\n    rX = async\n    bpf_timer_set_callback(...)\n\nasync:\n    foo()\n\nHere, async is not descended as it does not contribute to stack depth of\nmain (since it is referenced using bpf_pseudo_func and not\nbpf_pseudo_call). However, when async is invoked asynchronously, it will\nend up breaching the MAX_BPF_STACK limit by calling foo.\n\nHence, in addition to main, we also need to explore call chains\nbeginning at all async callback subprogs in a program.\n\nFixes: 7ddc80a476c2 (\"bpf: Teach stack depth check about async callbacks.\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230717161530.1238-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "ba7b3e7d5f9014be65879ede8fd599cb222901c9",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-07-18 15:21:09 -0700",
      "message": "bpf: Fix subprog idx logic in check_max_stack_depth\n\nThe assignment to idx in check_max_stack_depth happens once we see a\nbpf_pseudo_call or bpf_pseudo_func. This is not an issue as the rest of\nthe code performs a few checks and then pushes the frame to the frame\nstack, except the case of async callbacks. If the async callback case\ncauses the loop iteration to be skipped, the idx assignment will be\nincorrect on the next iteration of the loop. The value stored in the\nframe stack (as the subprogno of the current subprog) will be incorrect.\n\nThis leads to incorrect checks and incorrect tail_call_reachable\nmarking. Save the target subprog in a new variable and only assign to\nidx once we are done with the is_async_cb check which may skip pushing\nof frame to the frame stack and subsequent stack depth checks and tail\ncall markings.\n\nFixes: 7ddc80a476c2 (\"bpf: Teach stack depth check about async callbacks.\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230717161530.1238-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "7ce4dc3e4a9d954c8a1fb483c7a527e9b060b860",
      "author": "Yafang Shao <laoar.shao@gmail.com>",
      "date": "2023-07-13 16:24:29 -0700",
      "message": "bpf: Fix an error around PTR_UNTRUSTED\n\nPer discussion with Alexei, the PTR_UNTRUSTED flag should not been\ncleared when we start to walk a new struct, because the struct in\nquestion may be a struct nested in a union. We should also check and set\nthis flag before we walk its each member, in case itself is a union.\nWe will clear this flag if the field is BTF_TYPE_SAFE_RCU_OR_NULL.\n\nFixes: 6fcd486b3a0a (\"bpf: Refactor RCU enforcement in the verifier.\")\nSigned-off-by: Yafang Shao <laoar.shao@gmail.com>\nLink: https://lore.kernel.org/r/20230713025642.27477-2-laoar.shao@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "f42bcd168d034aa8abd9178c430b407be8c98827",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-07-12 07:57:18 -0700",
      "message": "bpf: teach verifier actual bounds of bpf_get_smp_processor_id() result\n\nbpf_get_smp_processor_id() helper returns current CPU on which BPF\nprogram runs. It can't return value that is bigger than maximum allowed\nnumber of CPUs (minus one, due to zero indexing). Teach BPF verifier to\nrecognize that. This makes it possible to use bpf_get_smp_processor_id()\nresult to index into arrays without extra checks, as demonstrated in\nsubsequent selftests/bpf patch.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230711232400.1658562-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "5415ccd50a8620c8cbaa32d6f18c946c453566f5",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-07-05 19:14:54 -0700",
      "message": "bpf: Fix max stack depth check for async callbacks\n\nThe check_max_stack_depth pass happens after the verifier's symbolic\nexecution, and attempts to walk the call graph of the BPF program,\nensuring that the stack usage stays within bounds for all possible call\nchains. There are two cases to consider: bpf_pseudo_func and\nbpf_pseudo_call. In the former case, the callback pointer is loaded into\na register, and is assumed that it is passed to some helper later which\ncalls it (however there is no way to be sure), but the check remains\nconservative and accounts the stack usage anyway. For this particular\ncase, asynchronous callbacks are skipped as they execute asynchronously\nwhen their corresponding event fires.\n\nThe case of bpf_pseudo_call is simpler and we know that the call is\ndefinitely made, hence the stack depth of the subprog is accounted for.\n\nHowever, the current check still skips an asynchronous callback even if\na bpf_pseudo_call was made for it. This is erroneous, as it will miss\naccounting for the stack usage of the asynchronous callback, which can\nbe used to breach the maximum stack depth limit.\n\nFix this by only skipping asynchronous callbacks when the instruction is\nnot a pseudo call to the subprog.\n\nFixes: 7ddc80a476c2 (\"bpf: Teach stack depth check about async callbacks.\")\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230705144730.235802-2-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1ffc85d9298e0ca0137ba65c93a786143fe167b8",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-06-13 15:15:08 -0700",
      "message": "bpf: Verify scalar ids mapping in regsafe() using check_ids()\n\nMake sure that the following unsafe example is rejected by verifier:\n\n1: r9 = ... some pointer with range X ...\n2: r6 = ... unbound scalar ID=a ...\n3: r7 = ... unbound scalar ID=b ...\n4: if (r6 > r7) goto +1\n5: r6 = r7\n6: if (r6 > X) goto ...\n--- checkpoint ---\n7: r9 += r7\n8: *(u64 *)r9 = Y\n\nThis example is unsafe because not all execution paths verify r7 range.\nBecause of the jump at (4) the verifier would arrive at (6) in two states:\nI.  r6{.id=b}, r7{.id=b} via path 1-6;\nII. r6{.id=a}, r7{.id=b} via path 1-4, 6.\n\nCurrently regsafe() does not call check_ids() for scalar registers,\nthus from POV of regsafe() states (I) and (II) are identical. If the\npath 1-6 is taken by verifier first, and checkpoint is created at (6)\nthe path [1-4, 6] would be considered safe.\n\nChanges in this commit:\n- check_ids() is modified to disallow mapping multiple old_id to the\n  same cur_id.\n- check_scalar_ids() is added, unlike check_ids() it treats ID zero as\n  a unique scalar ID.\n- check_scalar_ids() needs to generate temporary unique IDs, field\n  'tmp_id_gen' is added to bpf_verifier_env::idmap_scratch to\n  facilitate this.\n- regsafe() is updated to:\n  - use check_scalar_ids() for precise scalar registers.\n  - compare scalar registers using memcmp only for explore_alu_limits\n    branch. This simplifies control flow for scalar case, and has no\n    measurable performance impact.\n- check_alu_op() is updated to avoid generating bpf_reg_state::id for\n  constant scalar values when processing BPF_MOV. ID is needed to\n  propagate range information for identical values, but there is\n  nothing to propagate for constants.\n\nFixes: 75748837b7e5 (\"bpf: Propagate scalar ranges through register assignments.\")\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230613153824.3324830-4-eddyz87@gmail.com",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "904e6ddf4133c52fdb9654c2cd2ad90f320d48b9",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-06-13 15:14:27 -0700",
      "message": "bpf: Use scalar ids in mark_chain_precision()\n\nChange mark_chain_precision() to track precision in situations\nlike below:\n\n    r2 = unknown value\n    ...\n  --- state #0 ---\n    ...\n    r1 = r2                 // r1 and r2 now share the same ID\n    ...\n  --- state #1 {r1.id = A, r2.id = A} ---\n    ...\n    if (r2 > 10) goto exit; // find_equal_scalars() assigns range to r1\n    ...\n  --- state #2 {r1.id = A, r2.id = A} ---\n    r3 = r10\n    r3 += r1                // need to mark both r1 and r2\n\nAt the beginning of the processing of each state, ensure that if a\nregister with a scalar ID is marked as precise, all registers sharing\nthis ID are also marked as precise.\n\nThis property would be used by a follow-up change in regsafe().\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230613153824.3324830-2-eddyz87@gmail.com",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/verifier/precise.c"
      ]
    },
    {
      "hash": "0108a4e9f3584a7a2c026d1601b0682ff7335d95",
      "author": "Krister Johansen <kjlx@templeofstupid.com>",
      "date": "2023-06-13 15:13:52 -0700",
      "message": "bpf: ensure main program has an extable\n\nWhen subprograms are in use, the main program is not jit'd after the\nsubprograms because jit_subprogs sets a value for prog->bpf_func upon\nsuccess.  Subsequent calls to the JIT are bypassed when this value is\nnon-NULL.  This leads to a situation where the main program and its\nfunc[0] counterpart are both in the bpf kallsyms tree, but only func[0]\nhas an extable.  Extables are only created during JIT.  Now there are\ntwo nearly identical program ksym entries in the tree, but only one has\nan extable.  Depending upon how the entries are placed, there's a chance\nthat a fault will call search_extable on the aux with the NULL entry.\n\nSince jit_subprogs already copies state from func[0] to the main\nprogram, include the extable pointer in this state duplication.\nAdditionally, ensure that the copy of the main program in func[0] is not\nadded to the bpf_prog_kallsyms table. Instead, let the main program get\nadded later in bpf_prog_load().  This ensures there is only a single\ncopy of the main program in the kallsyms table, and that its tag matches\nthe tag observed by tooling like bpftool.\n\nCc: stable@vger.kernel.org\nFixes: 1c2a088a6626 (\"bpf: x64: add JIT support for multi-function programs\")\nSigned-off-by: Krister Johansen <kjlx@templeofstupid.com>\nAcked-by: Yonghong Song <yhs@fb.com>\nAcked-by: Ilya Leoshkevich <iii@linux.ibm.com>\nTested-by: Ilya Leoshkevich <iii@linux.ibm.com>\nLink: https://lore.kernel.org/r/6de9b2f4b4724ef56efbb0339daaa66c8b68b1e7.1686616663.git.kjlx@templeofstupid.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "713274f1f2c896d37017efee333fd44149710119",
      "author": "Maxim Mikityanskiy <maxim@isovalent.com>",
      "date": "2023-06-08 10:27:43 +0200",
      "message": "bpf: Fix verifier id tracking of scalars on spill\n\nThe following scenario describes a bug in the verifier where it\nincorrectly concludes about equivalent scalar IDs which could lead to\nverifier bypass in privileged mode:\n\n1. Prepare a 32-bit rogue number.\n2. Put the rogue number into the upper half of a 64-bit register, and\n   roll a random (unknown to the verifier) bit in the lower half. The\n   rest of the bits should be zero (although variations are possible).\n3. Assign an ID to the register by MOVing it to another arbitrary\n   register.\n4. Perform a 32-bit spill of the register, then perform a 32-bit fill to\n   another register. Due to a bug in the verifier, the ID will be\n   preserved, although the new register will contain only the lower 32\n   bits, i.e. all zeros except one random bit.\n\nAt this point there are two registers with different values but the same\nID, which means the integrity of the verifier state has been corrupted.\n\n5. Compare the new 32-bit register with 0. In the branch where it's\n   equal to 0, the verifier will believe that the original 64-bit\n   register is also 0, because it has the same ID, but its actual value\n   still contains the rogue number in the upper half.\n   Some optimizations of the verifier prevent the actual bypass, so\n   extra care is needed: the comparison must be between two registers,\n   and both branches must be reachable (this is why one random bit is\n   needed). Both branches are still suitable for the bypass.\n6. Right shift the original register by 32 bits to pop the rogue number.\n7. Use the rogue number as an offset with any pointer. The verifier will\n   believe that the offset is 0, while in reality it's the given number.\n\nThe fix is similar to the 32-bit BPF_MOV handling in check_alu_op for\nSCALAR_VALUE. If the spill is narrowing the actual register value, don't\nkeep the ID, make sure it's reset to 0.\n\nFixes: 354e8f1970f8 (\"bpf: Support <8-byte scalar spill and refill\")\nSigned-off-by: Maxim Mikityanskiy <maxim@isovalent.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nTested-by: Andrii Nakryiko <andrii@kernel.org> # Checked veristat delta\nAcked-by: Yonghong Song <yhs@fb.com>\nLink: https://lore.kernel.org/bpf/20230607123951.558971-2-maxtram95@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "51302c951c8fd5c298565c7127c855bf1d4550b6",
      "author": "David Vernet <void@manifault.com>",
      "date": "2023-06-05 14:36:57 -0700",
      "message": "bpf: Teach verifier that trusted PTR_TO_BTF_ID pointers are non-NULL\n\nIn reg_type_not_null(), we currently assume that a pointer may be NULL\nif it has the PTR_MAYBE_NULL modifier, or if it doesn't belong to one of\nseveral base type of pointers that are never NULL-able. For example,\nPTR_TO_CTX, PTR_TO_MAP_VALUE, etc.\n\nIt turns out that in some cases, PTR_TO_BTF_ID can never be NULL as\nwell, though we currently don't specify it. For example, if you had the\nfollowing program:\n\nSEC(\"tc\")\nlong example_refcnt_fail(void *ctx)\n{\n\tstruct bpf_cpumask *mask1, *mask2;\n\n\tmask1 = bpf_cpumask_create();\n\tmask2 = bpf_cpumask_create();\n\n        if (!mask1 || !mask2)\n\t\tgoto error_release;\n\n\tbpf_cpumask_test_cpu(0, (const struct cpumask *)mask1);\n\tbpf_cpumask_test_cpu(0, (const struct cpumask *)mask2);\n\nerror_release:\n\tif (mask1)\n\t\tbpf_cpumask_release(mask1);\n\tif (mask2)\n\t\tbpf_cpumask_release(mask2);\n\treturn ret;\n}\n\nThe verifier will incorrectly fail to load the program, thinking\n(unintuitively) that we have a possibly-unreleased reference if the mask\nis NULL, because we (correctly) don't issue a bpf_cpumask_release() on\nthe NULL path.\n\nThe reason the verifier gets confused is due to the fact that we don't\nexplicitly tell the verifier that trusted PTR_TO_BTF_ID pointers can\nnever be NULL. Basically, if we successfully get past the if check\n(meaning both pointers go from ptr_or_null_bpf_cpumask to\nptr_bpf_cpumask), the verifier will correctly assume that the references\nneed to be dropped on any possible branch that leads to program exit.\nHowever, it will _incorrectly_ think that the ptr == NULL branch is\npossible, and will erroneously detect it as a branch on which we failed\nto drop the reference.\n\nThe solution is of course to teach the verifier that trusted\nPTR_TO_BTF_ID pointers can never be NULL, so that it doesn't incorrectly\nthink it's possible for the reference to be present on the ptr == NULL\nbranch.\n\nA follow-on patch will add a selftest that verifies this behavior.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230602150112.1494194-1-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "503e4def5414fd0f9b6ffecb6eedbc4b1603693b",
      "author": "Daniel T. Lee <danieltimlee@gmail.com>",
      "date": "2023-06-05 14:33:17 -0700",
      "message": "bpf: Replace open code with for allocated object check\n\n>From commit 282de143ead9 (\"bpf: Introduce allocated objects support\"),\nWith this allocated object with BPF program, (PTR_TO_BTF_ID | MEM_ALLOC)\nhas been a way of indicating to check the type is the allocated object.\n\ncommit d8939cb0a03c (\"bpf: Loosen alloc obj test in verifier's\nreg_btf_record\")\n>From the commit, there has been helper function for checking this, named\ntype_is_ptr_alloc_obj(). But still, some of the code use open code to\nretrieve this info. This commit replaces the open code with the\ntype_is_alloc(), and the type_is_ptr_alloc_obj() function.\n\nSigned-off-by: Daniel T. Lee <danieltimlee@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230527122706.59315-1-danieltimlee@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "7793fc3babe9fea908e57f7c187ea819f9fd7e95",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-06-05 13:17:20 -0700",
      "message": "bpf: Make bpf_refcount_acquire fallible for non-owning refs\n\nThis patch fixes an incorrect assumption made in the original\nbpf_refcount series [0], specifically that the BPF program calling\nbpf_refcount_acquire on some node can always guarantee that the node is\nalive. In that series, the patch adding failure behavior to rbtree_add\nand list_push_{front, back} breaks this assumption for non-owning\nreferences.\n\nConsider the following program:\n\n  n = bpf_kptr_xchg(&mapval, NULL);\n  /* skip error checking */\n\n  bpf_spin_lock(&l);\n  if(bpf_rbtree_add(&t, &n->rb, less)) {\n    bpf_refcount_acquire(n);\n    /* Failed to add, do something else with the node */\n  }\n  bpf_spin_unlock(&l);\n\nIt's incorrect to assume that bpf_refcount_acquire will always succeed in this\nscenario. bpf_refcount_acquire is being called in a critical section\nhere, but the lock being held is associated with rbtree t, which isn't\nnecessarily the lock associated with the tree that the node is already\nin. So after bpf_rbtree_add fails to add the node and calls bpf_obj_drop\nin it, the program has no ownership of the node's lifetime. Therefore\nthe node's refcount can be decr'd to 0 at any time after the failing\nrbtree_add. If this happens before the refcount_acquire above, the node\nmight be free'd, and regardless refcount_acquire will be incrementing a\n0 refcount.\n\nLater patches in the series exercise this scenario, resulting in the\nexpected complaint from the kernel (without this patch's changes):\n\n  refcount_t: addition on 0; use-after-free.\n  WARNING: CPU: 1 PID: 207 at lib/refcount.c:25 refcount_warn_saturate+0xbc/0x110\n  Modules linked in: bpf_testmod(O)\n  CPU: 1 PID: 207 Comm: test_progs Tainted: G           O       6.3.0-rc7-02231-g723de1a718a2-dirty #371\n  Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.15.0-0-g2dd4b9b3f840-prebuilt.qemu.org 04/01/2014\n  RIP: 0010:refcount_warn_saturate+0xbc/0x110\n  Code: 6f 64 f6 02 01 e8 84 a3 5c ff 0f 0b eb 9d 80 3d 5e 64 f6 02 00 75 94 48 c7 c7 e0 13 d2 82 c6 05 4e 64 f6 02 01 e8 64 a3 5c ff <0f> 0b e9 7a ff ff ff 80 3d 38 64 f6 02 00 0f 85 6d ff ff ff 48 c7\n  RSP: 0018:ffff88810b9179b0 EFLAGS: 00010082\n  RAX: 0000000000000000 RBX: 0000000000000002 RCX: 0000000000000000\n  RDX: 0000000000000202 RSI: 0000000000000008 RDI: ffffffff857c3680\n  RBP: ffff88810027d3c0 R08: ffffffff8125f2a4 R09: ffff88810b9176e7\n  R10: ffffed1021722edc R11: 746e756f63666572 R12: ffff88810027d388\n  R13: ffff88810027d3c0 R14: ffffc900005fe030 R15: ffffc900005fe048\n  FS:  00007fee0584a700(0000) GS:ffff88811b280000(0000) knlGS:0000000000000000\n  CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n  CR2: 00005634a96f6c58 CR3: 0000000108ce9002 CR4: 0000000000770ee0\n  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n  DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n  PKRU: 55555554\n  Call Trace:\n   <TASK>\n   bpf_refcount_acquire_impl+0xb5/0xc0\n\n  (rest of output snipped)\n\nThe patch addresses this by changing bpf_refcount_acquire_impl to use\nrefcount_inc_not_zero instead of refcount_inc and marking\nbpf_refcount_acquire KF_RET_NULL.\n\nFor owning references, though, we know the above scenario is not possible\nand thus that bpf_refcount_acquire will always succeed. Some verifier\nbookkeeping is added to track \"is input owning ref?\" for bpf_refcount_acquire\ncalls and return false from is_kfunc_ret_null for bpf_refcount_acquire on\nowning refs despite it being marked KF_RET_NULL.\n\nExisting selftests using bpf_refcount_acquire are modified where\nnecessary to NULL-check its return value.\n\n  [0]: https://lore.kernel.org/bpf/20230415201811.343116-1-davemarchevsky@fb.com/\n\nFixes: d2dcc67df910 (\"bpf: Migrate bpf_rbtree_add and bpf_list_push_{front,back} to possibly fail\")\nReported-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230602022647.1571784-5-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/refcounted_kptr.c",
        "tools/testing/selftests/bpf/progs/refcounted_kptr_fail.c"
      ]
    },
    {
      "hash": "2140a6e3422de22e6ebe77d4d18b6c0c9c425426",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-06-05 13:17:19 -0700",
      "message": "bpf: Set kptr_struct_meta for node param to list and rbtree insert funcs\n\nIn verifier.c, fixup_kfunc_call uses struct bpf_insn_aux_data's\nkptr_struct_meta field to pass information about local kptr types to\nvarious helpers and kfuncs at runtime. The recent bpf_refcount series\nadded a few functions to the set that need this information:\n\n  * bpf_refcount_acquire\n    * Needs to know where the refcount field is in order to increment\n  * Graph collection insert kfuncs: bpf_rbtree_add, bpf_list_push_{front,back}\n    * Were migrated to possibly fail by the bpf_refcount series. If\n      insert fails, the input node is bpf_obj_drop'd. bpf_obj_drop needs\n      the kptr_struct_meta in order to decr refcount and properly free\n      special fields.\n\nUnfortunately the verifier handling of collection insert kfuncs was not\nmodified to actually populate kptr_struct_meta. Accordingly, when the\nnode input to those kfuncs is passed to bpf_obj_drop, it is done so\nwithout the information necessary to decr refcount.\n\nThis patch fixes the issue by populating kptr_struct_meta for those\nkfuncs.\n\nFixes: d2dcc67df910 (\"bpf: Migrate bpf_rbtree_add and bpf_list_push_{front,back} to possibly fail\")\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230602022647.1571784-3-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "e924e80ee6a39bc28d2ef8f51e19d336a98e3be0",
      "author": "Aditi Ghag <aditi.ghag@isovalent.com>",
      "date": "2023-05-19 22:44:14 -0700",
      "message": "bpf: Add kfunc filter function to 'struct btf_kfunc_id_set'\n\nThis commit adds the ability to filter kfuncs to certain BPF program\ntypes. This is required to limit bpf_sock_destroy kfunc implemented in\nfollow-up commits to programs with attach type 'BPF_TRACE_ITER'.\n\nThe commit adds a callback filter to 'struct btf_kfunc_id_set'.  The\nfilter has access to the `bpf_prog` construct including its properties\nsuch as `expected_attached_type`.\n\nSigned-off-by: Aditi Ghag <aditi.ghag@isovalent.com>\nLink: https://lore.kernel.org/r/20230519225157.760788-7-aditi.ghag@isovalent.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "include/linux/btf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "0613d8ca9ab382caabe9ed2dceb429e9781e443f",
      "author": "Will Deacon <will@kernel.org>",
      "date": "2023-05-19 09:58:37 -0700",
      "message": "bpf: Fix mask generation for 32-bit narrow loads of 64-bit fields\n\nA narrow load from a 64-bit context field results in a 64-bit load\nfollowed potentially by a 64-bit right-shift and then a bitwise AND\noperation to extract the relevant data.\n\nIn the case of a 32-bit access, an immediate mask of 0xffffffff is used\nto construct a 64-bit BPP_AND operation which then sign-extends the mask\nvalue and effectively acts as a glorified no-op. For example:\n\n0:\t61 10 00 00 00 00 00 00\tr0 = *(u32 *)(r1 + 0)\n\nresults in the following code generation for a 64-bit field:\n\n\tldr\tx7, [x7]\t// 64-bit load\n\tmov\tx10, #0xffffffffffffffff\n\tand\tx7, x7, x10\n\nFix the mask generation so that narrow loads always perform a 32-bit AND\noperation:\n\n\tldr\tx7, [x7]\t// 64-bit load\n\tmov\tw10, #0xffffffff\n\tand\tw7, w7, w10\n\nCc: Alexei Starovoitov <ast@kernel.org>\nCc: Daniel Borkmann <daniel@iogearbox.net>\nCc: John Fastabend <john.fastabend@gmail.com>\nCc: Krzesimir Nowak <krzesimir@kinvolk.io>\nCc: Andrey Ignatov <rdna@fb.com>\nAcked-by: Yonghong Song <yhs@fb.com>\nFixes: 31fd85816dbe (\"bpf: permits narrower load from bpf program context fields\")\nSigned-off-by: Will Deacon <will@kernel.org>\nLink: https://lore.kernel.org/r/20230518102528.1341-1-will@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d84b1a6708eec06b6cd9d33c5e0177bbd6ba4813",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-05-15 12:06:31 -0700",
      "message": "bpf: fix calculation of subseq_idx during precision backtracking\n\nSubsequent instruction index (subseq_idx) is an index of an instruction\nthat was verified/executed by verifier after the currently processed\ninstruction. It is maintained during precision backtracking processing\nand is used to detect various subprog calling conditions.\n\nThis patch fixes the bug with incorrectly resetting subseq_idx to -1\nwhen going from child state to parent state during backtracking. If we\ndon't maintain correct subseq_idx we can misidentify subprog calls\nleading to precision tracking bugs.\n\nOne such case was triggered by test_global_funcs/global_func9 test where\nglobal subprog call happened to be the very last instruction in parent\nstate, leading to subseq_idx==-1, triggering WARN_ONCE:\n\n  [   36.045754] verifier backtracking bug\n  [   36.045764] WARNING: CPU: 13 PID: 2073 at kernel/bpf/verifier.c:3503 __mark_chain_precision+0xcc6/0xde0\n  [   36.046819] Modules linked in: aesni_intel(E) crypto_simd(E) cryptd(E) kvm_intel(E) kvm(E) irqbypass(E) i2c_piix4(E) serio_raw(E) i2c_core(E) crc32c_intel)\n  [   36.048040] CPU: 13 PID: 2073 Comm: test_progs Tainted: G        W  OE      6.3.0-07976-g4d585f48ee6b-dirty #972\n  [   36.048783] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-59-gc9ba5276e321-prebuilt.qemu.org 04/01/2014\n  [   36.049648] RIP: 0010:__mark_chain_precision+0xcc6/0xde0\n  [   36.050038] Code: 3d 82 c6 05 bb 35 32 02 01 e8 66 21 ec ff 0f 0b b8 f2 ff ff ff e9 30 f5 ff ff 48 c7 c7 f3 61 3d 82 4c 89 0c 24 e8 4a 21 ec ff <0f> 0b 4c0\n\nWith the fix precision tracking across multiple states works correctly now:\n\nmark_precise: frame0: last_idx 45 first_idx 38 subseq_idx -1\nmark_precise: frame0: regs=r8 stack= before 44: (61) r7 = *(u32 *)(r10 -4)\nmark_precise: frame0: regs=r8 stack= before 43: (85) call pc+41\nmark_precise: frame0: regs=r8 stack= before 42: (07) r1 += -48\nmark_precise: frame0: regs=r8 stack= before 41: (bf) r1 = r10\nmark_precise: frame0: regs=r8 stack= before 40: (63) *(u32 *)(r10 -48) = r1\nmark_precise: frame0: regs=r8 stack= before 39: (b4) w1 = 0\nmark_precise: frame0: regs=r8 stack= before 38: (85) call pc+38\nmark_precise: frame0: parent state regs=r8 stack=:  R0_w=scalar() R1_w=map_value(off=4,ks=4,vs=8,imm=0) R6=1 R7_w=scalar() R8_r=P0 R10=fpm\nmark_precise: frame0: last_idx 36 first_idx 28 subseq_idx 38\nmark_precise: frame0: regs=r8 stack= before 36: (18) r1 = 0xffff888104f2ed14\nmark_precise: frame0: regs=r8 stack= before 35: (85) call pc+33\nmark_precise: frame0: regs=r8 stack= before 33: (18) r1 = 0xffff888104f2ed10\nmark_precise: frame0: regs=r8 stack= before 32: (85) call pc+36\nmark_precise: frame0: regs=r8 stack= before 31: (07) r1 += -4\nmark_precise: frame0: regs=r8 stack= before 30: (bf) r1 = r10\nmark_precise: frame0: regs=r8 stack= before 29: (63) *(u32 *)(r10 -4) = r7\nmark_precise: frame0: regs=r8 stack= before 28: (4c) w7 |= w0\nmark_precise: frame0: parent state regs=r8 stack=:  R0_rw=scalar() R6=1 R7_rw=scalar() R8_rw=P0 R10=fp0 fp-48_r=mmmmmmmm\nmark_precise: frame0: last_idx 27 first_idx 16 subseq_idx 28\nmark_precise: frame0: regs=r8 stack= before 27: (85) call pc+31\nmark_precise: frame0: regs=r8 stack= before 26: (b7) r1 = 0\nmark_precise: frame0: regs=r8 stack= before 25: (b7) r8 = 0\n\nNote how subseq_idx starts out as -1, then is preserved as 38 and then 28 as we\ngo up the parent state chain.\n\nReported-by: Alexei Starovoitov <ast@kernel.org>\nFixes: fde2a3882bd0 (\"bpf: support precision propagation in the presence of subprogs\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230515180710.1535018-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "4d585f48ee6b38c54c075b151c5efd2ff65f8ffd",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-05-15 07:17:12 -0700",
      "message": "bpf: Remove anonymous union in bpf_kfunc_call_arg_meta\n\nFor kfuncs like bpf_obj_drop and bpf_refcount_acquire - which take\nuser-defined types as input - the verifier needs to track the specific\ntype passed in when checking a particular kfunc call. This requires\ntracking (btf, btf_id) tuple. In commit 7c50b1cb76ac\n(\"bpf: Add bpf_refcount_acquire kfunc\") I added an anonymous union with\ninner structs named after the specific kfuncs tracking this information,\nwith the goal of making it more obvious which kfunc this data was being\ntracked / expected to be tracked on behalf of.\n\nIn a recent series adding a new user of this tuple, Alexei mentioned\nthat he didn't like this union usage as it doesn't really help with\nreadability or bug-proofing ([0]). In an offline convo we agreed to\nhave the tuple be fields (arg_btf, arg_btf_id), with comments in\nbpf_kfunc_call_arg_meta definition enumerating the uses of the fields by\nkfunc-specific handling logic. Such a pattern is used by struct\nbpf_reg_state without trouble.\n\nAccordingly, this patch removes the anonymous union in favor of arg_btf\nand arg_btf_id fields and comment enumerating their current uses. The\npatch also removes struct btf_and_id, which was only being used by the\nremoved union's inner structs.\n\nThis is a mechanical change, existing linked_list and rbtree tests will\nvalidate that correct (btf, btf_id) are being passed.\n\n  [0]: https://lore.kernel.org/bpf/20230505021707.vlyiwy57vwxglbka@dhcp-172-26-102-232.dhcp.thefacebook.com\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230510213047.1633612-1-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "2012c867c8005d72c949e274133df429ece78808",
      "author": "Daniel Rosenberg <drosen@google.com>",
      "date": "2023-05-06 16:42:57 -0700",
      "message": "bpf: verifier: Accept dynptr mem as mem in helpers\n\nThis allows using memory retrieved from dynptrs with helper functions\nthat accept ARG_PTR_TO_MEM. For instance, results from bpf_dynptr_data\ncan be passed along to bpf_strncmp.\n\nSigned-off-by: Daniel Rosenberg <drosen@google.com>\nLink: https://lore.kernel.org/r/20230506013134.2492210-5-drosen@google.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "3bda08b63670c39be390fcb00e7718775508e673",
      "author": "Daniel Rosenberg <drosen@google.com>",
      "date": "2023-05-06 16:42:57 -0700",
      "message": "bpf: Allow NULL buffers in bpf_dynptr_slice(_rw)\n\nbpf_dynptr_slice(_rw) uses a user provided buffer if it can not provide\na pointer to a block of contiguous memory. This buffer is unused in the\ncase of local dynptrs, and may be unused in other cases as well. There\nis no need to require the buffer, as the kfunc can just return NULL if\nit was needed and not provided.\n\nThis adds another kfunc annotation, __opt, which combines with __sz and\n__szk to allow the buffer associated with the size to be NULL. If the\nbuffer is NULL, the verifier does not check that the buffer is of\nsufficient size.\n\nSigned-off-by: Daniel Rosenberg <drosen@google.com>\nLink: https://lore.kernel.org/r/20230506013134.2492210-2-drosen@google.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "Documentation/bpf/kfuncs.rst",
        "include/linux/skbuff.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "fde2a3882bd07876c144f2e00f7ae6893c378180",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-05-04 22:35:35 -0700",
      "message": "bpf: support precision propagation in the presence of subprogs\n\nAdd support precision backtracking in the presence of subprogram frames in\njump history.\n\nThis means supporting a few different kinds of subprogram invocation\nsituations, all requiring a slightly different handling in precision\nbacktracking handling logic:\n  - static subprogram calls;\n  - global subprogram calls;\n  - callback-calling helpers/kfuncs.\n\nFor each of those we need to handle a few precision propagation cases:\n  - what to do with precision of subprog returns (r0);\n  - what to do with precision of input arguments;\n  - for all of them callee-saved registers in caller function should be\n    propagated ignoring subprog/callback part of jump history.\n\nN.B. Async callback-calling helpers (currently only\nbpf_timer_set_callback()) are transparent to all this because they set\na separate async callback environment and thus callback's history is not\nshared with main program's history. So as far as all the changes in this\ncommit goes, such helper is just a regular helper.\n\nLet's look at all these situation in more details. Let's start with\nstatic subprogram being called, using an exxerpt of a simple main\nprogram and its static subprog, indenting subprog's frame slightly to\nmake everything clear.\n\nframe 0\t\t\t\tframe 1\t\t\tprecision set\n=======\t\t\t\t=======\t\t\t=============\n\n 9: r6 = 456;\n10: r1 = 123;\t\t\t\t\t\tfr0: r6\n11: call pc+10;\t\t\t\t\t\tfr0: r1, r6\n\t\t\t\t22: r0 = r1;\t\tfr0: r6;     fr1: r1\n\t\t\t\t23: exit\t\tfr0: r6;     fr1: r0\n12: r1 = <map_pointer>\t\t\t\t\tfr0: r0, r6\n13: r1 += r0;\t\t\t\t\t\tfr0: r0, r6\n14: r1 += r6;\t\t\t\t\t\tfr0: r6\n15: exit\n\nAs can be seen above main function is passing 123 as single argument to\nan identity (`return x;`) subprog. Returned value is used to adjust map\npointer offset, which forces r0 to be marked as precise. Then\ninstruction #14 does the same for callee-saved r6, which will have to be\nbacktracked all the way to instruction #9. For brevity, precision sets\nfor instruction #13 and #14 are combined in the diagram above.\n\nFirst, for subprog calls, r0 returned from subprog (in frame 0) has to\ngo into subprog's frame 1, and should be cleared from frame 0. So we go\nback into subprog's frame knowing we need to mark r0 precise. We then\nsee that insn #22 sets r0 from r1, so now we care about marking r1\nprecise.  When we pop up from subprog's frame back into caller at\ninsn #11 we keep r1, as it's an argument-passing register, so we eventually\nfind `10: r1 = 123;` and satify precision propagation chain for insn #13.\n\nThis example demonstrates two sets of rules:\n  - r0 returned after subprog call has to be moved into subprog's r0 set;\n  - *static* subprog arguments (r1-r5) are moved back to caller precision set.\n\nLet's look at what happens with callee-saved precision propagation. Insn #14\nmark r6 as precise. When we get into subprog's frame, we keep r6 in\nframe 0's precision set *only*. Subprog itself has its own set of\nindependent r6-r10 registers and is not affected. When we eventually\nmade our way out of subprog frame we keep r6 in precision set until we\nreach `9: r6 = 456;`, satisfying propagation. r6-r10 propagation is\nperhaps the simplest aspect, it always stays in its original frame.\n\nThat's pretty much all we have to do to support precision propagation\nacross *static subprog* invocation.\n\nLet's look at what happens when we have global subprog invocation.\n\nframe 0\t\t\t\tframe 1\t\t\tprecision set\n=======\t\t\t\t=======\t\t\t=============\n\n 9: r6 = 456;\n10: r1 = 123;\t\t\t\t\t\tfr0: r6\n11: call pc+10; # global subprog\t\t\tfr0: r6\n12: r1 = <map_pointer>\t\t\t\t\tfr0: r0, r6\n13: r1 += r0;\t\t\t\t\t\tfr0: r0, r6\n14: r1 += r6;\t\t\t\t\t\tfr0: r6;\n15: exit\n\nStarting from insn #13, r0 has to be precise. We backtrack all the way\nto insn #11 (call pc+10) and see that subprog is global, so was already\nvalidated in isolation. As opposed to static subprog, global subprog\nalways returns unknown scalar r0, so that satisfies precision\npropagation and we drop r0 from precision set. We are done for insns #13.\n\nNow for insn #14. r6 is in precision set, we backtrack to `call pc+10;`.\nHere we need to recognize that this is effectively both exit and entry\nto global subprog, which means we stay in caller's frame. So we carry on\nwith r6 still in precision set, until we satisfy it at insn #9. The only\nhard part with global subprogs is just knowing when it's a global func.\n\nLastly, callback-calling helpers and kfuncs do simulate subprog calls,\nso jump history will have subprog instructions in between caller\nprogram's instructions, but the rules of propagating r0 and r1-r5\ndiffer, because we don't actually directly call callback. We actually\ncall helper/kfunc, which at runtime will call subprog, so the only\ndifference between normal helper/kfunc handling is that we need to make\nsure to skip callback simulatinog part of jump history.\nLet's look at an example to make this clearer.\n\nframe 0\t\t\t\tframe 1\t\t\tprecision set\n=======\t\t\t\t=======\t\t\t=============\n\n 8: r6 = 456;\n 9: r1 = 123;\t\t\t\t\t\tfr0: r6\n10: r2 = &callback;\t\t\t\t\tfr0: r6\n11: call bpf_loop;\t\t\t\t\tfr0: r6\n\t\t\t\t22: r0 = r1;\t\tfr0: r6      fr1:\n\t\t\t\t23: exit\t\tfr0: r6      fr1:\n12: r1 = <map_pointer>\t\t\t\t\tfr0: r0, r6\n13: r1 += r0;\t\t\t\t\t\tfr0: r0, r6\n14: r1 += r6;\t\t\t\t\t\tfr0: r6;\n15: exit\n\nAgain, insn #13 forces r0 to be precise. As soon as we get to `23: exit`\nwe see that this isn't actually a static subprog call (it's `call\nbpf_loop;` helper call instead). So we clear r0 from precision set.\n\nFor callee-saved register, there is no difference: it stays in frame 0's\nprecision set, we go through insn #22 and #23, ignoring them until we\nget back to caller frame 0, eventually satisfying precision backtrack\nlogic at insn #8 (`r6 = 456;`).\n\nAssuming callback needed to set r0 as precise at insn #23, we'd\nbacktrack to insn #22, switching from r0 to r1, and then at the point\nwhen we pop back to frame 0 at insn #11, we'll clear r1-r5 from\nprecision set, as we don't really do a subprog call directly, so there\nis no input argument precision propagation.\n\nThat's pretty much it. With these changes, it seems like the only still\nunsupported situation for precision backpropagation is the case when\nprogram is accessing stack through registers other than r10. This is\nstill left as unsupported (though rare) case for now.\n\nAs for results. For selftests, few positive changes for bigger programs,\ncls_redirect in dynptr variant benefitting the most:\n\n[vmuser@archvm bpf]$ ./veristat -C ~/subprog-precise-before-results.csv ~/subprog-precise-after-results.csv -f @veristat.cfg -e file,prog,insns -f 'insns_diff!=0'\nFile                                      Program        Insns (A)  Insns (B)  Insns     (DIFF)\n----------------------------------------  -------------  ---------  ---------  ----------------\npyperf600_bpf_loop.bpf.linked1.o          on_event            2060       2002      -58 (-2.82%)\ntest_cls_redirect_dynptr.bpf.linked1.o    cls_redirect       15660       2914  -12746 (-81.39%)\ntest_cls_redirect_subprogs.bpf.linked1.o  cls_redirect       61620      59088    -2532 (-4.11%)\nxdp_synproxy_kern.bpf.linked1.o           syncookie_tc      109980      86278  -23702 (-21.55%)\nxdp_synproxy_kern.bpf.linked1.o           syncookie_xdp      97716      85147  -12569 (-12.86%)\n\nCilium progress don't really regress. They don't use subprogs and are\nmostly unaffected, but some other fixes and improvements could have\nchanged something. This doesn't appear to be the case:\n\n[vmuser@archvm bpf]$ ./veristat -C ~/subprog-precise-before-results-cilium.csv ~/subprog-precise-after-results-cilium.csv -e file,prog,insns -f 'insns_diff!=0'\nFile           Program                         Insns (A)  Insns (B)  Insns (DIFF)\n-------------  ------------------------------  ---------  ---------  ------------\nbpf_host.o     tail_nodeport_nat_ingress_ipv6       4983       5003  +20 (+0.40%)\nbpf_lxc.o      tail_nodeport_nat_ingress_ipv6       4983       5003  +20 (+0.40%)\nbpf_overlay.o  tail_nodeport_nat_ingress_ipv6       4983       5003  +20 (+0.40%)\nbpf_xdp.o      tail_handle_nat_fwd_ipv6            12475      12504  +29 (+0.23%)\nbpf_xdp.o      tail_nodeport_nat_ingress_ipv6       6363       6371   +8 (+0.13%)\n\nLooking at (somewhat anonymized) Meta production programs, we see mostly\ninsignificant variation in number of instructions, with one program\n(syar_bind6_protect6) benefitting the most at -17%.\n\n[vmuser@archvm bpf]$ ./veristat -C ~/subprog-precise-before-results-fbcode.csv ~/subprog-precise-after-results-fbcode.csv -e prog,insns -f 'insns_diff!=0'\nProgram                   Insns (A)  Insns (B)  Insns     (DIFF)\n------------------------  ---------  ---------  ----------------\non_request_context_event        597        585      -12 (-2.01%)\nread_async_py_stack           43789      43657     -132 (-0.30%)\nread_sync_py_stack            35041      37599    +2558 (+7.30%)\nrrm_usdt                        946        940       -6 (-0.63%)\nsysarmor_inet6_bind           28863      28249     -614 (-2.13%)\nsysarmor_inet_bind            28845      28240     -605 (-2.10%)\nsyar_bind4_protect4          154145     147640    -6505 (-4.22%)\nsyar_bind6_protect6          165242     137088  -28154 (-17.04%)\nsyar_task_exit_setgid         21289      19720    -1569 (-7.37%)\nsyar_task_exit_setuid         21290      19721    -1569 (-7.37%)\ndo_uprobe                     19967      19413     -554 (-2.77%)\ntw_twfw_ingress              215877     204833   -11044 (-5.12%)\ntw_twfw_tc_in                215877     204833   -11044 (-5.12%)\n\nBut checking duration (wall clock) differences, that is the actual time taken\nby verifier to validate programs, we see a sometimes dramatic improvements, all\nthe way to about 16x improvements:\n\n[vmuser@archvm bpf]$ ./veristat -C ~/subprog-precise-before-results-meta.csv ~/subprog-precise-after-results-meta.csv -e prog,duration -s duration_diff^ | head -n20\nProgram                                   Duration (us) (A)  Duration (us) (B)  Duration (us) (DIFF)\n----------------------------------------  -----------------  -----------------  --------------------\ntw_twfw_ingress                                     4488374             272836    -4215538 (-93.92%)\ntw_twfw_tc_in                                       4339111             268175    -4070936 (-93.82%)\ntw_twfw_egress                                      3521816             270751    -3251065 (-92.31%)\ntw_twfw_tc_eg                                       3472878             284294    -3188584 (-91.81%)\nbalancer_ingress                                     343119             291391      -51728 (-15.08%)\nsyar_bind6_protect6                                   78992              64782      -14210 (-17.99%)\nttls_tc_ingress                                       11739               8176       -3563 (-30.35%)\nkprobe__security_inode_link                           13864              11341       -2523 (-18.20%)\nread_sync_py_stack                                    21927              19442       -2485 (-11.33%)\nread_async_py_stack                                   30444              28136        -2308 (-7.58%)\nsyar_task_exit_setuid                                 10256               8440       -1816 (-17.71%)\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c50c0b57a515826b5d2e1ce85cd85f24f0da10c2",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-05-04 22:35:35 -0700",
      "message": "bpf: fix mark_all_scalars_precise use in mark_chain_precision\n\nWhen precision backtracking bails out due to some unsupported sequence\nof instructions (e.g., stack access through register other than r10), we\nneed to mark all SCALAR registers as precise to be safe. Currently,\nthough, we mark SCALARs precise only starting from the state we detected\nunsupported condition, which could be one of the parent states of the\nactual current state. This will leave some registers potentially not\nmarked as precise, even though they should. So make sure we start\nmarking scalars as precise from current state (env->cur_state).\n\nFurther, we don't currently detect a situation when we end up with some\nstack slots marked as needing precision, but we ran out of available\nstates to find the instructions that populate those stack slots. This is\nakin the `i >= func->allocated_stack / BPF_REG_SIZE` check and should be\nhandled similarly by falling back to marking all SCALARs precise. Add\nthis check when we run out of states.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/verifier/precise.c"
      ]
    },
    {
      "hash": "f655badf2a8fc028433d9583bf86a6b473721f09",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-05-04 22:35:35 -0700",
      "message": "bpf: fix propagate_precision() logic for inner frames\n\nFix propagate_precision() logic to perform propagation of all necessary\nregisters and stack slots across all active frames *in one batch step*.\n\nDoing this for each register/slot in each individual frame is wasteful,\nbut the main problem is that backtracking of instruction in any frame\nexcept the deepest one just doesn't work. This is due to backtracking\nlogic relying on jump history, and available jump history always starts\n(or ends, depending how you view it) in current frame. So, if\nprog A (frame #0) called subprog B (frame #1) and we need to propagate\nprecision of, say, register R6 (callee-saved) within frame #0, we\nactually don't even know where jump history that corresponds to prog\nA even starts. We'd need to skip subprog part of jump history first to\nbe able to do this.\n\nLuckily, with struct backtrack_state and __mark_chain_precision()\nhandling bitmasks tracking/propagation across all active frames at the\nsame time (added in previous patch), propagate_precision() can be both\nfixed and sped up by setting all the necessary bits across all frames\nand then performing one __mark_chain_precision() pass. This makes it\nunnecessary to skip subprog parts of jump history.\n\nWe also improve logging along the way, to clearly specify which\nregisters' and slots' precision markings are propagated within which\nframe. Each frame will have dedicated line and all registers and stack\nslots from that frame will be reported in format similar to precision\nbacktrack regs/stack logging. E.g.:\n\nframe 1: propagating r1,r2,r3,fp-8,fp-16\nframe 0: propagating r3,r9,fp-120\n\nFixes: 529409ea92d5 (\"bpf: propagate precision across all frames, not just the last one\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1ef22b6865a73a8aed36d43375fe8c7b30869326",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-05-04 22:35:35 -0700",
      "message": "bpf: maintain bitmasks across all active frames in __mark_chain_precision\n\nTeach __mark_chain_precision logic to maintain register/stack masks\nacross all active frames when going from child state to parent state.\nCurrently this should be mostly no-op, as precision backtracking usually\nbails out when encountering subprog entry/exit.\n\nIt's not very apparent from the diff due to increased indentation, but\nthe logic remains the same, except everything is done on specific `fr`\nframe index. Calls to bt_clear_reg() and bt_clear_slot() are replaced\nwith frame-specific bt_clear_frame_reg() and bt_clear_frame_slot(),\nwhere frame index is passed explicitly, instead of using current frame\nnumber.\n\nWe also adjust logging to emit affected frame number. And we also add\nbetter logging of human-readable register and stack slot masks, similar\nto previous patch.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-6-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/verifier/precise.c"
      ]
    },
    {
      "hash": "d9439c21a9e4769bfd83a03ab39056164d44ac31",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-05-04 22:35:35 -0700",
      "message": "bpf: improve precision backtrack logging\n\nAdd helper to format register and stack masks in more human-readable\nformat. Adjust logging a bit during backtrack propagation and especially\nduring forcing precision fallback logic to make it clearer what's going\non (with log_level=2, of course), and also start reporting affected\nframe depth. This is in preparation for having more than one active\nframe later when precision propagation between subprog calls is added.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/verifier/precise.c"
      ]
    },
    {
      "hash": "407958a0e980b9e1842ab87b5a1040521e1e24e9",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-05-04 22:35:35 -0700",
      "message": "bpf: encapsulate precision backtracking bookkeeping\n\nAdd struct backtrack_state and straightforward API around it to keep\ntrack of register and stack masks used and maintained during precision\nbacktracking process. Having this logic separately allow to keep\nhigh-level backtracking algorithm cleaner, but also it sets us up to\ncleanly keep track of register and stack masks per frame, allowing (with\nsome further logic adjustments) to perform precision backpropagation\nacross multiple frames (i.e., subprog calls).\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "e0bf462276b6ee23203365eacb5c599f42a5a084",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-05-04 22:35:34 -0700",
      "message": "bpf: mark relevant stack slots scratched for register read instructions\n\nWhen handling instructions that read register slots, mark relevant stack\nslots as scratched so that verifier log would contain those slots' states, in\naddition to currently emitted registers with stack slot offsets.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230505043317.3629845-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "361f129f3cc185af6667aca0bec0be9a020a8abc",
      "author": "Joanne Koong <joannelkoong@gmail.com>",
      "date": "2023-04-27 10:40:47 +0200",
      "message": "bpf: Add bpf_dynptr_clone\n\nThe cloned dynptr will point to the same data as its parent dynptr,\nwith the same type, offset, size and read-only properties.\n\nAny writes to a dynptr will be reflected across all instances\n(by 'instance', this means any dynptrs that point to the same\nunderlying data).\n\nPlease note that data slice and dynptr invalidations will affect all\ninstances as well. For example, if bpf_dynptr_write() is called on an\nskb-type dynptr, all data slices of dynptr instances to that skb\nwill be invalidated as well (eg data slices of any clones, parents,\ngrandparents, ...). Another example is if a ringbuf dynptr is submitted,\nany instance of that dynptr will be invalidated.\n\nChanging the view of the dynptr (eg advancing the offset or\ntrimming the size) will only affect that dynptr and not affect any\nother instances.\n\nOne example use case where cloning may be helpful is for hashing or\niterating through dynptr data. Cloning will allow the user to maintain\nthe original view of the dynptr for future use, while also allowing\nviews to smaller subsets of the data after the offset is advanced or the\nsize is trimmed.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20230420071414.570108-5-joannelkoong@gmail.com",
      "modified_files": [
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "a0c109dcafb15b8bee187c49fb746779374f60f0",
      "author": "Yafang Shao <laoar.shao@gmail.com>",
      "date": "2023-04-24 14:16:01 -0700",
      "message": "bpf: Add __rcu_read_{lock,unlock} into btf id deny list\n\nThe tracing recursion prevention mechanism must be protected by rcu, that\nleaves __rcu_read_{lock,unlock} unprotected by this mechanism. If we trace\nthem, the recursion will happen. Let's add them into the btf id deny list.\n\nWhen CONFIG_PREEMPT_RCU is enabled, it can be reproduced with a simple bpf\nprogram as such:\n  SEC(\"fentry/__rcu_read_lock\")\n  int fentry_run()\n  {\n      return 0;\n  }\n\nSigned-off-by: Yafang Shao <laoar.shao@gmail.com>\nLink: https://lore.kernel.org/r/20230424161104.3737-2-laoar.shao@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "7deca5eae83389ca40ac1b1bde96e4af17cca84f",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-04-24 14:02:11 -0700",
      "message": "bpf: Disable bpf_refcount_acquire kfunc calls until race conditions are fixed\n\nAs reported by Kumar in [0], the shared ownership implementation for BPF\nprograms has some race conditions which need to be addressed before it\ncan safely be used. This patch does so in a minimal way instead of\nripping out shared ownership entirely, as proper fixes for the issues\nraised will follow ASAP, at which point this patch's commit can be\nreverted to re-enable shared ownership.\n\nThe patch removes the ability to call bpf_refcount_acquire_impl from BPF\nprograms. Programs can only bump refcount and obtain a new owning\nreference using this kfunc, so removing the ability to call it\neffectively disables shared ownership.\n\nInstead of changing success / failure expectations for\nbpf_refcount-related selftests, this patch just disables them from\nrunning for now.\n\n  [0]: https://lore.kernel.org/bpf/d7hyspcow5wtjcmw4fugdgyp3fwhljwuscp3xyut5qnwivyeru@ysdq543otzv2/\n\nReported-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230424204321.2680232-1-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/prog_tests/refcounted_kptr.c"
      ]
    },
    {
      "hash": "fd9c663b9ad67dedfc9a3fd3429ddd3e83782b4d",
      "author": "Florian Westphal <fw@strlen.de>",
      "date": "2023-04-21 11:34:14 -0700",
      "message": "bpf: minimal support for programs hooked into netfilter framework\n\nThis adds minimal support for BPF_PROG_TYPE_NETFILTER bpf programs\nthat will be invoked via the NF_HOOK() points in the ip stack.\n\nInvocation incurs an indirect call.  This is not a necessity: Its\npossible to add 'DEFINE_BPF_DISPATCHER(nf_progs)' and handle the\nprogram invocation with the same method already done for xdp progs.\n\nThis isn't done here to keep the size of this chunk down.\n\nVerifier restricts verdicts to either DROP or ACCEPT.\n\nSigned-off-by: Florian Westphal <fw@strlen.de>\nLink: https://lore.kernel.org/r/20230421170300.24115-3-fw@strlen.de\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_types.h",
        "include/net/netfilter/nf_bpf_link.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c",
        "net/core/filter.c",
        "net/netfilter/nf_bpf_link.c"
      ]
    },
    {
      "hash": "3be49f79555ee975acb4ad8b5de06fa4351264aa",
      "author": "Yonghong Song <yhs@fb.com>",
      "date": "2023-04-17 15:50:02 -0700",
      "message": "bpf: Improve verifier u32 scalar equality checking\n\nIn [1], I tried to remove bpf-specific codes to prevent certain\nllvm optimizations, and add llvm TTI (target transform info) hooks\nto prevent those optimizations. During this process, I found\nif I enable llvm SimplifyCFG:shouldFoldTwoEntryPHINode\ntransformation, I will hit the following verification failure with selftests:\n\n  ...\n  8: (18) r1 = 0xffffc900001b2230       ; R1_w=map_value(off=560,ks=4,vs=564,imm=0)\n  10: (61) r1 = *(u32 *)(r1 +0)         ; R1_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff))\n  ; if (skb->tstamp == EGRESS_ENDHOST_MAGIC)\n  11: (79) r2 = *(u64 *)(r6 +152)       ; R2_w=scalar() R6=ctx(off=0,imm=0)\n  ; if (skb->tstamp == EGRESS_ENDHOST_MAGIC)\n  12: (55) if r2 != 0xb9fbeef goto pc+10        ; R2_w=195018479\n  13: (bc) w2 = w1                      ; R1_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff)) R2_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff))\n  ; if (test < __NR_TESTS)\n  14: (a6) if w1 < 0x9 goto pc+1 16: R0=2 R1_w=scalar(umax=8,var_off=(0x0; 0xf)) R2_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff)) R6=ctx(off=0,imm=0) R10=fp0\n  ;\n  16: (27) r2 *= 28                     ; R2_w=scalar(umax=120259084260,var_off=(0x0; 0x1ffffffffc),s32_max=2147483644,u32_max=-4)\n  17: (18) r3 = 0xffffc900001b2118      ; R3_w=map_value(off=280,ks=4,vs=564,imm=0)\n  19: (0f) r3 += r2                     ; R2_w=scalar(umax=120259084260,var_off=(0x0; 0x1ffffffffc),s32_max=2147483644,u32_max=-4) R3_w=map_value(off=280,ks=4,vs=564,umax=120259084260,var_off=(0x0; 0x1ffffffffc),s32_max=2147483644,u32_max=-4)\n  20: (61) r2 = *(u32 *)(r3 +0)\n  R3 unbounded memory access, make sure to bounds check any such access\n  processed 97 insns (limit 1000000) max_states_per_insn 1 total_states 10 peak_states 10 mark_read 6\n  -- END PROG LOAD LOG --\n  libbpf: prog 'ingress_fwdns_prio100': failed to load: -13\n  libbpf: failed to load object 'test_tc_dtime'\n  libbpf: failed to load BPF skeleton 'test_tc_dtime': -13\n  ...\n\nAt insn 14, with condition 'w1 < 9', register r1 is changed from an arbitrary\nu32 value to `scalar(umax=8,var_off=(0x0; 0xf))`. Register r2, however, remains\nas an arbitrary u32 value. Current verifier won't claim r1/r2 equality if\nthe previous mov is alu32 ('w2 = w1').\n\nIf r1 upper 32bit value is not 0, we indeed cannot clamin r1/r2 equality\nafter 'w2 = w1'. But in this particular case, we know r1 upper 32bit value\nis 0, so it is safe to claim r1/r2 equality. This patch exactly did this.\nFor a 32bit subreg mov, if the src register upper 32bit is 0,\nit is okay to claim equality between src and dst registers.\n\nWith this patch, the above verification sequence becomes\n\n  ...\n  8: (18) r1 = 0xffffc9000048e230       ; R1_w=map_value(off=560,ks=4,vs=564,imm=0)\n  10: (61) r1 = *(u32 *)(r1 +0)         ; R1_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff))\n  ; if (skb->tstamp == EGRESS_ENDHOST_MAGIC)\n  11: (79) r2 = *(u64 *)(r6 +152)       ; R2_w=scalar() R6=ctx(off=0,imm=0)\n  ; if (skb->tstamp == EGRESS_ENDHOST_MAGIC)\n  12: (55) if r2 != 0xb9fbeef goto pc+10        ; R2_w=195018479\n  13: (bc) w2 = w1                      ; R1_w=scalar(id=6,umax=4294967295,var_off=(0x0; 0xffffffff)) R2_w=scalar(id=6,umax=4294967295,var_off=(0x0; 0xffffffff))\n  ; if (test < __NR_TESTS)\n  14: (a6) if w1 < 0x9 goto pc+1        ; R1_w=scalar(id=6,umin=9,umax=4294967295,var_off=(0x0; 0xffffffff))\n  ...\n  from 14 to 16: R0=2 R1_w=scalar(id=6,umax=8,var_off=(0x0; 0xf)) R2_w=scalar(id=6,umax=8,var_off=(0x0; 0xf)) R6=ctx(off=0,imm=0) R10=fp0\n  16: (27) r2 *= 28                     ; R2_w=scalar(umax=224,var_off=(0x0; 0xfc))\n  17: (18) r3 = 0xffffc9000048e118      ; R3_w=map_value(off=280,ks=4,vs=564,imm=0)\n  19: (0f) r3 += r2\n  20: (61) r2 = *(u32 *)(r3 +0)         ; R2_w=scalar(umax=4294967295,var_off=(0x0; 0xffffffff)) R3_w=map_value(off=280,ks=4,vs=564,umax=224,var_off=(0x0; 0xfc),s32_max=252,u32_max=252)\n  ...\n\nand eventually the bpf program can be verified successfully.\n\n  [1] https://reviews.llvm.org/D147968\n\nSigned-off-by: Yonghong Song <yhs@fb.com>\nLink: https://lore.kernel.org/r/20230417222134.359714-1-yhs@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "7b4ddf3920d247c2949073b9c274301c8131332a",
      "author": "David Vernet <void@manifault.com>",
      "date": "2023-04-16 08:51:24 -0700",
      "message": "bpf: Remove KF_KPTR_GET kfunc flag\n\nWe've managed to improve the UX for kptrs significantly over the last 9\nmonths. All of the existing use cases which previously had KF_KPTR_GET\nkfuncs (struct bpf_cpumask *, struct task_struct *, and struct cgroup *)\nhave all been updated to be synchronized using RCU. In other words,\ntheir KF_KPTR_GET kfuncs have been removed in favor of KF_RCU |\nKF_ACQUIRE kfuncs, with the pointers themselves also being readable from\nmaps in an RCU read region thanks to the types being RCU safe.\n\nWhile KF_KPTR_GET was a logical starting point for kptrs, it's become\nclear that they're not the correct abstraction. KF_KPTR_GET is a flag\nthat essentially does nothing other than enforcing that the argument to\na function is a pointer to a referenced kptr map value. At first glance,\nthat's a useful thing to guarantee to a kfunc. It gives kfuncs the\nability to try and acquire a reference on that kptr without requiring\nthe BPF prog to do something like this:\n\nstruct kptr_type *in_map, *new = NULL;\n\nin_map = bpf_kptr_xchg(&map->value, NULL);\nif (in_map) {\n        new = bpf_kptr_type_acquire(in_map);\n        in_map = bpf_kptr_xchg(&map->value, in_map);\n        if (in_map)\n                bpf_kptr_type_release(in_map);\n}\n\nThat's clearly a pretty ugly (and racy) UX, and if using KF_KPTR_GET is\nthe only alternative, it's better than nothing. However, the problem\nwith any KF_KPTR_GET kfunc lies in the fact that it always requires some\nkind of synchronization in order to safely do an opportunistic acquire\nof the kptr in the map. This is because a BPF program running on another\nCPU could do a bpf_kptr_xchg() on that map value, and free the kptr\nafter it's been read by the KF_KPTR_GET kfunc. For example, the\nnow-removed bpf_task_kptr_get() kfunc did the following:\n\nstruct task_struct *bpf_task_kptr_get(struct task_struct **pp)\n{\n            struct task_struct *p;\n\n        rcu_read_lock();\n        p = READ_ONCE(*pp);\n        /* If p is non-NULL, it could still be freed by another CPU,\n         * so we have to do an opportunistic refcount_inc_not_zero()\n         * and return NULL if the task will be freed after the\n         * current RCU read region.\n         */\n        |f (p && !refcount_inc_not_zero(&p->rcu_users))\n                p = NULL;\n        rcu_read_unlock();\n\n        return p;\n}\n\nIn other words, the kfunc uses RCU to ensure that the task remains valid\nafter it's been peeked from the map. However, this is completely\nredundant with just defining a KF_RCU kfunc that itself does a\nrefcount_inc_not_zero(), which is exactly what bpf_task_acquire() now\ndoes.\n\nSo, the question of whether KF_KPTR_GET is useful is actually, \"Are\nthere any synchronization mechanisms / safety flags that are required by\ncertain kptrs, but which are not provided by the verifier to kfuncs?\"\nThe answer to that question today is \"No\", because every kptr we\ncurrently care about is RCU protected.\n\nEven if the answer ever became \"yes\", the proper way to support that\nreferenced kptr type would be to add support for whatever\nsynchronization mechanism it requires in the verifier, rather than\ngiving kfuncs a flag that says, \"Here's a pointer to a referenced kptr\nin a map, do whatever you need to do.\"\n\nWith all that said -- so as to allow us to consolidate the kfunc API,\nand simplify the verifier a bit, this patch removes KF_KPTR_GET, and all\nrelevant logic from the verifier.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230416084928.326135-3-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/btf.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "404ad75a36fb1a1008e9fe803aa7d0212df9e240",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-04-15 17:36:50 -0700",
      "message": "bpf: Migrate bpf_rbtree_remove to possibly fail\n\nThis patch modifies bpf_rbtree_remove to account for possible failure\ndue to the input rb_node already not being in any collection.\nThe function can now return NULL, and does when the aforementioned\nscenario occurs. As before, on successful removal an owning reference to\nthe removed node is returned.\n\nAdding KF_RET_NULL to bpf_rbtree_remove's kfunc flags - now KF_RET_NULL |\nKF_ACQUIRE - provides the desired verifier semantics:\n\n  * retval must be checked for NULL before use\n  * if NULL, retval's ref_obj_id is released\n  * retval is a \"maybe acquired\" owning ref, not a non-owning ref,\n    so it will live past end of critical section (bpf_spin_unlock), and\n    thus can be checked for NULL after the end of the CS\n\nBPF programs must add checks\n============================\n\nThis does change bpf_rbtree_remove's verifier behavior. BPF program\nwriters will need to add NULL checks to their programs, but the\nresulting UX looks natural:\n\n  bpf_spin_lock(&glock);\n\n  n = bpf_rbtree_first(&ghead);\n  if (!n) { /* ... */}\n  res = bpf_rbtree_remove(&ghead, &n->node);\n\n  bpf_spin_unlock(&glock);\n\n  if (!res)  /* Newly-added check after this patch */\n    return 1;\n\n  n = container_of(res, /* ... */);\n  /* Do something else with n */\n  bpf_obj_drop(n);\n  return 0;\n\nThe \"if (!res)\" check above is the only addition necessary for the above\nprogram to pass verification after this patch.\n\nbpf_rbtree_remove no longer clobbers non-owning refs\n====================================================\n\nAn issue arises when bpf_rbtree_remove fails, though. Consider this\nexample:\n\n  struct node_data {\n    long key;\n    struct bpf_list_node l;\n    struct bpf_rb_node r;\n    struct bpf_refcount ref;\n  };\n\n  long failed_sum;\n\n  void bpf_prog()\n  {\n    struct node_data *n = bpf_obj_new(/* ... */);\n    struct bpf_rb_node *res;\n    n->key = 10;\n\n    bpf_spin_lock(&glock);\n\n    bpf_list_push_back(&some_list, &n->l); /* n is now a non-owning ref */\n    res = bpf_rbtree_remove(&some_tree, &n->r, /* ... */);\n    if (!res)\n      failed_sum += n->key;  /* not possible */\n\n    bpf_spin_unlock(&glock);\n    /* if (res) { do something useful and drop } ... */\n  }\n\nThe bpf_rbtree_remove in this example will always fail. Similarly to\nbpf_spin_unlock, bpf_rbtree_remove is a non-owning reference\ninvalidation point. The verifier clobbers all non-owning refs after a\nbpf_rbtree_remove call, so the \"failed_sum += n->key\" line will fail\nverification, and in fact there's no good way to get information about\nthe node which failed to add after the invalidation. This patch removes\nnon-owning reference invalidation from bpf_rbtree_remove to allow the\nabove usecase to pass verification. The logic for why this is now\npossible is as follows:\n\nBefore this series, bpf_rbtree_add couldn't fail and thus assumed that\nits input, a non-owning reference, was in the tree. But it's easy to\nconstruct an example where two non-owning references pointing to the same\nunderlying memory are acquired and passed to rbtree_remove one after\nanother (see rbtree_api_release_aliasing in\nselftests/bpf/progs/rbtree_fail.c).\n\nSo it was necessary to clobber non-owning refs to prevent this\ncase and, more generally, to enforce \"non-owning ref is definitely\nin some collection\" invariant. This series removes that invariant and\nthe failure / runtime checking added in this patch provide a clean way\nto deal with the aliasing issue - just fail to remove.\n\nBecause the aliasing issue prevented by clobbering non-owning refs is no\nlonger an issue, this patch removes the invalidate_non_owning_refs\ncall from verifier handling of bpf_rbtree_remove. Note that\nbpf_spin_unlock - the other caller of invalidate_non_owning_refs -\nclobbers non-owning refs for a different reason, so its clobbering\nbehavior remains unchanged.\n\nNo BPF program changes are necessary for programs to remain valid as a\nresult of this clobbering change. A valid program before this patch\npassed verification with its non-owning refs having shorter (or equal)\nlifetimes due to more aggressive clobbering.\n\nAlso, update existing tests to check bpf_rbtree_remove retval for NULL\nwhere necessary, and move rbtree_api_release_aliasing from\nprogs/rbtree_fail.c to progs/rbtree.c since it's now expected to pass\nverification.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-8-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/btf.c",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/prog_tests/linked_list.c",
        "tools/testing/selftests/bpf/prog_tests/rbtree.c",
        "tools/testing/selftests/bpf/progs/rbtree.c",
        "tools/testing/selftests/bpf/progs/rbtree_fail.c"
      ]
    },
    {
      "hash": "d2dcc67df910dd85253a701b6a5b747f955d28f5",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-04-15 17:36:50 -0700",
      "message": "bpf: Migrate bpf_rbtree_add and bpf_list_push_{front,back} to possibly fail\n\nConsider this code snippet:\n\n  struct node {\n    long key;\n    bpf_list_node l;\n    bpf_rb_node r;\n    bpf_refcount ref;\n  }\n\n  int some_bpf_prog(void *ctx)\n  {\n    struct node *n = bpf_obj_new(/*...*/), *m;\n\n    bpf_spin_lock(&glock);\n\n    bpf_rbtree_add(&some_tree, &n->r, /* ... */);\n    m = bpf_refcount_acquire(n);\n    bpf_rbtree_add(&other_tree, &m->r, /* ... */);\n\n    bpf_spin_unlock(&glock);\n\n    /* ... */\n  }\n\nAfter bpf_refcount_acquire, n and m point to the same underlying memory,\nand that node's bpf_rb_node field is being used by the some_tree insert,\nso overwriting it as a result of the second insert is an error. In order\nto properly support refcounted nodes, the rbtree and list insert\nfunctions must be allowed to fail. This patch adds such support.\n\nThe kfuncs bpf_rbtree_add, bpf_list_push_{front,back} are modified to\nreturn an int indicating success/failure, with 0 -> success, nonzero ->\nfailure.\n\nbpf_obj_drop on failure\n=======================\n\nCurrently the only reason an insert can fail is the example above: the\nbpf_{list,rb}_node is already in use. When such a failure occurs, the\ninsert kfuncs will bpf_obj_drop the input node. This allows the insert\noperations to logically fail without changing their verifier owning ref\nbehavior, namely the unconditional release_reference of the input\nowning ref.\n\nWith insert that always succeeds, ownership of the node is always passed\nto the collection, since the node always ends up in the collection.\n\nWith a possibly-failed insert w/ bpf_obj_drop, ownership of the node\nis always passed either to the collection (success), or to bpf_obj_drop\n(failure). Regardless, it's correct to continue unconditionally\nreleasing the input owning ref, as something is always taking ownership\nfrom the calling program on insert.\n\nKeeping owning ref behavior unchanged results in a nice default UX for\ninsert functions that can fail. If the program's reaction to a failed\ninsert is \"fine, just get rid of this owning ref for me and let me go\non with my business\", then there's no reason to check for failure since\nthat's default behavior. e.g.:\n\n  long important_failures = 0;\n\n  int some_bpf_prog(void *ctx)\n  {\n    struct node *n, *m, *o; /* all bpf_obj_new'd */\n\n    bpf_spin_lock(&glock);\n    bpf_rbtree_add(&some_tree, &n->node, /* ... */);\n    bpf_rbtree_add(&some_tree, &m->node, /* ... */);\n    if (bpf_rbtree_add(&some_tree, &o->node, /* ... */)) {\n      important_failures++;\n    }\n    bpf_spin_unlock(&glock);\n  }\n\nIf we instead chose to pass ownership back to the program on failed\ninsert - by returning NULL on success or an owning ref on failure -\nprograms would always have to do something with the returned ref on\nfailure. The most likely action is probably \"I'll just get rid of this\nowning ref and go about my business\", which ideally would look like:\n\n  if (n = bpf_rbtree_add(&some_tree, &n->node, /* ... */))\n    bpf_obj_drop(n);\n\nBut bpf_obj_drop isn't allowed in a critical section and inserts must\noccur within one, so in reality error handling would become a\nhard-to-parse mess.\n\nFor refcounted nodes, we can replicate the \"pass ownership back to\nprogram on failure\" logic with this patch's semantics, albeit in an ugly\nway:\n\n  struct node *n = bpf_obj_new(/* ... */), *m;\n\n  bpf_spin_lock(&glock);\n\n  m = bpf_refcount_acquire(n);\n  if (bpf_rbtree_add(&some_tree, &n->node, /* ... */)) {\n    /* Do something with m */\n  }\n\n  bpf_spin_unlock(&glock);\n  bpf_obj_drop(m);\n\nbpf_refcount_acquire is used to simulate \"return owning ref on failure\".\nThis should be an uncommon occurrence, though.\n\nAddition of two verifier-fixup'd args to collection inserts\n===========================================================\n\nThe actual bpf_obj_drop kfunc is\nbpf_obj_drop_impl(void *, struct btf_struct_meta *), with bpf_obj_drop\nmacro populating the second arg with 0 and the verifier later filling in\nthe arg during insn fixup.\n\nBecause bpf_rbtree_add and bpf_list_push_{front,back} now might do\nbpf_obj_drop, these kfuncs need a btf_struct_meta parameter that can be\npassed to bpf_obj_drop_impl.\n\nSimilarly, because the 'node' param to those insert functions is the\nbpf_{list,rb}_node within the node type, and bpf_obj_drop expects a\npointer to the beginning of the node, the insert functions need to be\nable to find the beginning of the node struct. A second\nverifier-populated param is necessary: the offset of {list,rb}_node within the\nnode type.\n\nThese two new params allow the insert kfuncs to correctly call\n__bpf_obj_drop_impl:\n\n  beginning_of_node = bpf_rb_node_ptr - offset\n  if (already_inserted)\n    __bpf_obj_drop_impl(beginning_of_node, btf_struct_meta->record);\n\nSimilarly to other kfuncs with \"hidden\" verifier-populated params, the\ninsert functions are renamed with _impl prefix and a macro is provided\nfor common usage. For example, bpf_rbtree_add kfunc is now\nbpf_rbtree_add_impl and bpf_rbtree_add is now a macro which sets\n\"hidden\" args to 0.\n\nDue to the two new args BPF progs will need to be recompiled to work\nwith the new _impl kfuncs.\n\nThis patch also rewrites the \"hidden argument\" explanation to more\ndirectly say why the BPF program writer doesn't need to populate the\narguments with anything meaningful.\n\nHow does this new logic affect non-owning references?\n=====================================================\n\nCurrently, non-owning refs are valid until the end of the critical\nsection in which they're created. We can make this guarantee because, if\na non-owning ref exists, the referent was added to some collection. The\ncollection will drop() its nodes when it goes away, but it can't go away\nwhile our program is accessing it, so that's not a problem. If the\nreferent is removed from the collection in the same CS that it was added\nin, it can't be bpf_obj_drop'd until after CS end. Those are the only\ntwo ways to free the referent's memory and neither can happen until\nafter the non-owning ref's lifetime ends.\n\nOn first glance, having these collection insert functions potentially\nbpf_obj_drop their input seems like it breaks the \"can't be\nbpf_obj_drop'd until after CS end\" line of reasoning. But we care about\nthe memory not being _freed_ until end of CS end, and a previous patch\nin the series modified bpf_obj_drop such that it doesn't free refcounted\nnodes until refcount == 0. So the statement can be more accurately\nrewritten as \"can't be free'd until after CS end\".\n\nWe can prove that this rewritten statement holds for any non-owning\nreference produced by collection insert functions:\n\n* If the input to the insert function is _not_ refcounted\n  * We have an owning reference to the input, and can conclude it isn't\n    in any collection\n    * Inserting a node in a collection turns owning refs into\n      non-owning, and since our input type isn't refcounted, there's no\n      way to obtain additional owning refs to the same underlying\n      memory\n  * Because our node isn't in any collection, the insert operation\n    cannot fail, so bpf_obj_drop will not execute\n  * If bpf_obj_drop is guaranteed not to execute, there's no risk of\n    memory being free'd\n\n* Otherwise, the input to the insert function is refcounted\n  * If the insert operation fails due to the node's list_head or rb_root\n    already being in some collection, there was some previous successful\n    insert which passed refcount to the collection\n  * We have an owning reference to the input, it must have been\n    acquired via bpf_refcount_acquire, which bumped the refcount\n  * refcount must be >= 2 since there's a valid owning reference and the\n    node is already in a collection\n  * Insert triggering bpf_obj_drop will decr refcount to >= 1, never\n    resulting in a free\n\nSo although we may do bpf_obj_drop during the critical section, this\nwill never result in memory being free'd, and no changes to non-owning\nref logic are needed in this patch.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-6-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/bpf_experimental.h"
      ]
    },
    {
      "hash": "7c50b1cb76aca4540aa917db5f2a302acddcadff",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-04-15 17:36:50 -0700",
      "message": "bpf: Add bpf_refcount_acquire kfunc\n\nCurrently, BPF programs can interact with the lifetime of refcounted\nlocal kptrs in the following ways:\n\n  bpf_obj_new  - Initialize refcount to 1 as part of new object creation\n  bpf_obj_drop - Decrement refcount and free object if it's 0\n  collection add - Pass ownership to the collection. No change to\n                   refcount but collection is responsible for\n\t\t   bpf_obj_dropping it\n\nIn order to be able to add a refcounted local kptr to multiple\ncollections we need to be able to increment the refcount and acquire a\nnew owning reference. This patch adds a kfunc, bpf_refcount_acquire,\nimplementing such an operation.\n\nbpf_refcount_acquire takes a refcounted local kptr and returns a new\nowning reference to the same underlying memory as the input. The input\ncan be either owning or non-owning. To reinforce why this is safe,\nconsider the following code snippets:\n\n  struct node *n = bpf_obj_new(typeof(*n)); // A\n  struct node *m = bpf_refcount_acquire(n); // B\n\nIn the above snippet, n will be alive with refcount=1 after (A), and\nsince nothing changes that state before (B), it's obviously safe. If\nn is instead added to some rbtree, we can still safely refcount_acquire\nit:\n\n  struct node *n = bpf_obj_new(typeof(*n));\n  struct node *m;\n\n  bpf_spin_lock(&glock);\n  bpf_rbtree_add(&groot, &n->node, less);   // A\n  m = bpf_refcount_acquire(n);              // B\n  bpf_spin_unlock(&glock);\n\nIn the above snippet, after (A) n is a non-owning reference, and after\n(B) m is an owning reference pointing to the same memory as n. Although\nn has no ownership of that memory's lifetime, it's guaranteed to be\nalive until the end of the critical section, and n would be clobbered if\nwe were past the end of the critical section, so it's safe to bump\nrefcount.\n\nImplementation details:\n\n* From verifier's perspective, bpf_refcount_acquire handling is similar\n  to bpf_obj_new and bpf_obj_drop. Like the former, it returns a new\n  owning reference matching input type, although like the latter, type\n  can be inferred from concrete kptr input. Verifier changes in\n  {check,fixup}_kfunc_call and check_kfunc_args are largely copied from\n  aforementioned functions' verifier changes.\n\n* An exception to the above is the new KF_ARG_PTR_TO_REFCOUNTED_KPTR\n  arg, indicated by new \"__refcounted_kptr\" kfunc arg suffix. This is\n  necessary in order to handle both owning and non-owning input without\n  adding special-casing to \"__alloc\" arg handling. Also a convenient\n  place to confirm that input type has bpf_refcount field.\n\n* The implemented kfunc is actually bpf_refcount_acquire_impl, with\n  'hidden' second arg that the verifier sets to the type's struct_meta\n  in fixup_kfunc_call.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230415201811.343116-5-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/bpf_experimental.h"
      ]
    },
    {
      "hash": "1cf3bfc60f9836f44da951f58b6ae24680484b35",
      "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
      "date": "2023-04-13 21:36:41 -0700",
      "message": "bpf: Support 64-bit pointers to kfuncs\n\ntest_ksyms_module fails to emit a kfunc call targeting a module on\ns390x, because the verifier stores the difference between kfunc\naddress and __bpf_call_base in bpf_insn.imm, which is s32, and modules\nare roughly (1 << 42) bytes away from the kernel on s390x.\n\nFix by keeping BTF id in bpf_insn.imm for BPF_PSEUDO_KFUNC_CALLs,\nand storing the absolute address in bpf_kfunc_desc.\n\nIntroduce bpf_jit_supports_far_kfunc_call() in order to limit this new\nbehavior to the s390x JIT. Otherwise other JITs need to be modified,\nwhich is not desired.\n\nIntroduce bpf_get_kfunc_addr() instead of exposing both\nfind_kfunc_desc() and struct bpf_kfunc_desc.\n\nIn addition to sorting kfuncs by imm, also sort them by offset, in\norder to handle conflicting imms from different modules. Do this on\nall architectures in order to simplify code.\n\nFactor out resolving specialized kfuncs (XPD and dynptr) from\nfixup_kfunc_call(). This was required in the first place, because\nfixup_kfunc_call() uses find_kfunc_desc(), which returns a const\npointer, so it's not possible to modify kfunc addr without stripping\nconst, which is not nice. It also removes repetition of code like:\n\n\tif (bpf_jit_supports_far_kfunc_call())\n\t\tdesc->addr = func;\n\telse\n\t\tinsn->imm = BPF_CALL_IMM(func);\n\nand separates kfunc_desc_tab fixups from kfunc_call fixups.\n\nSuggested-by: Jiri Olsa <olsajiri@gmail.com>\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nLink: https://lore.kernel.org/r/20230412230632.885985-1-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "arch/s390/net/bpf_jit_comp.c",
        "include/linux/bpf.h",
        "include/linux/filter.h",
        "kernel/bpf/core.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c11bd046485d7bf1ca200db0e7d0bdc4bafdd395",
      "author": "Yafang <laoar.shao@gmail.com>",
      "date": "2023-04-13 21:20:21 -0700",
      "message": "bpf: Add preempt_count_{sub,add} into btf id deny list\n\nThe recursion check in __bpf_prog_enter* and __bpf_prog_exit*\nleave preempt_count_{sub,add} unprotected. When attaching trampoline to\nthem we get panic as follows,\n\n[  867.843050] BUG: TASK stack guard page was hit at 0000000009d325cf (stack is 0000000046a46a15..00000000537e7b28)\n[  867.843064] stack guard page: 0000 [#1] PREEMPT SMP NOPTI\n[  867.843067] CPU: 8 PID: 11009 Comm: trace Kdump: loaded Not tainted 6.2.0+ #4\n[  867.843100] Call Trace:\n[  867.843101]  <TASK>\n[  867.843104]  asm_exc_int3+0x3a/0x40\n[  867.843108] RIP: 0010:preempt_count_sub+0x1/0xa0\n[  867.843135]  __bpf_prog_enter_recur+0x17/0x90\n[  867.843148]  bpf_trampoline_6442468108_0+0x2e/0x1000\n[  867.843154]  ? preempt_count_sub+0x1/0xa0\n[  867.843157]  preempt_count_sub+0x5/0xa0\n[  867.843159]  ? migrate_enable+0xac/0xf0\n[  867.843164]  __bpf_prog_exit_recur+0x2d/0x40\n[  867.843168]  bpf_trampoline_6442468108_0+0x55/0x1000\n...\n[  867.843788]  preempt_count_sub+0x5/0xa0\n[  867.843793]  ? migrate_enable+0xac/0xf0\n[  867.843829]  __bpf_prog_exit_recur+0x2d/0x40\n[  867.843837] BUG: IRQ stack guard page was hit at 0000000099bd8228 (stack is 00000000b23e2bc4..000000006d95af35)\n[  867.843841] BUG: IRQ stack guard page was hit at 000000005ae07924 (stack is 00000000ffd69623..0000000014eb594c)\n[  867.843843] BUG: IRQ stack guard page was hit at 00000000028320f0 (stack is 00000000034b6438..0000000078d1bcec)\n[  867.843842]  bpf_trampoline_6442468108_0+0x55/0x1000\n...\n\nThat is because in __bpf_prog_exit_recur, the preempt_count_{sub,add} are\ncalled after prog->active is decreased.\n\nFixing this by adding these two functions into btf ids deny list.\n\nSuggested-by: Steven Rostedt <rostedt@goodmis.org>\nSigned-off-by: Yafang <laoar.shao@gmail.com>\nCc: Masami Hiramatsu <mhiramat@kernel.org>\nCc: Steven Rostedt <rostedt@goodmis.org>\nCc: Jiri Olsa <olsajiri@gmail.com>\nAcked-by: Hao Luo <haoluo@google.com>\nLink: https://lore.kernel.org/r/20230413025248.79764-1-laoar.shao@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "bdcab4144f5da97cc0fa7e1dd63b8475e10c8f0a",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-04-11 18:05:44 +0200",
      "message": "bpf: Simplify internal verifier log interface\n\nSimplify internal verifier log API down to bpf_vlog_init() and\nbpf_vlog_finalize(). The former handles input arguments validation in\none place and makes it easier to change it. The latter subsumes -ENOSPC\n(truncation) and -EFAULT handling and simplifies both caller's code\n(bpf_check() and btf_parse()).\n\nFor btf_parse(), this patch also makes sure that verifier log\nfinalization happens even if there is some error condition during BTF\nverification process prior to normal finalization step.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-14-andrii@kernel.org",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/log.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "47a71c1f9af0a334c9dfa97633c41de4feda4287",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-04-11 18:05:43 +0200",
      "message": "bpf: Add log_true_size output field to return necessary log buffer size\n\nAdd output-only log_true_size and btf_log_true_size field to\nBPF_PROG_LOAD and BPF_BTF_LOAD commands, respectively. It will return\nthe size of log buffer necessary to fit in all the log contents at\nspecified log_level. This is very useful for BPF loader libraries like\nlibbpf to be able to size log buffer correctly, but could be used by\nusers directly, if necessary, as well.\n\nThis patch plumbs all this through the code, taking into account actual\nbpf_attr size provided by user to determine if these new fields are\nexpected by users. And if they are, set them from kernel on return.\n\nWe refactory btf_parse() function to accommodate this, moving attr and\nuattr handling inside it. The rest is very straightforward code, which\nis split from the logging accounting changes in the previous patch to\nmake it simpler to review logic vs UAPI changes.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-13-andrii@kernel.org",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/btf.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c",
        "tools/include/uapi/linux/bpf.h"
      ]
    },
    {
      "hash": "8a6ca6bc553e3c878fa53c506bc6ec281efdc039",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-04-11 18:05:43 +0200",
      "message": "bpf: Simplify logging-related error conditions handling\n\nMove log->level == 0 check into bpf_vlog_truncated() instead of doing it\nexplicitly. Also remove unnecessary goto in kernel/bpf/verifier.c.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-11-andrii@kernel.org",
      "modified_files": [
        "kernel/bpf/btf.c",
        "kernel/bpf/log.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "cbedb42a0da3bb48819b2200af4b4cb5d922c518",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-04-11 18:05:43 +0200",
      "message": "bpf: Avoid incorrect -EFAULT error in BPF_LOG_KERNEL mode\n\nIf verifier log is in BPF_LOG_KERNEL mode, no log->ubuf is expected and\nit stays NULL throughout entire verification process. Don't erroneously\nreturn -EFAULT in such case.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-10-andrii@kernel.org",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1216640938035e63bdbd32438e91c9bcc1fd8ee1",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-04-11 18:05:43 +0200",
      "message": "bpf: Switch BPF verifier log to be a rotating log by default\n\nCurrently, if user-supplied log buffer to collect BPF verifier log turns\nout to be too small to contain full log, bpf() syscall returns -ENOSPC,\nfails BPF program verification/load, and preserves first N-1 bytes of\nthe verifier log (where N is the size of user-supplied buffer).\n\nThis is problematic in a bunch of common scenarios, especially when\nworking with real-world BPF programs that tend to be pretty complex as\nfar as verification goes and require big log buffers. Typically, it's\nwhen debugging tricky cases at log level 2 (verbose). Also, when BPF program\nis successfully validated, log level 2 is the only way to actually see\nverifier state progression and all the important details.\n\nEven with log level 1, it's possible to get -ENOSPC even if the final\nverifier log fits in log buffer, if there is a code path that's deep\nenough to fill up entire log, even if normally it would be reset later\non (there is a logic to chop off successfully validated portions of BPF\nverifier log).\n\nIn short, it's not always possible to pre-size log buffer. Also, what's\nworse, in practice, the end of the log most often is way more important\nthan the beginning, but verifier stops emitting log as soon as initial\nlog buffer is filled up.\n\nThis patch switches BPF verifier log behavior to effectively behave as\nrotating log. That is, if user-supplied log buffer turns out to be too\nshort, verifier will keep overwriting previously written log,\neffectively treating user's log buffer as a ring buffer. -ENOSPC is\nstill going to be returned at the end, to notify user that log contents\nwas truncated, but the important last N bytes of the log would be\nreturned, which might be all that user really needs. This consistent\n-ENOSPC behavior, regardless of rotating or fixed log behavior, allows\nto prevent backwards compatibility breakage. The only user-visible\nchange is which portion of verifier log user ends up seeing *if buffer\nis too small*. Given contents of verifier log itself is not an ABI,\nthere is no breakage due to this behavior change. Specialized tools that\nrely on specific contents of verifier log in -ENOSPC scenario are\nexpected to be easily adapted to accommodate old and new behaviors.\n\nImportantly, though, to preserve good user experience and not require\nevery user-space application to adopt to this new behavior, before\nexiting to user-space verifier will rotate log (in place) to make it\nstart at the very beginning of user buffer as a continuous\nzero-terminated string. The contents will be a chopped off N-1 last\nbytes of full verifier log, of course.\n\nGiven beginning of log is sometimes important as well, we add\nBPF_LOG_FIXED (which equals 8) flag to force old behavior, which allows\ntools like veristat to request first part of verifier log, if necessary.\nBPF_LOG_FIXED flag is also a simple and straightforward way to check if\nBPF verifier supports rotating behavior.\n\nOn the implementation side, conceptually, it's all simple. We maintain\n64-bit logical start and end positions. If we need to truncate the log,\nstart position will be adjusted accordingly to lag end position by\nN bytes. We then use those logical positions to calculate their matching\nactual positions in user buffer and handle wrap around the end of the\nbuffer properly. Finally, right before returning from bpf_check(), we\nrotate user log buffer contents in-place as necessary, to make log\ncontents contiguous. See comments in relevant functions for details.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nReviewed-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-4-andrii@kernel.org",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/log.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/prog_tests/log_fixup.c"
      ]
    },
    {
      "hash": "4294a0a7ab6282c3d92f03de84e762dda993c93d",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-04-11 18:05:42 +0200",
      "message": "bpf: Split off basic BPF verifier log into separate file\n\nkernel/bpf/verifier.c file is large and growing larger all the time. So\nit's good to start splitting off more or less self-contained parts into\nseparate files to keep source code size (somewhat) somewhat under\ncontrol.\n\nThis patch is a one step in this direction, moving some of BPF verifier log\nroutines into a separate kernel/bpf/log.c. Right now it's most low-level\nand isolated routines to append data to log, reset log to previous\nposition, etc. Eventually we could probably move verifier state\nprinting logic here as well, but this patch doesn't attempt to do that\nyet.\n\nSubsequent patches will add more logic to verifier log management, so\nhaving basics in a separate file will make sure verifier.c doesn't grow\nmore with new changes.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Lorenz Bauer <lmb@isovalent.com>\nLink: https://lore.kernel.org/bpf/20230406234205.323208-2-andrii@kernel.org",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/Makefile",
        "kernel/bpf/log.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "953d9f5beaf75e88c69a13d70ce424cd606a29f5",
      "author": "Yonghong Song <yhs@fb.com>",
      "date": "2023-04-06 15:26:08 -0700",
      "message": "bpf: Improve handling of pattern '<const> <cond_op> <non_const>' in verifier\n\nCurrently, the verifier does not handle '<const> <cond_op> <non_const>' well.\nFor example,\n  ...\n  10: (79) r1 = *(u64 *)(r10 -16)       ; R1_w=scalar() R10=fp0\n  11: (b7) r2 = 0                       ; R2_w=0\n  12: (2d) if r2 > r1 goto pc+2\n  13: (b7) r0 = 0\n  14: (95) exit\n  15: (65) if r1 s> 0x1 goto pc+3\n  16: (0f) r0 += r1\n  ...\nAt insn 12, verifier decides both true and false branch are possible, but\nactually only false branch is possible.\n\nCurrently, the verifier already supports patterns '<non_const> <cond_op> <const>.\nAdd support for patterns '<const> <cond_op> <non_const>' in a similar way.\n\nAlso fix selftest 'verifier_bounds_mix_sign_unsign/bounds checks mixing signed and unsigned, variant 10'\ndue to this change.\n\nSigned-off-by: Yonghong Song <yhs@fb.com>\nAcked-by: Dave Marchevsky <davemarchevsky@fb.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230406164505.1046801-1-yhs@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/progs/verifier_bounds_mix_sign_unsign.c"
      ]
    },
    {
      "hash": "13fbcee55706db45ce047a7cea14811d68f94ee3",
      "author": "Yonghong Song <yhs@fb.com>",
      "date": "2023-04-06 15:26:08 -0700",
      "message": "bpf: Improve verifier JEQ/JNE insn branch taken checking\n\nCurrently, for BPF_JEQ/BPF_JNE insn, verifier determines\nwhether the branch is taken or not only if both operands\nare constants. Therefore, for the following code snippet,\n  0: (85) call bpf_ktime_get_ns#5       ; R0_w=scalar()\n  1: (a5) if r0 < 0x3 goto pc+2         ; R0_w=scalar(umin=3)\n  2: (b7) r2 = 2                        ; R2_w=2\n  3: (1d) if r0 == r2 goto pc+2 6\n\nAt insn 3, since r0 is not a constant, verifier assumes both branch\ncan be taken which may lead inproper verification failure.\n\nAdd comparing umin/umax value and the constant. If the umin value\nis greater than the constant, or umax value is smaller than the constant,\nfor JEQ the branch must be not-taken, and for JNE the branch must be taken.\nThe jmp32 mode JEQ/JNE branch taken checking is also handled similarly.\n\nThe following lists the veristat result w.r.t. changed number\nof processes insns during verification:\n\nFile                                                   Program                                               Insns (A)  Insns (B)  Insns    (DIFF)\n-----------------------------------------------------  ----------------------------------------------------  ---------  ---------  ---------------\ntest_cls_redirect.bpf.linked3.o                        cls_redirect                                              64980      73472  +8492 (+13.07%)\ntest_seg6_loop.bpf.linked3.o                           __add_egr_x                                               12425      12423      -2 (-0.02%)\ntest_tcp_hdr_options.bpf.linked3.o                     estab                                                      2634       2558     -76 (-2.89%)\ntest_parse_tcp_hdr_opt.bpf.linked3.o                   xdp_ingress_v6                                             1421       1420      -1 (-0.07%)\ntest_parse_tcp_hdr_opt_dynptr.bpf.linked3.o            xdp_ingress_v6                                             1238       1237      -1 (-0.08%)\ntest_tc_dtime.bpf.linked3.o                            egress_fwdns_prio100                                        414        411      -3 (-0.72%)\n\nMostly a small improvement but test_cls_redirect.bpf.linked3.o has a 13% regression.\nI checked with verifier log and found it this is due to pruning.\nFor some JEQ/JNE branches impacted by this patch,\none branch is explored and the other has state equivalence and\npruned.\n\nSigned-off-by: Yonghong Song <yhs@fb.com>\nAcked-by: Dave Marchevsky <davemarchevsky@fb.com>\nAcked-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230406164455.1045294-1-yhs@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "afeebf9f57a4965e0be90e4dc87f8f3a1417376d",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-04-04 16:57:24 -0700",
      "message": "bpf: Undo strict enforcement for walking untagged fields.\n\nThe commit 6fcd486b3a0a (\"bpf: Refactor RCU enforcement in the verifier.\")\nbroke several tracing bpf programs. Even in clang compiled kernels there are\nmany fields that are not marked with __rcu that are safe to read and pass into\nhelpers, but the verifier doesn't know that they're safe. Aggressively marking\nthem as PTR_UNTRUSTED was premature.\n\nFixes: 6fcd486b3a0a (\"bpf: Refactor RCU enforcement in the verifier.\")\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-8-alexei.starovoitov@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "30ee9821f9430c34a6254134a5f0e8db227510be",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-04-04 16:57:21 -0700",
      "message": "bpf: Allowlist few fields similar to __rcu tag.\n\nAllow bpf program access cgrp->kn, mm->exe_file, skb->sk, req->sk.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-7-alexei.starovoitov@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "add68b843f33d4e5dcbdc7ba6dffe7750a964159",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-04-04 16:57:18 -0700",
      "message": "bpf: Refactor NULL-ness check in check_reg_type().\n\ncheck_reg_type() unconditionally disallows PTR_TO_BTF_ID | PTR_MAYBE_NULL.\nIt's problematic for helpers that allow ARG_PTR_TO_BTF_ID_OR_NULL like\nbpf_sk_storage_get(). Allow passing PTR_TO_BTF_ID | PTR_MAYBE_NULL into such\nhelpers. That technically includes bpf_kptr_xchg() helper, but in practice:\n  bpf_kptr_xchg(..., bpf_cpumask_create());\nis still disallowed because bpf_cpumask_create() returns ref counted pointer\nwith ref_obj_id > 0.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-6-alexei.starovoitov@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "63260df1396578226ac3134cf7f764690002e70e",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-04-04 16:57:14 -0700",
      "message": "bpf: Refactor btf_nested_type_is_trusted().\n\nbtf_nested_type_is_trusted() tries to find a struct member at corresponding offset.\nIt works for flat structures and falls apart in more complex structs with nested structs.\nThe offset->member search is already performed by btf_struct_walk() including nested structs.\nReuse this work and pass {field name, field btf id} into btf_nested_type_is_trusted()\ninstead of offset to make BTF_TYPE_SAFE*() logic more robust.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-4-alexei.starovoitov@gmail.com",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b7e852a9ec96635168c04204fb7cf1f7390b9a8c",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-04-04 16:57:10 -0700",
      "message": "bpf: Remove unused arguments from btf_struct_access().\n\nRemove unused arguments from btf_struct_access() callback.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-3-alexei.starovoitov@gmail.com",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/filter.h",
        "kernel/bpf/verifier.c",
        "net/bpf/bpf_dummy_struct_ops.c",
        "net/core/filter.c",
        "net/ipv4/bpf_tcp_ca.c",
        "net/netfilter/nf_conntrack_bpf.c"
      ]
    },
    {
      "hash": "7d64c513284408fee5178a0953a686e9410f2399",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-04-04 16:57:03 -0700",
      "message": "bpf: Invoke btf_struct_access() callback only for writes.\n\nRemove duplicated if (atype == BPF_READ) btf_struct_access() from\nbtf_struct_access() callback and invoke it only for writes. This is\npossible to do because currently btf_struct_access() custom callback\nalways delegates to generic btf_struct_access() helper for BPF_READ\naccesses.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230404045029.82870-2-alexei.starovoitov@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "net/bpf/bpf_dummy_struct_ops.c",
        "net/core/filter.c",
        "net/ipv4/bpf_tcp_ca.c"
      ]
    },
    {
      "hash": "f6a6a5a976288e4d0d94eb1c6c9e983e8e5cdb31",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-04-03 14:54:21 -0700",
      "message": "bpf: Fix struct_meta lookup for bpf_obj_free_fields kfunc call\n\nbpf_obj_drop_impl has a void return type. In check_kfunc_call, the \"else\nif\" which sets insn_aux->kptr_struct_meta for bpf_obj_drop_impl is\nsurrounded by a larger if statement which checks btf_type_is_ptr. As a\nresult:\n\n  * The bpf_obj_drop_impl-specific code will never execute\n  * The btf_struct_meta input to bpf_obj_drop is always NULL\n  * __bpf_obj_drop_impl will always see a NULL btf_record when called\n    from BPF program, and won't call bpf_obj_free_fields\n  * program-allocated kptrs which have fields that should be cleaned up\n    by bpf_obj_free_fields may instead leak resources\n\nThis patch adds a btf_type_is_void branch to the larger if and moves\nspecial handling for bpf_obj_drop_impl there, fixing the issue.\n\nFixes: ac9f06050a35 (\"bpf: Introduce bpf_obj_drop\")\nCc: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230403200027.2271029-1-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d02c48fa113953aba0b330ec6c35f50c7d1d7986",
      "author": "David Vernet <void@manifault.com>",
      "date": "2023-04-01 09:07:20 -0700",
      "message": "bpf: Make struct task_struct an RCU-safe type\n\nstruct task_struct objects are a bit interesting in terms of how their\nlifetime is protected by refcounts. task structs have two refcount\nfields:\n\n1. refcount_t usage: Protects the memory backing the task struct. When\n   this refcount drops to 0, the task is immediately freed, without\n   waiting for an RCU grace period to elapse. This is the field that\n   most callers in the kernel currently use to ensure that a task\n   remains valid while it's being referenced, and is what's currently\n   tracked with bpf_task_acquire() and bpf_task_release().\n\n2. refcount_t rcu_users: A refcount field which, when it drops to 0,\n   schedules an RCU callback that drops a reference held on the 'usage'\n   field above (which is acquired when the task is first created). This\n   field therefore provides a form of RCU protection on the task by\n   ensuring that at least one 'usage' refcount will be held until an RCU\n   grace period has elapsed. The qualifier \"a form of\" is important\n   here, as a task can remain valid after task->rcu_users has dropped to\n   0 and the subsequent RCU gp has elapsed.\n\nIn terms of BPF, we want to use task->rcu_users to protect tasks that\nfunction as referenced kptrs, and to allow tasks stored as referenced\nkptrs in maps to be accessed with RCU protection.\n\nLet's first determine whether we can safely use task->rcu_users to\nprotect tasks stored in maps. All of the bpf_task* kfuncs can only be\ncalled from tracepoint, struct_ops, or BPF_PROG_TYPE_SCHED_CLS, program\ntypes. For tracepoint and struct_ops programs, the struct task_struct\npassed to a program handler will always be trusted, so it will always be\nsafe to call bpf_task_acquire() with any task passed to a program.\nNote, however, that we must update bpf_task_acquire() to be KF_RET_NULL,\nas it is possible that the task has exited by the time the program is\ninvoked, even if the pointer is still currently valid because the main\nkernel holds a task->usage refcount. For BPF_PROG_TYPE_SCHED_CLS, tasks\nshould never be passed as an argument to the any program handlers, so it\nshould not be relevant.\n\nThe second question is whether it's safe to use RCU to access a task\nthat was acquired with bpf_task_acquire(), and stored in a map. Because\nbpf_task_acquire() now uses task->rcu_users, it follows that if the task\nis present in the map, that it must have had at least one\ntask->rcu_users refcount by the time the current RCU cs was started.\nTherefore, it's safe to access that task until the end of the current\nRCU cs.\n\nWith all that said, this patch makes struct task_struct is an\nRCU-protected object. In doing so, we also change bpf_task_acquire() to\nbe KF_ACQUIRE | KF_RCU | KF_RET_NULL, and adjust any selftests as\nnecessary. A subsequent patch will remove bpf_task_kptr_get(), and\nbpf_task_acquire_not_zero() respectively.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230331195733.699708-2-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/prog_tests/task_kfunc.c",
        "tools/testing/selftests/bpf/progs/task_kfunc_common.h",
        "tools/testing/selftests/bpf/progs/task_kfunc_failure.c",
        "tools/testing/selftests/bpf/progs/task_kfunc_success.c"
      ]
    },
    {
      "hash": "e4c2acab95a5947fe7948140a83e4f4918c6c048",
      "author": "David Vernet <void@manifault.com>",
      "date": "2023-03-30 14:12:22 -0700",
      "message": "bpf: Handle PTR_MAYBE_NULL case in PTR_TO_BTF_ID helper call arg\n\nWhen validating a helper function argument, we use check_reg_type() to\nensure that the register containing the argument is of the correct type.\nWhen the register's base type is PTR_TO_BTF_ID, there is some\nsupplemental logic where we do extra checks for various combinations of\nPTR_TO_BTF_ID type modifiers. For example, for PTR_TO_BTF_ID,\nPTR_TO_BTF_ID | PTR_TRUSTED, and PTR_TO_BTF_ID | MEM_RCU, we call\nmap_kptr_match_type() for bpf_kptr_xchg() calls, and\nbtf_struct_ids_match() for other helper calls.\n\nWhen an unhandled PTR_TO_BTF_ID type modifier combination is passed to\ncheck_reg_type(), the verifier fails with an internal verifier error\nmessage. This can currently be triggered by passing a PTR_MAYBE_NULL\npointer to helper functions (currently just bpf_kptr_xchg()) with an\nARG_PTR_TO_BTF_ID_OR_NULL arg type. For example, by callin\nbpf_kptr_xchg(&v->kptr, bpf_cpumask_create()).\n\nWhether or not passing a PTR_MAYBE_NULL arg to an\nARG_PTR_TO_BTF_ID_OR_NULL argument is valid is an interesting question.\nIn a vacuum, it seems fine. A helper function with an\nARG_PTR_TO_BTF_ID_OR_NULL arg would seem to be implying that it can\nhandle either a NULL or non-NULL arg, and has logic in place to detect\nand gracefully handle each. This is the case for bpf_kptr_xchg(), which\nof course simply does an xchg(). On the other hand, bpf_kptr_xchg() also\nspecifies OBJ_RELEASE, and refcounting semantics for a PTR_MAYBE_NULL\npointer is different than handling it for a NULL _OR_ non-NULL pointer.\nFor example, with a non-NULL arg, we should always fail if there was not\na nonzero refcount for the value in the register being passed to the\nhelper. For PTR_MAYBE_NULL on the other hand, it's unclear. If the\npointer is NULL it would be fine, but if it's not NULL, it would be\nincorrect to load the program.\n\nThe current solution to this is to just fail if PTR_MAYBE_NULL is\npassed, and to instead require programs to have a NULL check to\nexplicitly handle the NULL and non-NULL cases. This seems reasonable.\nNot only would it possibly be quite complicated to correctly handle\nPTR_MAYBE_NULL refcounting in the verifier, but it's also an arguably\nodd programming pattern in general to not explicitly handle the NULL\ncase anyways. For example, it seems odd to not care about whether a\npointer you're passing to bpf_kptr_xchg() was successfully allocated in\na program such as the following:\n\nprivate(MASK) static struct bpf_cpumask __kptr * global_mask;\n\nSEC(\"tp_btf/task_newtask\")\nint BPF_PROG(example, struct task_struct *task, u64 clone_flags)\n{\n        struct bpf_cpumask *prev;\n\n\t/* bpf_cpumask_create() returns PTR_MAYBE_NULL */\n\tprev = bpf_kptr_xchg(&global_mask, bpf_cpumask_create());\n\tif (prev)\n\t\tbpf_cpumask_release(prev);\n\n\treturn 0;\n}\n\nThis patch therefore updates the verifier to explicitly check for\nPTR_MAYBE_NULL in check_reg_type(), and fail gracefully if it's\nobserved. This isn't really \"fixing\" anything unsafe or incorrect. We're\njust updating the verifier to fail gracefully, and explicitly handle\nthis pattern rather than unintentionally falling back to an internal\nverifier error path. A subsequent patch will update selftests.\n\nSigned-off-by: David Vernet <void@manifault.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/bpf/20230330145203.80506-1-void@manifault.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "6c831c4684124a544f73f7c9b83bc7b2eb0b23d3",
      "author": "David Vernet <void@manifault.com>",
      "date": "2023-03-25 16:56:22 -0700",
      "message": "bpf: Treat KF_RELEASE kfuncs as KF_TRUSTED_ARGS\n\nKF_RELEASE kfuncs are not currently treated as having KF_TRUSTED_ARGS,\neven though they have a superset of the requirements of KF_TRUSTED_ARGS.\nLike KF_TRUSTED_ARGS, KF_RELEASE kfuncs require a 0-offset argument, and\ndon't allow NULL-able arguments. Unlike KF_TRUSTED_ARGS which require\n_either_ an argument with ref_obj_id > 0, _or_ (ref->type &\nBPF_REG_TRUSTED_MODIFIERS) (and no unsafe modifiers allowed), KF_RELEASE\nonly allows for ref_obj_id > 0.  Because KF_RELEASE today doesn't\nautomatically imply KF_TRUSTED_ARGS, some of these requirements are\nenforced in different ways that can make the behavior of the verifier\nfeel unpredictable. For example, a KF_RELEASE kfunc with a NULL-able\nargument will currently fail in the verifier with a message like, \"arg#0\nis ptr_or_null_ expected ptr_ or socket\" rather than \"Possibly NULL\npointer passed to trusted arg0\". Our intention is the same, but the\nsemantics are different due to implemenetation details that kfunc authors\nand BPF program writers should not need to care about.\n\nLet's make the behavior of the verifier more consistent and intuitive by\nhaving KF_RELEASE kfuncs imply the presence of KF_TRUSTED_ARGS. Our\neventual goal is to have all kfuncs assume KF_TRUSTED_ARGS by default\nanyways, so this takes us a step in that direction.\n\nNote that it does not make sense to assume KF_TRUSTED_ARGS for all\nKF_ACQUIRE kfuncs. KF_ACQUIRE kfuncs can have looser semantics than\nKF_RELEASE, with e.g. KF_RCU | KF_RET_NULL. We may want to have\nKF_ACQUIRE imply KF_TRUSTED_ARGS _unless_ KF_RCU is specified, but that\ncan be left to another patch set, and there are no such subtleties to\naddress for KF_RELEASE.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230325213144.486885-4-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "Documentation/bpf/kfuncs.rst",
        "kernel/bpf/cpumask.c",
        "kernel/bpf/verifier.c",
        "net/bpf/test_run.c",
        "tools/testing/selftests/bpf/progs/cgrp_kfunc_failure.c",
        "tools/testing/selftests/bpf/progs/task_kfunc_failure.c",
        "tools/testing/selftests/bpf/verifier/calls.c",
        "tools/testing/selftests/bpf/verifier/ref_tracking.c"
      ]
    },
    {
      "hash": "b63cbc490e18d893632929b8faa55bb28da3fcd4",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-22 17:04:47 -0700",
      "message": "bpf: remember meta->iter info only for initialized iters\n\nFor iter_new() functions iterator state's slot might not be yet\ninitialized, in which case iter_get_spi() will return -ERANGE. This is\nexpected and is handled properly. But for iter_next() and iter_destroy()\ncases iter slot is supposed to be initialized and correct, so -ERANGE is\nnot possible.\n\nMove meta->iter.{spi,frameno} initialization into iter_next/iter_destroy\nhandling branch to make it more explicit that valid information will be\nremembered in meta->iter block for subsequent use in process_iter_next_call(),\navoiding confusingly looking -ERANGE assignment for meta->iter.spi.\n\nReported-by: Dan Carpenter <error27@gmail.com>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230322232502.836171-1-andrii@kernel.org\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "7be14c1c9030f73cc18b4ff23b78a0a081f16188",
      "author": "Daniel Borkmann <daniel@iogearbox.net>",
      "date": "2023-03-22 16:49:25 -0700",
      "message": "bpf: Fix __reg_bound_offset 64->32 var_off subreg propagation\n\nXu reports that after commit 3f50f132d840 (\"bpf: Verifier, do explicit ALU32\nbounds tracking\"), the following BPF program is rejected by the verifier:\n\n   0: (61) r2 = *(u32 *)(r1 +0)          ; R2_w=pkt(off=0,r=0,imm=0)\n   1: (61) r3 = *(u32 *)(r1 +4)          ; R3_w=pkt_end(off=0,imm=0)\n   2: (bf) r1 = r2\n   3: (07) r1 += 1\n   4: (2d) if r1 > r3 goto pc+8\n   5: (71) r1 = *(u8 *)(r2 +0)           ; R1_w=scalar(umax=255,var_off=(0x0; 0xff))\n   6: (18) r0 = 0x7fffffffffffff10\n   8: (0f) r1 += r0                      ; R1_w=scalar(umin=0x7fffffffffffff10,umax=0x800000000000000f)\n   9: (18) r0 = 0x8000000000000000\n  11: (07) r0 += 1\n  12: (ad) if r0 < r1 goto pc-2\n  13: (b7) r0 = 0\n  14: (95) exit\n\nAnd the verifier log says:\n\n  func#0 @0\n  0: R1=ctx(off=0,imm=0) R10=fp0\n  0: (61) r2 = *(u32 *)(r1 +0)          ; R1=ctx(off=0,imm=0) R2_w=pkt(off=0,r=0,imm=0)\n  1: (61) r3 = *(u32 *)(r1 +4)          ; R1=ctx(off=0,imm=0) R3_w=pkt_end(off=0,imm=0)\n  2: (bf) r1 = r2                       ; R1_w=pkt(off=0,r=0,imm=0) R2_w=pkt(off=0,r=0,imm=0)\n  3: (07) r1 += 1                       ; R1_w=pkt(off=1,r=0,imm=0)\n  4: (2d) if r1 > r3 goto pc+8          ; R1_w=pkt(off=1,r=1,imm=0) R3_w=pkt_end(off=0,imm=0)\n  5: (71) r1 = *(u8 *)(r2 +0)           ; R1_w=scalar(umax=255,var_off=(0x0; 0xff)) R2_w=pkt(off=0,r=1,imm=0)\n  6: (18) r0 = 0x7fffffffffffff10       ; R0_w=9223372036854775568\n  8: (0f) r1 += r0                      ; R0_w=9223372036854775568 R1_w=scalar(umin=9223372036854775568,umax=9223372036854775823,s32_min=-240,s32_max=15)\n  9: (18) r0 = 0x8000000000000000       ; R0_w=-9223372036854775808\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775807\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775807 R1_w=scalar(umin=9223372036854775568,umax=9223372036854775809)\n  13: (b7) r0 = 0                       ; R0_w=0\n  14: (95) exit\n\n  from 12 to 11: R0_w=-9223372036854775807 R1_w=scalar(umin=9223372036854775810,umax=9223372036854775823,var_off=(0x8000000000000000; 0xffffffff)) R2_w=pkt(off=0,r=1,imm=0) R3_w=pkt_end(off=0,imm=0) R10=fp0\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775806\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775806 R1_w=scalar(umin=9223372036854775810,umax=9223372036854775810,var_off=(0x8000000000000000; 0xffffffff))\n  13: safe\n\n  [...]\n\n  from 12 to 11: R0_w=-9223372036854775795 R1=scalar(umin=9223372036854775822,umax=9223372036854775823,var_off=(0x8000000000000000; 0xffffffff)) R2=pkt(off=0,r=1,imm=0) R3=pkt_end(off=0,imm=0) R10=fp0\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775794\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775794 R1=scalar(umin=9223372036854775822,umax=9223372036854775822,var_off=(0x8000000000000000; 0xffffffff))\n  13: safe\n\n  from 12 to 11: R0_w=-9223372036854775794 R1=scalar(umin=9223372036854775823,umax=9223372036854775823,var_off=(0x8000000000000000; 0xffffffff)) R2=pkt(off=0,r=1,imm=0) R3=pkt_end(off=0,imm=0) R10=fp0\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775793\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775793 R1=scalar(umin=9223372036854775823,umax=9223372036854775823,var_off=(0x8000000000000000; 0xffffffff))\n  13: safe\n\n  from 12 to 11: R0_w=-9223372036854775793 R1=scalar(umin=9223372036854775824,umax=9223372036854775823,var_off=(0x8000000000000000; 0xffffffff)) R2=pkt(off=0,r=1,imm=0) R3=pkt_end(off=0,imm=0) R10=fp0\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775792\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775792 R1=scalar(umin=9223372036854775824,umax=9223372036854775823,var_off=(0x8000000000000000; 0xffffffff))\n  13: safe\n\n  [...]\n\nThe 64bit umin=9223372036854775810 bound continuously bumps by +1 while\numax=9223372036854775823 stays as-is until the verifier complexity limit\nis reached and the program gets finally rejected. During this simulation,\nthe umin also eventually surpasses umax. Looking at the first 'from 12\nto 11' output line from the loop, R1 has the following state:\n\n  R1_w=scalar(umin=0x8000000000000002 (9223372036854775810),\n              umax=0x800000000000000f (9223372036854775823),\n          var_off=(0x8000000000000000;\n                           0xffffffff))\n\nThe var_off has technically not an inconsistent state but it's very\nimprecise and far off surpassing 64bit umax bounds whereas the expected\noutput with refined known bits in var_off should have been like:\n\n  R1_w=scalar(umin=0x8000000000000002 (9223372036854775810),\n              umax=0x800000000000000f (9223372036854775823),\n          var_off=(0x8000000000000000;\n                                  0xf))\n\nIn the above log, var_off stays as var_off=(0x8000000000000000; 0xffffffff)\nand does not converge into a narrower mask where more bits become known,\neventually transforming R1 into a constant upon umin=9223372036854775823,\numax=9223372036854775823 case where the verifier would have terminated and\nlet the program pass.\n\nThe __reg_combine_64_into_32() marks the subregister unknown and propagates\n64bit {s,u}min/{s,u}max bounds to their 32bit equivalents iff they are within\nthe 32bit universe. The question came up whether __reg_combine_64_into_32()\nshould special case the situation that when 64bit {s,u}min bounds have\nthe same value as 64bit {s,u}max bounds to then assign the latter as\nwell to the 32bit reg->{s,u}32_{min,max}_value. As can be seen from the\nabove example however, that is just /one/ special case and not a /generic/\nsolution given above example would still not be addressed this way and\nremain at an imprecise var_off=(0x8000000000000000; 0xffffffff).\n\nThe improvement is needed in __reg_bound_offset() to refine var32_off with\nthe updated var64_off instead of the prior reg->var_off. The reg_bounds_sync()\ncode first refines information about the register's min/max bounds via\n__update_reg_bounds() from the current var_off, then in __reg_deduce_bounds()\nfrom sign bit and with the potentially learned bits from bounds it'll\nupdate the var_off tnum in __reg_bound_offset(). For example, intersecting\nwith the old var_off might have improved bounds slightly, e.g. if umax\nwas 0x7f...f and var_off was (0; 0xf...fc), then new var_off will then\nresult in (0; 0x7f...fc). The intersected var64_off holds then the\nuniverse which is a superset of var32_off. The point for the latter is\nnot to broaden, but to further refine known bits based on the intersection\nof var_off with 32 bit bounds, so that we later construct the final var_off\nfrom upper and lower 32 bits. The final __update_reg_bounds() can then\npotentially still slightly refine bounds if more bits became known from the\nnew var_off.\n\nAfter the improvement, we can see R1 converging successively:\n\n  func#0 @0\n  0: R1=ctx(off=0,imm=0) R10=fp0\n  0: (61) r2 = *(u32 *)(r1 +0)          ; R1=ctx(off=0,imm=0) R2_w=pkt(off=0,r=0,imm=0)\n  1: (61) r3 = *(u32 *)(r1 +4)          ; R1=ctx(off=0,imm=0) R3_w=pkt_end(off=0,imm=0)\n  2: (bf) r1 = r2                       ; R1_w=pkt(off=0,r=0,imm=0) R2_w=pkt(off=0,r=0,imm=0)\n  3: (07) r1 += 1                       ; R1_w=pkt(off=1,r=0,imm=0)\n  4: (2d) if r1 > r3 goto pc+8          ; R1_w=pkt(off=1,r=1,imm=0) R3_w=pkt_end(off=0,imm=0)\n  5: (71) r1 = *(u8 *)(r2 +0)           ; R1_w=scalar(umax=255,var_off=(0x0; 0xff)) R2_w=pkt(off=0,r=1,imm=0)\n  6: (18) r0 = 0x7fffffffffffff10       ; R0_w=9223372036854775568\n  8: (0f) r1 += r0                      ; R0_w=9223372036854775568 R1_w=scalar(umin=9223372036854775568,umax=9223372036854775823,s32_min=-240,s32_max=15)\n  9: (18) r0 = 0x8000000000000000       ; R0_w=-9223372036854775808\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775807\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775807 R1_w=scalar(umin=9223372036854775568,umax=9223372036854775809)\n  13: (b7) r0 = 0                       ; R0_w=0\n  14: (95) exit\n\n  from 12 to 11: R0_w=-9223372036854775807 R1_w=scalar(umin=9223372036854775810,umax=9223372036854775823,var_off=(0x8000000000000000; 0xf),s32_min=0,s32_max=15,u32_max=15) R2_w=pkt(off=0,r=1,imm=0) R3_w=pkt_end(off=0,imm=0) R10=fp0\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775806\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775806 R1_w=-9223372036854775806\n  13: safe\n\n  from 12 to 11: R0_w=-9223372036854775806 R1_w=scalar(umin=9223372036854775811,umax=9223372036854775823,var_off=(0x8000000000000000; 0xf),s32_min=0,s32_max=15,u32_max=15) R2_w=pkt(off=0,r=1,imm=0) R3_w=pkt_end(off=0,imm=0) R10=fp0\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775805\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775805 R1_w=-9223372036854775805\n  13: safe\n\n  [...]\n\n  from 12 to 11: R0_w=-9223372036854775798 R1=scalar(umin=9223372036854775819,umax=9223372036854775823,var_off=(0x8000000000000008; 0x7),s32_min=8,s32_max=15,u32_min=8,u32_max=15) R2=pkt(off=0,r=1,imm=0) R3=pkt_end(off=0,imm=0) R10=fp0\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775797\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775797 R1=-9223372036854775797\n  13: safe\n\n  from 12 to 11: R0_w=-9223372036854775797 R1=scalar(umin=9223372036854775820,umax=9223372036854775823,var_off=(0x800000000000000c; 0x3),s32_min=12,s32_max=15,u32_min=12,u32_max=15) R2=pkt(off=0,r=1,imm=0) R3=pkt_end(off=0,imm=0) R10=fp0\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775796\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775796 R1=-9223372036854775796\n  13: safe\n\n  from 12 to 11: R0_w=-9223372036854775796 R1=scalar(umin=9223372036854775821,umax=9223372036854775823,var_off=(0x800000000000000c; 0x3),s32_min=12,s32_max=15,u32_min=12,u32_max=15) R2=pkt(off=0,r=1,imm=0) R3=pkt_end(off=0,imm=0) R10=fp0\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775795\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775795 R1=-9223372036854775795\n  13: safe\n\n  from 12 to 11: R0_w=-9223372036854775795 R1=scalar(umin=9223372036854775822,umax=9223372036854775823,var_off=(0x800000000000000e; 0x1),s32_min=14,s32_max=15,u32_min=14,u32_max=15) R2=pkt(off=0,r=1,imm=0) R3=pkt_end(off=0,imm=0) R10=fp0\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775794\n  12: (ad) if r0 < r1 goto pc-2         ; R0_w=-9223372036854775794 R1=-9223372036854775794\n  13: safe\n\n  from 12 to 11: R0_w=-9223372036854775794 R1=-9223372036854775793 R2=pkt(off=0,r=1,imm=0) R3=pkt_end(off=0,imm=0) R10=fp0\n  11: (07) r0 += 1                      ; R0_w=-9223372036854775793\n  12: (ad) if r0 < r1 goto pc-2\n  last_idx 12 first_idx 12\n  parent didn't have regs=1 stack=0 marks: R0_rw=P-9223372036854775801 R1_r=scalar(umin=9223372036854775815,umax=9223372036854775823,var_off=(0x8000000000000000; 0xf),s32_min=0,s32_max=15,u32_max=15) R2=pkt(off=0,r=1,imm=0) R3=pkt_end(off=0,imm=0) R10=fp0\n  last_idx 11 first_idx 11\n  regs=1 stack=0 before 11: (07) r0 += 1\n  parent didn't have regs=1 stack=0 marks: R0_rw=P-9223372036854775805 R1_rw=scalar(umin=9223372036854775812,umax=9223372036854775823,var_off=(0x8000000000000000; 0xf),s32_min=0,s32_max=15,u32_max=15) R2_w=pkt(off=0,r=1,imm=0) R3_w=pkt_end(off=0,imm=0) R10=fp0\n  last_idx 12 first_idx 0\n  regs=1 stack=0 before 12: (ad) if r0 < r1 goto pc-2\n  regs=1 stack=0 before 11: (07) r0 += 1\n  regs=1 stack=0 before 12: (ad) if r0 < r1 goto pc-2\n  regs=1 stack=0 before 11: (07) r0 += 1\n  regs=1 stack=0 before 12: (ad) if r0 < r1 goto pc-2\n  regs=1 stack=0 before 11: (07) r0 += 1\n  regs=1 stack=0 before 9: (18) r0 = 0x8000000000000000\n  last_idx 12 first_idx 12\n  parent didn't have regs=2 stack=0 marks: R0_rw=P-9223372036854775801 R1_r=Pscalar(umin=9223372036854775815,umax=9223372036854775823,var_off=(0x8000000000000000; 0xf),s32_min=0,s32_max=15,u32_max=15) R2=pkt(off=0,r=1,imm=0) R3=pkt_end(off=0,imm=0) R10=fp0\n  last_idx 11 first_idx 11\n  regs=2 stack=0 before 11: (07) r0 += 1\n  parent didn't have regs=2 stack=0 marks: R0_rw=P-9223372036854775805 R1_rw=Pscalar(umin=9223372036854775812,umax=9223372036854775823,var_off=(0x8000000000000000; 0xf),s32_min=0,s32_max=15,u32_max=15) R2_w=pkt(off=0,r=1,imm=0) R3_w=pkt_end(off=0,imm=0) R10=fp0\n  last_idx 12 first_idx 0\n  regs=2 stack=0 before 12: (ad) if r0 < r1 goto pc-2\n  regs=2 stack=0 before 11: (07) r0 += 1\n  regs=2 stack=0 before 12: (ad) if r0 < r1 goto pc-2\n  regs=2 stack=0 before 11: (07) r0 += 1\n  regs=2 stack=0 before 12: (ad) if r0 < r1 goto pc-2\n  regs=2 stack=0 before 11: (07) r0 += 1\n  regs=2 stack=0 before 9: (18) r0 = 0x8000000000000000\n  regs=2 stack=0 before 8: (0f) r1 += r0\n  regs=3 stack=0 before 6: (18) r0 = 0x7fffffffffffff10\n  regs=2 stack=0 before 5: (71) r1 = *(u8 *)(r2 +0)\n  13: safe\n\n  from 4 to 13: safe\n  verification time 322 usec\n  stack depth 0\n  processed 56 insns (limit 1000000) max_states_per_insn 1 total_states 3 peak_states 3 mark_read 1\n\nThis also fixes up a test case along with this improvement where we match\non the verifier log. The updated log now has a refined var_off, too.\n\nFixes: 3f50f132d840 (\"bpf: Verifier, do explicit ALU32 bounds tracking\")\nReported-by: Xu Kuohai <xukuohai@huaweicloud.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nReviewed-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20230314203424.4015351-2-xukuohai@huaweicloud.com\nLink: https://lore.kernel.org/bpf/20230322213056.2470-1-daniel@iogearbox.net",
      "modified_files": [
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/prog_tests/align.c"
      ]
    },
    {
      "hash": "d7ba4cc900bf1eea2d8c807c6b1fc6bd61f41237",
      "author": "JP Kobryn <inwardvessel@gmail.com>",
      "date": "2023-03-22 15:11:30 -0700",
      "message": "bpf: return long from bpf_map_ops funcs\n\nThis patch changes the return types of bpf_map_ops functions to long, where\npreviously int was returned. Using long allows for bpf programs to maintain\nthe sign bit in the absence of sign extension during situations where\ninlined bpf helper funcs make calls to the bpf_map_ops funcs and a negative\nerror is returned.\n\nThe definitions of the helper funcs are generated from comments in the bpf\nuapi header at `include/uapi/linux/bpf.h`. The return type of these\nhelpers was previously changed from int to long in commit bdb7b79b4ce8. For\nany case where one of the map helpers call the bpf_map_ops funcs that are\nstill returning 32-bit int, a compiler might not include sign extension\ninstructions to properly convert the 32-bit negative value a 64-bit\nnegative value.\n\nFor example:\nbpf assembly excerpt of an inlined helper calling a kernel function and\nchecking for a specific error:\n\n; err = bpf_map_update_elem(&mymap, &key, &val, BPF_NOEXIST);\n  ...\n  46:\tcall   0xffffffffe103291c\t; htab_map_update_elem\n; if (err && err != -EEXIST) {\n  4b:\tcmp    $0xffffffffffffffef,%rax ; cmp -EEXIST,%rax\n\nkernel function assembly excerpt of return value from\n`htab_map_update_elem` returning 32-bit int:\n\nmovl $0xffffffef, %r9d\n...\nmovl %r9d, %eax\n\n...results in the comparison:\ncmp $0xffffffffffffffef, $0x00000000ffffffef\n\nFixes: bdb7b79b4ce8 (\"bpf: Switch most helper return values from 32-bit int to 64-bit long\")\nTested-by: Eduard Zingerman <eddyz87@gmail.com>\nSigned-off-by: JP Kobryn <inwardvessel@gmail.com>\nLink: https://lore.kernel.org/r/20230322194754.185781-3-inwardvessel@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/filter.h",
        "kernel/bpf/arraymap.c",
        "kernel/bpf/bloom_filter.c",
        "kernel/bpf/bpf_cgrp_storage.c",
        "kernel/bpf/bpf_inode_storage.c",
        "kernel/bpf/bpf_struct_ops.c",
        "kernel/bpf/bpf_task_storage.c",
        "kernel/bpf/cpumap.c",
        "kernel/bpf/devmap.c",
        "kernel/bpf/hashtab.c",
        "kernel/bpf/local_storage.c",
        "kernel/bpf/lpm_trie.c",
        "kernel/bpf/queue_stack_maps.c",
        "kernel/bpf/reuseport_array.c",
        "kernel/bpf/ringbuf.c",
        "kernel/bpf/stackmap.c",
        "kernel/bpf/verifier.c",
        "net/core/bpf_sk_storage.c",
        "net/core/sock_map.c",
        "net/xdp/xskmap.c"
      ]
    },
    {
      "hash": "1057d299459657b85e593a4b6294a000f920672a",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-03-22 09:31:05 -0700",
      "message": "bpf: Teach the verifier to recognize rdonly_mem as not null.\n\nTeach the verifier to recognize PTR_TO_MEM | MEM_RDONLY as not NULL\notherwise if (!bpf_ksym_exists(known_kfunc)) doesn't go through\ndead code elimination.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230321203854.3035-3-alexei.starovoitov@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "58aa2afbb1e61fcf35bfcc819952a3c13d9f9203",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-03-17 15:44:26 -0700",
      "message": "bpf: Allow ld_imm64 instruction to point to kfunc.\n\nAllow ld_imm64 insn with BPF_PSEUDO_BTF_ID to hold the address of kfunc. The\nld_imm64 pointing to a valid kfunc will be seen as non-null PTR_TO_MEM by\nis_branch_taken() logic of the verifier, while libbpf will resolve address to\nunknown kfunc as ld_imm64 reg, 0 which will also be recognized by\nis_branch_taken() and the verifier will proceed dead code elimination. BPF\nprograms can use this logic to detect at load time whether kfunc is present in\nthe kernel with bpf_ksym_exists() macro that is introduced in the next patches.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nReviewed-by: Martin KaFai Lau <martin.lau@kernel.org>\nReviewed-by: Toke H\u00f8iland-J\u00f8rgensen <toke@redhat.com>\nAcked-by: John Fastabend <john.fastabend@gmail.com>\nLink: https://lore.kernel.org/bpf/20230317201920.62030-2-alexei.starovoitov@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "bd5314f8dd2d41330eecb60f0490c3fcfe1fc99d",
      "author": "Viktor Malik <vmalik@redhat.com>",
      "date": "2023-03-17 13:45:51 +0100",
      "message": "kallsyms, bpf: Move find_kallsyms_symbol_value out of internal header\n\nMoving find_kallsyms_symbol_value from kernel/module/internal.h to\ninclude/linux/module.h. The reason is that internal.h is not prepared to\nbe included when CONFIG_MODULES=n. find_kallsyms_symbol_value is used by\nkernel/bpf/verifier.c and including internal.h from it (without modules)\nleads into a compilation error:\n\n  In file included from ../include/linux/container_of.h:5,\n                   from ../include/linux/list.h:5,\n                   from ../include/linux/timer.h:5,\n                   from ../include/linux/workqueue.h:9,\n                   from ../include/linux/bpf.h:10,\n                   from ../include/linux/bpf-cgroup.h:5,\n                   from ../kernel/bpf/verifier.c:7:\n  ../kernel/bpf/../module/internal.h: In function 'mod_find':\n  ../include/linux/container_of.h:20:54: error: invalid use of undefined type 'struct module'\n     20 |         static_assert(__same_type(*(ptr), ((type *)0)->member) ||       \\\n        |                                                      ^~\n  [...]\n\nThis patch fixes the above error.\n\nFixes: 31bf1dbccfb0 (\"bpf: Fix attaching fentry/fexit/fmod_ret/lsm to modules\")\nReported-by: kernel test robot <lkp@intel.com>\nSigned-off-by: Viktor Malik <vmalik@redhat.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/oe-kbuild-all/202303161404.OrmfCy09-lkp@intel.com/\nLink: https://lore.kernel.org/bpf/20230317095601.386738-1-vmalik@redhat.com",
      "modified_files": [
        "include/linux/module.h",
        "kernel/bpf/verifier.c",
        "kernel/module/internal.h"
      ]
    },
    {
      "hash": "082cdc69a4651dd2a77539d69416a359ed1214f5",
      "author": "Luis Gerhorst <gerhorst@cs.fau.de>",
      "date": "2023-03-16 22:05:50 +0100",
      "message": "bpf: Remove misleading spec_v1 check on var-offset stack read\n\nFor every BPF_ADD/SUB involving a pointer, adjust_ptr_min_max_vals()\nensures that the resulting pointer has a constant offset if\nbypass_spec_v1 is false. This is ensured by calling sanitize_check_bounds()\nwhich in turn calls check_stack_access_for_ptr_arithmetic(). There,\n-EACCESS is returned if the register's offset is not constant, thereby\nrejecting the program.\n\nIn summary, an unprivileged user must never be able to create stack\npointers with a variable offset. That is also the case, because a\nrespective check in check_stack_write() is missing. If they were able\nto create a variable-offset pointer, users could still use it in a\nstack-write operation to trigger unsafe speculative behavior [1].\n\nBecause unprivileged users must already be prevented from creating\nvariable-offset stack pointers, viable options are to either remove\nthis check (replacing it with a clarifying comment), or to turn it\ninto a \"verifier BUG\"-message, also adding a similar check in\ncheck_stack_write() (for consistency, as a second-level defense).\nThis patch implements the first option to reduce verifier bloat.\n\nThis check was introduced by commit 01f810ace9ed (\"bpf: Allow\nvariable-offset stack access\") which correctly notes that\n\"variable-offset reads and writes are disallowed (they were already\ndisallowed for the indirect access case) because the speculative\nexecution checking code doesn't support them\". However, it does not\nfurther discuss why the check in check_stack_read() is necessary.\nThe code which made this check obsolete was also introduced in this\ncommit.\n\nI have compiled ~650 programs from the Linux selftests, Linux samples,\nCilium, and libbpf/examples projects and confirmed that none of these\ntrigger the check in check_stack_read() [2]. Instead, all of these\nprograms are, as expected, already rejected when constructing the\nvariable-offset pointers. Note that the check in\ncheck_stack_access_for_ptr_arithmetic() also prints \"off=%d\" while the\ncode removed by this patch does not (the error removed does not appear\nin the \"verification_error\" values). For reproducibility, the\nrepository linked includes the raw data and scripts used to create\nthe plot.\n\n  [1] https://arxiv.org/pdf/1807.03757.pdf\n  [2] https://gitlab.cs.fau.de/un65esoq/bpf-spectre/-/raw/53dc19fcf459c186613b1156a81504b39c8d49db/data/plots/23-02-26_23-56_bpftool/bpftool/0004-errors.pdf?inline=false\n\nFixes: 01f810ace9ed (\"bpf: Allow variable-offset stack access\")\nSigned-off-by: Luis Gerhorst <gerhorst@cs.fau.de>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20230315165358.23701-1-gerhorst@cs.fau.de",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "63d2d83d21a6e2c6f019da5b2d5cdabe6d1cb951",
      "author": "David Vernet <void@manifault.com>",
      "date": "2023-03-16 12:28:30 -0700",
      "message": "bpf: Mark struct bpf_cpumask as rcu protected\n\nstruct bpf_cpumask is a BPF-wrapper around the struct cpumask type which\ncan be instantiated by a BPF program, and then queried as a cpumask in\nsimilar fashion to normal kernel code. The previous patch in this series\nmakes the type fully RCU safe, so the type can be included in the\nrcu_protected_type BTF ID list.\n\nA subsequent patch will remove bpf_cpumask_kptr_get(), as it's no longer\nuseful now that we can just treat the type as RCU safe by default and do\nour own if check.\n\nSigned-off-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230316054028.88924-3-void@manifault.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "31bf1dbccfb0a9861d4846755096b3fff5687f8a",
      "author": "Viktor Malik <vmalik@redhat.com>",
      "date": "2023-03-15 18:38:21 -0700",
      "message": "bpf: Fix attaching fentry/fexit/fmod_ret/lsm to modules\n\nThis resolves two problems with attachment of fentry/fexit/fmod_ret/lsm\nto functions located in modules:\n\n1. The verifier tries to find the address to attach to in kallsyms. This\n   is always done by searching the entire kallsyms, not respecting the\n   module in which the function is located. Such approach causes an\n   incorrect attachment address to be computed if the function to attach\n   to is shadowed by a function of the same name located earlier in\n   kallsyms.\n\n2. If the address to attach to is located in a module, the module\n   reference is only acquired in register_fentry. If the module is\n   unloaded between the place where the address is found\n   (bpf_check_attach_target in the verifier) and register_fentry, it is\n   possible that another module is loaded to the same address which may\n   lead to potential errors.\n\nSince the attachment must contain the BTF of the program to attach to,\nwe extract the module from it and search for the function address in the\ncorrect module (resolving problem no. 1). Then, the module reference is\ntaken directly in bpf_check_attach_target and stored in the bpf program\n(in bpf_prog_aux). The reference is only released when the program is\nunloaded (resolving problem no. 2).\n\nSigned-off-by: Viktor Malik <vmalik@redhat.com>\nAcked-by: Jiri Olsa <jolsa@kernel.org>\nReviewed-by: Luis Chamberlain <mcgrof@kernel.org>\nLink: https://lore.kernel.org/r/3f6a9d8ae850532b5ef864ef16327b0f7a669063.1678432753.git.vmalik@redhat.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "kernel/bpf/syscall.c",
        "kernel/bpf/trampoline.c",
        "kernel/bpf/verifier.c",
        "kernel/module/internal.h"
      ]
    },
    {
      "hash": "3e30be4288b31702d4898487a74e80ba14150a9f",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-03-13 23:08:21 -0700",
      "message": "bpf: Allow helpers access trusted PTR_TO_BTF_ID.\n\nThe verifier rejects the code:\n  bpf_strncmp(task->comm, 16, \"my_task\");\nwith the message:\n  16: (85) call bpf_strncmp#182\n  R1 type=trusted_ptr_ expected=fp, pkt, pkt_meta, map_key, map_value, mem, ringbuf_mem, buf\n\nTeach the verifier that such access pattern is safe.\nDo not allow untrusted and legacy ptr_to_btf_id to be passed into helpers.\n\nReported-by: David Vernet <void@manifault.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/r/20230313235845.61029-3-alexei.starovoitov@gmail.com\nSigned-off-by: Martin KaFai Lau <martin.lau@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "34f0677e7afd3a292bc1aadda7ce8e35faedb204",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-13 11:46:44 -0700",
      "message": "bpf: fix precision propagation verbose logging\n\nFix wrong order of frame index vs register/slot index in precision\npropagation verbose (level 2) output. It's wrong and very confusing as is.\n\nFixes: 529409ea92d5 (\"bpf: propagate precision across all frames, not just the last one\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230313184017.4083374-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "738c96d5e2e3700b370f02ac84a9dc159ca81f25",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-03-10 16:38:05 -0800",
      "message": "bpf: Allow local kptrs to be exchanged via bpf_kptr_xchg\n\nThe previous patch added necessary plumbing for verifier and runtime to\nknow what to do with non-kernel PTR_TO_BTF_IDs in map values, but didn't\nprovide any way to get such local kptrs into a map value. This patch\nmodifies verifier handling of bpf_kptr_xchg to allow MEM_ALLOC kptr\ntypes.\n\ncheck_reg_type is modified accept MEM_ALLOC-flagged input to\nbpf_kptr_xchg despite such types not being in btf_ptr_types. This could\nhave been done with a MAYBE_MEM_ALLOC equivalent to MAYBE_NULL, but\nbpf_kptr_xchg is the only helper that I can forsee using\nMAYBE_MEM_ALLOC, so keep it special-cased for now.\n\nThe verifier tags bpf_kptr_xchg retval MEM_ALLOC if and only if the BTF\nassociated with the retval is not kernel BTF.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230310230743.2320707-3-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "b32a5dae44cc7346835839c2e92356ff4609c823",
      "author": "Dave Marchevsky <davemarchevsky@fb.com>",
      "date": "2023-03-10 12:16:37 -0800",
      "message": "bpf: verifier: Rename kernel_type_name helper to btf_type_name\n\nkernel_type_name was introduced in commit 9e15db66136a (\"bpf: Implement accurate raw_tp context access via BTF\")\nwith type signature:\n\n  const char *kernel_type_name(u32 id)\n\nAt that time the function used global btf_vmlinux BTF for all id lookups. Later,\nin commit 22dc4a0f5ed1 (\"bpf: Remove hard-coded btf_vmlinux assumption from BPF verifier\"),\nthe type signature was changed to:\n\n  static const char *kernel_type_name(const struct btf* btf, u32 id)\n\nWith the btf parameter used for lookups instead of global btf_vmlinux.\n\nThe helper will function as expected for type name lookup using non-kernel BTFs,\nand will be used for such in further patches in the series. Let's rename it to\navoid incorrect assumptions that might arise when seeing the current name.\n\nSigned-off-by: Dave Marchevsky <davemarchevsky@fb.com>\nLink: https://lore.kernel.org/r/20230309180111.1618459-2-davemarchevsky@fb.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "52c2b005a3c18c565fc70cfd0ca49375f301e952",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-10 10:11:42 -0800",
      "message": "bpf: take into account liveness when propagating precision\n\nWhen doing state comparison, if old state has register that is not\nmarked as REG_LIVE_READ, then we just skip comparison, regardless what's\nthe state of corresponing register in current state. This is because not\nREG_LIVE_READ register is irrelevant for further program execution and\ncorrectness. All good here.\n\nBut when we get to precision propagation, after two states were declared\nequivalent, we don't take into account old register's liveness, and thus\nattempt to propagate precision for register in current state even if\nthat register in old state was not REG_LIVE_READ anymore. This is bad,\nbecause register in current state could be anything at all and this\ncould cause -EFAULT due to internal logic bugs.\n\nFix by taking into account REG_LIVE_READ liveness mark to keep the logic\nin state comparison in sync with precision propagation.\n\nFixes: a3ce685dd01a (\"bpf: fix precision tracking\")\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230309224131.57449-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "4b5ce570dbef57a20acdd71b0c65376009012354",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-10 08:31:42 -0800",
      "message": "bpf: ensure state checkpointing at iter_next() call sites\n\nState equivalence check and checkpointing performed in is_state_visited()\nemploys certain heuristics to try to save memory by avoiding state checkpoints\nif not enough jumps and instructions happened since last checkpoint. This leads\nto unpredictability of whether a particular instruction will be checkpointed\nand how regularly. While normally this is not causing much problems (except\ninconveniences for predictable verifier tests, which we overcome with\nBPF_F_TEST_STATE_FREQ flag), turns out it's not the case for open-coded\niterators.\n\nChecking and saving state checkpoints at iter_next() call is crucial for fast\nconvergence of open-coded iterator loop logic, so we need to force it. If we\ndon't do that, is_state_visited() might skip saving a checkpoint, causing\nunnecessarily long sequence of not checkpointed instructions and jumps, leading\nto exhaustion of jump history buffer, and potentially other undesired outcomes.\nIt is expected that with correct open-coded iterators convergence will happen\nquickly, so we don't run a risk of exhausting memory.\n\nThis patch adds, in addition to prune and jump instruction marks, also a\n\"forced checkpoint\" mark, and makes sure that any iter_next() call instruction\nis marked as such.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230310060149.625887-1-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "06accc8779c1d558a5b5a21f2ac82b0c95827ddd",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-08 16:19:50 -0800",
      "message": "bpf: add support for open-coded iterator loops\n\nTeach verifier about the concept of the open-coded (or inline) iterators.\n\nThis patch adds generic iterator loop verification logic, new STACK_ITER\nstack slot type to contain iterator state, and necessary kfunc plumbing\nfor iterator's constructor, destructor and next methods. Next patch\nimplements first specific iterator (numbers iterator for implementing\nfor() loop logic). Such split allows to have more focused commits for\nverifier logic and separate commit that we could point later to\ndemonstrating  what does it take to add a new kind of iterator.\n\nEach kind of iterator has its own associated struct bpf_iter_<type>,\nwhere <type> denotes a specific type of iterator. struct bpf_iter_<type>\nstate is supposed to live on BPF program stack, so there will be no way\nto change its size later on without breaking backwards compatibility, so\nchoose wisely! But given this struct is specific to a given <type> of\niterator, this allows a lot of flexibility: simple iterators could be\nfine with just one stack slot (8 bytes), like numbers iterator in the\nnext patch, while some other more complicated iterators might need way\nmore to keep their iterator state. Either way, such design allows to\navoid runtime memory allocations, which otherwise would be necessary if\nwe fixed on-the-stack size and it turned out to be too small for a given\niterator implementation.\n\nThe way BPF verifier logic is implemented, there are no artificial\nrestrictions on a number of active iterators, it should work correctly\nusing multiple active iterators at the same time. This also means you\ncan have multiple nested iteration loops. struct bpf_iter_<type>\nreference can be safely passed to subprograms as well.\n\nGeneral flow is easiest to demonstrate with a simple example using\nnumber iterator implemented in next patch. Here's the simplest possible\nloop:\n\n  struct bpf_iter_num it;\n  int *v;\n\n  bpf_iter_num_new(&it, 2, 5);\n  while ((v = bpf_iter_num_next(&it))) {\n      bpf_printk(\"X = %d\", *v);\n  }\n  bpf_iter_num_destroy(&it);\n\nAbove snippet should output \"X = 2\", \"X = 3\", \"X = 4\". Note that 5 is\nexclusive and is not returned. This matches similar APIs (e.g., slices\nin Go or Rust) that implement a range of elements, where end index is\nnon-inclusive.\n\nIn the above example, we see a trio of function:\n  - constructor, bpf_iter_num_new(), which initializes iterator state\n  (struct bpf_iter_num it) on the stack. If any of the input arguments\n  are invalid, constructor should make sure to still initialize it such\n  that subsequent bpf_iter_num_next() calls will return NULL. I.e., on\n  error, return error and construct empty iterator.\n  - next method, bpf_iter_num_next(), which accepts pointer to iterator\n  state and produces an element. Next method should always return\n  a pointer. The contract between BPF verifier is that next method will\n  always eventually return NULL when elements are exhausted. Once NULL is\n  returned, subsequent next calls should keep returning NULL. In the\n  case of numbers iterator, bpf_iter_num_next() returns a pointer to an int\n  (storage for this integer is inside the iterator state itself),\n  which can be dereferenced after corresponding NULL check.\n  - once done with the iterator, it's mandated that user cleans up its\n  state with the call to destructor, bpf_iter_num_destroy() in this\n  case. Destructor frees up any resources and marks stack space used by\n  struct bpf_iter_num as usable for something else.\n\nAny other iterator implementation will have to implement at least these\nthree methods. It is enforced that for any given type of iterator only\napplicable constructor/destructor/next are callable. I.e., verifier\nensures you can't pass number iterator state into, say, cgroup\niterator's next method.\n\nIt is important to keep the naming pattern consistent to be able to\ncreate generic macros to help with BPF iter usability. E.g., one\nof the follow up patches adds generic bpf_for_each() macro to bpf_misc.h\nin selftests, which allows to utilize iterator \"trio\" nicely without\nhaving to code the above somewhat tedious loop explicitly every time.\nThis is enforced at kfunc registration point by one of the previous\npatches in this series.\n\nAt the implementation level, iterator state tracking for verification\npurposes is very similar to dynptr. We add STACK_ITER stack slot type,\nreserve necessary number of slots, depending on\nsizeof(struct bpf_iter_<type>), and keep track of necessary extra state\nin the \"main\" slot, which is marked with non-zero ref_obj_id. Other\nslots are also marked as STACK_ITER, but have zero ref_obj_id. This is\nsimpler than having a separate \"is_first_slot\" flag.\n\nAnother big distinction is that STACK_ITER is *always refcounted*, which\nsimplifies implementation without sacrificing usability. So no need for\nextra \"iter_id\", no need to anticipate reuse of STACK_ITER slots for new\nconstructors, etc. Keeping it simple here.\n\nAs far as the verification logic goes, there are two extensive comments:\nin process_iter_next_call() and iter_active_depths_differ() explaining\nsome important and sometimes subtle aspects. Please refer to them for\ndetails.\n\nBut from 10,000-foot point of view, next methods are the points of\nforking a verification state, which are conceptually similar to what\nverifier is doing when validating conditional jump. We branch out at\na `call bpf_iter_<type>_next` instruction and simulate two outcomes:\nNULL (iteration is done) and non-NULL (new element is returned). NULL is\nsimulated first and is supposed to reach exit without looping. After\nthat non-NULL case is validated and it either reaches exit (for trivial\nexamples with no real loop), or reaches another `call bpf_iter_<type>_next`\ninstruction with the state equivalent to already (partially) validated\none. State equivalency at that point means we technically are going to\nbe looping forever without \"breaking out\" out of established \"state\nenvelope\" (i.e., subsequent iterations don't add any new knowledge or\nconstraints to the verifier state, so running 1, 2, 10, or a million of\nthem doesn't matter). But taking into account the contract stating that\niterator next method *has to* return NULL eventually, we can conclude\nthat loop body is safe and will eventually terminate. Given we validated\nlogic outside of the loop (NULL case), and concluded that loop body is\nsafe (though potentially looping many times), verifier can claim safety\nof the overall program logic.\n\nThe rest of the patch is necessary plumbing for state tracking, marking,\nvalidation, and necessary further kfunc plumbing to allow implementing\niterator constructor, destructor, and next methods.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230308184121.1165081-4-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "07236eab7a3139da97aef9f5f21f403be82a82ea",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-08 16:19:50 -0800",
      "message": "bpf: factor out fetching basic kfunc metadata\n\nFactor out logic to fetch basic kfunc metadata based on struct bpf_insn.\nThis is not exactly short or trivial code to just copy/paste and this\ninformation is sometimes necessary in other parts of the verifier logic.\nSubsequent patches will rely on this to determine if an instruction is\na kfunc call to iterator next method.\n\nNo functional changes intended, including that verbose() warning\nbehavior when kfunc is not allowed for a particular program type.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230308184121.1165081-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "f4b4eee6169bb33c5157ebe07e53d7e4be7631c0",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-04 11:14:32 -0800",
      "message": "bpf: add support for fixed-size memory pointer returns for kfuncs\n\nSupport direct fixed-size (and for now, read-only) memory access when\nkfunc's return type is a pointer to non-struct type. Calculate type size\nand let BPF program access that many bytes directly. This is crucial for\nnumbers iterator.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230302235015.2044271-13-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "a461f5adf17756e99ee0903d1a40961b0342ebb3",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-04 11:14:32 -0800",
      "message": "bpf: generalize dynptr_get_spi to be usable for iters\n\nGeneralize the logic of fetching special stack slot object state using\nspi (stack slot index). This will be used by STACK_ITER logic next.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230302235015.2044271-12-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d5271c5b1950b887def1663b75e2d710cc16535f",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-04 11:14:32 -0800",
      "message": "bpf: mark PTR_TO_MEM as non-null register type\n\nPTR_TO_MEM register without PTR_MAYBE_NULL is indeed non-null. This is\nimportant for BPF verifier to be able to prune guaranteed not to be\ntaken branches. This is always the case with open-coded iterators.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230302235015.2044271-11-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d0e1ac227945c6af616c003365c6feb986dc0839",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-04 11:14:32 -0800",
      "message": "bpf: move kfunc_call_arg_meta higher in the file\n\nMove struct bpf_kfunc_call_arg_meta higher in the file and put it next\nto struct bpf_call_arg_meta, so it can be used from more functions.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230302235015.2044271-10-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "553a64a85c5d1dac277325a0f51a31c056593048",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-04 11:14:32 -0800",
      "message": "bpf: ensure that r0 is marked scratched after any function call\n\nr0 is important (unless called function is void-returning, but that's\ntaken care of by print_verifier_state() anyways) in verifier logs.\nCurrently for helpers we seem to print it in verifier log, but for\nkfuncs we don't.\n\nInstead of figuring out where in the maze of code we accidentally set r0\nas scratched for helpers and why we don't do that for kfuncs, just\nenforce that after any function call r0 is marked as scratched.\n\nAlso, perhaps, we should reconsider \"scratched\" terminology, as it's\nmightily confusing. \"Touched\" would seem more appropriate. But I left\nthat for follow ups for now.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230302235015.2044271-9-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "c1ee85a9806a720aa054f68fe7f9c79418f36c2b",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-04 11:14:32 -0800",
      "message": "bpf: fix visit_insn()'s detection of BPF_FUNC_timer_set_callback helper\n\nIt's not correct to assume that any BPF_CALL instruction is a helper\ncall. Fix visit_insn()'s detection of bpf_timer_set_callback() helper by\nalso checking insn->code == 0. For kfuncs insn->code would be set to\nBPF_PSEUDO_KFUNC_CALL, and for subprog calls it will be BPF_PSEUDO_CALL.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230302235015.2044271-8-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "653ae3a874aca6764a4c1f5a8bf1b072ade0d6f4",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-04 11:14:32 -0800",
      "message": "bpf: clean up visit_insn()'s instruction processing\n\nInstead of referencing processed instruction repeatedly as insns[t]\nthroughout entire visit_insn() function, take a local insn pointer and\nwork with it in a cleaner way.\n\nIt makes enhancing this function further a bit easier as well.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230302235015.2044271-7-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "98ddcf389d1bb7a407d49c23dfe6443680812f24",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-04 11:14:31 -0800",
      "message": "bpf: honor env->test_state_freq flag in is_state_visited()\n\nenv->test_state_freq flag can be set by user by passing\nBPF_F_TEST_STATE_FREQ program flag. This is used in a bunch of selftests\nto have predictable state checkpoints at every jump and so on.\n\nCurrently, bounded loop handling heuristic ignores this flag if number\nof processed jumps and/or number of processed instructions is below some\nthresholds, which throws off that reliable state checkpointing.\n\nHonor this flag in all circumstances by disabling heuristic if\nenv->test_state_freq is set.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230302235015.2044271-5-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "567da5d253cd6b41c6d015adac1af653725bef9d",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-04 11:14:31 -0800",
      "message": "bpf: improve regsafe() checks for PTR_TO_{MEM,BUF,TP_BUFFER}\n\nTeach regsafe() logic to handle PTR_TO_MEM, PTR_TO_BUF, and\nPTR_TO_TP_BUFFER similarly to PTR_TO_MAP_{KEY,VALUE}. That is, instead of\nexact match for var_off and range, use tnum_in() and range_within()\nchecks, allowing more general verified state to subsume more specific\ncurrent state. This allows to match wider range of valid and safe\nstates, speeding up verification and detecting wider range of equivalent\nstates for upcoming open-coded iteration looping logic.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230302235015.2044271-3-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "d54e0f6c1adffbf72f2cf4aebe6122899c3b851c",
      "author": "Andrii Nakryiko <andrii@kernel.org>",
      "date": "2023-03-04 11:14:31 -0800",
      "message": "bpf: improve stack slot state printing\n\nImprove stack slot state printing to provide more useful and relevant\ninformation, especially for dynptrs. While previously we'd see something\nlike:\n\n  8: (85) call bpf_ringbuf_reserve_dynptr#198   ; R0_w=scalar() fp-8_w=dddddddd fp-16_w=dddddddd refs=2\n\nNow we'll see way more useful:\n\n  8: (85) call bpf_ringbuf_reserve_dynptr#198   ; R0_w=scalar() fp-16_w=dynptr_ringbuf(ref_id=2) refs=2\n\nI experimented with printing the range of slots taken by dynptr,\nsomething like:\n\n  fp-16..8_w=dynptr_ringbuf(ref_id=2)\n\nBut it felt very awkward and pretty useless. So we print the lowest\naddress (most negative offset) only.\n\nThe general structure of this code is now also set up for easier\nextension and will accommodate ITER slots naturally.\n\nSigned-off-by: Andrii Nakryiko <andrii@kernel.org>\nLink: https://lore.kernel.org/r/20230302235015.2044271-2-andrii@kernel.org\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "0d80a619c113d0e216dbffa56b2d5ccc079ee520",
      "author": "Eduard Zingerman <eddyz87@gmail.com>",
      "date": "2023-03-03 21:41:46 -0800",
      "message": "bpf: allow ctx writes using BPF_ST_MEM instruction\n\nLift verifier restriction to use BPF_ST_MEM instructions to write to\ncontext data structures. This requires the following changes:\n - verifier.c:do_check() for BPF_ST updated to:\n   - no longer forbid writes to registers of type PTR_TO_CTX;\n   - track dst_reg type in the env->insn_aux_data[...].ptr_type field\n     (same way it is done for BPF_STX and BPF_LDX instructions).\n - verifier.c:convert_ctx_access() and various callbacks invoked by\n   it are updated to handled BPF_ST instruction alongside BPF_STX.\n\nSigned-off-by: Eduard Zingerman <eddyz87@gmail.com>\nLink: https://lore.kernel.org/r/20230304011247.566040-2-eddyz87@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/cgroup.c",
        "kernel/bpf/verifier.c",
        "net/core/filter.c",
        "tools/testing/selftests/bpf/verifier/ctx.c"
      ]
    },
    {
      "hash": "6fcd486b3a0a628c41f12b3a7329a18a2c74b351",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-03-03 17:42:20 +0100",
      "message": "bpf: Refactor RCU enforcement in the verifier.\n\nbpf_rcu_read_lock/unlock() are only available in clang compiled kernels. Lack\nof such key mechanism makes it impossible for sleepable bpf programs to use RCU\npointers.\n\nAllow bpf_rcu_read_lock/unlock() in GCC compiled kernels (though GCC doesn't\nsupport btf_type_tag yet) and allowlist certain field dereferences in important\ndata structures like tast_struct, cgroup, socket that are used by sleepable\nprograms either as RCU pointer or full trusted pointer (which is valid outside\nof RCU CS). Use BTF_TYPE_SAFE_RCU and BTF_TYPE_SAFE_TRUSTED macros for such\ntagging. They will be removed once GCC supports btf_type_tag.\n\nWith that refactor check_ptr_to_btf_access(). Make it strict in enforcing\nPTR_TRUSTED and PTR_UNTRUSTED while deprecating old PTR_TO_BTF_ID without\nmodifier flags. There is a chance that this strict enforcement might break\nexisting programs (especially on GCC compiled kernels), but this cleanup has to\nstart sooner than later. Note PTR_TO_CTX access still yields old deprecated\nPTR_TO_BTF_ID. Once it's converted to strict PTR_TRUSTED or PTR_UNTRUSTED the\nkfuncs and helpers will be able to default to KF_TRUSTED_ARGS. KF_RCU will\nremain as a weaker version of KF_TRUSTED_ARGS where obj refcnt could be 0.\n\nAdjust rcu_read_lock selftest to run on gcc and clang compiled kernels.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230303041446.3630-7-alexei.starovoitov@gmail.com",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/bpf_verifier.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/cpumask.c",
        "kernel/bpf/verifier.c",
        "tools/testing/selftests/bpf/prog_tests/cgrp_local_storage.c",
        "tools/testing/selftests/bpf/prog_tests/rcu_read_lock.c",
        "tools/testing/selftests/bpf/progs/cgrp_ls_sleepable.c",
        "tools/testing/selftests/bpf/progs/cpumask_failure.c",
        "tools/testing/selftests/bpf/progs/nested_trust_failure.c",
        "tools/testing/selftests/bpf/progs/rcu_read_lock.c",
        "tools/testing/selftests/bpf/verifier/calls.c"
      ]
    },
    {
      "hash": "20c09d92faeefb8536f705d3a4629e0dc314c8a1",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-03-03 17:42:20 +0100",
      "message": "bpf: Introduce kptr_rcu.\n\nThe life time of certain kernel structures like 'struct cgroup' is protected by RCU.\nHence it's safe to dereference them directly from __kptr tagged pointers in bpf maps.\nThe resulting pointer is MEM_RCU and can be passed to kfuncs that expect KF_RCU.\nDerefrence of other kptr-s returns PTR_UNTRUSTED.\n\nFor example:\nstruct map_value {\n   struct cgroup __kptr *cgrp;\n};\n\nSEC(\"tp_btf/cgroup_mkdir\")\nint BPF_PROG(test_cgrp_get_ancestors, struct cgroup *cgrp_arg, const char *path)\n{\n  struct cgroup *cg, *cg2;\n\n  cg = bpf_cgroup_acquire(cgrp_arg); // cg is PTR_TRUSTED and ref_obj_id > 0\n  bpf_kptr_xchg(&v->cgrp, cg);\n\n  cg2 = v->cgrp; // This is new feature introduced by this patch.\n  // cg2 is PTR_MAYBE_NULL | MEM_RCU.\n  // When cg2 != NULL, it's a valid cgroup, but its percpu_ref could be zero\n\n  if (cg2)\n    bpf_cgroup_ancestor(cg2, level); // safe to do.\n}\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: Tejun Heo <tj@kernel.org>\nAcked-by: David Vernet <void@manifault.com>\nLink: https://lore.kernel.org/bpf/20230303041446.3630-4-alexei.starovoitov@gmail.com",
      "modified_files": [
        "Documentation/bpf/kfuncs.rst",
        "include/linux/btf.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "net/bpf/test_run.c",
        "tools/testing/selftests/bpf/progs/cgrp_kfunc_failure.c",
        "tools/testing/selftests/bpf/progs/map_kptr_fail.c",
        "tools/testing/selftests/bpf/verifier/calls.c",
        "tools/testing/selftests/bpf/verifier/map_kptr.c"
      ]
    },
    {
      "hash": "8d093b4e95a2a16a2cfcd36869b348a17112fabe",
      "author": "Alexei Starovoitov <ast@kernel.org>",
      "date": "2023-03-03 17:42:20 +0100",
      "message": "bpf: Mark cgroups and dfl_cgrp fields as trusted.\n\nbpf programs sometimes do:\nbpf_cgrp_storage_get(&map, task->cgroups->dfl_cgrp, ...);\nIt is safe to do, because cgroups->dfl_cgrp pointer is set diring init and\nnever changes. The task->cgroups is also never NULL. It is also set during init\nand will change when task switches cgroups. For any trusted task pointer\ndereference of cgroups and dfl_cgrp should yield trusted pointers. The verifier\nwasn't aware of this. Hence in gcc compiled kernels task->cgroups dereference\nwas producing PTR_TO_BTF_ID without modifiers while in clang compiled kernels\nthe verifier recognizes __rcu tag in cgroups field and produces\nPTR_TO_BTF_ID | MEM_RCU | MAYBE_NULL.\nTag cgroups and dfl_cgrp as trusted to equalize clang and gcc behavior.\nWhen GCC supports btf_type_tag such tagging will done directly in the type.\n\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nAcked-by: David Vernet <void@manifault.com>\nAcked-by: Tejun Heo <tj@kernel.org>\nLink: https://lore.kernel.org/bpf/20230303041446.3630-3-alexei.starovoitov@gmail.com",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "9db44fdd8105da00669d425acab887c668df75f6",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-03-01 10:24:33 -0800",
      "message": "bpf: Support kptrs in local storage maps\n\nEnable support for kptrs in local storage maps by wiring up the freeing\nof these kptrs from map value. Freeing of bpf_local_storage_map is only\ndelayed in case there are special fields, therefore bpf_selem_free_*\npath can also only dereference smap safely in that case. This is\nrecorded using a bool utilizing a hole in bpF_local_storage_elem. It\ncould have been tagged in the pointer value smap using the lowest bit\n(since alignment > 1), but since there was already a hole I went with\nthe simpler option. Only the map structure freeing is delayed using RCU\nbarriers, as the buckets aren't used when selem is being freed, so they\ncan be freed once all readers of the bucket lists can no longer access\nit.\n\nCc: Martin KaFai Lau <martin.lau@kernel.org>\nCc: KP Singh <kpsingh@kernel.org>\nCc: Paul E. McKenney <paulmck@kernel.org>\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230225154010.391965-3-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_local_storage.h",
        "kernel/bpf/bpf_local_storage.c",
        "kernel/bpf/syscall.c",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "66e3a13e7c2c44d0c9dd6bb244680ca7529a8845",
      "author": "Joanne Koong <joannelkoong@gmail.com>",
      "date": "2023-03-01 09:55:24 -0800",
      "message": "bpf: Add bpf_dynptr_slice and bpf_dynptr_slice_rdwr\n\nTwo new kfuncs are added, bpf_dynptr_slice and bpf_dynptr_slice_rdwr.\nThe user must pass in a buffer to store the contents of the data slice\nif a direct pointer to the data cannot be obtained.\n\nFor skb and xdp type dynptrs, these two APIs are the only way to obtain\na data slice. However, for other types of dynptrs, there is no\ndifference between bpf_dynptr_slice(_rdwr) and bpf_dynptr_data.\n\nFor skb type dynptrs, the data is copied into the user provided buffer\nif any of the data is not in the linear portion of the skb. For xdp type\ndynptrs, the data is copied into the user provided buffer if the data is\nbetween xdp frags.\n\nIf the skb is cloned and a call to bpf_dynptr_data_rdwr is made, then\nthe skb will be uncloned (see bpf_unclone_prologue()).\n\nPlease note that any bpf_dynptr_write() automatically invalidates any prior\ndata slices of the skb dynptr. This is because the skb may be cloned or\nmay need to pull its paged buffer into the head. As such, any\nbpf_dynptr_write() will automatically have its prior data slices\ninvalidated, even if the write is to data in the skb head of an uncloned\nskb. Please note as well that any other helper calls that change the\nunderlying packet buffer (eg bpf_skb_pull_data()) invalidates any data\nslices of the skb dynptr as well, for the same reasons.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-10-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/filter.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "net/core/filter.c",
        "tools/include/uapi/linux/bpf.h"
      ]
    },
    {
      "hash": "05421aecd4ed65da0dc17b0c3c13779ef334e9e5",
      "author": "Joanne Koong <joannelkoong@gmail.com>",
      "date": "2023-03-01 09:55:24 -0800",
      "message": "bpf: Add xdp dynptrs\n\nAdd xdp dynptrs, which are dynptrs whose underlying pointer points\nto a xdp_buff. The dynptr acts on xdp data. xdp dynptrs have two main\nbenefits. One is that they allow operations on sizes that are not\nstatically known at compile-time (eg variable-sized accesses).\nAnother is that parsing the packet data through dynptrs (instead of\nthrough direct access of xdp->data and xdp->data_end) can be more\nergonomic and less brittle (eg does not need manual if checking for\nbeing within bounds of data_end).\n\nFor reads and writes on the dynptr, this includes reading/writing\nfrom/to and across fragments. Data slices through the bpf_dynptr_data\nAPI are not supported; instead bpf_dynptr_slice() and\nbpf_dynptr_slice_rdwr() should be used.\n\nFor examples of how xdp dynptrs can be used, please see the attached\nselftests.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-9-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/filter.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "net/core/filter.c",
        "tools/include/uapi/linux/bpf.h"
      ]
    },
    {
      "hash": "b5964b968ac64c2ec2debee7518499113b27c34e",
      "author": "Joanne Koong <joannelkoong@gmail.com>",
      "date": "2023-03-01 09:55:24 -0800",
      "message": "bpf: Add skb dynptrs\n\nAdd skb dynptrs, which are dynptrs whose underlying pointer points\nto a skb. The dynptr acts on skb data. skb dynptrs have two main\nbenefits. One is that they allow operations on sizes that are not\nstatically known at compile-time (eg variable-sized accesses).\nAnother is that parsing the packet data through dynptrs (instead of\nthrough direct access of skb->data and skb->data_end) can be more\nergonomic and less brittle (eg does not need manual if checking for\nbeing within bounds of data_end).\n\nFor bpf prog types that don't support writes on skb data, the dynptr is\nread-only (bpf_dynptr_write() will return an error)\n\nFor reads and writes through the bpf_dynptr_read() and bpf_dynptr_write()\ninterfaces, reading and writing from/to data in the head as well as from/to\nnon-linear paged buffers is supported. Data slices through the\nbpf_dynptr_data API are not supported; instead bpf_dynptr_slice() and\nbpf_dynptr_slice_rdwr() (added in subsequent commit) should be used.\n\nFor examples of how skb dynptrs can be used, please see the attached\nselftests.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-8-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf.h",
        "include/linux/filter.h",
        "include/uapi/linux/bpf.h",
        "kernel/bpf/btf.c",
        "kernel/bpf/helpers.c",
        "kernel/bpf/verifier.c",
        "net/core/filter.c",
        "tools/include/uapi/linux/bpf.h"
      ]
    },
    {
      "hash": "d96d937d7c5c12237dce1f14bf0fc9900cabba09",
      "author": "Joanne Koong <joannelkoong@gmail.com>",
      "date": "2023-03-01 09:55:24 -0800",
      "message": "bpf: Add __uninit kfunc annotation\n\nThis patch adds __uninit as a kfunc annotation.\n\nThis will be useful for scenarios such as for example in dynptrs,\nindicating whether the dynptr should be checked by the verifier as an\ninitialized or an uninitialized dynptr.\n\nWithout this annotation, the alternative would be needing to hard-code\nin the verifier the specific kfunc to indicate that arg should be\ntreated as an uninitialized arg.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-7-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "Documentation/bpf/kfuncs.rst",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "485ec51ef9764c0f67d35cabba0a963936b9126e",
      "author": "Joanne Koong <joannelkoong@gmail.com>",
      "date": "2023-03-01 09:55:23 -0800",
      "message": "bpf: Refactor verifier dynptr into get_dynptr_arg_reg\n\nThis commit refactors the logic for determining which register in a\nfunction is the dynptr into \"get_dynptr_arg_reg\". This will be used\nin the future when the dynptr reg for BPF_FUNC_dynptr_write will need\nto be obtained in order to support writes for skb dynptrs.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-6-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "1d18feb2c915c5ad0a9a61d04b8560e8efb78ce8",
      "author": "Joanne Koong <joannelkoong@gmail.com>",
      "date": "2023-03-01 09:55:23 -0800",
      "message": "bpf: Allow initializing dynptrs in kfuncs\n\nThis change allows kfuncs to take in an uninitialized dynptr as a\nparameter. Before this change, only helper functions could successfully\nuse uninitialized dynptrs. This change moves the memory access check\n(including stack state growing and slot marking) into\nprocess_dynptr_func(), which both helpers and kfuncs call into.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-4-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "7e0dac2807e6c4ae8c56941d74971fdb0763b4f9",
      "author": "Joanne Koong <joannelkoong@gmail.com>",
      "date": "2023-03-01 09:55:23 -0800",
      "message": "bpf: Refactor process_dynptr_func\n\nThis change cleans up process_dynptr_func's flow to be more intuitive\nand updates some comments with more context.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nLink: https://lore.kernel.org/r/20230301154953.641654-3-joannelkoong@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "include/linux/bpf_verifier.h",
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "df2ccc180a2e6f6e4343ebee99dcfab4f8af2816",
      "author": "Ilya Leoshkevich <iii@linux.ibm.com>",
      "date": "2023-02-22 13:08:52 -0800",
      "message": "bpf: Check for helper calls in check_subprogs()\n\nThe condition src_reg != BPF_PSEUDO_CALL && imm == BPF_FUNC_tail_call\nmay be satisfied by a kfunc call. This would lead to unnecessarily\nsetting has_tail_call. Use src_reg == 0 instead.\n\nSigned-off-by: Ilya Leoshkevich <iii@linux.ibm.com>\nAcked-by: Stanislav Fomichev <sdf@google.com>\nLink: https://lore.kernel.org/r/20230220163756.753713-1-iii@linux.ibm.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "dbd8d22863e83ee2834642e4cfd3bdacb8a1c975",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-02-22 12:50:34 -0800",
      "message": "bpf: Wrap register invalidation with a helper\n\nTypically, verifier should use env->allow_ptr_leaks when invaliding\nregisters for users that don't have CAP_PERFMON or CAP_SYS_ADMIN to\navoid leaking the pointer value. This is similar in spirit to\nc67cae551f0d (\"bpf: Tighten ptr_to_btf_id checks.\"). In a lot of the\nexisting checks, we know the capabilities are present, hence we don't do\nthe check.\n\nInstead of being inconsistent in the application of the check, wrap the\naction of invalidating a register into a helper named 'mark_invalid_reg'\nand use it in a uniform fashion to replace open coded invalidation\noperations, so that the check is always made regardless of the call site\nand we don't have to remember whether it needs to be done or not for\neach case.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230221200646.2500777-7-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "da03e43a8c500fcfb11ac5eeb03c1b4a9c1dd958",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-02-22 12:50:15 -0800",
      "message": "bpf: Fix check_reg_type for PTR_TO_BTF_ID\n\nThe current code does type matching for the case where reg->type is\nPTR_TO_BTF_ID or has the PTR_TRUSTED flag. However, this only needs to\noccur for non-MEM_ALLOC and non-MEM_PERCPU cases, but will include both\nas per the current code.\n\nThe MEM_ALLOC case with or without PTR_TRUSTED needs to be handled\nspecially by the code for type_is_alloc case, while MEM_PERCPU case must\nbe ignored. Hence, to restore correct behavior and for clarity,\nexplicitly list out the handled PTR_TO_BTF_ID types which should be\nhandled for each case using a switch statement.\n\nHelpers currently only take:\n\tPTR_TO_BTF_ID\n\tPTR_TO_BTF_ID | PTR_TRUSTED\n\tPTR_TO_BTF_ID | MEM_RCU\n\tPTR_TO_BTF_ID | MEM_ALLOC\n\tPTR_TO_BTF_ID | MEM_PERCPU\n\tPTR_TO_BTF_ID | MEM_PERCPU | PTR_TRUSTED\n\nThis fix was also described (for the MEM_ALLOC case) in [0].\n\n  [0]: https://lore.kernel.org/bpf/20221121160657.h6z7xuvedybp5y7s@apollo\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230221200646.2500777-6-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    },
    {
      "hash": "521d3c0a1730c29c96870919a7a115577e17f8c7",
      "author": "Kumar Kartikeya Dwivedi <memxor@gmail.com>",
      "date": "2023-02-22 12:49:52 -0800",
      "message": "bpf: Remove unused MEM_ALLOC | PTR_TRUSTED checks\n\nThe plan is to supposedly tag everything with PTR_TRUSTED eventually,\nhowever those changes should bring in their respective code, instead\nof leaving it around right now. It is arguable whether PTR_TRUSTED is\nrequired for all types, when it's only use case is making PTR_TO_BTF_ID\na bit stronger, while all other types are trusted by default.\n\nHence, just drop the two instances which do not occur in the verifier\nfor now to avoid reader confusion.\n\nSigned-off-by: Kumar Kartikeya Dwivedi <memxor@gmail.com>\nLink: https://lore.kernel.org/r/20230221200646.2500777-5-memxor@gmail.com\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>",
      "modified_files": [
        "kernel/bpf/verifier.c"
      ]
    }
  ]
}
